{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/robots.txt","path":"robots.txt","modified":0,"renderable":0},{"_id":"themes/next/source/CNAME","path":"CNAME","modified":0,"renderable":1},{"_id":"source/about/index/公众号.jpg","path":"about/index/公众号.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.png","path":"images/avatar.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"fdcb271fe911c9378fead3ac217762897bd7bf1a","modified":1534250143412},{"_id":"source/.me_configs.data","hash":"be0d1ba93f31dadd375f266a8a131bab230e5522","modified":1528393970000},{"_id":"source/.md_configs.data","hash":"8842f205e55d4dee88e990327ceeebfc55470fe5","modified":1528393970000},{"_id":"source/.z_sync_configs.data","hash":"d35bb00a1634c739a93585c06ebea3d272c9a023","modified":1528393970000},{"_id":"source/robots.txt","hash":"2be28bdb6f77c8f7627632bea984e2b3d735b411","modified":1528393971000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1528393971000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1528393971000},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1528393971000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1528393971000},{"_id":"themes/next/.gitignore","hash":"a18c2e83bb20991b899b58e6aeadcb87dd8aa16e","modified":1528393971000},{"_id":"themes/next/.stickler.yml","hash":"b7939095038cbdc4883fc10950e163a60a643b43","modified":1528393971000},{"_id":"themes/next/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1528393971000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1528393971000},{"_id":"themes/next/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1528393971000},{"_id":"themes/next/README.md","hash":"807c28ad6473b221101251d244aa08e2a61b0d60","modified":1528393971000},{"_id":"themes/next/bower.json","hash":"a8c832da6aad5245052aed7ff26c246f85d68c6c","modified":1528393971000},{"_id":"themes/next/_config.yml","hash":"73524f29e86492ca8e92b4861614a998115ebe82","modified":1528682554000},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1528393971000},{"_id":"themes/next/gulpfile.coffee","hash":"48d2f9fa88a4210308fc41cc7d3f6d53989f71b7","modified":1528393971000},{"_id":"themes/next/package.json","hash":"11a0b27f92da8abf1efbea6e7a0af4271d7bff9e","modified":1528393971000},{"_id":"source/about/.DS_Store","hash":"894a1066d062f778f4734ec041b15051406bc25f","modified":1530437356073},{"_id":"source/_posts/.DS_Store","hash":"ede48feacb060ab50d74976154597896bd604c86","modified":1534248670883},{"_id":"source/categories/index.md","hash":"953b432e5293289c926e7eff1b5594652d0bab40","modified":1528393971000},{"_id":"source/about/index.md","hash":"c2528bd72377de6a0a35b0686597cd7eaff1aaa6","modified":1530435455075},{"_id":"source/_posts/.md_configs.data","hash":"8842f205e55d4dee88e990327ceeebfc55470fe5","modified":1528393970000},{"_id":"source/_posts/.me_configs.data","hash":"65f09b99475d152a6e9f0bb6fe935aa0013c06b7","modified":1528393970000},{"_id":"source/_posts/.基于google-protobuf的RPC实现-python版.md.swp","hash":"becb4ac1df37615f41482d21d03a35790b229274","modified":1534229232626},{"_id":"source/_posts/Linux常用命令.md","hash":"910ef02f644ec69a70db49e2c59c2f0a9c313f5f","modified":1534217029153},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程.md","hash":"7d29cba206b63a40203d3340875f57115224cf9b","modified":1528393970000},{"_id":"source/_posts/Markdown写作教程.md","hash":"6c8017412a5aa44a943d5c7f39222c8f35a53397","modified":1528393971000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E.md","hash":"163c6a75705bb410d70e20e205bc6f9661fabba1","modified":1529982413132},{"_id":"source/_posts/Python之MatPlotLib使用教程.md","hash":"b254ef50305fe8d9c7e5cb97696b86aeb5bd6fbb","modified":1529996444699},{"_id":"source/_posts/Python之NumPy使用教程.md","hash":"f192bd1ffceece2f6dea491223cdb4d5daa7f3e9","modified":1529996137248},{"_id":"source/_posts/Python之Pandas使用教程.md","hash":"af16cbfd1f10950e7e6249164d6bb7d21d63e666","modified":1529996088386},{"_id":"source/_posts/Python之Sklearn使用教程.md","hash":"38fe9c817675e9421c07c1bc00bd87d4b6548253","modified":1528393971000},{"_id":"source/_posts/基于google-protobuf的gRPC实现-python版.md","hash":"680434928584071073d9c5f84aae77b5080d4267","modified":1534260831053},{"_id":"source/_posts/效率软件推荐（一）.md","hash":"129caa86df23cc604f2acc6f9e130bae071bae23","modified":1528876800537},{"_id":"source/_posts/智慧考古探测.md","hash":"bd310963d2f83b447a408546fc38600a5d84a126","modified":1529982973852},{"_id":"source/_posts/机器学习之Apriori算法.md","hash":"55f7ae126ac06dc917b83caaea11e06f17108a4c","modified":1528393971000},{"_id":"source/_posts/机器学习之K均值-K-Means-算法.md","hash":"a8f791be195d47169b4f1aa96297c8a73e02daaf","modified":1529985784710},{"_id":"source/_posts/机器学习之K近邻-KNN-算法.md","hash":"944624aa9ecd239025ead832f43993b8e23b7557","modified":1528393971000},{"_id":"source/_posts/机器学习之Logistic回归.md","hash":"c4237876b04f71831a90d84e2a87fd95c558dac6","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）.md","hash":"a3619c763746e5362f1b52835e49ac5be990dcd6","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（二）.md","hash":"c1a1e2a1d1b25ef7569fbe74f032997b1ae33261","modified":1528393971000},{"_id":"source/_posts/机器学习之决策树-C4-5算法.md","hash":"df8a167449c12d6e80596004e6f3515ed6a40f10","modified":1528393971000},{"_id":"source/_posts/机器学习之分类与回归树-CART.md","hash":"74944aa24e13cd967e5d92b63ac636265731864e","modified":1528393971000},{"_id":"source/_posts/机器学习之最大期望-EM-算法.md","hash":"7254dca2a152ec811f5fe43a3a465189df52fc7a","modified":1529986249801},{"_id":"source/_posts/机器学习之朴素贝叶斯算法.md","hash":"ac142f72119738cb3408e88eb2f77f9a0dac13e5","modified":1529985536909},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT.md","hash":"2c2f8b3ea056b8819eae246a0632d23d2e71e2d0","modified":1528393971000},{"_id":"source/_posts/机器学习之线性回归.md","hash":"21d5f4943139f0e6e0b9a565d160c60f2cf475a1","modified":1528393971000},{"_id":"source/_posts/机器学习之自适应增强-Adaboost.md","hash":"6582495e08c7e7aec46d2461d3fb1f7590fa2aa5","modified":1529994941919},{"_id":"source/_posts/机器学习之随机森林.md","hash":"011f24111a0ceba4208fe298a315c9d674357dff","modified":1530071620656},{"_id":"source/_posts/深度神经网络之前向传播算法.md","hash":"50dd2387f47ff0f21b005b64ad1f6d0728158cf6","modified":1530087429591},{"_id":"source/_posts/机器学习知识体系.md","hash":"0e24a6d9ec95380259b42e344e4600adf35dc5ae","modified":1528393971000},{"_id":"source/_posts/深度神经网络之反向传播算法.md","hash":"d1b0a58b8b09f757606d6db25e4d0a6850c216fc","modified":1530173196732},{"_id":"source/_posts/深度神经网络之损失函数和激活函数.md","hash":"e61baee83d475cc470d3088b761af427c239333e","modified":1530355553662},{"_id":"source/_posts/深度神经网络之正则化.md","hash":"ffffe1f4e88e9e5ea18a7a1772fb142a5a0bc161","modified":1530673605367},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文.md","hash":"3588081be49c460036f9d3181c2caaca551a65a4","modified":1529984965409},{"_id":"source/_posts/网店工商信息图片文字提取.md","hash":"1e49e6487a3276111ed685332f30b88133e1b858","modified":1528721827000},{"_id":"source/favorite/index.md","hash":"207c3024e538eb0b91375ada3bd7db83cc27abba","modified":1533189726589},{"_id":"source/tags/index.md","hash":"04610e91300b8e0df44c872460f285b6a6595f14","modified":1528393971000},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"b63696d41f022525e40d7e7870c3785b6bc7536b","modified":1528393971000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1528393971000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"f846118d7fc68c053df47b24e1f661241645373f","modified":1528393971000},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"7abbb4c8a29b2c14e576a00f53dbc0b4f5669c13","modified":1528393971000},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1528393971000},{"_id":"themes/next/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1528393971000},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1528393971000},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"141e989844d0b5ae2e09fb162a280715afb39b0d","modified":1528393971000},{"_id":"themes/next/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1528393971000},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1528393971000},{"_id":"themes/next/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1528393971000},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"120750c03ec30ccaa470b113bbe39f3d423c67f0","modified":1528393971000},{"_id":"themes/next/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1528393971000},{"_id":"themes/next/docs/MATH.md","hash":"0ae4258950de01a457ea8123a8d13ec6db496e53","modified":1528393971000},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1528393971000},{"_id":"themes/next/layout/_layout.swig","hash":"09e8a6bfe5aa901c66d314601c872e57f05509e8","modified":1528393971000},{"_id":"themes/next/layout/archive.swig","hash":"2b6450c6b6d2bcbcd123ad9f59922a5e323d77a5","modified":1528393971000},{"_id":"themes/next/layout/category.swig","hash":"5d955284a42f802a48560b4452c80906a5d1da02","modified":1528393971000},{"_id":"themes/next/layout/index.swig","hash":"53300ca42c00cba050bc98b0a3f2d888d71829b1","modified":1528393971000},{"_id":"themes/next/layout/page.swig","hash":"79040bae5ec14291441b33eea341a24a7c0e9f93","modified":1528393971000},{"_id":"themes/next/layout/post.swig","hash":"e7458f896ac33086d9427979f0f963475b43338e","modified":1528393971000},{"_id":"themes/next/layout/schedule.swig","hash":"3e9cba5313bf3b98a38ccb6ef78b56ffa11d66ee","modified":1528393971000},{"_id":"themes/next/layout/tag.swig","hash":"ba402ce8fd55e80b240e019e8d8c48949b194373","modified":1528393971000},{"_id":"themes/next/languages/de.yml","hash":"fb478c5040a4e58a4c1ad5fb52a91e5983d65a3a","modified":1528393971000},{"_id":"themes/next/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1528393971000},{"_id":"themes/next/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1528393971000},{"_id":"themes/next/languages/fr.yml","hash":"0162a85ae4175e66882a9ead1249fedb89200467","modified":1528393971000},{"_id":"themes/next/languages/id.yml","hash":"e7fb582e117a0785036dcdbb853a6551263d6aa6","modified":1528393971000},{"_id":"themes/next/languages/it.yml","hash":"62ef41d0a9a3816939cb4d93a524e6930ab9c517","modified":1528393971000},{"_id":"themes/next/languages/ja.yml","hash":"5f8e54c666393d1ca2e257f6b1e3b4116f6657d8","modified":1528393971000},{"_id":"themes/next/languages/ko.yml","hash":"fae155018ae0efdf68669b2c7dd3f959c2e45cc9","modified":1528393971000},{"_id":"themes/next/languages/nl.yml","hash":"bb9ce8adfa5ee94bc6b5fac6ad24ba4605d180d3","modified":1528393971000},{"_id":"themes/next/languages/pt-BR.yml","hash":"bfc80c8a363fa2e8dde38ea2bc85cd19e15ab653","modified":1528393971000},{"_id":"themes/next/languages/pt.yml","hash":"3cb51937d13ff12fcce747f972ccb664840a9ef3","modified":1528393971000},{"_id":"themes/next/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1528393971000},{"_id":"themes/next/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1528393971000},{"_id":"themes/next/languages/vi.yml","hash":"8da921dd8335dd676efce31bf75fdd4af7ce6448","modified":1528393971000},{"_id":"themes/next/languages/zh-CN.yml","hash":"041fd4769f133952e093afd2a9782513cae0b2bb","modified":1528393971000},{"_id":"themes/next/languages/zh-hk.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1528393971000},{"_id":"themes/next/scripts/helpers.js","hash":"392cda207757d4c055b53492a98f81386379fc4f","modified":1528393971000},{"_id":"themes/next/languages/zh-tw.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1528393971000},{"_id":"themes/next/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1528393971000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1528393971000},{"_id":"themes/next/source/CNAME","hash":"5455120583e7a62be755f95d1b53d5efbbec5bf4","modified":1528393971000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1528393971000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1528393971000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1528393971000},{"_id":"source/_posts/《剑指Offer》Python版.md","hash":"7527afbfe85e11160b1798b765fcfd2bb6ccdf15","modified":1533527783764},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528393971000},{"_id":"source/about/index/公众号.jpg","hash":"df033d0e4e9de400ba207e9b6b7f28817d5996d8","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/02.png","hash":"e68d02686e078ee777ecb9c62586843dc0fa4d95","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/03.png","hash":"613cb5e4cfc842bd987ed3a3da446ff779605ff4","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/05.png","hash":"abe67138ba5186ca80125b35f68a24ada5da397a","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/04.png","hash":"29b910fc5dd61d9dc017a43ebe5fa7835a7cd33c","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/06.png","hash":"07003112ee77f408788693ccdb71fa6578d635f2","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/07.png","hash":"7f0e85e07ca1ae562025c8031cfc87eb9a99b51a","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/08.png","hash":"4e3061fc73c47c5040c2920567185f906298fab7","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/09.png","hash":"352b9d0ab5678717465660ca2f7d243ac605166a","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/11.png","hash":"44dfb34c533a659f62e0213a9b6320329503857c","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/13.png","hash":"e10a1daaaac28c0d04bf4a3183e08ae9b5e0b985","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像20.png","hash":"414c76614aa0e1f9b0832aced52ed8e0702fcb1d","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程03.png","hash":"ef9781638e5f177bb192245d906da889af3f6aa3","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程10.png","hash":"b89f99c644b86aec73b4f6b0a93566ddd765ad15","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程09.png","hash":"56374280cab86a969f853c967201d3d3729cf920","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程11.png","hash":"bddde196beb8e1f8b8fe1ec4b9ddb82682953f38","modified":1528393971000},{"_id":"source/_posts/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片02.png","hash":"945254cd8be0e45015bdd8c7574219cf221752a6","modified":1534241298686},{"_id":"source/_posts/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片01.png","hash":"314ffc4aee9d0e92346eddb5d48cf43483914b46","modified":1534240604311},{"_id":"source/_posts/基于google-protobuf的gRPC实现-python版/.DS_Store","hash":"1745316bd518687ffd4d11b51a3cc13c8cc5dad3","modified":1534248376024},{"_id":"source/_posts/效率软件推荐（一）/软件推荐夸克浏览器01.PNG","hash":"bca19e29af9bf3ffe308c91a6c2e1ee81d0fceb0","modified":1528865071455},{"_id":"source/_posts/效率软件推荐（一）/.DS_Store","hash":"eecb1f4a715d49862eb6eb58bb75397ca4542eeb","modified":1528874083503},{"_id":"source/_posts/智慧考古探测/.DS_Store","hash":"2c602e41cd682dde3bfc70e0324653b30c2fc2b5","modified":1529982910764},{"_id":"source/_posts/效率软件推荐（一）/软件推荐夸克浏览器02.PNG","hash":"164e7e45ffbe688e1e427f24da9f420049a6767a","modified":1528864730286},{"_id":"source/_posts/效率软件推荐（一）/软件推荐滴答清单01.PNG","hash":"5c5094d85ea2d08320fc7994485d51876efaffce","modified":1528822323000},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png","hash":"ad73ccab95e08bb762f941a4630dd3c6a9388a2e","modified":1528393971000},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png","hash":"c577553d106ea0027b569274eb357e96cda02ed3","modified":1528393971000},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png","hash":"a21052a33e97fc36bd411a2b42eb7abec2ae6736","modified":1528393971000},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png","hash":"a1324b9077b0916ac0936bf0440e907d4d70e137","modified":1528393971000},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png","hash":"8ae7791d2a6604a0ceda324af19292ab54e092f3","modified":1528393971000},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png","hash":"4cccd6b50cdd9d4933ea11bb74c175d27ec3d57f","modified":1528393971000},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png","hash":"fe51a131f9553b96995b546ec9515dc9a3b59c27","modified":1528393971000},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归01.png","hash":"1817f152628ed2d0e63b71a9071cbcb7765edd41","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png","hash":"2c766de1fa96945fbadff336ad36ddb470026c58","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像01.png","hash":"f5b42a7d64bcf4e42dbddea25f9d91dacd47ae8e","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像02.png","hash":"911aacc5c8458e2a9789dabcf11bee3b8fb08254","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像03.png","hash":"db3af154b414462add94cafcf711c95dc8fce7d2","modified":1528393971000},{"_id":"source/_posts/机器学习之决策树-C4-5算法/机器学习之决策树图片01.png","hash":"94ede414f10507f9d1a5433b3db69e819d36b605","modified":1528393971000},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png","hash":"b99b737fc9473f0ffda6ba8a3c3f225a9bf04908","modified":1528393971000},{"_id":"source/_posts/机器学习之线性回归/图片01.png","hash":"2b715466ee24cacfb4596159b9e5d548c7508a98","modified":1528393971000},{"_id":"source/_posts/机器学习知识体系/图片01.png","hash":"2242e0694775cb96fac26c3d88feabbc965c0c59","modified":1528393971000},{"_id":"source/_posts/机器学习知识体系/图片02.png","hash":"0a2bb3b98d4750f7b7f7830aefb1347ea1e6414c","modified":1528393971000},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络01.png","hash":"6d05a8e90581cbd6c5ac2dc501559e33f1c596c9","modified":1530065254872},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络02.png","hash":"a011fd645733681b936dfb334bbce4caf5d296ba","modified":1530066037323},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络03.png","hash":"a4878a8a23d141d3da0f0e951921fd96eb768ed4","modified":1530066323431},{"_id":"source/_posts/深度神经网络之前向传播算法/.DS_Store","hash":"105626d556a959314d008d48fe47ccd8857de657","modified":1530071476869},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络04.png","hash":"9975bce863a82ec307b0c4e530d4d2c33edc998a","modified":1530067464522},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络06.png","hash":"2bd04361c64d2c2c2f01c8b3d41ad46676ac7938","modified":1530069465884},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络05.png","hash":"305b1e06cc809638d05c77ba39ca01b1237fcfee","modified":1530067828428},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片01.png","hash":"36ea9198dd3d411d2c8009df179b454872006ff0","modified":1530336063751},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片05.png","hash":"6725bb33e47d4db1eda2353839fb11f02e6d44e5","modified":1530338523399},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/.DS_Store","hash":"a4bdef03d7fc87f3250c8a03cc0875bc8793ce9f","modified":1530354509901},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片03.png","hash":"85a2bf7f1d5093c19a0c6b1ccda38919130a4336","modified":1530337127173},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片06.png","hash":"4a480163104179e57df0bea4979e5f9c8bb32a30","modified":1530343206825},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片07.png","hash":"7f824024095010343cc0e07e1c68da769ab151bf","modified":1530348375850},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片03.png","hash":"2ab98f528ab0d0b81b233e14f8314fcf97fb6dfe","modified":1530606048901},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片04.png","hash":"f22098c7b05b0bdff45a4d1fd532a62a7f092adf","modified":1530609485367},{"_id":"source/_posts/深度神经网络之正则化/.DS_Store","hash":"36492800b8699019c0554c2f7d4378fcc9e3ddb9","modified":1530670750892},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片05.png","hash":"00cd1a9f4d1dc193a7e17e51b97401e93ed04f26","modified":1530609509255},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片08.png","hash":"eebee8620200b5580c880fde142fd74d1847bf25","modified":1530348515428},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片02.png","hash":"b0c9a5b7784233e2749525c90ff2784caeec87d3","modified":1530602674494},{"_id":"source/_posts/网店工商信息图片文字提取/网店工商信息图片文字提取01.png","hash":"f000c96bda08707cffe2da2d904381a3d8defcd3","modified":1528078000000},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/.DS_Store","hash":"e70832415720c11bd97c6bdfaa6c0049358a360d","modified":1529984893508},{"_id":"themes/next/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1528393971000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1528393971000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1528393971000},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1528393971000},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1528393971000},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1528393971000},{"_id":"themes/next/docs/ru/README.md","hash":"712d9a9a557c54dd6638adfb0e1d2bb345b60756","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"6855402e2ef59aae307e8bd2a990647d3a605eb8","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"a45a791b49954331390d548ac34169d573ea5922","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"44e4fb7ce2eca20dfa98cdd1700b50d6def4086f","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"24cf2618d164440b047bb9396263de83bee5b993","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"e03607b608db4aa7d46f6726827c51ac16623339","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"c1ba919f70efe87a39e6217883e1625af0b2c23c","modified":1528393971000},{"_id":"themes/next/docs/zh-CN/README.md","hash":"84d349fda6b9973c81a9ad4677db9d9ee1828506","modified":1528393971000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1528393971000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"05e67c50a4f3a20fad879ed61b890de8ca6ba4ea","modified":1528393971000},{"_id":"themes/next/layout/_macro/post-related.swig","hash":"08fe30ce8909b920540231e36c97e28cfbce62b6","modified":1528393971000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"bd5778d509c51f4b1d8da3a2bc35462929f08c75","modified":1528393971000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"1f3121ef66a4698fd78f34bf2594ef79a407c92c","modified":1528393971000},{"_id":"themes/next/layout/_macro/post.swig","hash":"686e60ede86547bdd7bc34c3629e4c9dbd134a21","modified":1528393971000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"a9e1346b83cf99e06bed59a53fc069279751e52a","modified":1528393971000},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"60001c8e08b21bf3a7afaf029839e1455340e95d","modified":1528393971000},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"a8ab2035654dd06d94faf11a35750529e922d719","modified":1528393971000},{"_id":"themes/next/layout/_third-party/github-banner.swig","hash":"cabd9640dc3027a0b3ac06f5ebce777e50754065","modified":1528393971000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"f532ce257fca6108e84b8f35329c53f272c2ce84","modified":1528393971000},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"927f19160ae14e7030df306fc7114ba777476282","modified":1528393971000},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"6b75c5fd76ae7cf0a7b04024510bd5221607eab3","modified":1528393971000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1528393971000},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1528393971000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"b0ca46e0d1ff4c08cb0a3a8c1994f20d0260cef9","modified":1528393971000},{"_id":"themes/next/layout/_partials/breadcrumb.swig","hash":"6994d891e064f10607bce23f6e2997db7994010e","modified":1528393971000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"1ae77b6a369f83c9986408f2ab448090e37cd2dc","modified":1528393971000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"5df32b286a8265ba82a4ef5e1439ff34751545ad","modified":1528393971000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1aaf32bed57b976c4c1913fd801be34d4838cc72","modified":1528393971000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1528393971000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"0a0129e926c27fffc6e7ef87fe370016bc7a4564","modified":1528393971000},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"ac3ad2c0eccdf16edaa48816d111aaf51200a54b","modified":1528393971000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"6fc63d5da49cb6157b8792f39c7305b55a0d1593","modified":1528393971000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1528393971000},{"_id":"themes/next/scripts/tags/button.js","hash":"5a61c2da25970a4981fbd65f4a57c5e85db4dcda","modified":1528393971000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"db70a841e7c1708f95ca97b44413b526b267fa9b","modified":1528393971000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"2b3a4dc15dea33972c0b6d46a1483dabbf06fb5b","modified":1528393971000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"a98fc19a90924f2368e1982f8c449cbc09df8439","modified":1528393971000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"1b97b1b5364945b8ab3e50813bef84273055234f","modified":1528393971000},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"b7600f6b868d8f4f7032126242d9738cd1e6ad71","modified":1528393971000},{"_id":"themes/next/scripts/tags/label.js","hash":"621004f2836040b12c4e8fef77e62cf22c561297","modified":1528393971000},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"460e5e1f305847dcd4bcab9da2038a85f0a1c273","modified":1528393971000},{"_id":"themes/next/scripts/tags/note.js","hash":"4975d4433e11161b2e9a5744b7287c2d667b3c76","modified":1528393971000},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1528393971000},{"_id":"themes/next/source/css/main.styl","hash":"c26ca6e7b5bd910b9046d6722c8e00be672890e0","modified":1528393971000},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1528393971000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1528393971000},{"_id":"themes/next/source/images/avatar.png","hash":"d985c17188c7c966df863e797976d98ce8b40771","modified":1528393971000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1528393971000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1528393971000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1528393971000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1528393971000},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"78aec0fd86b154f1483a3fddf8bacb4632404bb5","modified":1528393971000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1528393971000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1528393971000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1528393971000},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"78aec0fd86b154f1483a3fddf8bacb4632404bb5","modified":1528393971000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1528393971000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1528393971000},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1528393971000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1528393971000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1528393971000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1528393971000},{"_id":"source/_posts/Linux常用命令/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/Markdown写作教程/图片01.png","hash":"37bf22348a323e3708cfe540ae74e6d8e89bd4d6","modified":1528393971000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/Markdown写作教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/01.png","hash":"33e230638dfea5aecb04dfbdd3a6099c248ef2c3","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像18.png","hash":"3eb66a4726829f310e9317433813bafcae824183","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像19.png","hash":"a9ce5acc9a9bd616faf50581b802aa69d4ec5e5d","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像21.png","hash":"6c844ab24a7b2fa744f14a70bac46b613fc4cca3","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像22.png","hash":"e9cf3a459468a6696855da633138be48e6ee3d7b","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像23.png","hash":"84f4972fd31cc00510f9cd5b5b5c23bce1661072","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/图片16.png","hash":"f61b216e3015ff8461953b80a41c404ab9c05f7a","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/图片17.png","hash":"05d26db61bbd1b7ad73219630d6423e1c712d91c","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程04.png","hash":"54e7eb64aeff4e62baa1f465b9ecd8b0163c298d","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程07.png","hash":"f73d270ea01afc9494a85cb642779e25ceb87aa7","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程06.png","hash":"397f01c175a32a327bd0f5e25737bebfb30a429a","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/《剑指Offer》Python版/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/基于google-protobuf的gRPC实现-python版/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记01.PNG","hash":"c1fdb7a17862fccbdd3b96c549387c549fb5dadd","modified":1528858730611},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记04.PNG","hash":"d3ca4f10fe671b28c89000775e17acf871afd0d0","modified":1528861954905},{"_id":"source/_posts/效率软件推荐（一）/软件推荐夸克浏览器03.PNG","hash":"2e49a00dcd4f66b944a4bdf48cfa63ba6cdeb577","modified":1528864730507},{"_id":"source/_posts/效率软件推荐（一）/软件推荐滴答清单02.PNG","hash":"5a409d47f7235849c9931077e41af0493ec8c1ad","modified":1528821126000},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png","hash":"305df6ad6e8549a280eda7178c796df59c3efc7a","modified":1528393971000},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png","hash":"8be383416d0e78739d78ee2332876bbc366a75c9","modified":1528393971000},{"_id":"source/_posts/机器学习之Logistic回归/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归03.png","hash":"414ee5813fc11417114610e2e6af70a89650f185","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png","hash":"b44c4687d5db6136f7ad2d8c07ce5f8810315df5","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png","hash":"7efc5e982bb03590dadca9b1d7bcff20937026a4","modified":1528393971000},{"_id":"source/_posts/机器学习之线性回归/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png","hash":"7efc5e982bb03590dadca9b1d7bcff20937026a4","modified":1528393971000},{"_id":"source/_posts/机器学习知识体系/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/深度神经网络之前向传播算法/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络07.png","hash":"8f2ba2ecb496d9a8fa1a6695fbb3c4dbb1f16dd1","modified":1530069779529},{"_id":"source/_posts/深度神经网络之反向传播算法/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片02.png","hash":"2f24b00c060c1c240daa6f575b823c7b1c10d55c","modified":1530336426411},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片04.png","hash":"7eaaf664001cb7cb3b27d6ab559a29651c60253b","modified":1530337223612},{"_id":"source/_posts/深度神经网络之正则化/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528393971000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528393971000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528393971000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528393971000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528393971000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528393971000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528393971000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528393971000},{"_id":"source/_posts/.Archive/Python之MatPlotLib使用教程.md/2018-03-25 19-55-55.md","hash":"eb5a4de8344a6843371745efa3cbccee02fef7a3","modified":1528725785510},{"_id":"source/_posts/.Archive/Python之MatPlotLib使用教程.md/2018-03-25 19-59-25.md","hash":"d4f91fd5d44d6beddd58eec9f426f0887583e1a6","modified":1528725784990},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 20-10-45.md","hash":"049fb26ee773eaf41946ef49b09daf6bb243fe2f","modified":1528725785339},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 21-52-20.md","hash":"f46cc95f707d6ef281191a30f6184020cff607fd","modified":1528725784204},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 20-40-57.md","hash":"d0cfca39104d291dee12c76bcfcce956d20b39d7","modified":1528725785661},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 20-19-07.md","hash":"4b912fca9b44721a93d346f24902fcc1d9518a4b","modified":1528725784958},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 21-55-10.md","hash":"32350f1cd9bc33d88c34ad2627911fd2c896b220","modified":1528725785040},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 21-57-50.md","hash":"3fbf2ab4576c8e8e5142583adcfbef684a5ede9b","modified":1528725786752},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 21-56-10.md","hash":"c168af8ad8b522e4e48e7cf3b148376e70396ade","modified":1528725787747},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-02-50.md","hash":"e452abb9fca7c44053cd2e998f9eb1d82ac273de","modified":1528725787148},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-04-40.md","hash":"e9c2d0a19630ffb04a3eae8540d8841ed8c71d56","modified":1528725787614},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-01-20.md","hash":"c39ea2086a8b044afe3e18c065fe8b10f04843c1","modified":1528725786632},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-07-10.md","hash":"bf01038be7871345ae5e13d67fcf2bf40dd3a116","modified":1528725786578},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-08-30.md","hash":"f9b76652cee53c59c03f4c0d83b8db0ad616b8e4","modified":1528725785005},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-16-30.md","hash":"a0dd34efea9fe196efe06452b3b2d7b8f87774a7","modified":1528725784524},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-10-40.md","hash":"ad568931dda35f76d84d7b8403c47a2ebe36c7ea","modified":1528725787175},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-20-30.md","hash":"d9579c4616c8cb15464eafe998c00bc98f8fc033","modified":1528725787418},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-22-00.md","hash":"7af87c891b584f41c215ee7d1ce56ef05d39cb27","modified":1528725786881},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-12-00.md","hash":"22cc0ff5ac4cc3818c442182f02bbd4a7bce1982","modified":1528725787828},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-15-30.md","hash":"74864eb152380179f7e65e77fc154c234a03779e","modified":1528725785028},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-23-40.md","hash":"19cfae3d212a34d0c0de41ccc2534d96f1376e2a","modified":1528725787490},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 23-25-31.md","hash":"be501dc660ce5477c996116d45d4c509a013a836","modified":1528725786947},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-40-21.md","hash":"483109e9481cff47d6780cde0909c38b45a254ef","modified":1528725785397},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 23-32-38.md","hash":"54af3335021febd12841463741bac435ce78aed3","modified":1528725786717},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 00-40-11.md","hash":"3eef9d1b2d90e4cbd9ada9e6828956a699fd7c2e","modified":1528725786150},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 00-41-36.md","hash":"a16b36716e6bd24fd078622801c7abe58e97d8ac","modified":1528725785355},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 23-17-48.md","hash":"d7378f3302b3427c4ef0aa51d87bbbcdab7f2e42","modified":1528725787444},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-21-06.md","hash":"6db531e0dffd53d94d09af155f768568699f5259","modified":1528725787299},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-23-56.md","hash":"c66db085f5534adccc3f2a29958b2f6ca908954a","modified":1528725785931},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-22-26.md","hash":"69e3b51eab2d30816d8b8b1326dfa738e34242e8","modified":1528725786032},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-31-16.md","hash":"df215088593c0372eabcaa8f86814f48bb2fce68","modified":1528725784814},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-34-26.md","hash":"ef03e21c450b5c01291525905bf3d6e47f556dcb","modified":1528725787123},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-36-46.md","hash":"fbdbeff0a51aeacd2ebd0a07e3bfd45f34345390","modified":1528725785918},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-35-36.md","hash":"bd1dafa423779089e951fed3b68fffda1b649af7","modified":1528725786700},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-33-06.md","hash":"aac252ce042db8a06086a02d4f45cd094fe0b4b0","modified":1528725786543},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-37-46.md","hash":"58c78bf8b049e9b29b10c2979bed346583ee424f","modified":1528725786933},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-39-26.md","hash":"bac24c55eedc782459b12c8b26c2b02c61ad8e79","modified":1528725784893},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-41-46.md","hash":"b80095e406003ec53a4d6c7a26fdbd0cc0cd08c6","modified":1528725786976},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-42-46.md","hash":"33a3d5d5e4fb23a23ec5fc5c127dfc4baf7ce0b4","modified":1528725785299},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-40-46.md","hash":"20651a1855db4f2389033635e5d20ee7758f113d","modified":1528725785419},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-44-06.md","hash":"e827a734db603b0b4a48b63f9b71250833268059","modified":1528725784651},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-45-06.md","hash":"6a78156cf700bf4d80bd4ac9bb0ca799d44a6646","modified":1528725784907},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-46-44.md","hash":"7a7096c08001cb5c21eb1de2011a0d00d1d7fa32","modified":1528725785699},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-22-38.md","hash":"56f392a82fafaa724043fcc173b0ed60aba88559","modified":1528725785270},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-21-28.md","hash":"a1f09efb8dfbcd09c5acd39c66feae094721a6ff","modified":1528725786591},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-23-48.md","hash":"da3d957091a84bf19391bcc51cb9b840eef82026","modified":1528725786676},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-31-58.md","hash":"34b3ef1e8a026e9079f33a9463729e13a1f827a3","modified":1528725785688},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-33-29.md","hash":"6ee8200a93119f6221cfd4384d68753e8d18792d","modified":1528725786103},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-25-38.md","hash":"87a8eb8d631ecba93341319704ba8e27d74707fa","modified":1528725785562},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-42-49.md","hash":"33a29aad02da9bd2d15b8a4e39dd6d3072734b57","modified":1528725786139},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-38-49.md","hash":"151224915158d949e45a855f869a5fdbcaf32230","modified":1528725786689},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-43-59.md","hash":"6ead39fb81ce29b51a3dc45a5e7b60f29313d666","modified":1528725786989},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-34-29.md","hash":"ab40e56cdf66dc96cf2e616d6b262b8a83f0029d","modified":1528725785370},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-45-29.md","hash":"88cd551cb991c96982436b921d72abbda9812656","modified":1528725786868},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-47-09.md","hash":"a78752c4a66a15abe83e8e17012a2b6bf7c99bf4","modified":1528725785991},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-48-09.md","hash":"40998f1150b08de3784b3e1b217fe49830d9517a","modified":1528725787210},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-50-29.md","hash":"cc5d24a1e98f874dc399c5c524c51c3e269ef87b","modified":1528725787162},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-51-39.md","hash":"218a24d8cbd0f93077d86faf8bef3185160d4802","modified":1528725786173},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-52-39.md","hash":"f7b85011363e9533f202e843b254fa312c2fc464","modified":1528725785905},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-53-49.md","hash":"cec8c17a758b931b28659172e7eff49a6b7f14e2","modified":1528725786556},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-55-59.md","hash":"890b192408dc5f4e27a9101151082331a156fadb","modified":1528725786728},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-54-59.md","hash":"ab08bc472e72d309786c16e77f03caf6746f387e","modified":1528725785110},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-57-59.md","hash":"6d9f2de0e3cd46f82c351422777655371ca87ad9","modified":1528725787095},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-00-39.md","hash":"373df51de30881e6d6943be5f9a4768db52dbb1d","modified":1528725785863},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-07-09.md","hash":"b9039a1b1b0a792611fbe471bc23fa4ade6077d7","modified":1528725784513},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-08-09.md","hash":"ab3a5085c37fc851054050d5477ca9606fbf6d2b","modified":1528725786741},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-59-29.md","hash":"33d5e6ec3b7c9c89ce5ae8ceb676e166176d06be","modified":1528725785097},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-10-19.md","hash":"21d1eed734d5425872fb57e15049efb8a19eebde","modified":1528725786653},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-11-29.md","hash":"b30fe788bb1acff82e9bae45017f59f64958f2fc","modified":1528725786665},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-17-49.md","hash":"57f684edb79da0ca486029db3379deb06f76b579","modified":1528725785325},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-19-09.md","hash":"187db145fa18ea582f965dbf849ffd66fb09de95","modified":1528725784540},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-21-09.md","hash":"fa41b1fda333357495ff41d7fd2470417d94f90e","modified":1528725785891},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-22-19.md","hash":"9925fe82ebd2b81377e12e85fc024583e5dc4caf","modified":1528725787344},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-24-29.md","hash":"a233a10461172c85b8c0f77aa403df112a2358c3","modified":1528725786513},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-04-32.md","hash":"68fd4dd7c304738f8ed74cea3bfc7c7578324b4a","modified":1528725785959},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-05-42.md","hash":"c0abdf7d187ef4ef2779c9c308c40dfd3024987e","modified":1528725787113},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-06-42.md","hash":"2439b5a290bdf02d0b90ecc99a8870ab060a055f","modified":1528725785773},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-08-22.md","hash":"0f9dd1f345389999e3abcca0118eb7d1abce791b","modified":1528725784324},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-09-22.md","hash":"ee47ba3255a6528bd3887a557b6312334c8367d4","modified":1528725786128},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-10-32.md","hash":"22ca7f872bf5bdcd8bfe56cec7fe58289c38476d","modified":1528725785597},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-11-42.md","hash":"7e9c35e90f602ee22419a4bd851505f55b6cef8c","modified":1528725785070},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-13-22.md","hash":"46f1e288b3ae476af2ba9a6d1516e8d031091e18","modified":1528725785582},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-14-22.md","hash":"94b4a9eb03304f851dc4ee069584aed9d81810ed","modified":1528725787134},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-15-22.md","hash":"75b349c8f2df5af30e3418b42a508f248b420653","modified":1528725785639},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-16-42.md","hash":"b92eef8a9f13713d6aeba4ed75eea9199560a293","modified":1528725786853},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-18-22.md","hash":"ca5df8507ac02d0c686037e50888b9fbf8929df8","modified":1528725786162},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-22-22.md","hash":"219c7feb17f955025807a13fa83ea12c0842e0d6","modified":1528725785083},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-19-52.md","hash":"237f34cb4e7b927abd307445dd7bff3bfa454e9c","modified":1528725784495},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-24-13.md","hash":"17eb8fd7a29cf7b1b77725e05d659d2c6a840feb","modified":1528725785725},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-50-13.md","hash":"def480dd8ca7d7e7863b09112d7df1a1e307e167","modified":1528725786009},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-46-23.md","hash":"2f31e399a623554c0c26a915151d3ab25f85d4ae","modified":1528725785609},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-52-32.md","hash":"0ab39dbd96b12445c3df9139e996bcd67117309c","modified":1528725786568},{"_id":"source/_posts/Markdown写作教程/图片03.png","hash":"244052c1fc5344f9d755cb942517d9d5da14afe3","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/12.png","hash":"4812b5371a7cb9d823327f8e95dffa5aaf97934b","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png","hash":"2be19ce3efa76780e9a18a93a1dac729b1b7a3f5","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png","hash":"ed89cf8ee5f651f2e1769321716565748b043bc3","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png","hash":"2f7a3455e1fd13aaa582f7274d4c7d50e8fae5d9","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png","hash":"ed008a6f9a39ee597bf965da9cf3349181e88980","modified":1528393971000},{"_id":"source/_posts/机器学习之线性回归/公式01.png","hash":"e3afd52d851957f06f23168c198ccd1814e9de25","modified":1528393971000},{"_id":"source/_posts/机器学习之线性回归/公式03.png","hash":"3ce320dce755145da770585a40c55c22da3d671e","modified":1528393971000},{"_id":"source/_posts/机器学习之线性回归/图片03.png","hash":"e322d56b0e286a2ca13cbbb470ce25010f8b1c5b","modified":1528393971000},{"_id":"source/_posts/网店工商信息图片文字提取/网店工商信息图片文字提取02.png","hash":"0049e281739aa61de9bff80884a86e177b73a119","modified":1528681623000},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"d1b73c926109145e52605929b75914cc8b60fb89","modified":1528393971000},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"67f0cb55e6702c492e99a9f697827629da036a0c","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"94b26dfbcd1cf2eb87dd9752d58213338926af27","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"beb53371c035b62e1a2c7bb76c63afbb595fe6e5","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"cee047575ae324398025423696b760db64d04e6f","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1528393971000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1528393971000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1528393971000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"8878241797f8494a70968756c57cacdfc77b61c7","modified":1528393971000},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1528393971000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"17a54796f6e03fc834880a58efca45c286e40e40","modified":1528393971000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"40e3cacbd5fa5f2948d0179eff6dd88053e8648e","modified":1528393971000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"6f340d122a9816ccdf4b45b662880a4b2d087671","modified":1528393971000},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"c0eb6123464d745ac5324ce6deac8ded601f432f","modified":1528393971000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"42f62695029834d45934705c619035733762309e","modified":1528393971000},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"a6fc00ec7f5642aabd66aa1cf51c6acc5b10e012","modified":1528393971000},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"97dbc2035bcb5aa7eafb80a4202dc827cce34983","modified":1528393971000},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"9b9ff4cc6d5474ab03f09835a2be80e0dba9fe89","modified":1528393971000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1528393971000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"b15e10abe85b4270860a56c970b559baa258b2a8","modified":1528393971000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1528393971000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1528393971000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1528393971000},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"1dd6cc34d041705e960831615f22f66deb1026ff","modified":1528393971000},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"fd780171713aada5eb4f4ffed8e714617c8ae6be","modified":1528393971000},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"a7e376b087ae77f2e2a61ba6af81cde5af693174","modified":1528393971000},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1528393971000},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"3db735d0cd2d449edf2674310ac1e7c0043cb357","modified":1528393971000},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"88b4b6051592d26bff59788acb76346ce4e398c2","modified":1528393971000},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a33b29ccbdc2248aedff23b04e0627f435824406","modified":1528393971000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1528393971000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1528393971000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1528393971000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1528393971000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1528393971000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1528393971000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1528393971000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"cc865af4a3cb6d25a0be171b7fc919ade306bb50","modified":1528393971000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1528393971000},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1528393971000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1528393971000},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1528393971000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"2640a54fa63bdd4c547eab7ce2fc1192cf0ccec8","modified":1528393971000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"81ca13d6d0beff8b1a4b542a51e3b0fb68f08efd","modified":1528393971000},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"7a2706304465b9e673d5561b715e7c72a238437c","modified":1528393971000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1528393971000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"32392d213f5d05bc26b2dc452f2fc6fea9d44f6d","modified":1528393971000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"cfb03ec629f13883509eac66e561e9dba562333f","modified":1528393971000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1528393971000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1528393971000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1528393971000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1528393971000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1528393971000},{"_id":"themes/next/source/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1528393971000},{"_id":"themes/next/source/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1528393971000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1528393971000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1528393971000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1528393971000},{"_id":"themes/next/source/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1528393971000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1528393971000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1528393971000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1528393971000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1528393971000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1528393971000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1528393971000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1528393971000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.3.png","hash":"9c5d6ffa38f3ff94979444e7ac5e38943c2404f8","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程05.png","hash":"256d9aadf22876b4fe5beea1c6fb126d245df262","modified":1528393971000},{"_id":"source/_posts/机器学习之Apriori算法/机器学习之Apriori算法图片01.png","hash":"f8ae81474b95fe79f07ebc5473e45574bec40829","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png","hash":"c6c24fbb1b1f06aee99d1c7ed558d54edf48e1ed","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png","hash":"f6aea31b36824cf4958541fd47067965a5057d81","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png","hash":"8e40abc5b044df8a0b256edbbca383df20e1a485","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png","hash":"28a4092973b644020d1b7b8f4d7ce80619006334","modified":1528393971000},{"_id":"source/_posts/机器学习之线性回归/公式02.png","hash":"09937e4c48b4bfaf597eb85131c7db518c73fce9","modified":1528393971000},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png","hash":"578f8b3f574ea4f6969c28ac881582e550b6df8e","modified":1528393971000},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png","hash":"07ef180e8842ac7765b0d07f3f15a4ffb9841dc7","modified":1528393971000},{"_id":"source/_posts/机器学习知识体系/图片03.png","hash":"8d75e5be4fbaa92f0ab0e411159cbfbb201673df","modified":1528393971000},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片01.png","hash":"dacb86e19945f305013025d44f3e9de09dba359c","modified":1530667993736},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1528393971000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片7.2.png","hash":"02359d570ab5830d815f61d8fe16ab25179c9c89","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/15.png","hash":"9adb17a88751a53d34588418786ee941c412ee1f","modified":1528393971000},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记02.PNG","hash":"3cd672901d649f34bbc25e86161d21fe7fd4cb5c","modified":1528858832000},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归02.png","hash":"bfa20ad1c15607d24bd562d56286006dad364d9f","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png","hash":"6cb18abfc66ca9a0570bcd2dd2149a9568714ef2","modified":1528393971000},{"_id":"source/_posts/机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png","hash":"492fd9787fc36e529f216bf8f8453b4b9e4835d1","modified":1528393971000},{"_id":"source/_posts/机器学习之线性回归/图片02.png","hash":"bfa20ad1c15607d24bd562d56286006dad364d9f","modified":1528393971000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1528393971000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"6958a97fde63e03983ec2394a4f8e408860fb42b","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1528393971000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1528393971000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"18309b68ff33163a6f76a39437e618bb6ed411f8","modified":1528393971000},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1528393971000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1528393971000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1528393971000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1528393971000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"0810e7c43d6c8adc8434a8fa66eabe0436ab8178","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"f362fbc791dafb378807cabbc58abf03e097af6d","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"f43c821ea272f80703862260b140932fe4aa0e1f","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"2212511ae14258d93bec57993c0385e5ffbb382b","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"5e12572b18846250e016a872a738026478ceef37","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"35f093fe4c1861661ac1542d6e8ea5a9bbfeb659","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"d5e8ea6336bc2e237d501ed0d5bbcbbfe296c832","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"05a5abf02e84ba8f639b6f9533418359f0ae4ecb","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"ba1842dbeb97e46c6c4d2ae0e7a2ca6d610ada67","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"41f9cdafa00e256561c50ae0b97ab7fcd7c1d6a2","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"ffa870c3fa37a48b01dc6f967e66f5df508d02bf","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"5779cc8086b1cfde9bc4f1afdd85223bdc45f0a0","modified":1528393971000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1528393971000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"a5913612c237bb7443c6006a386edd775201d423","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1528393971000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1528393971000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1528393971000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.1.png","hash":"6e71a79873cf1b4ce889c6bfd9bfdf7be1ac9009","modified":1528393970000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片4.1.png","hash":"faa30796bf710258a987c3277c77de1e5af0e709","modified":1528393970000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.4.png","hash":"36ce82cebff199c501a9cba78fef0169aae62225","modified":1528393971000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png","hash":"4d8692aec2e5ac5ead5da05c1833bdef07aa1d6e","modified":1528393971000},{"_id":"source/_posts/智慧考古探测/0003.jpg","hash":"3f58f88dd2cf745248da3f459c1c7f190d24b543","modified":1529953530000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png","hash":"6def4e9f93f518f2210e44d9c7d23aad339ffa56","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png","hash":"29d5e6087b241a8a4eae416a0997028f28b066e2","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png","hash":"11f3b12ef3ea65d97c6a92fffc71d73c094807b4","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png","hash":"840d9444a423bb1e43200f77bf6b2b6b4ef5f200","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png","hash":"4cf2f1a3596e6381019ac52c02a6fb7d78b02ac2","modified":1528393971000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1528393971000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0020.jpg","hash":"123e36968e83ae9771bce309da9b353110fa58ad","modified":1529881130000},{"_id":"source/_posts/智慧考古探测/0007.jpg","hash":"5cea74dfe1d770886099ce2c39483d4f15dc40e8","modified":1529953530000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png","hash":"7f105dfa87769b6894feb53226af206130ae2a0a","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png","hash":"c4aa4e10466b8af8ee430875266251cfd8669822","modified":1528393971000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png","hash":"562f9f4ef0d0b6fe7b1dbdd274ea417385b2e8cb","modified":1528393971000},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/001.png","hash":"53edfa978f907f2bd291231420d45fca3ac1b8d6","modified":1529984752283},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"7cc3f36222494c9a1325c5347d7eb9ae53755a32","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"39dee82d481dd9d44e33658960ec63e47cd0a715","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"ee37e6c465b9b2a7e39175fccfcbed14f2db039b","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"17b95828f9db7f131ec0361a8c0e89b0b5c9bff5","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"7dd9a0378ccff3e4a2003f486b1a34e74c20dac6","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"fb451dc4cc0355b57849c27d3eb110c73562f794","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"417f05ff12a2aaca6ceeac8b7e7eb26e9440c4c3","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ca89b167d368eac50a4f808fa53ba67e69cbef94","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"c0ac49fadd33ca4a9a0a04d5ff2ac6560d0ecd9e","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8bf095377d28881f63a30bd7db97526829103bf2","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"35c0350096921dd8e2222ec41b6c17a4ea6b44f2","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"bbe0d111f6451fc04e52719fd538bd0753ec17f9","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"4427ed3250483ed5b7baad74fa93474bd1eda729","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"43bc58daa8d35d5d515dc787ceb21dd77633fe49","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"5d15cc8bbefe44c77a9b9f96bf04a6033a4b35b8","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"10e9bb3392826a5a8f4cabfc14c6d81645f33fe6","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"76937db9702053d772f6758d9cea4088c2a6e2a3","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1528393971000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1c06be422bc41fd35e5c7948cdea2c09961207f6","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1528393971000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1528393971000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"fec45d19179030b8f43ebd0eabc5467892c14136","modified":1528393971000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1528393971000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"fec45d19179030b8f43ebd0eabc5467892c14136","modified":1528393971000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.5.png","hash":"bec5459c79cfa6fdabea06979fe34404781e106b","modified":1528393971000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片7.1.png","hash":"f70709d5d8b754a9fc2869781dc69a935d711de0","modified":1528393971000},{"_id":"source/_posts/Python之MatPlotLib使用教程/10.png","hash":"edb7d8ae73e7c28e9dc0af865e00422e93ae9fc4","modified":1528393971000},{"_id":"source/_posts/智慧考古探测/0009.jpg","hash":"85a9ad5b6ff553d64cf162e586fb5b9ffb7f256b","modified":1529953532000},{"_id":"source/_posts/智慧考古探测/0015.jpg","hash":"2cbcad1299dd6ae2c2505875917b1de196a02755","modified":1529953532000},{"_id":"source/_posts/智慧考古探测/0016.jpg","hash":"7ea9642d9b54ee928aaf5d15496eca682880be1e","modified":1529953534000},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png","hash":"d8ed9a84f2cb143da24a448a746eb27a6b948110","modified":1528393971000},{"_id":"source/_posts/机器学习知识体系/图片04.png","hash":"078a6395cfe259faaaafc9509152721c309ae446","modified":1528393971000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0002.jpg","hash":"49d32a5fe75fa675517c44940a19324f0de0c441","modified":1529881122000},{"_id":"source/_posts/智慧考古探测/0008.jpg","hash":"789b815e158d1e9ada5aa6f222cbaa009198a629","modified":1529953532000},{"_id":"source/_posts/机器学习之决策树-C4-5算法/机器学习之决策树图片02.png","hash":"a0bd7baea72ce6eb49b8e443fffa61cb4ce8fafc","modified":1528393971000},{"_id":"source/_posts/机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png","hash":"c0dda60bc9bb970d061a4e1abb9c12a301b0978d","modified":1528393971000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0004.jpg","hash":"b410a783bec6c7ee65e7ce063b9ef2f0d761427a","modified":1529881124000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0012.jpg","hash":"731afc7e20415e9774dbf8be4257657e524e7c3a","modified":1529881126000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0013.jpg","hash":"ecec9159ad22504fb9ddcae6edf7000038fc711a","modified":1529881126000},{"_id":"source/_posts/智慧考古探测/0005.jpg","hash":"764d8f349cd5a2474c8087dba6925f2a7eaeb1f4","modified":1529953530000},{"_id":"source/_posts/智慧考古探测/0006.jpg","hash":"8d74a65a51250ac78e07078d9fb6ca4643b68682","modified":1529953530000},{"_id":"source/_posts/智慧考古探测/0011.jpg","hash":"c9a208a59ba69aa756c48d22c84a8fd2a1e64f70","modified":1529953532000},{"_id":"source/_posts/智慧考古探测/0013.jpg","hash":"b0bc7bf235ff86634188f89f4f4e200c4065f8db","modified":1529953532000},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/005.png","hash":"c37bcbbdc516e7e33b4f721219c40363071aea5b","modified":1529984835474},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/004.png","hash":"94f7aae6c9ae8fd47214f8ff9ad8feac92276448","modified":1529984825042},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1528393971000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0005.jpg","hash":"b46f542aeead8244d1689c483441d66185208501","modified":1529881124000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0009.jpg","hash":"6eddd751cf3477d6ac67b97ace061eaaeca80262","modified":1529881126000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0008.jpg","hash":"649b3879851afbd86f4658eb57fc6c354370bd9f","modified":1529881124000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0011.jpg","hash":"61db61490cd945128829fe67800fdb418c92e9e4","modified":1529881126000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0010.jpg","hash":"fdc63c2404471580c1714a3b8142446a5eb69229","modified":1529881126000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0015.jpg","hash":"0bbb4ceb50e9dba1a56d9ede984a99fcfb5c33ed","modified":1529881128000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0018.jpg","hash":"010057b2f0b63cc30f2e69fec0c23004ec053036","modified":1529881128000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0022.jpg","hash":"3ea127bb6645c9f279d1bc058968a99ab0cd7abb","modified":1529881130000},{"_id":"source/_posts/智慧考古探测/0004.jpg","hash":"1c174bb5124eb7eaf72f92db34eeba78403d0adb","modified":1529953530000},{"_id":"source/_posts/智慧考古探测/0010.jpg","hash":"51270e284833d809d5312324146f82fd935b9ab9","modified":1529953532000},{"_id":"source/_posts/智慧考古探测/0012.jpg","hash":"f27e40b8578f7eee1f97f49092a3acb26c835925","modified":1529953532000},{"_id":"source/_posts/智慧考古探测/0014.jpg","hash":"1394e01dda5681718820cfe691dab11d38b6a485","modified":1529953532000},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/003.png","hash":"9ecc48b3835d9f36b8fbbecac9b733649b744fef","modified":1529984782022},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0001.jpg","hash":"a7b8847ac3621662c5daa3ea2e0447110f410ce8","modified":1529881122000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0003.jpg","hash":"b653861339d76b47be9bc4fb8057007ae0b55024","modified":1529881124000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0006.jpg","hash":"cedf2684b3b5d011615db8b941663c47f6563a03","modified":1529881124000},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/002.png","hash":"73e236cab9c64efeb518f507ff9d4afe88d8d4d4","modified":1529984771967},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0007.jpg","hash":"f3d275ca23e14efa92e4b4a8a68c1cca3f43a6e7","modified":1529881124000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0014.jpg","hash":"bf0b45886e64157a442a0d7edaeeb13b96543c08","modified":1529881126000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0021.jpg","hash":"649393c704f2d241992d996f7fab4c5987a59ba3","modified":1529881130000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0023.jpg","hash":"f02dab5ff9e1af103d236cd5255c2cc62cdb77ec","modified":1529881130000},{"_id":"source/_posts/Python之MatPlotLib使用教程/14.png","hash":"43f3564c467b1029cf25e46c2749b93e408be3cf","modified":1528393971000},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记03.PNG","hash":"49191fb3ab85b29f393948bc235f49a29a4b0aa4","modified":1528861710600},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0016.jpg","hash":"e9b25047b6ee568a5e1dd91318be415580dbcf45","modified":1529881128000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0017.jpg","hash":"7234297c25b32d9e65db0264de6a73debeceeae0","modified":1529881128000},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0019.jpg","hash":"67c79d1a83769eb82fe4e55585e8f35b02f416d7","modified":1529881128000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程08.png","hash":"12a6e73e9a4c6a21addd5542ebc96addfd889498","modified":1528393971000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.6.png","hash":"fdfdc025a22244646018a4105b4ffaa6b3ecdef6","modified":1528393971000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片3.3.png","hash":"4e466a4caae62c4d477208251f8f9ad259faeebe","modified":1528393970000},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.2.png","hash":"34877fc9266906c05387350a9c9d13b0c8411f92","modified":1528393971000},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记05.PNG","hash":"96e90a370328798086f8033c6a0aa9fe7dc8e49a","modified":1528860412014},{"_id":"source/_posts/机器学习之随机森林/机器学习之随机森林图片01.png","hash":"b17a64c76af269ced0330cc80c64bb4f67f637c1","modified":1528393971000},{"_id":"source/_posts/Markdown写作教程/图片02.png","hash":"4becc8fdd53dbe2c6e7117270d2297a27abb3c2d","modified":1528393971000},{"_id":"public/categories/index.html","hash":"68c10b781c334b743577d734e34ba81bc9c36bea","modified":1534260612762},{"_id":"public/about/index.html","hash":"c5bc2bdc85bfb4f6e2b6c4b1c8011670f4cd7c6d","modified":1534260612763},{"_id":"public/favorite/index.html","hash":"e65fb59ea0799f173b5c9e81a4b975fb11a529de","modified":1534260612763},{"_id":"public/2018/08/14/基于google-protobuf的gRPC实现-python版/index.html","hash":"d72fc6f9e350353e06fe63322012dc4b70e27257","modified":1534260612763},{"_id":"public/tags/index.html","hash":"6f2d3af166681194e5e602f633e11f91a9eb0008","modified":1534260612764},{"_id":"public/2018/07/29/《剑指Offer》Python版/index.html","hash":"577a8796f88a2ed82e7dffd6b75bf11cc451d518","modified":1534260612764},{"_id":"public/2018/07/03/深度神经网络之正则化/index.html","hash":"f1d2903266d2a2b497bcd0797ccc74a0a4e588c2","modified":1534260612764},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/index.html","hash":"698a34f4cdfdeee7885de3768e5e822117d6531f","modified":1534260612764},{"_id":"public/2018/06/27/深度神经网络之反向传播算法/index.html","hash":"3a3254c9dc483f7ace3ca34769bf589f537c6f9c","modified":1534260612764},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/index.html","hash":"f2205bf76cf3e897ce83c24b735a3a1dadd60118","modified":1534260612764},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/index.html","hash":"f3a0e587638fc0b433274cae7172838d189f68f3","modified":1534260612764},{"_id":"public/2018/06/12/效率软件推荐（一）/index.html","hash":"4774f62e741142532c92301058946fc0fff89caf","modified":1534260612765},{"_id":"public/2018/06/10/网店工商信息图片文字提取/index.html","hash":"0e6353e59772d399d7f734e66013c746594ef2de","modified":1534260612765},{"_id":"public/2018/05/18/机器学习之Apriori算法/index.html","hash":"b0779292ed6ff666cf51951ae6d2dc22c61dde39","modified":1534260612765},{"_id":"public/2018/08/14/Linux常用命令/index.html","hash":"ed98e650f6c424f063f588f110dcf4509fdb91ee","modified":1534260612775},{"_id":"public/2018/05/14/机器学习之朴素贝叶斯算法/index.html","hash":"75fc8720bd78ca3a7af869cc52f63de45f75cb46","modified":1534260612775},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/index.html","hash":"9f29d492a260a8e569ca591f42967bade12bd352","modified":1534260612776},{"_id":"public/2018/05/09/机器学习之最大期望-EM-算法/index.html","hash":"e8b4b9ae1dc3b75cc55754a671a8188f8dee9593","modified":1534260612776},{"_id":"public/2018/04/30/机器学习之梯度提升决策树-GBDT/index.html","hash":"6d7068da7afdd0103cafb1d9fb6edfdb0e832eab","modified":1534260612776},{"_id":"public/2018/04/30/机器学习之随机森林/index.html","hash":"49fdcf735f463647125168aa7c27d2f8508ba0ad","modified":1534260612776},{"_id":"public/2018/04/22/机器学习之分类与回归树-CART/index.html","hash":"faf28b5186af9c2060e76ab4ecde55f8a5725521","modified":1534260612776},{"_id":"public/2018/04/19/机器学习之决策树-C4-5算法/index.html","hash":"1df190ec1a0d65ca71a0c3cf54275bf82d2fccdb","modified":1534260612776},{"_id":"public/2018/04/15/Python之Sklearn使用教程/index.html","hash":"4693a2b06a7ef284ce11b5ee6b2723a91a1c64f9","modified":1534260612777},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/index.html","hash":"fe02939acae7cb19fc52a03af6b3015bb6d03b86","modified":1534260612777},{"_id":"public/2018/03/27/机器学习之Logistic回归/index.html","hash":"a4951b88c94bf30f4a97c7124e567dd1fa30199f","modified":1534260612777},{"_id":"public/2018/03/24/机器学习之线性回归/index.html","hash":"9462edd505250c3bc5287bb52836227f68366652","modified":1534260612777},{"_id":"public/2018/05/12/机器学习之K均值-K-Means-算法/index.html","hash":"84141d564938413293e9d646c739a8fcd2674340","modified":1534260612777},{"_id":"public/2018/05/03/机器学习之自适应增强-Adaboost/index.html","hash":"1a0c6a3fcc15fef1b892ac0d1ba3a237efb2c15f","modified":1534260612777},{"_id":"public/2018/03/24/机器学习知识体系/index.html","hash":"6c5502634bde6674cd8a654929c6be217e0bd2dc","modified":1534260612777},{"_id":"public/2018/03/20/智慧考古探测/index.html","hash":"7c588728c111f18d64f75cf171ad5b9e5b804a19","modified":1534260612778},{"_id":"public/2018/03/18/Markdown写作教程/index.html","hash":"9268c053ae3b2259fe5af340947f0c589e49bd8a","modified":1534260612778},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/index.html","hash":"a54810dcda6523938b64004be5f88964d5ba9917","modified":1534260612778},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/index.html","hash":"ac989d8aef3f1557c487d7ebd06691721ee4e496","modified":1534260612778},{"_id":"public/2018/03/12/面向知乎的个性化推荐模型研究论文/index.html","hash":"a8eaf1374cb19f908af8936a05c2c08a8a2120d0","modified":1534260612778},{"_id":"public/2018/03/12/Python之Pandas使用教程/index.html","hash":"a64327f595effaf9d012853b3cdc7e4ed76b25be","modified":1534260612778},{"_id":"public/categories/Linux/index.html","hash":"c16f61ce441e4d7e97e78c16fc3641738b2efba9","modified":1534260612778},{"_id":"public/categories/Python库/index.html","hash":"27f10f0785b99c5dfb2fac9840325d944ca84f1a","modified":1534260612778},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/index.html","hash":"f378a46243f9431fa8e603a68320881baa2fb0c3","modified":1534260612779},{"_id":"public/2018/03/13/Python之NumPy使用教程/index.html","hash":"f48e95648a052f29ad4df1ca306aaea54b9bf659","modified":1534260612779},{"_id":"public/categories/软件/index.html","hash":"c75911df0f8b73dc3a9f51df5fa576cfcfce153b","modified":1534260612779},{"_id":"public/categories/机器学习/index.html","hash":"2889d904f849b85b8e9cd5b1083382753addc94e","modified":1534260612779},{"_id":"public/categories/protobuf/index.html","hash":"29e5914216d8c172c146363798f0146b2e159e5a","modified":1534260612780},{"_id":"public/categories/深度学习/index.html","hash":"2d51313ffeb2283d274c70ee7d582ec15eb1c3ba","modified":1534260612780},{"_id":"public/categories/推荐系统/index.html","hash":"4a7995c48e546a77ace76dda03c524eef0267005","modified":1534260612780},{"_id":"public/categories/教程/index.html","hash":"e6380257a9952e68079d79f40d62bea15c01c34f","modified":1534260612780},{"_id":"public/categories/算法/index.html","hash":"31ba265bc6886869427a87cd99dae9a8dc2d32ed","modified":1534260612780},{"_id":"public/archives/index.html","hash":"3421deb1706969949614edd7669b2fc54f38ab00","modified":1534260612780},{"_id":"public/categories/比赛/index.html","hash":"352fe33e4aded7bb899a2402cf4b473103e1f8f4","modified":1534260612780},{"_id":"public/archives/page/2/index.html","hash":"302207ce253c4f5cbfb906d603f6ebcba9ea941c","modified":1534260612780},{"_id":"public/archives/page/3/index.html","hash":"08081935f1baac97b005e9a3dd6743e048b97541","modified":1534260612781},{"_id":"public/categories/文字识别/index.html","hash":"633ac3d1e17f911879eef2f44c4a10d16089da40","modified":1534260612781},{"_id":"public/archives/page/4/index.html","hash":"43eb7ed1d55b086afc90017612595408174e18f8","modified":1534260612781},{"_id":"public/archives/2018/page/2/index.html","hash":"8eb3747841b456c4b4533015f08edf04a89ffb2f","modified":1534260612781},{"_id":"public/categories/机器学习/page/2/index.html","hash":"ab730c29b9d2e661b4db32970950258227dcce5d","modified":1534260612781},{"_id":"public/archives/2018/page/3/index.html","hash":"b2a9e9c7b1251e2ffae87e9e4f21140256538e65","modified":1534260612781},{"_id":"public/archives/2018/page/4/index.html","hash":"845820031023dd3be8d9ba88cd0f247b35f97170","modified":1534260612781},{"_id":"public/archives/2018/03/page/2/index.html","hash":"8c0adae263c24e01de2c6784365aee6857ed4d78","modified":1534260612781},{"_id":"public/archives/2018/04/index.html","hash":"1f78fec6744e3b54c3419ec453506dce4a725dc1","modified":1534260612781},{"_id":"public/archives/2018/05/index.html","hash":"7c158cb1955a77d5bf2e5368fb6e81800492912a","modified":1534260612781},{"_id":"public/archives/2018/07/index.html","hash":"b3f324b39ef85c2ab50ffc3ab226636b7552654a","modified":1534260612781},{"_id":"public/archives/2018/08/index.html","hash":"7a1c89012d0b6b435c8feee3ff887fb35b2a0b79","modified":1534260612781},{"_id":"public/index.html","hash":"6051008533211e6c248befbf70abec9eeb89c4eb","modified":1534260612782},{"_id":"public/page/2/index.html","hash":"62655e53e9fb393b8a38ad1ad60c4aaac5d73697","modified":1534260612782},{"_id":"public/page/3/index.html","hash":"99b36616dfe8d66cc28c692503225f2ab46148c4","modified":1534260612782},{"_id":"public/page/4/index.html","hash":"92d5f3d704576a863690a6e1b0a98cd8ae4b1266","modified":1534260612782},{"_id":"public/page/5/index.html","hash":"6fcb01099690c28e0d54f0abbeed14e0604384d6","modified":1534260612782},{"_id":"public/tags/Linux/index.html","hash":"5b0701f13b97630dbbc0d0b13c927702ff4de28b","modified":1534260612782},{"_id":"public/tags/Mac/index.html","hash":"f08b6b210328e3da9e0a7f2a69f3c84e95d9b5ac","modified":1534260612782},{"_id":"public/tags/Hexo/index.html","hash":"d311353d54d1dc82ef5827bfcb61003ff35822da","modified":1534260612782},{"_id":"public/archives/2018/index.html","hash":"7401dc392a42e1f2f51d4cd460d7edca72ed120f","modified":1534260612782},{"_id":"public/tags/博客/index.html","hash":"077599fbbdb69facea42995390f99b50da230f96","modified":1534260612782},{"_id":"public/tags/Markdown/index.html","hash":"5d29ed6b6c7d674be86852e45467d4fef20dce30","modified":1534260612782},{"_id":"public/tags/GitHub/index.html","hash":"1b72733ab74a1f6517870cc733df028c3f306a78","modified":1534260612783},{"_id":"public/tags/教程/index.html","hash":"c5b3c9389c5bd0db2801c936901f66c0234276a7","modified":1534260612783},{"_id":"public/tags/python/index.html","hash":"3e22f596381b5f3b4f06f23c2439d72cb48c7576","modified":1534260612783},{"_id":"public/tags/Python/index.html","hash":"95115e5a9cd8efb74c3ab97ab652c3631d13771f","modified":1534260612783},{"_id":"public/tags/机器学习/index.html","hash":"3f399c74f38af520ff6aeceb6c35f844b202933e","modified":1534260612783},{"_id":"public/tags/机器学习/page/2/index.html","hash":"84f4f9cb1cb8926b11ec7d0c19f95ad8773a8618","modified":1534260612783},{"_id":"public/tags/protobuf/index.html","hash":"8cae9a266e7683db79832acbcc76192e0e3d7754","modified":1534260612783},{"_id":"public/tags/RPC/index.html","hash":"6d7b97510d0b2c89c31a786fb8ff68a27c68d5d9","modified":1534260612783},{"_id":"public/tags/gRPC/index.html","hash":"4573736dd21766535f76dcabf592c00a0fa09b25","modified":1534260612783},{"_id":"public/archives/2018/03/index.html","hash":"3f22002e22d90a10187b9bce4fd208b93678b0a8","modified":1534260612783},{"_id":"public/tags/效率软件/index.html","hash":"416f01589ae02176969cd5779838e907a575b8b7","modified":1534260612783},{"_id":"public/archives/2018/06/index.html","hash":"3701f3ce447010ca253d8b83d57c75a624644d44","modified":1534260612783},{"_id":"public/tags/算法/index.html","hash":"a13e7a78c606412eac473944745f7b50ec5bca9b","modified":1534260612783},{"_id":"public/tags/算法/page/2/index.html","hash":"23200fd7aa970d5b7f2606d6c02bf6e8807221b3","modified":1534260612783},{"_id":"public/tags/深度学习/index.html","hash":"80a1b5fba10fac86d7e9eb36c5f8d64a18f9ed7f","modified":1534260612783},{"_id":"public/tags/推荐系统/index.html","hash":"8cc66e1cab238712e539d21b678341a611495649","modified":1534260612784},{"_id":"public/tags/文字识别/index.html","hash":"f0b4ad79fee9905fe1a3ce5efa7bd4be5ea05296","modified":1534260612784},{"_id":"public/tags/数学建模/index.html","hash":"bdfebbcc32bf37c1c1f49358815255b0d1b6455e","modified":1534260612825},{"_id":"public/CNAME","hash":"5455120583e7a62be755f95d1b53d5efbbec5bf4","modified":1534260612852},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1534260612852},{"_id":"public/robots.txt","hash":"2be28bdb6f77c8f7627632bea984e2b3d735b411","modified":1534260612852},{"_id":"public/about/index/公众号.jpg","hash":"df033d0e4e9de400ba207e9b6b7f28817d5996d8","modified":1534260612852},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1534260612852},{"_id":"public/images/avatar.png","hash":"d985c17188c7c966df863e797976d98ce8b40771","modified":1534260612852},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1534260612852},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1534260612852},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1534260612852},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1534260612852},{"_id":"public/images/favicon-16x16-next.png","hash":"78aec0fd86b154f1483a3fddf8bacb4632404bb5","modified":1534260612852},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1534260612852},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1534260612853},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1534260612853},{"_id":"public/images/favicon-32x32-next.png","hash":"78aec0fd86b154f1483a3fddf8bacb4632404bb5","modified":1534260612853},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1534260612853},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1534260612853},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1534260612853},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1534260612853},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1534260612853},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1534260612853},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1534260612853},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1534260612853},{"_id":"public/2018/04/19/机器学习之决策树-C4-5算法/机器学习之决策树图片01.png","hash":"94ede414f10507f9d1a5433b3db69e819d36b605","modified":1534260612853},{"_id":"public/2018/04/30/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png","hash":"b99b737fc9473f0ffda6ba8a3c3f225a9bf04908","modified":1534260612853},{"_id":"public/2018/08/14/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片01.png","hash":"314ffc4aee9d0e92346eddb5d48cf43483914b46","modified":1534260612853},{"_id":"public/2018/06/10/网店工商信息图片文字提取/网店工商信息图片文字提取01.png","hash":"f000c96bda08707cffe2da2d904381a3d8defcd3","modified":1534260612853},{"_id":"public/2018/08/14/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片02.png","hash":"945254cd8be0e45015bdd8c7574219cf221752a6","modified":1534260612853},{"_id":"public/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png","hash":"ad73ccab95e08bb762f941a4630dd3c6a9388a2e","modified":1534260612853},{"_id":"public/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归01.png","hash":"1817f152628ed2d0e63b71a9071cbcb7765edd41","modified":1534260612853},{"_id":"public/2018/03/24/机器学习知识体系/图片01.png","hash":"2242e0694775cb96fac26c3d88feabbc965c0c59","modified":1534260612853},{"_id":"public/2018/03/24/机器学习知识体系/图片02.png","hash":"0a2bb3b98d4750f7b7f7830aefb1347ea1e6414c","modified":1534260612854},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png","hash":"a21052a33e97fc36bd411a2b42eb7abec2ae6736","modified":1534260612854},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png","hash":"a1324b9077b0916ac0936bf0440e907d4d70e137","modified":1534260612854},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png","hash":"c577553d106ea0027b569274eb357e96cda02ed3","modified":1534260612854},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png","hash":"8ae7791d2a6604a0ceda324af19292ab54e092f3","modified":1534260612854},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png","hash":"4cccd6b50cdd9d4933ea11bb74c175d27ec3d57f","modified":1534260612854},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png","hash":"fe51a131f9553b96995b546ec9515dc9a3b59c27","modified":1534260612854},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像01.png","hash":"f5b42a7d64bcf4e42dbddea25f9d91dacd47ae8e","modified":1534260612854},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像03.png","hash":"db3af154b414462add94cafcf711c95dc8fce7d2","modified":1534260612854},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像02.png","hash":"911aacc5c8458e2a9789dabcf11bee3b8fb08254","modified":1534260612854},{"_id":"public/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片02.png","hash":"b0c9a5b7784233e2749525c90ff2784caeec87d3","modified":1534260612854},{"_id":"public/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片03.png","hash":"2ab98f528ab0d0b81b233e14f8314fcf97fb6dfe","modified":1534260612854},{"_id":"public/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片04.png","hash":"f22098c7b05b0bdff45a4d1fd532a62a7f092adf","modified":1534260612854},{"_id":"public/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片05.png","hash":"00cd1a9f4d1dc193a7e17e51b97401e93ed04f26","modified":1534260612854},{"_id":"public/2018/03/24/机器学习之线性回归/图片01.png","hash":"2b715466ee24cacfb4596159b9e5d548c7508a98","modified":1534260612854},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/深度神经网络01.png","hash":"6d05a8e90581cbd6c5ac2dc501559e33f1c596c9","modified":1534260612854},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/深度神经网络02.png","hash":"a011fd645733681b936dfb334bbce4caf5d296ba","modified":1534260612854},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/深度神经网络03.png","hash":"a4878a8a23d141d3da0f0e951921fd96eb768ed4","modified":1534260612854},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/深度神经网络05.png","hash":"305b1e06cc809638d05c77ba39ca01b1237fcfee","modified":1534260612855},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/深度神经网络06.png","hash":"2bd04361c64d2c2c2f01c8b3d41ad46676ac7938","modified":1534260612855},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/深度神经网络04.png","hash":"9975bce863a82ec307b0c4e530d4d2c33edc998a","modified":1534260612855},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片01.png","hash":"36ea9198dd3d411d2c8009df179b454872006ff0","modified":1534260612855},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片03.png","hash":"85a2bf7f1d5093c19a0c6b1ccda38919130a4336","modified":1534260612855},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片05.png","hash":"6725bb33e47d4db1eda2353839fb11f02e6d44e5","modified":1534260612855},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片07.png","hash":"7f824024095010343cc0e07e1c68da769ab151bf","modified":1534260612855},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片06.png","hash":"4a480163104179e57df0bea4979e5f9c8bb32a30","modified":1534260612855},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片08.png","hash":"eebee8620200b5580c880fde142fd74d1847bf25","modified":1534260612855},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐夸克浏览器01.PNG","hash":"bca19e29af9bf3ffe308c91a6c2e1ee81d0fceb0","modified":1534260612855},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐夸克浏览器02.PNG","hash":"164e7e45ffbe688e1e427f24da9f420049a6767a","modified":1534260612855},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程09.png","hash":"56374280cab86a969f853c967201d3d3729cf920","modified":1534260612855},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐滴答清单01.PNG","hash":"5c5094d85ea2d08320fc7994485d51876efaffce","modified":1534260612855},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程03.png","hash":"ef9781638e5f177bb192245d906da889af3f6aa3","modified":1534260612856},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程10.png","hash":"b89f99c644b86aec73b4f6b0a93566ddd765ad15","modified":1534260612856},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程11.png","hash":"bddde196beb8e1f8b8fe1ec4b9ddb82682953f38","modified":1534260612856},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png","hash":"2c766de1fa96945fbadff336ad36ddb470026c58","modified":1534260612856},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/02.png","hash":"e68d02686e078ee777ecb9c62586843dc0fa4d95","modified":1534260612856},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/03.png","hash":"613cb5e4cfc842bd987ed3a3da446ff779605ff4","modified":1534260612856},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/04.png","hash":"29b910fc5dd61d9dc017a43ebe5fa7835a7cd33c","modified":1534260612856},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/06.png","hash":"07003112ee77f408788693ccdb71fa6578d635f2","modified":1534260612856},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/05.png","hash":"abe67138ba5186ca80125b35f68a24ada5da397a","modified":1534260612856},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/07.png","hash":"7f0e85e07ca1ae562025c8031cfc87eb9a99b51a","modified":1534260612856},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/08.png","hash":"4e3061fc73c47c5040c2920567185f906298fab7","modified":1534260612856},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/09.png","hash":"352b9d0ab5678717465660ca2f7d243ac605166a","modified":1534260612857},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/11.png","hash":"44dfb34c533a659f62e0213a9b6320329503857c","modified":1534260612857},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/13.png","hash":"e10a1daaaac28c0d04bf4a3183e08ae9b5e0b985","modified":1534260612857},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像20.png","hash":"414c76614aa0e1f9b0832aced52ed8e0702fcb1d","modified":1534260612857},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1534260613551},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1534260613555},{"_id":"public/2018/08/14/Linux常用命令/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613557},{"_id":"public/2018/06/27/深度神经网络之反向传播算法/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613557},{"_id":"public/2018/04/30/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png","hash":"7efc5e982bb03590dadca9b1d7bcff20937026a4","modified":1534260613558},{"_id":"public/2018/08/14/基于google-protobuf的gRPC实现-python版/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613558},{"_id":"public/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png","hash":"305df6ad6e8549a280eda7178c796df59c3efc7a","modified":1534260613558},{"_id":"public/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png","hash":"8be383416d0e78739d78ee2332876bbc366a75c9","modified":1534260613558},{"_id":"public/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png","hash":"7efc5e982bb03590dadca9b1d7bcff20937026a4","modified":1534260613558},{"_id":"public/2018/03/18/Markdown写作教程/图片01.png","hash":"37bf22348a323e3708cfe540ae74e6d8e89bd4d6","modified":1534260613558},{"_id":"public/2018/03/18/Markdown写作教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613559},{"_id":"public/2018/03/27/机器学习之Logistic回归/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613559},{"_id":"public/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归03.png","hash":"414ee5813fc11417114610e2e6af70a89650f185","modified":1534260613559},{"_id":"public/2018/03/24/机器学习知识体系/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613559},{"_id":"public/2018/07/03/深度神经网络之正则化/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613559},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613559},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png","hash":"b44c4687d5db6136f7ad2d8c07ce5f8810315df5","modified":1534260613559},{"_id":"public/2018/03/24/机器学习之线性回归/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613559},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613559},{"_id":"public/2018/06/27/深度神经网络之前向传播算法/深度神经网络07.png","hash":"8f2ba2ecb496d9a8fa1a6695fbb3c4dbb1f16dd1","modified":1534260613559},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613560},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片02.png","hash":"2f24b00c060c1c240daa6f575b823c7b1c10d55c","modified":1534260613560},{"_id":"public/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片04.png","hash":"7eaaf664001cb7cb3b27d6ab559a29651c60253b","modified":1534260613560},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐印象笔记01.PNG","hash":"c1fdb7a17862fccbdd3b96c549387c549fb5dadd","modified":1534260613560},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐印象笔记04.PNG","hash":"d3ca4f10fe671b28c89000775e17acf871afd0d0","modified":1534260613560},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐夸克浏览器03.PNG","hash":"2e49a00dcd4f66b944a4bdf48cfa63ba6cdeb577","modified":1534260613560},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613560},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程04.png","hash":"54e7eb64aeff4e62baa1f465b9ecd8b0163c298d","modified":1534260613560},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程06.png","hash":"397f01c175a32a327bd0f5e25737bebfb30a429a","modified":1534260613560},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程07.png","hash":"f73d270ea01afc9494a85cb642779e25ceb87aa7","modified":1534260613560},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐滴答清单02.PNG","hash":"5a409d47f7235849c9931077e41af0493ec8c1ad","modified":1534260613560},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613560},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613561},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/01.png","hash":"33e230638dfea5aecb04dfbdd3a6099c248ef2c3","modified":1534260613561},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像18.png","hash":"3eb66a4726829f310e9317433813bafcae824183","modified":1534260613561},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像19.png","hash":"a9ce5acc9a9bd616faf50581b802aa69d4ec5e5d","modified":1534260613561},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像21.png","hash":"6c844ab24a7b2fa744f14a70bac46b613fc4cca3","modified":1534260613561},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像23.png","hash":"84f4972fd31cc00510f9cd5b5b5c23bce1661072","modified":1534260613561},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像22.png","hash":"e9cf3a459468a6696855da633138be48e6ee3d7b","modified":1534260613561},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图片16.png","hash":"f61b216e3015ff8461953b80a41c404ab9c05f7a","modified":1534260613561},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613562},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图片17.png","hash":"05d26db61bbd1b7ad73219630d6423e1c712d91c","modified":1534260613562},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1534260613617},{"_id":"public/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1534260613617},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1534260613617},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1534260613617},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1534260613618},{"_id":"public/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1534260613618},{"_id":"public/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1534260613618},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1534260613618},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1534260613618},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1534260613618},{"_id":"public/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1534260613618},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1534260613618},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1534260613618},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1534260613618},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1534260613618},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1534260613618},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1534260613619},{"_id":"public/css/main.css","hash":"753eae8fd610970eb093c6db3893049f51fee6f5","modified":1534260613619},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1534260613619},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1534260613619},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1534260613619},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1534260613619},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1534260613619},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1534260613619},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"a5913612c237bb7443c6006a386edd775201d423","modified":1534260613619},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1534260613619},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1534260613620},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1534260613620},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"fec45d19179030b8f43ebd0eabc5467892c14136","modified":1534260613621},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"fec45d19179030b8f43ebd0eabc5467892c14136","modified":1534260613623},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1534260613625},{"_id":"public/2018/07/29/《剑指Offer》Python版/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1534260613626},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1534260613631},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/12.png","hash":"4812b5371a7cb9d823327f8e95dffa5aaf97934b","modified":1534260613631},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png","hash":"2be19ce3efa76780e9a18a93a1dac729b1b7a3f5","modified":1534260613632},{"_id":"public/2018/03/18/Markdown写作教程/图片03.png","hash":"244052c1fc5344f9d755cb942517d9d5da14afe3","modified":1534260613632},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png","hash":"ed89cf8ee5f651f2e1769321716565748b043bc3","modified":1534260613632},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png","hash":"2f7a3455e1fd13aaa582f7274d4c7d50e8fae5d9","modified":1534260613633},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png","hash":"ed008a6f9a39ee597bf965da9cf3349181e88980","modified":1534260613633},{"_id":"public/2018/03/24/机器学习之线性回归/公式01.png","hash":"e3afd52d851957f06f23168c198ccd1814e9de25","modified":1534260613633},{"_id":"public/2018/03/24/机器学习之线性回归/公式03.png","hash":"3ce320dce755145da770585a40c55c22da3d671e","modified":1534260613634},{"_id":"public/2018/03/24/机器学习之线性回归/图片03.png","hash":"e322d56b0e286a2ca13cbbb470ce25010f8b1c5b","modified":1534260613634},{"_id":"public/2018/06/10/网店工商信息图片文字提取/网店工商信息图片文字提取02.png","hash":"0049e281739aa61de9bff80884a86e177b73a119","modified":1534260613634},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.3.png","hash":"9c5d6ffa38f3ff94979444e7ac5e38943c2404f8","modified":1534260613977},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程05.png","hash":"256d9aadf22876b4fe5beea1c6fb126d245df262","modified":1534260613978},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png","hash":"c6c24fbb1b1f06aee99d1c7ed558d54edf48e1ed","modified":1534260613979},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png","hash":"f6aea31b36824cf4958541fd47067965a5057d81","modified":1534260613980},{"_id":"public/2018/05/18/机器学习之Apriori算法/机器学习之Apriori算法图片01.png","hash":"f8ae81474b95fe79f07ebc5473e45574bec40829","modified":1534260613980},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png","hash":"8e40abc5b044df8a0b256edbbca383df20e1a485","modified":1534260613981},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png","hash":"28a4092973b644020d1b7b8f4d7ce80619006334","modified":1534260613981},{"_id":"public/2018/03/24/机器学习之线性回归/公式02.png","hash":"09937e4c48b4bfaf597eb85131c7db518c73fce9","modified":1534260613982},{"_id":"public/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png","hash":"07ef180e8842ac7765b0d07f3f15a4ffb9841dc7","modified":1534260613983},{"_id":"public/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png","hash":"578f8b3f574ea4f6969c28ac881582e550b6df8e","modified":1534260613984},{"_id":"public/2018/03/24/机器学习知识体系/图片03.png","hash":"8d75e5be4fbaa92f0ab0e411159cbfbb201673df","modified":1534260613984},{"_id":"public/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片01.png","hash":"dacb86e19945f305013025d44f3e9de09dba359c","modified":1534260613985},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片7.2.png","hash":"02359d570ab5830d815f61d8fe16ab25179c9c89","modified":1534260614056},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/15.png","hash":"9adb17a88751a53d34588418786ee941c412ee1f","modified":1534260614056},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐印象笔记02.PNG","hash":"3cd672901d649f34bbc25e86161d21fe7fd4cb5c","modified":1534260614056},{"_id":"public/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归02.png","hash":"bfa20ad1c15607d24bd562d56286006dad364d9f","modified":1534260614057},{"_id":"public/2018/05/09/机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png","hash":"492fd9787fc36e529f216bf8f8453b4b9e4835d1","modified":1534260614057},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png","hash":"6cb18abfc66ca9a0570bcd2dd2149a9568714ef2","modified":1534260614059},{"_id":"public/2018/03/24/机器学习之线性回归/图片02.png","hash":"bfa20ad1c15607d24bd562d56286006dad364d9f","modified":1534260614060},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片4.1.png","hash":"faa30796bf710258a987c3277c77de1e5af0e709","modified":1534260614148},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.1.png","hash":"6e71a79873cf1b4ce889c6bfd9bfdf7be1ac9009","modified":1534260614148},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.4.png","hash":"36ce82cebff199c501a9cba78fef0169aae62225","modified":1534260614149},{"_id":"public/2018/03/20/智慧考古探测/0003.jpg","hash":"3f58f88dd2cf745248da3f459c1c7f190d24b543","modified":1534260614150},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png","hash":"4d8692aec2e5ac5ead5da05c1833bdef07aa1d6e","modified":1534260614150},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png","hash":"6def4e9f93f518f2210e44d9c7d23aad339ffa56","modified":1534260614151},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png","hash":"29d5e6087b241a8a4eae416a0997028f28b066e2","modified":1534260614152},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png","hash":"11f3b12ef3ea65d97c6a92fffc71d73c094807b4","modified":1534260614152},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png","hash":"840d9444a423bb1e43200f77bf6b2b6b4ef5f200","modified":1534260614153},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png","hash":"4cf2f1a3596e6381019ac52c02a6fb7d78b02ac2","modified":1534260614154},{"_id":"public/2018/03/12/面向知乎的个性化推荐模型研究论文/001.png","hash":"53edfa978f907f2bd291231420d45fca3ac1b8d6","modified":1534260614216},{"_id":"public/2018/03/20/智慧考古探测/0007.jpg","hash":"5cea74dfe1d770886099ce2c39483d4f15dc40e8","modified":1534260614216},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png","hash":"7f105dfa87769b6894feb53226af206130ae2a0a","modified":1534260614217},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png","hash":"c4aa4e10466b8af8ee430875266251cfd8669822","modified":1534260614217},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png","hash":"562f9f4ef0d0b6fe7b1dbdd274ea417385b2e8cb","modified":1534260614218},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0020.jpg","hash":"123e36968e83ae9771bce309da9b353110fa58ad","modified":1534260614218},{"_id":"public/2018/03/24/机器学习知识体系/图片04.png","hash":"078a6395cfe259faaaafc9509152721c309ae446","modified":1534260614243},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.5.png","hash":"bec5459c79cfa6fdabea06979fe34404781e106b","modified":1534260614243},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片7.1.png","hash":"f70709d5d8b754a9fc2869781dc69a935d711de0","modified":1534260614244},{"_id":"public/2018/03/20/智慧考古探测/0009.jpg","hash":"85a9ad5b6ff553d64cf162e586fb5b9ffb7f256b","modified":1534260614245},{"_id":"public/2018/03/20/智慧考古探测/0015.jpg","hash":"2cbcad1299dd6ae2c2505875917b1de196a02755","modified":1534260614246},{"_id":"public/2018/03/20/智慧考古探测/0016.jpg","hash":"7ea9642d9b54ee928aaf5d15496eca682880be1e","modified":1534260614247},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png","hash":"d8ed9a84f2cb143da24a448a746eb27a6b948110","modified":1534260614247},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/10.png","hash":"edb7d8ae73e7c28e9dc0af865e00422e93ae9fc4","modified":1534260614248},{"_id":"public/2018/04/22/机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png","hash":"c0dda60bc9bb970d061a4e1abb9c12a301b0978d","modified":1534260614272},{"_id":"public/2018/04/19/机器学习之决策树-C4-5算法/机器学习之决策树图片02.png","hash":"a0bd7baea72ce6eb49b8e443fffa61cb4ce8fafc","modified":1534260614272},{"_id":"public/2018/03/20/智慧考古探测/0008.jpg","hash":"789b815e158d1e9ada5aa6f222cbaa009198a629","modified":1534260614273},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0002.jpg","hash":"49d32a5fe75fa675517c44940a19324f0de0c441","modified":1534260614273},{"_id":"public/2018/03/12/面向知乎的个性化推荐模型研究论文/004.png","hash":"94f7aae6c9ae8fd47214f8ff9ad8feac92276448","modified":1534260614282},{"_id":"public/2018/03/12/面向知乎的个性化推荐模型研究论文/005.png","hash":"c37bcbbdc516e7e33b4f721219c40363071aea5b","modified":1534260614282},{"_id":"public/2018/03/20/智慧考古探测/0006.jpg","hash":"8d74a65a51250ac78e07078d9fb6ca4643b68682","modified":1534260614284},{"_id":"public/2018/03/20/智慧考古探测/0005.jpg","hash":"764d8f349cd5a2474c8087dba6925f2a7eaeb1f4","modified":1534260614285},{"_id":"public/2018/03/20/智慧考古探测/0011.jpg","hash":"c9a208a59ba69aa756c48d22c84a8fd2a1e64f70","modified":1534260614286},{"_id":"public/2018/03/20/智慧考古探测/0013.jpg","hash":"b0bc7bf235ff86634188f89f4f4e200c4065f8db","modified":1534260614286},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0004.jpg","hash":"b410a783bec6c7ee65e7ce063b9ef2f0d761427a","modified":1534260614289},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0013.jpg","hash":"ecec9159ad22504fb9ddcae6edf7000038fc711a","modified":1534260614295},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0012.jpg","hash":"731afc7e20415e9774dbf8be4257657e524e7c3a","modified":1534260614297},{"_id":"public/2018/03/12/面向知乎的个性化推荐模型研究论文/003.png","hash":"9ecc48b3835d9f36b8fbbecac9b733649b744fef","modified":1534260614314},{"_id":"public/2018/03/20/智慧考古探测/0004.jpg","hash":"1c174bb5124eb7eaf72f92db34eeba78403d0adb","modified":1534260614314},{"_id":"public/2018/03/20/智慧考古探测/0010.jpg","hash":"51270e284833d809d5312324146f82fd935b9ab9","modified":1534260614315},{"_id":"public/2018/03/20/智慧考古探测/0012.jpg","hash":"f27e40b8578f7eee1f97f49092a3acb26c835925","modified":1534260614316},{"_id":"public/2018/03/20/智慧考古探测/0014.jpg","hash":"1394e01dda5681718820cfe691dab11d38b6a485","modified":1534260614317},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0005.jpg","hash":"b46f542aeead8244d1689c483441d66185208501","modified":1534260614318},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0008.jpg","hash":"649b3879851afbd86f4658eb57fc6c354370bd9f","modified":1534260614319},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0009.jpg","hash":"6eddd751cf3477d6ac67b97ace061eaaeca80262","modified":1534260614320},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0010.jpg","hash":"fdc63c2404471580c1714a3b8142446a5eb69229","modified":1534260614321},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0011.jpg","hash":"61db61490cd945128829fe67800fdb418c92e9e4","modified":1534260614322},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0015.jpg","hash":"0bbb4ceb50e9dba1a56d9ede984a99fcfb5c33ed","modified":1534260614324},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0018.jpg","hash":"010057b2f0b63cc30f2e69fec0c23004ec053036","modified":1534260614328},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0022.jpg","hash":"3ea127bb6645c9f279d1bc058968a99ab0cd7abb","modified":1534260614329},{"_id":"public/2018/03/12/面向知乎的个性化推荐模型研究论文/002.png","hash":"73e236cab9c64efeb518f507ff9d4afe88d8d4d4","modified":1534260614345},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0001.jpg","hash":"a7b8847ac3621662c5daa3ea2e0447110f410ce8","modified":1534260614345},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0006.jpg","hash":"cedf2684b3b5d011615db8b941663c47f6563a03","modified":1534260614346},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0003.jpg","hash":"b653861339d76b47be9bc4fb8057007ae0b55024","modified":1534260614348},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐印象笔记03.PNG","hash":"49191fb3ab85b29f393948bc235f49a29a4b0aa4","modified":1534260614378},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0007.jpg","hash":"f3d275ca23e14efa92e4b4a8a68c1cca3f43a6e7","modified":1534260614378},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0014.jpg","hash":"bf0b45886e64157a442a0d7edaeeb13b96543c08","modified":1534260614379},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0021.jpg","hash":"649393c704f2d241992d996f7fab4c5987a59ba3","modified":1534260614380},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0023.jpg","hash":"f02dab5ff9e1af103d236cd5255c2cc62cdb77ec","modified":1534260614382},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/14.png","hash":"43f3564c467b1029cf25e46c2749b93e408be3cf","modified":1534260614384},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0016.jpg","hash":"e9b25047b6ee568a5e1dd91318be415580dbcf45","modified":1534260614409},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0017.jpg","hash":"7234297c25b32d9e65db0264de6a73debeceeae0","modified":1534260614466},{"_id":"public/2018/06/25/Out-of-Gas-and-Driving-on-E/0019.jpg","hash":"67c79d1a83769eb82fe4e55585e8f35b02f416d7","modified":1534260614466},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程08.png","hash":"12a6e73e9a4c6a21addd5542ebc96addfd889498","modified":1534260614635},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.6.png","hash":"fdfdc025a22244646018a4105b4ffaa6b3ecdef6","modified":1534260614661},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片3.3.png","hash":"4e466a4caae62c4d477208251f8f9ad259faeebe","modified":1534260614665},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.2.png","hash":"34877fc9266906c05387350a9c9d13b0c8411f92","modified":1534260614665},{"_id":"public/2018/04/30/机器学习之随机森林/机器学习之随机森林图片01.png","hash":"b17a64c76af269ced0330cc80c64bb4f67f637c1","modified":1534260614691},{"_id":"public/2018/06/12/效率软件推荐（一）/软件推荐印象笔记05.PNG","hash":"96e90a370328798086f8033c6a0aa9fe7dc8e49a","modified":1534260614695},{"_id":"public/2018/03/18/Markdown写作教程/图片02.png","hash":"4becc8fdd53dbe2c6e7117270d2297a27abb3c2d","modified":1534260614756}],"Category":[{"name":"Linux","_id":"cjktv5d340004jiz5pebey8p1"},{"name":"教程","_id":"cjktv5d3d000ajiz5q16j4bwx"},{"name":"Python库","_id":"cjktv5d3i000gjiz5rmoxy1sw"},{"name":"比赛","_id":"cjktv5d3q000qjiz531g44954"},{"name":"protobuf","_id":"cjktv5d480018jiz58qcrxvhg"},{"name":"软件","_id":"cjktv5d4f001gjiz5vupefi1h"},{"name":"机器学习","_id":"cjktv5d4j001mjiz5p6oc9nbl"},{"name":"深度学习","_id":"cjktv5d5i0036jiz5ycqzam5h"},{"name":"推荐系统","_id":"cjktv5d5k003ejiz5b4wvc5co"},{"name":"文字识别","_id":"cjktv5d5o003sjiz5gyvxfjwu"},{"name":"算法","_id":"cjktv5de0005wjiz58ksp725c"}],"Data":[],"Page":[{"title":"分类","type":"categories","comments":0,"date":"2018-03-13T15:29:19.000Z","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ntype: \"categories\"\ncomments: false\ndate: 2018-03-13 23:29:19\n---\n","updated":"2018-06-07T17:52:51.000Z","path":"categories/index.html","layout":"page","_id":"cjktv5d2u0000jiz5l7eggziw","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"关于","date":"2018-03-13T13:34:44.000Z","comments":1,"_content":"「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...欢迎关注公众号阅读更多内容。[**个人简历**](http://p66yyzg4i.bkt.clouddn.com/%E7%8E%8B%E6%8C%AF%E6%B5%B7%28Eric%29_C++.pdf)\n\n![公众号](index/公众号.jpg)\n\n","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2018-03-13 21:34:44\ncomments: true\n---\n「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...欢迎关注公众号阅读更多内容。[**个人简历**](http://p66yyzg4i.bkt.clouddn.com/%E7%8E%8B%E6%8C%AF%E6%B5%B7%28Eric%29_C++.pdf)\n\n![公众号](index/公众号.jpg)\n\n","updated":"2018-07-01T08:57:35.075Z","path":"about/index.html","layout":"page","_id":"cjktv5d310002jiz50u95epps","content":"<p>「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…欢迎关注公众号阅读更多内容。<a href=\"http://p66yyzg4i.bkt.clouddn.com/%E7%8E%8B%E6%8C%AF%E6%B5%B7%28Eric%29_C++.pdf\" target=\"_blank\" rel=\"noopener\"><strong>个人简历</strong></a></p>\n<p><img src=\"/about/index/公众号.jpg\" alt=\"公众号\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…欢迎关注公众号阅读更多内容。<a href=\"http://p66yyzg4i.bkt.clouddn.com/%E7%8E%8B%E6%8C%AF%E6%B5%B7%28Eric%29_C++.pdf\" target=\"_blank\" rel=\"noopener\"><strong>个人简历</strong></a></p>\n<p><img src=\"/about/index/公众号.jpg\" alt=\"公众号\"></p>\n"},{"title":"喜欢","date":"2018-05-20T10:00:03.000Z","_content":"\n## 1.书籍\n\n{% stream %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%E7%8E%8B%E9%98%B3%E6%98%8E.png [《知行合一王阳明》](https://book.douban.com/subject/25911978/)%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%82%96%E7%94%B3%E5%85%8B%E7%9A%84%E6%95%91%E8%B5%8E.png [《肖申克的救赎》](https://book.douban.com/subject/1829226/)%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png 《明朝那些事儿》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png  《沉默的大多数》%}\n\n{% figure https://img3.doubanio.com/lpic/s28357056.jpg  《三体 》 %}\n\n{% figure https://img3.doubanio.com/lpic/s1168991.jpg 《阿甘正传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png  《骄傲的印度》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png  《斯里兰卡 》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png  《越禁忌越美丽 》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png  《看见》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png  《瓦尔登湖》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png  《小王子》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png  《围城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png  《挪威的森林》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png  《呼兰河传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png  《边城》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png  《白鹿原》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png  《平凡的世界》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png  《老人与海》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png  《雪国》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png  《千只鹤》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png  [《羊脂球》 ](https://book.douban.com/subject/3144827/) %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png  《解密》 %}\n\n{% figure https://img3.doubanio.com/lpic/s6384944.jpg 《百年孤独》%}\n\n{% figure https://img3.doubanio.com/lpic/s3278363.jpg 《穆斯林的葬礼》%}\n\n{% figure https://img3.doubanio.com/lpic/s3735710.jpg 《岛》%}\n\n{% figure https://img3.doubanio.com/lpic/s11284102.jpg 《霍乱时期的爱情》%}\n\n{% figure https://img3.doubanio.com/lpic/s6509536.jpg 《悟空传》%}\n\n{% figure https://img3.doubanio.com/lpic/s27988606.jpg [《硅谷之火》](https://book.douban.com/subject/26306584/)%}\n\n{% figure https://img3.doubanio.com/lpic/s9034256.jpg [《千年一叹》](https://book.douban.com/subject/6808159/)%}\n\n{% figure https://img1.doubanio.com/lpic/s27226968.jpg 《文化苦旅》%}\n\n{% figure https://img3.doubanio.com/lpic/s1466042.jpg 《狼图腾》%}\n\n{% figure https://img3.doubanio.com/lpic/s27314106.jpg 《生活的艺术》%}\n\n{% figure https://img1.doubanio.com/lpic/s27595957.jpg 《荆棘鸟》%}\n\n{% endstream %}\n\n## 2.电影\n\n{% stream %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png 《凸变英雄Leaf》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png 《妖猫传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png 《暴雪将至》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png 《钢之炼金术师》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png 《十月围城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png 《无间道3》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png 《无间道2》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png 《西西里的美丽传说》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/K.png 《K》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png 《我的英雄学院》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\n《暴裂无声》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\n《南极之恋》%}\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\n《未闻花名》%}\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png 《刻刻》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\n《万物理论》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\n《来自风平浪静的明天》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png 《唐人街探案（二）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png 《捉妖记（二）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg 《疯狂动物城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png 《机器人总动员》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\n《盗梦空间》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\n《摔跤吧！爸爸》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\n《寻梦环游记》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\n《星际穿越》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\n《霸王别姬（1993）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png 《少年派的奇幻漂流》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg 《飞屋环游记》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png 《二十二》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png 《V字仇杀队》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\n《驯龙高手》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\n《让子弹飞》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\n《血战钢锯岭》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\n《超能陆战队》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png 《无敌破坏王》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\n《你的名字》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\n《神偷奶爸》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\n《啪嗒啪嗒》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\n《小羊肖恩》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png 《麦兜当当伴我心》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png 《麦兜我和我妈妈》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png 《哆啦A梦：伴我同行》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png 《西游记之大圣归来》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png 《昆虫总动员》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png  《湄公河行动》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png 《冰雪奇缘》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\n《百鸟朝凤》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png 《魁拔》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\n《老炮儿》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\n《大护法》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\n《喊山》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\n《功夫熊猫》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\n《金蝉脱壳》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\n《绣春刀》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png 《寒战》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\n《人在囧途》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png 《北京遇上西雅图》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png 《诺亚方舟漂流记》%}\n\n{% endstream %}\n\n## 3.音乐\n\n<!-- 胭脂雪-->\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=28660030&auto=1&height=66\"></iframe>\n\n<!-- 红昭愿-->\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=525317068&auto=1&height=66\"></iframe>\n\n","source":"favorite/index.md","raw":"---\ntitle: 喜欢\ndate: 2018-05-20 18:00:03\n---\n\n## 1.书籍\n\n{% stream %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%E7%8E%8B%E9%98%B3%E6%98%8E.png [《知行合一王阳明》](https://book.douban.com/subject/25911978/)%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%82%96%E7%94%B3%E5%85%8B%E7%9A%84%E6%95%91%E8%B5%8E.png [《肖申克的救赎》](https://book.douban.com/subject/1829226/)%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png 《明朝那些事儿》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png  《沉默的大多数》%}\n\n{% figure https://img3.doubanio.com/lpic/s28357056.jpg  《三体 》 %}\n\n{% figure https://img3.doubanio.com/lpic/s1168991.jpg 《阿甘正传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png  《骄傲的印度》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png  《斯里兰卡 》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png  《越禁忌越美丽 》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png  《看见》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png  《瓦尔登湖》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png  《小王子》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png  《围城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png  《挪威的森林》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png  《呼兰河传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png  《边城》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png  《白鹿原》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png  《平凡的世界》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png  《老人与海》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png  《雪国》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png  《千只鹤》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png  [《羊脂球》 ](https://book.douban.com/subject/3144827/) %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png  《解密》 %}\n\n{% figure https://img3.doubanio.com/lpic/s6384944.jpg 《百年孤独》%}\n\n{% figure https://img3.doubanio.com/lpic/s3278363.jpg 《穆斯林的葬礼》%}\n\n{% figure https://img3.doubanio.com/lpic/s3735710.jpg 《岛》%}\n\n{% figure https://img3.doubanio.com/lpic/s11284102.jpg 《霍乱时期的爱情》%}\n\n{% figure https://img3.doubanio.com/lpic/s6509536.jpg 《悟空传》%}\n\n{% figure https://img3.doubanio.com/lpic/s27988606.jpg [《硅谷之火》](https://book.douban.com/subject/26306584/)%}\n\n{% figure https://img3.doubanio.com/lpic/s9034256.jpg [《千年一叹》](https://book.douban.com/subject/6808159/)%}\n\n{% figure https://img1.doubanio.com/lpic/s27226968.jpg 《文化苦旅》%}\n\n{% figure https://img3.doubanio.com/lpic/s1466042.jpg 《狼图腾》%}\n\n{% figure https://img3.doubanio.com/lpic/s27314106.jpg 《生活的艺术》%}\n\n{% figure https://img1.doubanio.com/lpic/s27595957.jpg 《荆棘鸟》%}\n\n{% endstream %}\n\n## 2.电影\n\n{% stream %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png 《凸变英雄Leaf》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png 《妖猫传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png 《暴雪将至》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png 《钢之炼金术师》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png 《十月围城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png 《无间道3》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png 《无间道2》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png 《西西里的美丽传说》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/K.png 《K》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png 《我的英雄学院》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\n《暴裂无声》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\n《南极之恋》%}\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\n《未闻花名》%}\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png 《刻刻》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\n《万物理论》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\n《来自风平浪静的明天》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png 《唐人街探案（二）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png 《捉妖记（二）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg 《疯狂动物城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png 《机器人总动员》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\n《盗梦空间》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\n《摔跤吧！爸爸》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\n《寻梦环游记》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\n《星际穿越》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\n《霸王别姬（1993）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png 《少年派的奇幻漂流》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg 《飞屋环游记》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png 《二十二》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png 《V字仇杀队》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\n《驯龙高手》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\n《让子弹飞》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\n《血战钢锯岭》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\n《超能陆战队》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png 《无敌破坏王》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\n《你的名字》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\n《神偷奶爸》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\n《啪嗒啪嗒》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\n《小羊肖恩》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png 《麦兜当当伴我心》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png 《麦兜我和我妈妈》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png 《哆啦A梦：伴我同行》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png 《西游记之大圣归来》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png 《昆虫总动员》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png  《湄公河行动》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png 《冰雪奇缘》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\n《百鸟朝凤》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png 《魁拔》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\n《老炮儿》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\n《大护法》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\n《喊山》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\n《功夫熊猫》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\n《金蝉脱壳》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\n《绣春刀》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png 《寒战》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\n《人在囧途》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png 《北京遇上西雅图》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png 《诺亚方舟漂流记》%}\n\n{% endstream %}\n\n## 3.音乐\n\n<!-- 胭脂雪-->\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=28660030&auto=1&height=66\"></iframe>\n\n<!-- 红昭愿-->\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=525317068&auto=1&height=66\"></iframe>\n\n","updated":"2018-08-02T06:02:06.589Z","path":"favorite/index.html","comments":1,"layout":"page","_id":"cjktv5d370006jiz51nkja58l","content":"<h2 id=\"1-书籍\"><a href=\"#1-书籍\" class=\"headerlink\" title=\"1.书籍\"></a>1.书籍</h2><script src=\"//cdn.bootcss.com/jquery/2.1.0/jquery.min.js\"></script><script src=\"//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js\"></script><div class=\"hexo-img-stream\"><style type=\"text/css\">.hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}</style>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%E7%8E%8B%E9%98%B3%E6%98%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%E7%8E%8B%E9%98%B3%E6%98%8E.png\"></noscript><figcaption><a href=\"https://book.douban.com/subject/25911978/\" target=\"_blank\" rel=\"noopener\">《知行合一王阳明》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%82%96%E7%94%B3%E5%85%8B%E7%9A%84%E6%95%91%E8%B5%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%82%96%E7%94%B3%E5%85%8B%E7%9A%84%E6%95%91%E8%B5%8E.png\"></noscript><figcaption><a href=\"https://book.douban.com/subject/1829226/\" target=\"_blank\" rel=\"noopener\">《肖申克的救赎》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png\"></noscript><figcaption>《明朝那些事儿》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png\"></noscript><figcaption>《沉默的大多数》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s28357056.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s28357056.jpg\"></noscript><figcaption>《三体 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s1168991.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s1168991.jpg\"></noscript><figcaption>《阿甘正传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png\"></noscript><figcaption>《骄傲的印度》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png\"></noscript><figcaption>《斯里兰卡 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png\"></noscript><figcaption>《越禁忌越美丽 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png\"></noscript><figcaption>《看见》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png\"></noscript><figcaption>《瓦尔登湖》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png\"></noscript><figcaption>《小王子》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png\"></noscript><figcaption>《围城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png\"></noscript><figcaption>《挪威的森林》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png\"></noscript><figcaption>《呼兰河传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png\"></noscript><figcaption>《边城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png\"></noscript><figcaption>《白鹿原》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png\"></noscript><figcaption>《平凡的世界》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png\"></noscript><figcaption>《老人与海》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png\"></noscript><figcaption>《雪国》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png\"></noscript><figcaption>《千只鹤》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png\"></noscript><figcaption><a href=\"https://book.douban.com/subject/3144827/\" target=\"_blank\" rel=\"noopener\">《羊脂球》 </a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png\"></noscript><figcaption>《解密》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s6384944.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s6384944.jpg\"></noscript><figcaption>《百年孤独》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s3278363.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s3278363.jpg\"></noscript><figcaption>《穆斯林的葬礼》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s3735710.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s3735710.jpg\"></noscript><figcaption>《岛》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s11284102.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s11284102.jpg\"></noscript><figcaption>《霍乱时期的爱情》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s6509536.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s6509536.jpg\"></noscript><figcaption>《悟空传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s27988606.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s27988606.jpg\"></noscript><figcaption><a href=\"https://book.douban.com/subject/26306584/\" target=\"_blank\" rel=\"noopener\">《硅谷之火》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s9034256.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s9034256.jpg\"></noscript><figcaption><a href=\"https://book.douban.com/subject/6808159/\" target=\"_blank\" rel=\"noopener\">《千年一叹》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img1.doubanio.com/lpic/s27226968.jpg\"><noscript><img src=\"https://img1.doubanio.com/lpic/s27226968.jpg\"></noscript><figcaption>《文化苦旅》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s1466042.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s1466042.jpg\"></noscript><figcaption>《狼图腾》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s27314106.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s27314106.jpg\"></noscript><figcaption>《生活的艺术》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img1.doubanio.com/lpic/s27595957.jpg\"><noscript><img src=\"https://img1.doubanio.com/lpic/s27595957.jpg\"></noscript><figcaption>《荆棘鸟》\n</figcaption></figure>\n</div><script type=\"text/javascript\">$('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });</script>\n<h2 id=\"2-电影\"><a href=\"#2-电影\" class=\"headerlink\" title=\"2.电影\"></a>2.电影</h2><script src=\"//cdn.bootcss.com/jquery/2.1.0/jquery.min.js\"></script><script src=\"//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js\"></script><div class=\"hexo-img-stream\"><style type=\"text/css\">.hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}</style>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png\"></noscript><figcaption>《凸变英雄Leaf》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png\"></noscript><figcaption>《妖猫传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png\"></noscript><figcaption>《暴雪将至》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png\"></noscript><figcaption>《钢之炼金术师》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png\"></noscript><figcaption>《十月围城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png\"></noscript><figcaption>《无间道3》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png\"></noscript><figcaption>《无间道2》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png\"></noscript><figcaption>《西西里的美丽传说》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/K.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/K.png\"></noscript><figcaption>《K》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png\"></noscript><figcaption>《我的英雄学院》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\"></noscript><figcaption>《暴裂无声》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\"></noscript><figcaption>《南极之恋》\n</figcaption></figure>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\"></noscript><figcaption>《未闻花名》\n</figcaption></figure>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png\"></noscript><figcaption>《刻刻》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\"></noscript><figcaption>《万物理论》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\"></noscript><figcaption>《来自风平浪静的明天》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png\"></noscript><figcaption>《唐人街探案（二）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png\"></noscript><figcaption>《捉妖记（二）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg\"></noscript><figcaption>《疯狂动物城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png\"></noscript><figcaption>《机器人总动员》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\"></noscript><figcaption>《盗梦空间》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\"></noscript><figcaption>《摔跤吧！爸爸》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\"></noscript><figcaption>《寻梦环游记》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\"></noscript><figcaption>《星际穿越》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\"></noscript><figcaption>《霸王别姬（1993）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png\"></noscript><figcaption>《少年派的奇幻漂流》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg\"></noscript><figcaption>《飞屋环游记》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png\"></noscript><figcaption>《二十二》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png\"></noscript><figcaption>《V字仇杀队》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\"></noscript><figcaption>《驯龙高手》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\"></noscript><figcaption>《让子弹飞》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\"></noscript><figcaption>《血战钢锯岭》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\"></noscript><figcaption>《超能陆战队》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png\"></noscript><figcaption>《无敌破坏王》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\"></noscript><figcaption>《你的名字》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\"></noscript><figcaption>《神偷奶爸》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\"></noscript><figcaption>《啪嗒啪嗒》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\"></noscript><figcaption>《小羊肖恩》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png\"></noscript><figcaption>《麦兜当当伴我心》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png\"></noscript><figcaption>《麦兜我和我妈妈》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png\"></noscript><figcaption>《哆啦A梦：伴我同行》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png\"></noscript><figcaption>《西游记之大圣归来》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png\"></noscript><figcaption>《昆虫总动员》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png\"></noscript><figcaption>《湄公河行动》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png\"></noscript><figcaption>《冰雪奇缘》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\"></noscript><figcaption>《百鸟朝凤》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png\"></noscript><figcaption>《魁拔》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\"></noscript><figcaption>《老炮儿》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\"></noscript><figcaption>《大护法》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\"></noscript><figcaption>《喊山》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\"></noscript><figcaption>《功夫熊猫》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\"></noscript><figcaption>《金蝉脱壳》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\"></noscript><figcaption>《绣春刀》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png\"></noscript><figcaption>《寒战》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\"></noscript><figcaption>《人在囧途》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png\"></noscript><figcaption>《北京遇上西雅图》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png\"></noscript><figcaption>《诺亚方舟漂流记》\n</figcaption></figure>\n</div><script type=\"text/javascript\">$('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });</script>\n<h2 id=\"3-音乐\"><a href=\"#3-音乐\" class=\"headerlink\" title=\"3.音乐\"></a>3.音乐</h2><!-- 胭脂雪-->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=28660030&auto=1&height=66\"></iframe>\n\n<!-- 红昭愿-->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=525317068&auto=1&height=66\"></iframe>\n\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-书籍\"><a href=\"#1-书籍\" class=\"headerlink\" title=\"1.书籍\"></a>1.书籍</h2><script src=\"//cdn.bootcss.com/jquery/2.1.0/jquery.min.js\"></script><script src=\"//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js\"></script><div class=\"hexo-img-stream\"><style type=\"text/css\">.hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}</style>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%E7%8E%8B%E9%98%B3%E6%98%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%E7%8E%8B%E9%98%B3%E6%98%8E.png\"></noscript><figcaption><a href=\"https://book.douban.com/subject/25911978/\" target=\"_blank\" rel=\"noopener\">《知行合一王阳明》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%82%96%E7%94%B3%E5%85%8B%E7%9A%84%E6%95%91%E8%B5%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%82%96%E7%94%B3%E5%85%8B%E7%9A%84%E6%95%91%E8%B5%8E.png\"></noscript><figcaption><a href=\"https://book.douban.com/subject/1829226/\" target=\"_blank\" rel=\"noopener\">《肖申克的救赎》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png\"></noscript><figcaption>《明朝那些事儿》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png\"></noscript><figcaption>《沉默的大多数》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s28357056.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s28357056.jpg\"></noscript><figcaption>《三体 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s1168991.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s1168991.jpg\"></noscript><figcaption>《阿甘正传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png\"></noscript><figcaption>《骄傲的印度》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png\"></noscript><figcaption>《斯里兰卡 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png\"></noscript><figcaption>《越禁忌越美丽 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png\"></noscript><figcaption>《看见》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png\"></noscript><figcaption>《瓦尔登湖》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png\"></noscript><figcaption>《小王子》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png\"></noscript><figcaption>《围城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png\"></noscript><figcaption>《挪威的森林》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png\"></noscript><figcaption>《呼兰河传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png\"></noscript><figcaption>《边城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png\"></noscript><figcaption>《白鹿原》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png\"></noscript><figcaption>《平凡的世界》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png\"></noscript><figcaption>《老人与海》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png\"></noscript><figcaption>《雪国》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png\"></noscript><figcaption>《千只鹤》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png\"></noscript><figcaption><a href=\"https://book.douban.com/subject/3144827/\" target=\"_blank\" rel=\"noopener\">《羊脂球》 </a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png\"></noscript><figcaption>《解密》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s6384944.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s6384944.jpg\"></noscript><figcaption>《百年孤独》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s3278363.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s3278363.jpg\"></noscript><figcaption>《穆斯林的葬礼》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s3735710.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s3735710.jpg\"></noscript><figcaption>《岛》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s11284102.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s11284102.jpg\"></noscript><figcaption>《霍乱时期的爱情》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s6509536.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s6509536.jpg\"></noscript><figcaption>《悟空传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s27988606.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s27988606.jpg\"></noscript><figcaption><a href=\"https://book.douban.com/subject/26306584/\" target=\"_blank\" rel=\"noopener\">《硅谷之火》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s9034256.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s9034256.jpg\"></noscript><figcaption><a href=\"https://book.douban.com/subject/6808159/\" target=\"_blank\" rel=\"noopener\">《千年一叹》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img1.doubanio.com/lpic/s27226968.jpg\"><noscript><img src=\"https://img1.doubanio.com/lpic/s27226968.jpg\"></noscript><figcaption>《文化苦旅》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s1466042.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s1466042.jpg\"></noscript><figcaption>《狼图腾》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s27314106.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s27314106.jpg\"></noscript><figcaption>《生活的艺术》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img1.doubanio.com/lpic/s27595957.jpg\"><noscript><img src=\"https://img1.doubanio.com/lpic/s27595957.jpg\"></noscript><figcaption>《荆棘鸟》\n</figcaption></figure>\n</div><script type=\"text/javascript\">$('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });</script>\n<h2 id=\"2-电影\"><a href=\"#2-电影\" class=\"headerlink\" title=\"2.电影\"></a>2.电影</h2><script src=\"//cdn.bootcss.com/jquery/2.1.0/jquery.min.js\"></script><script src=\"//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js\"></script><div class=\"hexo-img-stream\"><style type=\"text/css\">.hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}</style>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png\"></noscript><figcaption>《凸变英雄Leaf》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png\"></noscript><figcaption>《妖猫传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png\"></noscript><figcaption>《暴雪将至》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png\"></noscript><figcaption>《钢之炼金术师》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png\"></noscript><figcaption>《十月围城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png\"></noscript><figcaption>《无间道3》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png\"></noscript><figcaption>《无间道2》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png\"></noscript><figcaption>《西西里的美丽传说》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/K.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/K.png\"></noscript><figcaption>《K》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png\"></noscript><figcaption>《我的英雄学院》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\"></noscript><figcaption>《暴裂无声》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\"></noscript><figcaption>《南极之恋》\n</figcaption></figure>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\"></noscript><figcaption>《未闻花名》\n</figcaption></figure>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png\"></noscript><figcaption>《刻刻》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\"></noscript><figcaption>《万物理论》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\"></noscript><figcaption>《来自风平浪静的明天》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png\"></noscript><figcaption>《唐人街探案（二）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png\"></noscript><figcaption>《捉妖记（二）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg\"></noscript><figcaption>《疯狂动物城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png\"></noscript><figcaption>《机器人总动员》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\"></noscript><figcaption>《盗梦空间》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\"></noscript><figcaption>《摔跤吧！爸爸》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\"></noscript><figcaption>《寻梦环游记》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\"></noscript><figcaption>《星际穿越》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\"></noscript><figcaption>《霸王别姬（1993）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png\"></noscript><figcaption>《少年派的奇幻漂流》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg\"></noscript><figcaption>《飞屋环游记》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png\"></noscript><figcaption>《二十二》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png\"></noscript><figcaption>《V字仇杀队》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\"></noscript><figcaption>《驯龙高手》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\"></noscript><figcaption>《让子弹飞》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\"></noscript><figcaption>《血战钢锯岭》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\"></noscript><figcaption>《超能陆战队》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png\"></noscript><figcaption>《无敌破坏王》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\"></noscript><figcaption>《你的名字》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\"></noscript><figcaption>《神偷奶爸》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\"></noscript><figcaption>《啪嗒啪嗒》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\"></noscript><figcaption>《小羊肖恩》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png\"></noscript><figcaption>《麦兜当当伴我心》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png\"></noscript><figcaption>《麦兜我和我妈妈》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png\"></noscript><figcaption>《哆啦A梦：伴我同行》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png\"></noscript><figcaption>《西游记之大圣归来》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png\"></noscript><figcaption>《昆虫总动员》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png\"></noscript><figcaption>《湄公河行动》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png\"></noscript><figcaption>《冰雪奇缘》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\"></noscript><figcaption>《百鸟朝凤》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png\"></noscript><figcaption>《魁拔》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\"></noscript><figcaption>《老炮儿》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\"></noscript><figcaption>《大护法》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\"></noscript><figcaption>《喊山》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\"></noscript><figcaption>《功夫熊猫》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\"></noscript><figcaption>《金蝉脱壳》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\"></noscript><figcaption>《绣春刀》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png\"></noscript><figcaption>《寒战》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\"></noscript><figcaption>《人在囧途》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png\"></noscript><figcaption>《北京遇上西雅图》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png\"></noscript><figcaption>《诺亚方舟漂流记》\n</figcaption></figure>\n</div><script type=\"text/javascript\">$('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });</script>\n<h2 id=\"3-音乐\"><a href=\"#3-音乐\" class=\"headerlink\" title=\"3.音乐\"></a>3.音乐</h2><!-- 胭脂雪-->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=28660030&auto=1&height=66\"></iframe>\n\n<!-- 红昭愿-->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=525317068&auto=1&height=66\"></iframe>\n\n"},{"title":"标签","type":"tags","comments":0,"date":"2018-03-13T15:29:35.000Z","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ntype: \"tags\"\ncomments: false\ndate: 2018-03-13 23:29:35\n---\n","updated":"2018-06-07T17:52:51.000Z","path":"tags/index.html","layout":"page","_id":"cjktv5ddw005ujiz5vra7m0xo","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Linux常用命令","date":"2018-08-14T03:12:37.000Z","mathjax":true,"_content":"\n### 1.常用指令 \n\n> ls显示文件或目录 \n>\n> ​\t-l列出文件详细信息(list) \n>\n> ​\t-a列出当前目录下所有文件及目录，包含隐藏的a(all) \n>\n> mkdir创建目录 \n>\n> ​\t-p创建目录，若无父目录，则创建p（parent） \n>\n> cd切换目录 \n>\n> touch创建空文件 touch a.txt \n>\n> echo创建带有内容的文件 echo ‘It is a good test’ >b.txt \n>\n> cat查看文件内容 cat a.txt \n>\n> cp拷贝文件 cp b.txt c.txt \n>\n> mv移动或重命名 mv c.txt cc.txt \n>\n> rm删除文件 rm cc.txt \n>\n> ​\t-r递归删除，可删除子目录及文件 \n>\n> ​\t-f强制删除 \n>\n> find在文件系统中搜索某文件 find / -name 'hadoop-streaming' \n>\n> wc统计文本中行数、字数、字符数 wc b.txt \n>\n> grep在文本文件中查找某个字符串 grep ‘good’ b.txt 显示b.txt中包含good的行 \n>\n> rmdir删除空目录 \n>\n> tree树形结构显示目录 tree -a \n>\n> pwd显示当前目录 pwd \n>\n> In创建链接文件 \n>\n> more、less分页显示文本文件内容 more a.txt| less a.txt \n>\n> head、tail显示文件头、尾内容 head a.txt| tail a.txt \n\n### 2.系统管理命令 \n\n> stat显示指定文件的详细信息，比ls更详细 stat a.txt \n>\n> who显示在线登陆用户 \n>\n> whoami显示当前操作用户 \n>\n> hostname显示主机名 \n>\n> uname显示系统信息 \n>\n> top动态显示耗费资源最多的进程信息 \n>\n> ps显示瞬间进程状态 \n>\n> du查看目录大小 du -h /home带有单位显示目录信息 \n>\n> df查看磁盘大小 df -h 带有单位显示磁盘信息 \n>\n> ifconfig查看网络情况 \n>\n> ping测试网络连通 \n>\n> netstat显示网络状态信息 \n>\n> man查看命令 man ping查看ping信息 \n>\n> clear清屏 \n>\n> kill杀死进程 \n\n### 3.打包压缩相关命令 \n\n> tar:打包压缩 \n>\n> ​    -c归档文件 \n>\n> ​    -x压缩文件 \n>\n> ​    -z gzip压缩文件 \n>\n> ​    -j bzip2压缩文件 \n>\n> ​    -v显示压缩或解压缩过程v(view) \n>\n> ​    -f使用档名 \n>\n> tar -cvf Test.tar Test只打包，不压缩 \n>\n> tar -zcvf Test.tar.gz Test打包并使用gzip压缩 \n>\n> tar -jcvf Test.tar.bz2 Test打包并使用bzip2压缩 \n>\n> 解压缩只需要将上述c替换成x便可以 \n\n### 4.关机/重启机器 \n\n> shutdown \n>\n> ​    -r关机重启 \n>\n> ​    -h关机不重启 \n>\n> ​    Now立刻关机 \n>\n> halt关机 \n>\n> reboot重启 \n\n### 5.Vim简单实用\n\n> vim使用 \n>\n> vim三种模式：命令模式、插入模式、编辑模式。使用ESC或i来切换模式。 \n>\n> 命令模式如下 \n>\n> :q退出 \n>\n> :q！强制退出 \n>\n> :wq保存并退出 \n>\n> :set number显示行号 \n>\n> :set nonnumber隐藏行号 \n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Linux常用命令/推广.png)","source":"_posts/Linux常用命令.md","raw":"---\ntitle: Linux常用命令\ndate: 2018-08-14 11:12:37\ntags: [Linux]\ncategories: Linux\nmathjax: true\n---\n\n### 1.常用指令 \n\n> ls显示文件或目录 \n>\n> ​\t-l列出文件详细信息(list) \n>\n> ​\t-a列出当前目录下所有文件及目录，包含隐藏的a(all) \n>\n> mkdir创建目录 \n>\n> ​\t-p创建目录，若无父目录，则创建p（parent） \n>\n> cd切换目录 \n>\n> touch创建空文件 touch a.txt \n>\n> echo创建带有内容的文件 echo ‘It is a good test’ >b.txt \n>\n> cat查看文件内容 cat a.txt \n>\n> cp拷贝文件 cp b.txt c.txt \n>\n> mv移动或重命名 mv c.txt cc.txt \n>\n> rm删除文件 rm cc.txt \n>\n> ​\t-r递归删除，可删除子目录及文件 \n>\n> ​\t-f强制删除 \n>\n> find在文件系统中搜索某文件 find / -name 'hadoop-streaming' \n>\n> wc统计文本中行数、字数、字符数 wc b.txt \n>\n> grep在文本文件中查找某个字符串 grep ‘good’ b.txt 显示b.txt中包含good的行 \n>\n> rmdir删除空目录 \n>\n> tree树形结构显示目录 tree -a \n>\n> pwd显示当前目录 pwd \n>\n> In创建链接文件 \n>\n> more、less分页显示文本文件内容 more a.txt| less a.txt \n>\n> head、tail显示文件头、尾内容 head a.txt| tail a.txt \n\n### 2.系统管理命令 \n\n> stat显示指定文件的详细信息，比ls更详细 stat a.txt \n>\n> who显示在线登陆用户 \n>\n> whoami显示当前操作用户 \n>\n> hostname显示主机名 \n>\n> uname显示系统信息 \n>\n> top动态显示耗费资源最多的进程信息 \n>\n> ps显示瞬间进程状态 \n>\n> du查看目录大小 du -h /home带有单位显示目录信息 \n>\n> df查看磁盘大小 df -h 带有单位显示磁盘信息 \n>\n> ifconfig查看网络情况 \n>\n> ping测试网络连通 \n>\n> netstat显示网络状态信息 \n>\n> man查看命令 man ping查看ping信息 \n>\n> clear清屏 \n>\n> kill杀死进程 \n\n### 3.打包压缩相关命令 \n\n> tar:打包压缩 \n>\n> ​    -c归档文件 \n>\n> ​    -x压缩文件 \n>\n> ​    -z gzip压缩文件 \n>\n> ​    -j bzip2压缩文件 \n>\n> ​    -v显示压缩或解压缩过程v(view) \n>\n> ​    -f使用档名 \n>\n> tar -cvf Test.tar Test只打包，不压缩 \n>\n> tar -zcvf Test.tar.gz Test打包并使用gzip压缩 \n>\n> tar -jcvf Test.tar.bz2 Test打包并使用bzip2压缩 \n>\n> 解压缩只需要将上述c替换成x便可以 \n\n### 4.关机/重启机器 \n\n> shutdown \n>\n> ​    -r关机重启 \n>\n> ​    -h关机不重启 \n>\n> ​    Now立刻关机 \n>\n> halt关机 \n>\n> reboot重启 \n\n### 5.Vim简单实用\n\n> vim使用 \n>\n> vim三种模式：命令模式、插入模式、编辑模式。使用ESC或i来切换模式。 \n>\n> 命令模式如下 \n>\n> :q退出 \n>\n> :q！强制退出 \n>\n> :wq保存并退出 \n>\n> :set number显示行号 \n>\n> :set nonnumber隐藏行号 \n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Linux常用命令/推广.png)","slug":"Linux常用命令","published":1,"updated":"2018-08-14T03:23:49.153Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjktv5d2x0001jiz5qil4o1sm","content":"<h3 id=\"1-常用指令\"><a href=\"#1-常用指令\" class=\"headerlink\" title=\"1.常用指令\"></a>1.常用指令</h3><blockquote>\n<p>ls显示文件或目录 </p>\n<p>​    -l列出文件详细信息(list) </p>\n<p>​    -a列出当前目录下所有文件及目录，包含隐藏的a(all) </p>\n<p>mkdir创建目录 </p>\n<p>​    -p创建目录，若无父目录，则创建p（parent） </p>\n<p>cd切换目录 </p>\n<p>touch创建空文件 touch a.txt </p>\n<p>echo创建带有内容的文件 echo ‘It is a good test’ &gt;b.txt </p>\n<p>cat查看文件内容 cat a.txt </p>\n<p>cp拷贝文件 cp b.txt c.txt </p>\n<p>mv移动或重命名 mv c.txt cc.txt </p>\n<p>rm删除文件 rm cc.txt </p>\n<p>​    -r递归删除，可删除子目录及文件 </p>\n<p>​    -f强制删除 </p>\n<p>find在文件系统中搜索某文件 find / -name ‘hadoop-streaming’ </p>\n<p>wc统计文本中行数、字数、字符数 wc b.txt </p>\n<p>grep在文本文件中查找某个字符串 grep ‘good’ b.txt 显示b.txt中包含good的行 </p>\n<p>rmdir删除空目录 </p>\n<p>tree树形结构显示目录 tree -a </p>\n<p>pwd显示当前目录 pwd </p>\n<p>In创建链接文件 </p>\n<p>more、less分页显示文本文件内容 more a.txt| less a.txt </p>\n<p>head、tail显示文件头、尾内容 head a.txt| tail a.txt </p>\n</blockquote>\n<h3 id=\"2-系统管理命令\"><a href=\"#2-系统管理命令\" class=\"headerlink\" title=\"2.系统管理命令\"></a>2.系统管理命令</h3><blockquote>\n<p>stat显示指定文件的详细信息，比ls更详细 stat a.txt </p>\n<p>who显示在线登陆用户 </p>\n<p>whoami显示当前操作用户 </p>\n<p>hostname显示主机名 </p>\n<p>uname显示系统信息 </p>\n<p>top动态显示耗费资源最多的进程信息 </p>\n<p>ps显示瞬间进程状态 </p>\n<p>du查看目录大小 du -h /home带有单位显示目录信息 </p>\n<p>df查看磁盘大小 df -h 带有单位显示磁盘信息 </p>\n<p>ifconfig查看网络情况 </p>\n<p>ping测试网络连通 </p>\n<p>netstat显示网络状态信息 </p>\n<p>man查看命令 man ping查看ping信息 </p>\n<p>clear清屏 </p>\n<p>kill杀死进程 </p>\n</blockquote>\n<h3 id=\"3-打包压缩相关命令\"><a href=\"#3-打包压缩相关命令\" class=\"headerlink\" title=\"3.打包压缩相关命令\"></a>3.打包压缩相关命令</h3><blockquote>\n<p>tar:打包压缩 </p>\n<p>​    -c归档文件 </p>\n<p>​    -x压缩文件 </p>\n<p>​    -z gzip压缩文件 </p>\n<p>​    -j bzip2压缩文件 </p>\n<p>​    -v显示压缩或解压缩过程v(view) </p>\n<p>​    -f使用档名 </p>\n<p>tar -cvf Test.tar Test只打包，不压缩 </p>\n<p>tar -zcvf Test.tar.gz Test打包并使用gzip压缩 </p>\n<p>tar -jcvf Test.tar.bz2 Test打包并使用bzip2压缩 </p>\n<p>解压缩只需要将上述c替换成x便可以 </p>\n</blockquote>\n<h3 id=\"4-关机-重启机器\"><a href=\"#4-关机-重启机器\" class=\"headerlink\" title=\"4.关机/重启机器\"></a>4.关机/重启机器</h3><blockquote>\n<p>shutdown </p>\n<p>​    -r关机重启 </p>\n<p>​    -h关机不重启 </p>\n<p>​    Now立刻关机 </p>\n<p>halt关机 </p>\n<p>reboot重启 </p>\n</blockquote>\n<h3 id=\"5-Vim简单实用\"><a href=\"#5-Vim简单实用\" class=\"headerlink\" title=\"5.Vim简单实用\"></a>5.Vim简单实用</h3><blockquote>\n<p>vim使用 </p>\n<p>vim三种模式：命令模式、插入模式、编辑模式。使用ESC或i来切换模式。 </p>\n<p>命令模式如下 </p>\n<p>:q退出 </p>\n<p>:q！强制退出 </p>\n<p>:wq保存并退出 </p>\n<p>:set number显示行号 </p>\n<p>:set nonnumber隐藏行号 </p>\n</blockquote>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/08/14/Linux常用命令/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-常用指令\"><a href=\"#1-常用指令\" class=\"headerlink\" title=\"1.常用指令\"></a>1.常用指令</h3><blockquote>\n<p>ls显示文件或目录 </p>\n<p>​    -l列出文件详细信息(list) </p>\n<p>​    -a列出当前目录下所有文件及目录，包含隐藏的a(all) </p>\n<p>mkdir创建目录 </p>\n<p>​    -p创建目录，若无父目录，则创建p（parent） </p>\n<p>cd切换目录 </p>\n<p>touch创建空文件 touch a.txt </p>\n<p>echo创建带有内容的文件 echo ‘It is a good test’ &gt;b.txt </p>\n<p>cat查看文件内容 cat a.txt </p>\n<p>cp拷贝文件 cp b.txt c.txt </p>\n<p>mv移动或重命名 mv c.txt cc.txt </p>\n<p>rm删除文件 rm cc.txt </p>\n<p>​    -r递归删除，可删除子目录及文件 </p>\n<p>​    -f强制删除 </p>\n<p>find在文件系统中搜索某文件 find / -name ‘hadoop-streaming’ </p>\n<p>wc统计文本中行数、字数、字符数 wc b.txt </p>\n<p>grep在文本文件中查找某个字符串 grep ‘good’ b.txt 显示b.txt中包含good的行 </p>\n<p>rmdir删除空目录 </p>\n<p>tree树形结构显示目录 tree -a </p>\n<p>pwd显示当前目录 pwd </p>\n<p>In创建链接文件 </p>\n<p>more、less分页显示文本文件内容 more a.txt| less a.txt </p>\n<p>head、tail显示文件头、尾内容 head a.txt| tail a.txt </p>\n</blockquote>\n<h3 id=\"2-系统管理命令\"><a href=\"#2-系统管理命令\" class=\"headerlink\" title=\"2.系统管理命令\"></a>2.系统管理命令</h3><blockquote>\n<p>stat显示指定文件的详细信息，比ls更详细 stat a.txt </p>\n<p>who显示在线登陆用户 </p>\n<p>whoami显示当前操作用户 </p>\n<p>hostname显示主机名 </p>\n<p>uname显示系统信息 </p>\n<p>top动态显示耗费资源最多的进程信息 </p>\n<p>ps显示瞬间进程状态 </p>\n<p>du查看目录大小 du -h /home带有单位显示目录信息 </p>\n<p>df查看磁盘大小 df -h 带有单位显示磁盘信息 </p>\n<p>ifconfig查看网络情况 </p>\n<p>ping测试网络连通 </p>\n<p>netstat显示网络状态信息 </p>\n<p>man查看命令 man ping查看ping信息 </p>\n<p>clear清屏 </p>\n<p>kill杀死进程 </p>\n</blockquote>\n<h3 id=\"3-打包压缩相关命令\"><a href=\"#3-打包压缩相关命令\" class=\"headerlink\" title=\"3.打包压缩相关命令\"></a>3.打包压缩相关命令</h3><blockquote>\n<p>tar:打包压缩 </p>\n<p>​    -c归档文件 </p>\n<p>​    -x压缩文件 </p>\n<p>​    -z gzip压缩文件 </p>\n<p>​    -j bzip2压缩文件 </p>\n<p>​    -v显示压缩或解压缩过程v(view) </p>\n<p>​    -f使用档名 </p>\n<p>tar -cvf Test.tar Test只打包，不压缩 </p>\n<p>tar -zcvf Test.tar.gz Test打包并使用gzip压缩 </p>\n<p>tar -jcvf Test.tar.bz2 Test打包并使用bzip2压缩 </p>\n<p>解压缩只需要将上述c替换成x便可以 </p>\n</blockquote>\n<h3 id=\"4-关机-重启机器\"><a href=\"#4-关机-重启机器\" class=\"headerlink\" title=\"4.关机/重启机器\"></a>4.关机/重启机器</h3><blockquote>\n<p>shutdown </p>\n<p>​    -r关机重启 </p>\n<p>​    -h关机不重启 </p>\n<p>​    Now立刻关机 </p>\n<p>halt关机 </p>\n<p>reboot重启 </p>\n</blockquote>\n<h3 id=\"5-Vim简单实用\"><a href=\"#5-Vim简单实用\" class=\"headerlink\" title=\"5.Vim简单实用\"></a>5.Vim简单实用</h3><blockquote>\n<p>vim使用 </p>\n<p>vim三种模式：命令模式、插入模式、编辑模式。使用ESC或i来切换模式。 </p>\n<p>命令模式如下 </p>\n<p>:q退出 </p>\n<p>:q！强制退出 </p>\n<p>:wq保存并退出 </p>\n<p>:set number显示行号 </p>\n<p>:set nonnumber隐藏行号 </p>\n</blockquote>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/08/14/Linux常用命令/推广.png\" alt=\"推广\"></p>\n"},{"title":"Mac+Hexo+GitHub博客搭建教程","date":"2018-03-16T03:41:35.000Z","toc":true,"comments":1,"_content":"\n### 1.为什么写博客\n\n以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。\n\n### 2.Mac+Hexo+GitHub博客\n\n现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。\n\n+ Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。\n+ Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。\n\n选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是*百度爬虫无法爬去博客内容*，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。\n\n### 3.博客本地环境搭建\n\n#### 3.1安装Node.js和Git\n\nMac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录*/usr/local/bin*目录下。测试Node.js和npm，出现下述信息则安装成功。\n\n```\nnode -v\nv8.10.0\n```\n\n```\nnpm -v\n5.6.0\n```\n\nGit官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。\n\n> Git --version \n>\n> git version 2.15.0\n\n#### 3.2安装Hexo\n\nNode.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。\n\n```mac\nsudo npm install -g hexo\n```\n\n#### 3.3博客初始化\n\n创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。\n\n```\ncd myblog\n```\n\n执行下述命令初始化本地博客，下载一些列文件。\n\n```\nhexo init\n```\n\n执行下述命令安装npm。\n\n```\nsudo npm install\n```\n\n执行下述命令生成本地html文件并开启服务器，然后通过http://localhost:4000查看本地博客。\n\n```\nhexo g\nhexo s\n```\n\n![图片3.3](Mac+Hexo+GitHub博客搭建教程/图片3.3.png)\n\n### 4.本地博客关联GitHub\n\n#### 4.1本地博客代码上传GitHub\n\n注册并登陆GitHub账号后，新建仓库，名称必须为`user.github.io`，如`weizhixiaoyi.github.io`。\n\n![图片01](Mac+Hexo+GitHub博客搭建教程/图片4.1.png)\n\n终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。\n\n```Vim\nvim _config.yml\n```\n\n打开后至文档最后部分，将deploy配置如下。\n\n```Python\ndeploy:\n  type: git\n  repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git\n  branch: master\n```\n\n其中将repository中`weizhixiaoyi`改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。\n\n```\nhexo g\nhexo d\n```\n\n若执行`hexo g`出错则执行`npm install hexo --save`，若执行`hexo d`出错则执行`npm install hexo-deployer-git --save `。错误修正后再次执行`hexo g`和`hexo d`。\n\n若未关联GitHub，执行`hexo d`时会提示输入GitHub账号用户名和密码，即:\n\n```\nusername for 'https://github.com':\npassword for 'https://github.com':\n```\n\n`hexo d`执行成功后便可通过https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。\n\n#### 4.2添加ssh keys到GitHub\n\n添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。\n\n执行下述命令生成新的ssh key，将`your_email@example.com`改成自己以注册的GitHub邮箱地址。默认会在`~/.ssh/id_rsa.pub`中生成`id_rsa`和`id_rsa.pub`文件。\n\n```\nssh-keygen -t rsa -C \"your_email@exampl\"\t\t\n```\n\nMac下利用`open ~/.ssh  `打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key`路径GitHub->Setting->SSH keys->add SSH key`界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。\n\n此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过`hexo g`和`hexo d`便可更新到GitHub之中，通过https://weizhixiaoyi.github.io访问便可看到更新内容。\n\n### 5.更换Hexo主题\n\n可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。\n\n终端cd到myblog目录下执行如下所示命令。\n\n```\ngit clone https://github.com/iissnan/hexo-theme-next themes/next\n```\n\n将blog目录下_config.yml里的theme的名称`landscape`更改为`next`。\n\n执行如下命令（每次部署文章的步骤）\n\n```\nhexo g  //生成缓存和静态文件\nhexo d  //重新部署到服务器\n```\n\n当本地博客部署到服务器后，网页端无变化时可以采用下述命令。\n\n```\nhexo clean  //清楚缓存文件(db.json)和已生成的静态文件(public)\n```\n\n### 6.配置Hexo-theme-next主题\n\nHexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问[next官网](http://theme-next.iissnan.com/)查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.1.png)\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.2.png)\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.3.png)\n\n#### 6.1增加标签、分类、归档页\n\n首先将next/config.yml文件中将`menu`中`tags` ` catagories` `archive`前面的`#`。例如增加标签页，通过`hexo new page 'tags'`增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为`tags`。利用`hexo g`和`hexo d`将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。\n\n#### 6.2增加喜欢界面\n\n喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。\n\n从GitHub上https://github.com/weizhixiaoyi 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。\n\n```Query\nimage_stream:\n\tjquery: false\n```\n\n在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。\n\n```\nmenu:\n  home: / || home\n  about: /about/ || user\n  favorite: /favorite/ || heart\n  tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive\n```\n\n然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。\n\n```\n{% stream %}\n\n{% figure https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg\n[《万物理论》]（https://movie.douban.com/subject/24815950/）%}\n\n{% endstream %}\n```\n#### 6.3文章阅读统计\n\n文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。\n\n注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。\n\n```\nleancloud_visitors:\n  enable: true \n  app_id: Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz #<app_id>\n  app_key: qJejurdHKM06N75OQedX4SDK #<app_key>\n```\n\n![图片6.4](Mac+Hexo+GitHub博客搭建教程/图片6.4.png)\n\n#### 6.4增加百度统计\n\n百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到`代码获取`。\n\n```javascript\n<script>\nvar _hmt = _hmt || [];\n(function() {\n  var hm = document.createElement(\"script\");\n  hm.src = \"https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645\";\n  var s = document.getElementsByTagName(\"script\")[0]; \n  s.parentNode.insertBefore(hm, s);\n})();\n</script>\n```\n\n将代码中`b54e835b3551fd0696954b3aedf5d645`复制到next主题_config.yml的`baidu_analytics`中。接下来通过`代码安装检查`来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。\n\n![图片6.5](Mac+Hexo+GitHub博客搭建教程/图片6.5.png)\n\n#### 6.4增加评论功能\n\n多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。\n\n![图片6.6](Mac+Hexo+GitHub博客搭建教程/图片6.6.png)进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。`编写文章时应在头部添加comments: true`\n\n### 7.绑定个人域名\n\n现在使用的域名`weizhixiaoyi.github.io`是github提供的二级域名，也可绑定自己的个性域名`weizhixiaoyi.com`。域名是在阿里云购买，年费为55元，也可以在狗爹`https://sg.godaddy.com`购买，购买好域名之后便可以直接解析。\n\n#### 7.1GitHub端\n\n在next主题中source文件夹中创建`CNAME`文件，没有后缀名，然后将个人域名`weizhixiaoyi.com`添加进`CNAME`文件即可，然后通过`hexo g` `hexo d`重新部署网站。\n\n#### 7.2域名解析\n\n如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。\n\n+ 记录类型：CNAME\n+ 主机记录：@\n+ 解析线路：默认\n+ 记录值：weizhixiaoyi.github.io\n\n解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。\n\n### 7.博客SEO优化\n\nSEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。\n\n#### 7.1确认收录情况\n\n在谷歌上搜索`site:weizhixiaoyi.com`，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。\n\n![图片7.1](Mac+Hexo+GitHub博客搭建教程/图片7.1.png)\n\n#### 7.1网站身份验证\n\n验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过`shadowsock`+`搬瓦工`自行搭建。\n\n进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。\n\n```\n{% if theme.google_site_verification %}\n  <meta name=\"google-site-verification\" content=\"E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo\" />\n{% endif %}\n```\n\n然后回到`myblog`文件夹下将_config.yml中google_site_vertification设置为`true`。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用`hexo g`和`hexo d`更新博客内容，至此网站身份验证结束。\n\n#### 7.2添加Sitemap\n\nsitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。\n\n首先安装针对谷歌的插件`npm install hexo-generator-sitemap --save`，然后进入`myblog`文件夹下将`sitemap`设置如下。\n\n```\n# sitemap\nsitemap:\n  path: sitemap.xml\n```\n\n#### 7.3谷歌收录博客\n\n谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过`site:weizhixiaoyi.com`能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。\n\n![图片7.2](Mac+Hexo+GitHub博客搭建教程/图片7.2.png)\n\n另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。\n\n### 8.ToDoList\n\n+ 寻找更好的方法解决百度爬虫无法爬取博客内容的问题\n+ 博客增加转发功能\n\n------\n\n### 9.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Mac+Hexo+GitHub博客搭建教程/推广.png)","source":"_posts/Mac+Hexo+GitHub博客搭建教程.md","raw":"---\ntitle: Mac+Hexo+GitHub博客搭建教程\ndate: 2018-03-16 11:41:35\ntags: [Mac,Hexo,GitHub,博客]\ncategories: 教程\ntoc: true\ncomments: true\n---\n\n### 1.为什么写博客\n\n以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。\n\n### 2.Mac+Hexo+GitHub博客\n\n现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。\n\n+ Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。\n+ Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。\n\n选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是*百度爬虫无法爬去博客内容*，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。\n\n### 3.博客本地环境搭建\n\n#### 3.1安装Node.js和Git\n\nMac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录*/usr/local/bin*目录下。测试Node.js和npm，出现下述信息则安装成功。\n\n```\nnode -v\nv8.10.0\n```\n\n```\nnpm -v\n5.6.0\n```\n\nGit官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。\n\n> Git --version \n>\n> git version 2.15.0\n\n#### 3.2安装Hexo\n\nNode.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。\n\n```mac\nsudo npm install -g hexo\n```\n\n#### 3.3博客初始化\n\n创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。\n\n```\ncd myblog\n```\n\n执行下述命令初始化本地博客，下载一些列文件。\n\n```\nhexo init\n```\n\n执行下述命令安装npm。\n\n```\nsudo npm install\n```\n\n执行下述命令生成本地html文件并开启服务器，然后通过http://localhost:4000查看本地博客。\n\n```\nhexo g\nhexo s\n```\n\n![图片3.3](Mac+Hexo+GitHub博客搭建教程/图片3.3.png)\n\n### 4.本地博客关联GitHub\n\n#### 4.1本地博客代码上传GitHub\n\n注册并登陆GitHub账号后，新建仓库，名称必须为`user.github.io`，如`weizhixiaoyi.github.io`。\n\n![图片01](Mac+Hexo+GitHub博客搭建教程/图片4.1.png)\n\n终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。\n\n```Vim\nvim _config.yml\n```\n\n打开后至文档最后部分，将deploy配置如下。\n\n```Python\ndeploy:\n  type: git\n  repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git\n  branch: master\n```\n\n其中将repository中`weizhixiaoyi`改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。\n\n```\nhexo g\nhexo d\n```\n\n若执行`hexo g`出错则执行`npm install hexo --save`，若执行`hexo d`出错则执行`npm install hexo-deployer-git --save `。错误修正后再次执行`hexo g`和`hexo d`。\n\n若未关联GitHub，执行`hexo d`时会提示输入GitHub账号用户名和密码，即:\n\n```\nusername for 'https://github.com':\npassword for 'https://github.com':\n```\n\n`hexo d`执行成功后便可通过https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。\n\n#### 4.2添加ssh keys到GitHub\n\n添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。\n\n执行下述命令生成新的ssh key，将`your_email@example.com`改成自己以注册的GitHub邮箱地址。默认会在`~/.ssh/id_rsa.pub`中生成`id_rsa`和`id_rsa.pub`文件。\n\n```\nssh-keygen -t rsa -C \"your_email@exampl\"\t\t\n```\n\nMac下利用`open ~/.ssh  `打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key`路径GitHub->Setting->SSH keys->add SSH key`界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。\n\n此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过`hexo g`和`hexo d`便可更新到GitHub之中，通过https://weizhixiaoyi.github.io访问便可看到更新内容。\n\n### 5.更换Hexo主题\n\n可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。\n\n终端cd到myblog目录下执行如下所示命令。\n\n```\ngit clone https://github.com/iissnan/hexo-theme-next themes/next\n```\n\n将blog目录下_config.yml里的theme的名称`landscape`更改为`next`。\n\n执行如下命令（每次部署文章的步骤）\n\n```\nhexo g  //生成缓存和静态文件\nhexo d  //重新部署到服务器\n```\n\n当本地博客部署到服务器后，网页端无变化时可以采用下述命令。\n\n```\nhexo clean  //清楚缓存文件(db.json)和已生成的静态文件(public)\n```\n\n### 6.配置Hexo-theme-next主题\n\nHexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问[next官网](http://theme-next.iissnan.com/)查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.1.png)\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.2.png)\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.3.png)\n\n#### 6.1增加标签、分类、归档页\n\n首先将next/config.yml文件中将`menu`中`tags` ` catagories` `archive`前面的`#`。例如增加标签页，通过`hexo new page 'tags'`增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为`tags`。利用`hexo g`和`hexo d`将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。\n\n#### 6.2增加喜欢界面\n\n喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。\n\n从GitHub上https://github.com/weizhixiaoyi 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。\n\n```Query\nimage_stream:\n\tjquery: false\n```\n\n在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。\n\n```\nmenu:\n  home: / || home\n  about: /about/ || user\n  favorite: /favorite/ || heart\n  tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive\n```\n\n然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。\n\n```\n{% stream %}\n\n{% figure https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg\n[《万物理论》]（https://movie.douban.com/subject/24815950/）%}\n\n{% endstream %}\n```\n#### 6.3文章阅读统计\n\n文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。\n\n注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。\n\n```\nleancloud_visitors:\n  enable: true \n  app_id: Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz #<app_id>\n  app_key: qJejurdHKM06N75OQedX4SDK #<app_key>\n```\n\n![图片6.4](Mac+Hexo+GitHub博客搭建教程/图片6.4.png)\n\n#### 6.4增加百度统计\n\n百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到`代码获取`。\n\n```javascript\n<script>\nvar _hmt = _hmt || [];\n(function() {\n  var hm = document.createElement(\"script\");\n  hm.src = \"https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645\";\n  var s = document.getElementsByTagName(\"script\")[0]; \n  s.parentNode.insertBefore(hm, s);\n})();\n</script>\n```\n\n将代码中`b54e835b3551fd0696954b3aedf5d645`复制到next主题_config.yml的`baidu_analytics`中。接下来通过`代码安装检查`来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。\n\n![图片6.5](Mac+Hexo+GitHub博客搭建教程/图片6.5.png)\n\n#### 6.4增加评论功能\n\n多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。\n\n![图片6.6](Mac+Hexo+GitHub博客搭建教程/图片6.6.png)进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。`编写文章时应在头部添加comments: true`\n\n### 7.绑定个人域名\n\n现在使用的域名`weizhixiaoyi.github.io`是github提供的二级域名，也可绑定自己的个性域名`weizhixiaoyi.com`。域名是在阿里云购买，年费为55元，也可以在狗爹`https://sg.godaddy.com`购买，购买好域名之后便可以直接解析。\n\n#### 7.1GitHub端\n\n在next主题中source文件夹中创建`CNAME`文件，没有后缀名，然后将个人域名`weizhixiaoyi.com`添加进`CNAME`文件即可，然后通过`hexo g` `hexo d`重新部署网站。\n\n#### 7.2域名解析\n\n如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。\n\n+ 记录类型：CNAME\n+ 主机记录：@\n+ 解析线路：默认\n+ 记录值：weizhixiaoyi.github.io\n\n解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。\n\n### 7.博客SEO优化\n\nSEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。\n\n#### 7.1确认收录情况\n\n在谷歌上搜索`site:weizhixiaoyi.com`，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。\n\n![图片7.1](Mac+Hexo+GitHub博客搭建教程/图片7.1.png)\n\n#### 7.1网站身份验证\n\n验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过`shadowsock`+`搬瓦工`自行搭建。\n\n进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。\n\n```\n{% if theme.google_site_verification %}\n  <meta name=\"google-site-verification\" content=\"E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo\" />\n{% endif %}\n```\n\n然后回到`myblog`文件夹下将_config.yml中google_site_vertification设置为`true`。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用`hexo g`和`hexo d`更新博客内容，至此网站身份验证结束。\n\n#### 7.2添加Sitemap\n\nsitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。\n\n首先安装针对谷歌的插件`npm install hexo-generator-sitemap --save`，然后进入`myblog`文件夹下将`sitemap`设置如下。\n\n```\n# sitemap\nsitemap:\n  path: sitemap.xml\n```\n\n#### 7.3谷歌收录博客\n\n谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过`site:weizhixiaoyi.com`能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。\n\n![图片7.2](Mac+Hexo+GitHub博客搭建教程/图片7.2.png)\n\n另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。\n\n### 8.ToDoList\n\n+ 寻找更好的方法解决百度爬虫无法爬取博客内容的问题\n+ 博客增加转发功能\n\n------\n\n### 9.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Mac+Hexo+GitHub博客搭建教程/推广.png)","slug":"Mac+Hexo+GitHub博客搭建教程","published":1,"updated":"2018-06-07T17:52:50.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d310003jiz5jrlbe751","content":"<h3 id=\"1-为什么写博客\"><a href=\"#1-为什么写博客\" class=\"headerlink\" title=\"1.为什么写博客\"></a>1.为什么写博客</h3><p>以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。</p>\n<h3 id=\"2-Mac-Hexo-GitHub博客\"><a href=\"#2-Mac-Hexo-GitHub博客\" class=\"headerlink\" title=\"2.Mac+Hexo+GitHub博客\"></a>2.Mac+Hexo+GitHub博客</h3><p>现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。</p>\n<ul>\n<li>Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。</li>\n<li>Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。</li>\n</ul>\n<p>选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是<em>百度爬虫无法爬去博客内容</em>，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。</p>\n<h3 id=\"3-博客本地环境搭建\"><a href=\"#3-博客本地环境搭建\" class=\"headerlink\" title=\"3.博客本地环境搭建\"></a>3.博客本地环境搭建</h3><h4 id=\"3-1安装Node-js和Git\"><a href=\"#3-1安装Node-js和Git\" class=\"headerlink\" title=\"3.1安装Node.js和Git\"></a>3.1安装Node.js和Git</h4><p>Mac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录<em>/usr/local/bin</em>目录下。测试Node.js和npm，出现下述信息则安装成功。</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">node</span> <span class=\"title\">-v</span></span><br><span class=\"line\">v8.<span class=\"number\">10.0</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-tag\">npm</span> <span class=\"selector-tag\">-v</span></span><br><span class=\"line\">5<span class=\"selector-class\">.6</span><span class=\"selector-class\">.0</span></span><br></pre></td></tr></table></figure>\n<p>Git官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。</p>\n<blockquote>\n<p>Git —version </p>\n<p>git version 2.15.0</p>\n</blockquote>\n<h4 id=\"3-2安装Hexo\"><a href=\"#3-2安装Hexo\" class=\"headerlink\" title=\"3.2安装Hexo\"></a>3.2安装Hexo</h4><p>Node.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm install -g hexo</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-3博客初始化\"><a href=\"#3-3博客初始化\" class=\"headerlink\" title=\"3.3博客初始化\"></a>3.3博客初始化</h4><p>创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> myblog</span><br></pre></td></tr></table></figure>\n<p>执行下述命令初始化本地博客，下载一些列文件。</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo init</span></span><br></pre></td></tr></table></figure>\n<p>执行下述命令安装npm。</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm <span class=\"keyword\">install</span></span><br></pre></td></tr></table></figure>\n<p>执行下述命令生成本地html文件并开启服务器，然后通过<a href=\"http://localhost:4000查看本地博客。\" target=\"_blank\" rel=\"noopener\">http://localhost:4000查看本地博客。</a></p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo g</span></span><br><span class=\"line\"><span class=\"attribute\">hexo s</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片3.3.png\" alt=\"图片3.3\"></p>\n<h3 id=\"4-本地博客关联GitHub\"><a href=\"#4-本地博客关联GitHub\" class=\"headerlink\" title=\"4.本地博客关联GitHub\"></a>4.本地博客关联GitHub</h3><h4 id=\"4-1本地博客代码上传GitHub\"><a href=\"#4-1本地博客代码上传GitHub\" class=\"headerlink\" title=\"4.1本地博客代码上传GitHub\"></a>4.1本地博客代码上传GitHub</h4><p>注册并登陆GitHub账号后，新建仓库，名称必须为<code>user.github.io</code>，如<code>weizhixiaoyi.github.io</code>。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片4.1.png\" alt=\"图片01\"></p>\n<p>终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">vim</span> _config.yml</span><br></pre></td></tr></table></figure>\n<p>打开后至文档最后部分，将deploy配置如下。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure>\n<p>其中将repository中<code>weizhixiaoyi</code>改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo g</span></span><br><span class=\"line\"><span class=\"attribute\">hexo d</span></span><br></pre></td></tr></table></figure>\n<p>若执行<code>hexo g</code>出错则执行<code>npm install hexo --save</code>，若执行<code>hexo d</code>出错则执行<code>npm install hexo-deployer-git --save</code>。错误修正后再次执行<code>hexo g</code>和<code>hexo d</code>。</p>\n<p>若未关联GitHub，执行<code>hexo d</code>时会提示输入GitHub账号用户名和密码，即:</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">username <span class=\"keyword\">for</span> <span class=\"symbol\">'https</span>:<span class=\"comment\">//github.com':</span></span><br><span class=\"line\">password <span class=\"keyword\">for</span> <span class=\"symbol\">'https</span>:<span class=\"comment\">//github.com':</span></span><br></pre></td></tr></table></figure>\n<p><code>hexo d</code>执行成功后便可通过<a href=\"https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。\" target=\"_blank\" rel=\"noopener\">https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。</a></p>\n<h4 id=\"4-2添加ssh-keys到GitHub\"><a href=\"#4-2添加ssh-keys到GitHub\" class=\"headerlink\" title=\"4.2添加ssh keys到GitHub\"></a>4.2添加ssh keys到GitHub</h4><p>添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。</p>\n<p>执行下述命令生成新的ssh key，将<code>your_email@example.com</code>改成自己以注册的GitHub邮箱地址。默认会在<code>~/.ssh/id_rsa.pub</code>中生成<code>id_rsa</code>和<code>id_rsa.pub</code>文件。</p>\n<figure class=\"highlight excel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -<span class=\"built_in\">t</span> rsa -C <span class=\"string\">\"your_email@exampl\"</span></span><br></pre></td></tr></table></figure>\n<p>Mac下利用<code>open ~/.ssh</code>打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key<code>路径GitHub-&gt;Setting-&gt;SSH keys-&gt;add SSH key</code>界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。</p>\n<p>此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过<code>hexo g</code>和<code>hexo d</code>便可更新到GitHub之中，通过<a href=\"https://weizhixiaoyi.github.io访问便可看到更新内容。\" target=\"_blank\" rel=\"noopener\">https://weizhixiaoyi.github.io访问便可看到更新内容。</a></p>\n<h3 id=\"5-更换Hexo主题\"><a href=\"#5-更换Hexo主题\" class=\"headerlink\" title=\"5.更换Hexo主题\"></a>5.更换Hexo主题</h3><p>可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。</p>\n<p>终端cd到myblog目录下执行如下所示命令。</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone http<span class=\"variable\">s:</span>//github.<span class=\"keyword\">com</span>/iissnan/hexo-theme-<span class=\"keyword\">next</span> themes/<span class=\"keyword\">next</span></span><br></pre></td></tr></table></figure>\n<p>将blog目录下_config.yml里的theme的名称<code>landscape</code>更改为<code>next</code>。</p>\n<p>执行如下命令（每次部署文章的步骤）</p>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g  <span class=\"comment\">//生成缓存和静态文件</span></span><br><span class=\"line\">hexo d  <span class=\"comment\">//重新部署到服务器</span></span><br></pre></td></tr></table></figure>\n<p>当本地博客部署到服务器后，网页端无变化时可以采用下述命令。</p>\n<figure class=\"highlight x86asm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean  //清楚缓存文件(<span class=\"built_in\">db</span>.json)和已生成的静态文件(<span class=\"meta\">public</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-配置Hexo-theme-next主题\"><a href=\"#6-配置Hexo-theme-next主题\" class=\"headerlink\" title=\"6.配置Hexo-theme-next主题\"></a>6.配置Hexo-theme-next主题</h3><p>Hexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问<a href=\"http://theme-next.iissnan.com/\" target=\"_blank\" rel=\"noopener\">next官网</a>查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.1.png\" alt=\"图片6.1\"></p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.2.png\" alt=\"图片6.1\"></p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.3.png\" alt=\"图片6.1\"></p>\n<h4 id=\"6-1增加标签、分类、归档页\"><a href=\"#6-1增加标签、分类、归档页\" class=\"headerlink\" title=\"6.1增加标签、分类、归档页\"></a>6.1增加标签、分类、归档页</h4><p>首先将next/config.yml文件中将<code>menu</code>中<code>tags</code> <code>catagories</code> <code>archive</code>前面的<code>#</code>。例如增加标签页，通过<code>hexo new page &#39;tags&#39;</code>增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为<code>tags</code>。利用<code>hexo g</code>和<code>hexo d</code>将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。</p>\n<h4 id=\"6-2增加喜欢界面\"><a href=\"#6-2增加喜欢界面\" class=\"headerlink\" title=\"6.2增加喜欢界面\"></a>6.2增加喜欢界面</h4><p>喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。</p>\n<p>从GitHub上<a href=\"https://github.com/weizhixiaoyi\" target=\"_blank\" rel=\"noopener\">https://github.com/weizhixiaoyi</a> 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">image_stream:</span><br><span class=\"line\">\tjquery: false</span><br></pre></td></tr></table></figure>\n<p>在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。</p>\n<figure class=\"highlight dts\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">menu:</span></span><br><span class=\"line\"><span class=\"symbol\">  home:</span> / || home</span><br><span class=\"line\"><span class=\"symbol\">  about:</span> <span class=\"meta-keyword\">/about/</span> || user</span><br><span class=\"line\"><span class=\"symbol\">  favorite:</span> <span class=\"meta-keyword\">/favorite/</span> || heart</span><br><span class=\"line\"><span class=\"symbol\">  tags:</span> <span class=\"meta-keyword\">/tags/</span> || tags</span><br><span class=\"line\"><span class=\"symbol\">  categories:</span> <span class=\"meta-keyword\">/categories/</span> || th</span><br><span class=\"line\"><span class=\"symbol\">  archives:</span> <span class=\"meta-keyword\">/archives/</span> || archive</span><br></pre></td></tr></table></figure>\n<p>然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">stream</span> %&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">figure</span> https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg</span></span><br><span class=\"line\"><span class=\"template-tag\">[《万物理论》]（https://movie.douban.com/subject/24815950/）%&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">endstream</span> %&#125;</span><span class=\"xml\"></span></span><br></pre></td></tr></table></figure>\n<h4 id=\"6-3文章阅读统计\"><a href=\"#6-3文章阅读统计\" class=\"headerlink\" title=\"6.3文章阅读统计\"></a>6.3文章阅读统计</h4><p>文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。</p>\n<p>注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">leancloud_visitors:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span> </span><br><span class=\"line\"><span class=\"attr\">  app_id:</span> <span class=\"string\">Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz</span> <span class=\"comment\">#&lt;app_id&gt;</span></span><br><span class=\"line\"><span class=\"attr\">  app_key:</span> <span class=\"string\">qJejurdHKM06N75OQedX4SDK</span> <span class=\"comment\">#&lt;app_key&gt;</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.4.png\" alt=\"图片6.4\"></p>\n<h4 id=\"6-4增加百度统计\"><a href=\"#6-4增加百度统计\" class=\"headerlink\" title=\"6.4增加百度统计\"></a>6.4增加百度统计</h4><p>百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到<code>代码获取</code>。</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;script&gt;</span><br><span class=\"line\"><span class=\"keyword\">var</span> _hmt = _hmt || [];</span><br><span class=\"line\">(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> hm = <span class=\"built_in\">document</span>.createElement(<span class=\"string\">\"script\"</span>);</span><br><span class=\"line\">  hm.src = <span class=\"string\">\"https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645\"</span>;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> s = <span class=\"built_in\">document</span>.getElementsByTagName(<span class=\"string\">\"script\"</span>)[<span class=\"number\">0</span>]; </span><br><span class=\"line\">  s.parentNode.insertBefore(hm, s);</span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\">&lt;<span class=\"regexp\">/script&gt;</span></span><br></pre></td></tr></table></figure>\n<p>将代码中<code>b54e835b3551fd0696954b3aedf5d645</code>复制到next主题_config.yml的<code>baidu_analytics</code>中。接下来通过<code>代码安装检查</code>来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.5.png\" alt=\"图片6.5\"></p>\n<h4 id=\"6-4增加评论功能\"><a href=\"#6-4增加评论功能\" class=\"headerlink\" title=\"6.4增加评论功能\"></a>6.4增加评论功能</h4><p>多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.6.png\" alt=\"图片6.6\">进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。<code>编写文章时应在头部添加comments: true</code></p>\n<h3 id=\"7-绑定个人域名\"><a href=\"#7-绑定个人域名\" class=\"headerlink\" title=\"7.绑定个人域名\"></a>7.绑定个人域名</h3><p>现在使用的域名<code>weizhixiaoyi.github.io</code>是github提供的二级域名，也可绑定自己的个性域名<code>weizhixiaoyi.com</code>。域名是在阿里云购买，年费为55元，也可以在狗爹<code>https://sg.godaddy.com</code>购买，购买好域名之后便可以直接解析。</p>\n<h4 id=\"7-1GitHub端\"><a href=\"#7-1GitHub端\" class=\"headerlink\" title=\"7.1GitHub端\"></a>7.1GitHub端</h4><p>在next主题中source文件夹中创建<code>CNAME</code>文件，没有后缀名，然后将个人域名<code>weizhixiaoyi.com</code>添加进<code>CNAME</code>文件即可，然后通过<code>hexo g</code> <code>hexo d</code>重新部署网站。</p>\n<h4 id=\"7-2域名解析\"><a href=\"#7-2域名解析\" class=\"headerlink\" title=\"7.2域名解析\"></a>7.2域名解析</h4><p>如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。</p>\n<ul>\n<li>记录类型：CNAME</li>\n<li>主机记录：@</li>\n<li>解析线路：默认</li>\n<li>记录值：weizhixiaoyi.github.io</li>\n</ul>\n<p>解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。</p>\n<h3 id=\"7-博客SEO优化\"><a href=\"#7-博客SEO优化\" class=\"headerlink\" title=\"7.博客SEO优化\"></a>7.博客SEO优化</h3><p>SEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。</p>\n<h4 id=\"7-1确认收录情况\"><a href=\"#7-1确认收录情况\" class=\"headerlink\" title=\"7.1确认收录情况\"></a>7.1确认收录情况</h4><p>在谷歌上搜索<code>site:weizhixiaoyi.com</code>，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片7.1.png\" alt=\"图片7.1\"></p>\n<h4 id=\"7-1网站身份验证\"><a href=\"#7-1网站身份验证\" class=\"headerlink\" title=\"7.1网站身份验证\"></a>7.1网站身份验证</h4><p>验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过<code>shadowsock</code>+<code>搬瓦工</code>自行搭建。</p>\n<p>进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">if</span></span> theme.google_site_verification %&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\">  <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">\"google-site-verification\"</span> <span class=\"attr\">content</span>=<span class=\"string\">\"E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo\"</span> /&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">endif</span></span> %&#125;</span><span class=\"xml\"></span></span><br></pre></td></tr></table></figure>\n<p>然后回到<code>myblog</code>文件夹下将_config.yml中google_site_vertification设置为<code>true</code>。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用<code>hexo g</code>和<code>hexo d</code>更新博客内容，至此网站身份验证结束。</p>\n<h4 id=\"7-2添加Sitemap\"><a href=\"#7-2添加Sitemap\" class=\"headerlink\" title=\"7.2添加Sitemap\"></a>7.2添加Sitemap</h4><p>sitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。</p>\n<p>首先安装针对谷歌的插件<code>npm install hexo-generator-sitemap --save</code>，然后进入<code>myblog</code>文件夹下将<code>sitemap</code>设置如下。</p>\n<figure class=\"highlight dts\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"># sitemap</span></span><br><span class=\"line\"><span class=\"symbol\">sitemap:</span></span><br><span class=\"line\"><span class=\"symbol\">  path:</span> sitemap.xml</span><br></pre></td></tr></table></figure>\n<h4 id=\"7-3谷歌收录博客\"><a href=\"#7-3谷歌收录博客\" class=\"headerlink\" title=\"7.3谷歌收录博客\"></a>7.3谷歌收录博客</h4><p>谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过<code>site:weizhixiaoyi.com</code>能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片7.2.png\" alt=\"图片7.2\"></p>\n<p>另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。</p>\n<h3 id=\"8-ToDoList\"><a href=\"#8-ToDoList\" class=\"headerlink\" title=\"8.ToDoList\"></a>8.ToDoList</h3><ul>\n<li>寻找更好的方法解决百度爬虫无法爬取博客内容的问题</li>\n<li>博客增加转发功能</li>\n</ul>\n<hr>\n<h3 id=\"9-推广\"><a href=\"#9-推广\" class=\"headerlink\" title=\"9.推广\"></a>9.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-为什么写博客\"><a href=\"#1-为什么写博客\" class=\"headerlink\" title=\"1.为什么写博客\"></a>1.为什么写博客</h3><p>以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。</p>\n<h3 id=\"2-Mac-Hexo-GitHub博客\"><a href=\"#2-Mac-Hexo-GitHub博客\" class=\"headerlink\" title=\"2.Mac+Hexo+GitHub博客\"></a>2.Mac+Hexo+GitHub博客</h3><p>现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。</p>\n<ul>\n<li>Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。</li>\n<li>Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。</li>\n</ul>\n<p>选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是<em>百度爬虫无法爬去博客内容</em>，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。</p>\n<h3 id=\"3-博客本地环境搭建\"><a href=\"#3-博客本地环境搭建\" class=\"headerlink\" title=\"3.博客本地环境搭建\"></a>3.博客本地环境搭建</h3><h4 id=\"3-1安装Node-js和Git\"><a href=\"#3-1安装Node-js和Git\" class=\"headerlink\" title=\"3.1安装Node.js和Git\"></a>3.1安装Node.js和Git</h4><p>Mac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录<em>/usr/local/bin</em>目录下。测试Node.js和npm，出现下述信息则安装成功。</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">node</span> <span class=\"title\">-v</span></span><br><span class=\"line\">v8.<span class=\"number\">10.0</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-tag\">npm</span> <span class=\"selector-tag\">-v</span></span><br><span class=\"line\">5<span class=\"selector-class\">.6</span><span class=\"selector-class\">.0</span></span><br></pre></td></tr></table></figure>\n<p>Git官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。</p>\n<blockquote>\n<p>Git —version </p>\n<p>git version 2.15.0</p>\n</blockquote>\n<h4 id=\"3-2安装Hexo\"><a href=\"#3-2安装Hexo\" class=\"headerlink\" title=\"3.2安装Hexo\"></a>3.2安装Hexo</h4><p>Node.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm install -g hexo</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-3博客初始化\"><a href=\"#3-3博客初始化\" class=\"headerlink\" title=\"3.3博客初始化\"></a>3.3博客初始化</h4><p>创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> myblog</span><br></pre></td></tr></table></figure>\n<p>执行下述命令初始化本地博客，下载一些列文件。</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo init</span></span><br></pre></td></tr></table></figure>\n<p>执行下述命令安装npm。</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm <span class=\"keyword\">install</span></span><br></pre></td></tr></table></figure>\n<p>执行下述命令生成本地html文件并开启服务器，然后通过<a href=\"http://localhost:4000查看本地博客。\" target=\"_blank\" rel=\"noopener\">http://localhost:4000查看本地博客。</a></p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo g</span></span><br><span class=\"line\"><span class=\"attribute\">hexo s</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片3.3.png\" alt=\"图片3.3\"></p>\n<h3 id=\"4-本地博客关联GitHub\"><a href=\"#4-本地博客关联GitHub\" class=\"headerlink\" title=\"4.本地博客关联GitHub\"></a>4.本地博客关联GitHub</h3><h4 id=\"4-1本地博客代码上传GitHub\"><a href=\"#4-1本地博客代码上传GitHub\" class=\"headerlink\" title=\"4.1本地博客代码上传GitHub\"></a>4.1本地博客代码上传GitHub</h4><p>注册并登陆GitHub账号后，新建仓库，名称必须为<code>user.github.io</code>，如<code>weizhixiaoyi.github.io</code>。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片4.1.png\" alt=\"图片01\"></p>\n<p>终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">vim</span> _config.yml</span><br></pre></td></tr></table></figure>\n<p>打开后至文档最后部分，将deploy配置如下。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure>\n<p>其中将repository中<code>weizhixiaoyi</code>改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo g</span></span><br><span class=\"line\"><span class=\"attribute\">hexo d</span></span><br></pre></td></tr></table></figure>\n<p>若执行<code>hexo g</code>出错则执行<code>npm install hexo --save</code>，若执行<code>hexo d</code>出错则执行<code>npm install hexo-deployer-git --save</code>。错误修正后再次执行<code>hexo g</code>和<code>hexo d</code>。</p>\n<p>若未关联GitHub，执行<code>hexo d</code>时会提示输入GitHub账号用户名和密码，即:</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">username <span class=\"keyword\">for</span> <span class=\"symbol\">'https</span>:<span class=\"comment\">//github.com':</span></span><br><span class=\"line\">password <span class=\"keyword\">for</span> <span class=\"symbol\">'https</span>:<span class=\"comment\">//github.com':</span></span><br></pre></td></tr></table></figure>\n<p><code>hexo d</code>执行成功后便可通过<a href=\"https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。\" target=\"_blank\" rel=\"noopener\">https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。</a></p>\n<h4 id=\"4-2添加ssh-keys到GitHub\"><a href=\"#4-2添加ssh-keys到GitHub\" class=\"headerlink\" title=\"4.2添加ssh keys到GitHub\"></a>4.2添加ssh keys到GitHub</h4><p>添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。</p>\n<p>执行下述命令生成新的ssh key，将<code>your_email@example.com</code>改成自己以注册的GitHub邮箱地址。默认会在<code>~/.ssh/id_rsa.pub</code>中生成<code>id_rsa</code>和<code>id_rsa.pub</code>文件。</p>\n<figure class=\"highlight excel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -<span class=\"built_in\">t</span> rsa -C <span class=\"string\">\"your_email@exampl\"</span></span><br></pre></td></tr></table></figure>\n<p>Mac下利用<code>open ~/.ssh</code>打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key<code>路径GitHub-&gt;Setting-&gt;SSH keys-&gt;add SSH key</code>界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。</p>\n<p>此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过<code>hexo g</code>和<code>hexo d</code>便可更新到GitHub之中，通过<a href=\"https://weizhixiaoyi.github.io访问便可看到更新内容。\" target=\"_blank\" rel=\"noopener\">https://weizhixiaoyi.github.io访问便可看到更新内容。</a></p>\n<h3 id=\"5-更换Hexo主题\"><a href=\"#5-更换Hexo主题\" class=\"headerlink\" title=\"5.更换Hexo主题\"></a>5.更换Hexo主题</h3><p>可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。</p>\n<p>终端cd到myblog目录下执行如下所示命令。</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone http<span class=\"variable\">s:</span>//github.<span class=\"keyword\">com</span>/iissnan/hexo-theme-<span class=\"keyword\">next</span> themes/<span class=\"keyword\">next</span></span><br></pre></td></tr></table></figure>\n<p>将blog目录下_config.yml里的theme的名称<code>landscape</code>更改为<code>next</code>。</p>\n<p>执行如下命令（每次部署文章的步骤）</p>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g  <span class=\"comment\">//生成缓存和静态文件</span></span><br><span class=\"line\">hexo d  <span class=\"comment\">//重新部署到服务器</span></span><br></pre></td></tr></table></figure>\n<p>当本地博客部署到服务器后，网页端无变化时可以采用下述命令。</p>\n<figure class=\"highlight x86asm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean  //清楚缓存文件(<span class=\"built_in\">db</span>.json)和已生成的静态文件(<span class=\"meta\">public</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-配置Hexo-theme-next主题\"><a href=\"#6-配置Hexo-theme-next主题\" class=\"headerlink\" title=\"6.配置Hexo-theme-next主题\"></a>6.配置Hexo-theme-next主题</h3><p>Hexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问<a href=\"http://theme-next.iissnan.com/\" target=\"_blank\" rel=\"noopener\">next官网</a>查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.1.png\" alt=\"图片6.1\"></p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.2.png\" alt=\"图片6.1\"></p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.3.png\" alt=\"图片6.1\"></p>\n<h4 id=\"6-1增加标签、分类、归档页\"><a href=\"#6-1增加标签、分类、归档页\" class=\"headerlink\" title=\"6.1增加标签、分类、归档页\"></a>6.1增加标签、分类、归档页</h4><p>首先将next/config.yml文件中将<code>menu</code>中<code>tags</code> <code>catagories</code> <code>archive</code>前面的<code>#</code>。例如增加标签页，通过<code>hexo new page &#39;tags&#39;</code>增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为<code>tags</code>。利用<code>hexo g</code>和<code>hexo d</code>将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。</p>\n<h4 id=\"6-2增加喜欢界面\"><a href=\"#6-2增加喜欢界面\" class=\"headerlink\" title=\"6.2增加喜欢界面\"></a>6.2增加喜欢界面</h4><p>喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。</p>\n<p>从GitHub上<a href=\"https://github.com/weizhixiaoyi\" target=\"_blank\" rel=\"noopener\">https://github.com/weizhixiaoyi</a> 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">image_stream:</span><br><span class=\"line\">\tjquery: false</span><br></pre></td></tr></table></figure>\n<p>在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。</p>\n<figure class=\"highlight dts\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">menu:</span></span><br><span class=\"line\"><span class=\"symbol\">  home:</span> / || home</span><br><span class=\"line\"><span class=\"symbol\">  about:</span> <span class=\"meta-keyword\">/about/</span> || user</span><br><span class=\"line\"><span class=\"symbol\">  favorite:</span> <span class=\"meta-keyword\">/favorite/</span> || heart</span><br><span class=\"line\"><span class=\"symbol\">  tags:</span> <span class=\"meta-keyword\">/tags/</span> || tags</span><br><span class=\"line\"><span class=\"symbol\">  categories:</span> <span class=\"meta-keyword\">/categories/</span> || th</span><br><span class=\"line\"><span class=\"symbol\">  archives:</span> <span class=\"meta-keyword\">/archives/</span> || archive</span><br></pre></td></tr></table></figure>\n<p>然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">stream</span> %&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">figure</span> https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg</span></span><br><span class=\"line\"><span class=\"template-tag\">[《万物理论》]（https://movie.douban.com/subject/24815950/）%&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">endstream</span> %&#125;</span><span class=\"xml\"></span></span><br></pre></td></tr></table></figure>\n<h4 id=\"6-3文章阅读统计\"><a href=\"#6-3文章阅读统计\" class=\"headerlink\" title=\"6.3文章阅读统计\"></a>6.3文章阅读统计</h4><p>文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。</p>\n<p>注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">leancloud_visitors:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span> </span><br><span class=\"line\"><span class=\"attr\">  app_id:</span> <span class=\"string\">Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz</span> <span class=\"comment\">#&lt;app_id&gt;</span></span><br><span class=\"line\"><span class=\"attr\">  app_key:</span> <span class=\"string\">qJejurdHKM06N75OQedX4SDK</span> <span class=\"comment\">#&lt;app_key&gt;</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.4.png\" alt=\"图片6.4\"></p>\n<h4 id=\"6-4增加百度统计\"><a href=\"#6-4增加百度统计\" class=\"headerlink\" title=\"6.4增加百度统计\"></a>6.4增加百度统计</h4><p>百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到<code>代码获取</code>。</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;script&gt;</span><br><span class=\"line\"><span class=\"keyword\">var</span> _hmt = _hmt || [];</span><br><span class=\"line\">(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> hm = <span class=\"built_in\">document</span>.createElement(<span class=\"string\">\"script\"</span>);</span><br><span class=\"line\">  hm.src = <span class=\"string\">\"https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645\"</span>;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> s = <span class=\"built_in\">document</span>.getElementsByTagName(<span class=\"string\">\"script\"</span>)[<span class=\"number\">0</span>]; </span><br><span class=\"line\">  s.parentNode.insertBefore(hm, s);</span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\">&lt;<span class=\"regexp\">/script&gt;</span></span><br></pre></td></tr></table></figure>\n<p>将代码中<code>b54e835b3551fd0696954b3aedf5d645</code>复制到next主题_config.yml的<code>baidu_analytics</code>中。接下来通过<code>代码安装检查</code>来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.5.png\" alt=\"图片6.5\"></p>\n<h4 id=\"6-4增加评论功能\"><a href=\"#6-4增加评论功能\" class=\"headerlink\" title=\"6.4增加评论功能\"></a>6.4增加评论功能</h4><p>多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.6.png\" alt=\"图片6.6\">进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。<code>编写文章时应在头部添加comments: true</code></p>\n<h3 id=\"7-绑定个人域名\"><a href=\"#7-绑定个人域名\" class=\"headerlink\" title=\"7.绑定个人域名\"></a>7.绑定个人域名</h3><p>现在使用的域名<code>weizhixiaoyi.github.io</code>是github提供的二级域名，也可绑定自己的个性域名<code>weizhixiaoyi.com</code>。域名是在阿里云购买，年费为55元，也可以在狗爹<code>https://sg.godaddy.com</code>购买，购买好域名之后便可以直接解析。</p>\n<h4 id=\"7-1GitHub端\"><a href=\"#7-1GitHub端\" class=\"headerlink\" title=\"7.1GitHub端\"></a>7.1GitHub端</h4><p>在next主题中source文件夹中创建<code>CNAME</code>文件，没有后缀名，然后将个人域名<code>weizhixiaoyi.com</code>添加进<code>CNAME</code>文件即可，然后通过<code>hexo g</code> <code>hexo d</code>重新部署网站。</p>\n<h4 id=\"7-2域名解析\"><a href=\"#7-2域名解析\" class=\"headerlink\" title=\"7.2域名解析\"></a>7.2域名解析</h4><p>如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。</p>\n<ul>\n<li>记录类型：CNAME</li>\n<li>主机记录：@</li>\n<li>解析线路：默认</li>\n<li>记录值：weizhixiaoyi.github.io</li>\n</ul>\n<p>解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。</p>\n<h3 id=\"7-博客SEO优化\"><a href=\"#7-博客SEO优化\" class=\"headerlink\" title=\"7.博客SEO优化\"></a>7.博客SEO优化</h3><p>SEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。</p>\n<h4 id=\"7-1确认收录情况\"><a href=\"#7-1确认收录情况\" class=\"headerlink\" title=\"7.1确认收录情况\"></a>7.1确认收录情况</h4><p>在谷歌上搜索<code>site:weizhixiaoyi.com</code>，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片7.1.png\" alt=\"图片7.1\"></p>\n<h4 id=\"7-1网站身份验证\"><a href=\"#7-1网站身份验证\" class=\"headerlink\" title=\"7.1网站身份验证\"></a>7.1网站身份验证</h4><p>验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过<code>shadowsock</code>+<code>搬瓦工</code>自行搭建。</p>\n<p>进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">if</span></span> theme.google_site_verification %&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\">  <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">\"google-site-verification\"</span> <span class=\"attr\">content</span>=<span class=\"string\">\"E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo\"</span> /&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">endif</span></span> %&#125;</span><span class=\"xml\"></span></span><br></pre></td></tr></table></figure>\n<p>然后回到<code>myblog</code>文件夹下将_config.yml中google_site_vertification设置为<code>true</code>。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用<code>hexo g</code>和<code>hexo d</code>更新博客内容，至此网站身份验证结束。</p>\n<h4 id=\"7-2添加Sitemap\"><a href=\"#7-2添加Sitemap\" class=\"headerlink\" title=\"7.2添加Sitemap\"></a>7.2添加Sitemap</h4><p>sitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。</p>\n<p>首先安装针对谷歌的插件<code>npm install hexo-generator-sitemap --save</code>，然后进入<code>myblog</code>文件夹下将<code>sitemap</code>设置如下。</p>\n<figure class=\"highlight dts\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"># sitemap</span></span><br><span class=\"line\"><span class=\"symbol\">sitemap:</span></span><br><span class=\"line\"><span class=\"symbol\">  path:</span> sitemap.xml</span><br></pre></td></tr></table></figure>\n<h4 id=\"7-3谷歌收录博客\"><a href=\"#7-3谷歌收录博客\" class=\"headerlink\" title=\"7.3谷歌收录博客\"></a>7.3谷歌收录博客</h4><p>谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过<code>site:weizhixiaoyi.com</code>能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片7.2.png\" alt=\"图片7.2\"></p>\n<p>另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。</p>\n<h3 id=\"8-ToDoList\"><a href=\"#8-ToDoList\" class=\"headerlink\" title=\"8.ToDoList\"></a>8.ToDoList</h3><ul>\n<li>寻找更好的方法解决百度爬虫无法爬取博客内容的问题</li>\n<li>博客增加转发功能</li>\n</ul>\n<hr>\n<h3 id=\"9-推广\"><a href=\"#9-推广\" class=\"headerlink\" title=\"9.推广\"></a>9.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/16/Mac+Hexo+GitHub博客搭建教程/推广.png\" alt=\"推广\"></p>\n"},{"title":"Markdown写作教程","date":"2018-03-18T15:29:14.000Z","toc":true,"comments":1,"_content":"\n[博客搭建教程](https://weizhixiaoyi.com/2018/03/16/Mac+Hexo+GitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/)写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。\n\n### 1.为什么选择Markdown\n\n首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用**印象笔记**至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。\n\n### 2.标题\n\n标题通过`#`的个数进行区分，Markdown共支持6级标题。\n\n![标题](Markdown写作教程/图片01.png)\n\n### 3.字体设置\n\n#### 3.1粗体\n\n文字前后加`**`来表示粗体。\n\n```markdown\n**粗体**\n```\n\n**粗体**\n\n#### 3.2斜体\n\n文字前后加`*`来表示斜体。\n\n```markdown\n*斜体*\n```\n\n*斜体*\n\n#### 3.3粗斜体\n\n文字前后加`***`来表示粗斜体。\n\n```markdown\n***粗斜体***\n```\n\n***粗斜体***\n\n#### 3.4下划线\n\n文字前后加`<u>` `</u>`来表示下划线。\n\n```markdown\n<u>下滑线</u>\n```\n\n<u>下划线</u>\n\n#### 3.5删除线\n\n文字前后加`~~`来表示删除线。\n\n```markdown\n~~删除线~~\n```\n\n~~删除线~~\n\n#### 3.6标记\n\n文字前后加`` `来表示标记，该符号位于Esc键下面。\n\n```markdown\n`标记`\n```\n\n`标记`\n\n#### 3.7Html标签\n\n```Markdown\n<font face=\"微软雅黑\" color=\"red\" size=\"6\">字体及字体颜色和大小</font>\n```\n\n<font face=\"微软雅黑\" color=\"red\" size=\"3\">字体及字体颜色和大小</font>\n\n### 4.列表\n\n#### 4.1有序列表\n\n采用`1.  ` 后加空格形式表示有序列表。\n\n```markdown\n1. 有序列表1\n2. 有序列表2\n3. 有序列表3\n```\n\n1. 有序列表1\n2. 有序列表2\n3. 有序列表3\n\n#### 4.2无序列表\n\n采用`+` `-` `* ` `=`符号表示无序列表，支持多级嵌套。\n\n```markdown\n+ 有序列表1\n+ + 有序列表1.1\n+ + 有序列表1.2\n+ 有序列表2\n+ 有序列表3\n```\n\n- 无序列表1\n  - 无序列表1.1\n  - 无序列表1.2\n- 无序列表2\n- 无序列表3\n\n#### 4.3未完成列表\n\n采用`- []`表示未完成任务，各符号间均有空格。\n\n```markdown\n- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3\n```\n\n- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3\n\n#### 4.4已完成任务\n\n采用`- [x] `表示已完成任务，各符号间均有空格。同时可直接在未完成任务间`打勾`来转换成已完成任务。\n\n```markdown\n- [x] 已完成任务1\n- [x] 已完成任务2\n- [x] 已完成任务3\n```\n\n- [ ] 已完成任务1\n- [ ] 已完成任务2\n- [ ] 已完成任务3\n\n### 5.表格\n\n表格对齐方式\n\n- 居左：:----\n- 居中：:----:或-----\n- 居由：----:\n\n```markdown\n| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |\n```\n\n| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |\n\n### 6.段落和换行\n\n#### 6.1首行缩进方式\n\n+ `&emsp;`中文空格\n+ `&ensp;`半中文空格\n+ `&nbsp;`英文空格\n+ ` `输入法切换到全角双击空格\n\n#### 6.2换行\n\n+ ` ` ` `换行处连续打两个空格\n+ 换行处使用`<br>`进行换行\n\n#### 6.3空行\n\n+ ` ` ` ` 空行处连续打两个空格\n+ 换行处使用`<br>`进行空行\n\n### 6.引用和代码块\n\n#### 6.1引用\n\n若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。\n\n```Markdown\n> 引用1\n> > 引用1.1\n> > 引用1.2\n> 引用2\n```\n\n> 引用1\n>\n> > 引用1.1\n> >\n> > 引用1.2\n>\n> 引用2\n\n#### 6.2代码块\n\n代码前后添加```` `表示代码块。\n\n```markdown\n​```Python\nprint('代码块')\n​```\n```\n\n```python\nprint（'代码块'）\n```\n\n### 7.链接\n\n#### 7.1图片链接\n\n采用`![]()`来表示图片链接。\n\n```Markdown\n![图片名称](链接地址)\n```\n\n![图片名称](Markdown写作教程/图片02.png)\n\n#### 7.2文字链接\n\n采用`[]()`表示文字链接。\n\n```Markdown\n[链接名称](链接地址)\n```\n\n[文字链接](weizhixiaoyi.com)\n\n#### 7.3参考链接\n\n采用`[ ]: `表示参考链接，注意符号后有空格。\n\n```markdown\n[ ]: url title\n```\n\n[参考链接]: https://weizhixiaoyi.com\t\"谓之小一\"\n\n### 8.分割线\n\n上下文无关时可使用分割符进行分开。\n\n- 连续多个`-`  (>=3)\n- 连续多个`*` （>=3）\n- 连续多个下划线`_` （>=3）\n\n```\n---分割线\n***分割线\n___分割线\n```\n\n------\n\n------\n\n------\n\n### 9.脚注和注释\n\n#### 9.1脚注\n\n采用`[^]:表示脚注，注意空格。\n\n```markdown\n[^]: 脚注\n```\n\n[^]: 脚注\n\n#### 9.2注释\n\n采用`<!---->`表示注释.\n\n```markdown\n<!--注释-->\n```\n\n<!--注释-->\n\n### 11.转义\n\nMarkdown通过反斜杠`\\`来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。\n\n```markdown\n\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号\n```\n\n\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号\n\n### 12.目录\n\n采用`[TOC]`来生成文章目录。\n\n```Markdown\n[TOC]\n```\n\n![图片03](Markdown写作教程/图片03.png)\n\n-----\n\n### 13.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Markdown写作教程/推广.png)\n\n","source":"_posts/Markdown写作教程.md","raw":"---\ntitle: Markdown写作教程\ndate: 2018-03-18 23:29:14\ntags: [Markdown,博客,教程]\ntoc: true\ncomments: true\n---\n\n[博客搭建教程](https://weizhixiaoyi.com/2018/03/16/Mac+Hexo+GitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/)写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。\n\n### 1.为什么选择Markdown\n\n首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用**印象笔记**至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。\n\n### 2.标题\n\n标题通过`#`的个数进行区分，Markdown共支持6级标题。\n\n![标题](Markdown写作教程/图片01.png)\n\n### 3.字体设置\n\n#### 3.1粗体\n\n文字前后加`**`来表示粗体。\n\n```markdown\n**粗体**\n```\n\n**粗体**\n\n#### 3.2斜体\n\n文字前后加`*`来表示斜体。\n\n```markdown\n*斜体*\n```\n\n*斜体*\n\n#### 3.3粗斜体\n\n文字前后加`***`来表示粗斜体。\n\n```markdown\n***粗斜体***\n```\n\n***粗斜体***\n\n#### 3.4下划线\n\n文字前后加`<u>` `</u>`来表示下划线。\n\n```markdown\n<u>下滑线</u>\n```\n\n<u>下划线</u>\n\n#### 3.5删除线\n\n文字前后加`~~`来表示删除线。\n\n```markdown\n~~删除线~~\n```\n\n~~删除线~~\n\n#### 3.6标记\n\n文字前后加`` `来表示标记，该符号位于Esc键下面。\n\n```markdown\n`标记`\n```\n\n`标记`\n\n#### 3.7Html标签\n\n```Markdown\n<font face=\"微软雅黑\" color=\"red\" size=\"6\">字体及字体颜色和大小</font>\n```\n\n<font face=\"微软雅黑\" color=\"red\" size=\"3\">字体及字体颜色和大小</font>\n\n### 4.列表\n\n#### 4.1有序列表\n\n采用`1.  ` 后加空格形式表示有序列表。\n\n```markdown\n1. 有序列表1\n2. 有序列表2\n3. 有序列表3\n```\n\n1. 有序列表1\n2. 有序列表2\n3. 有序列表3\n\n#### 4.2无序列表\n\n采用`+` `-` `* ` `=`符号表示无序列表，支持多级嵌套。\n\n```markdown\n+ 有序列表1\n+ + 有序列表1.1\n+ + 有序列表1.2\n+ 有序列表2\n+ 有序列表3\n```\n\n- 无序列表1\n  - 无序列表1.1\n  - 无序列表1.2\n- 无序列表2\n- 无序列表3\n\n#### 4.3未完成列表\n\n采用`- []`表示未完成任务，各符号间均有空格。\n\n```markdown\n- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3\n```\n\n- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3\n\n#### 4.4已完成任务\n\n采用`- [x] `表示已完成任务，各符号间均有空格。同时可直接在未完成任务间`打勾`来转换成已完成任务。\n\n```markdown\n- [x] 已完成任务1\n- [x] 已完成任务2\n- [x] 已完成任务3\n```\n\n- [ ] 已完成任务1\n- [ ] 已完成任务2\n- [ ] 已完成任务3\n\n### 5.表格\n\n表格对齐方式\n\n- 居左：:----\n- 居中：:----:或-----\n- 居由：----:\n\n```markdown\n| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |\n```\n\n| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |\n\n### 6.段落和换行\n\n#### 6.1首行缩进方式\n\n+ `&emsp;`中文空格\n+ `&ensp;`半中文空格\n+ `&nbsp;`英文空格\n+ ` `输入法切换到全角双击空格\n\n#### 6.2换行\n\n+ ` ` ` `换行处连续打两个空格\n+ 换行处使用`<br>`进行换行\n\n#### 6.3空行\n\n+ ` ` ` ` 空行处连续打两个空格\n+ 换行处使用`<br>`进行空行\n\n### 6.引用和代码块\n\n#### 6.1引用\n\n若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。\n\n```Markdown\n> 引用1\n> > 引用1.1\n> > 引用1.2\n> 引用2\n```\n\n> 引用1\n>\n> > 引用1.1\n> >\n> > 引用1.2\n>\n> 引用2\n\n#### 6.2代码块\n\n代码前后添加```` `表示代码块。\n\n```markdown\n​```Python\nprint('代码块')\n​```\n```\n\n```python\nprint（'代码块'）\n```\n\n### 7.链接\n\n#### 7.1图片链接\n\n采用`![]()`来表示图片链接。\n\n```Markdown\n![图片名称](链接地址)\n```\n\n![图片名称](Markdown写作教程/图片02.png)\n\n#### 7.2文字链接\n\n采用`[]()`表示文字链接。\n\n```Markdown\n[链接名称](链接地址)\n```\n\n[文字链接](weizhixiaoyi.com)\n\n#### 7.3参考链接\n\n采用`[ ]: `表示参考链接，注意符号后有空格。\n\n```markdown\n[ ]: url title\n```\n\n[参考链接]: https://weizhixiaoyi.com\t\"谓之小一\"\n\n### 8.分割线\n\n上下文无关时可使用分割符进行分开。\n\n- 连续多个`-`  (>=3)\n- 连续多个`*` （>=3）\n- 连续多个下划线`_` （>=3）\n\n```\n---分割线\n***分割线\n___分割线\n```\n\n------\n\n------\n\n------\n\n### 9.脚注和注释\n\n#### 9.1脚注\n\n采用`[^]:表示脚注，注意空格。\n\n```markdown\n[^]: 脚注\n```\n\n[^]: 脚注\n\n#### 9.2注释\n\n采用`<!---->`表示注释.\n\n```markdown\n<!--注释-->\n```\n\n<!--注释-->\n\n### 11.转义\n\nMarkdown通过反斜杠`\\`来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。\n\n```markdown\n\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号\n```\n\n\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号\n\n### 12.目录\n\n采用`[TOC]`来生成文章目录。\n\n```Markdown\n[TOC]\n```\n\n![图片03](Markdown写作教程/图片03.png)\n\n-----\n\n### 13.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Markdown写作教程/推广.png)\n\n","slug":"Markdown写作教程","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d380007jiz5unucdms0","content":"<p><a href=\"https://weizhixiaoyi.com/2018/03/16/Mac+Hexo+GitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/\">博客搭建教程</a>写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。</p>\n<h3 id=\"1-为什么选择Markdown\"><a href=\"#1-为什么选择Markdown\" class=\"headerlink\" title=\"1.为什么选择Markdown\"></a>1.为什么选择Markdown</h3><p>首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用<strong>印象笔记</strong>至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。</p>\n<h3 id=\"2-标题\"><a href=\"#2-标题\" class=\"headerlink\" title=\"2.标题\"></a>2.标题</h3><p>标题通过<code>#</code>的个数进行区分，Markdown共支持6级标题。</p>\n<p><img src=\"/2018/03/18/Markdown写作教程/图片01.png\" alt=\"标题\"></p>\n<h3 id=\"3-字体设置\"><a href=\"#3-字体设置\" class=\"headerlink\" title=\"3.字体设置\"></a>3.字体设置</h3><h4 id=\"3-1粗体\"><a href=\"#3-1粗体\" class=\"headerlink\" title=\"3.1粗体\"></a>3.1粗体</h4><p>文字前后加<code>**</code>来表示粗体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">**粗体**</span></span><br></pre></td></tr></table></figure>\n<p><strong>粗体</strong></p>\n<h4 id=\"3-2斜体\"><a href=\"#3-2斜体\" class=\"headerlink\" title=\"3.2斜体\"></a>3.2斜体</h4><p>文字前后加<code>*</code>来表示斜体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"emphasis\">*斜体*</span></span><br></pre></td></tr></table></figure>\n<p><em>斜体</em></p>\n<h4 id=\"3-3粗斜体\"><a href=\"#3-3粗斜体\" class=\"headerlink\" title=\"3.3粗斜体\"></a>3.3粗斜体</h4><p>文字前后加<code>***</code>来表示粗斜体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">***粗斜体**</span>*</span><br></pre></td></tr></table></figure>\n<p><strong><em>粗斜体</em></strong></p>\n<h4 id=\"3-4下划线\"><a href=\"#3-4下划线\" class=\"headerlink\" title=\"3.4下划线\"></a>3.4下划线</h4><p>文字前后加<code>&lt;u&gt;</code> <code>&lt;/u&gt;</code>来表示下划线。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">u</span>&gt;</span></span>下滑线<span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">u</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p><u>下划线</u></p>\n<h4 id=\"3-5删除线\"><a href=\"#3-5删除线\" class=\"headerlink\" title=\"3.5删除线\"></a>3.5删除线</h4><p>文字前后加<code>~~</code>来表示删除线。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~~删除线~~</span><br></pre></td></tr></table></figure>\n<p><del>删除线</del></p>\n<h4 id=\"3-6标记\"><a href=\"#3-6标记\" class=\"headerlink\" title=\"3.6标记\"></a>3.6标记</h4><p>文字前后加<code>` </code>来表示标记，该符号位于Esc键下面。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"code\">`标记`</span></span><br></pre></td></tr></table></figure>\n<p><code>标记</code></p>\n<h4 id=\"3-7Html标签\"><a href=\"#3-7Html标签\" class=\"headerlink\" title=\"3.7Html标签\"></a>3.7Html标签</h4><figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">font</span> <span class=\"attr\">face</span>=<span class=\"string\">\"微软雅黑\"</span> <span class=\"attr\">color</span>=<span class=\"string\">\"red\"</span> <span class=\"attr\">size</span>=<span class=\"string\">\"6\"</span>&gt;</span></span>字体及字体颜色和大小<span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">font</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<font face=\"微软雅黑\" color=\"red\" size=\"3\">字体及字体颜色和大小</font>\n\n<h3 id=\"4-列表\"><a href=\"#4-列表\" class=\"headerlink\" title=\"4.列表\"></a>4.列表</h3><h4 id=\"4-1有序列表\"><a href=\"#4-1有序列表\" class=\"headerlink\" title=\"4.1有序列表\"></a>4.1有序列表</h4><p>采用<code>1.</code> 后加空格形式表示有序列表。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">1. </span>有序列表1</span><br><span class=\"line\"><span class=\"bullet\">2. </span>有序列表2</span><br><span class=\"line\"><span class=\"bullet\">3. </span>有序列表3</span><br></pre></td></tr></table></figure>\n<ol>\n<li>有序列表1</li>\n<li>有序列表2</li>\n<li>有序列表3</li>\n</ol>\n<h4 id=\"4-2无序列表\"><a href=\"#4-2无序列表\" class=\"headerlink\" title=\"4.2无序列表\"></a>4.2无序列表</h4><p>采用<code>+</code> <code>-</code> <code>*</code> <code>=</code>符号表示无序列表，支持多级嵌套。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">+ </span>有序列表1</span><br><span class=\"line\"><span class=\"bullet\">+ </span>+ 有序列表1.1</span><br><span class=\"line\"><span class=\"bullet\">+ </span>+ 有序列表1.2</span><br><span class=\"line\"><span class=\"bullet\">+ </span>有序列表2</span><br><span class=\"line\"><span class=\"bullet\">+ </span>有序列表3</span><br></pre></td></tr></table></figure>\n<ul>\n<li>无序列表1<ul>\n<li>无序列表1.1</li>\n<li>无序列表1.2</li>\n</ul>\n</li>\n<li>无序列表2</li>\n<li>无序列表3</li>\n</ul>\n<h4 id=\"4-3未完成列表\"><a href=\"#4-3未完成列表\" class=\"headerlink\" title=\"4.3未完成列表\"></a>4.3未完成列表</h4><p>采用<code>- []</code>表示未完成任务，各符号间均有空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务1</span><br><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务2</span><br><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务3</span><br></pre></td></tr></table></figure>\n<ul>\n<li>[ ] 未完成任务1</li>\n<li>[ ] 未完成任务2</li>\n<li>[ ] 未完成任务3</li>\n</ul>\n<h4 id=\"4-4已完成任务\"><a href=\"#4-4已完成任务\" class=\"headerlink\" title=\"4.4已完成任务\"></a>4.4已完成任务</h4><p>采用<code>- [x]</code>表示已完成任务，各符号间均有空格。同时可直接在未完成任务间<code>打勾</code>来转换成已完成任务。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务1</span><br><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务2</span><br><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务3</span><br></pre></td></tr></table></figure>\n<ul>\n<li>[ ] 已完成任务1</li>\n<li>[ ] 已完成任务2</li>\n<li>[ ] 已完成任务3</li>\n</ul>\n<h3 id=\"5-表格\"><a href=\"#5-表格\" class=\"headerlink\" title=\"5.表格\"></a>5.表格</h3><p>表格对齐方式</p>\n<ul>\n<li>居左：:——</li>\n<li>居中：:——:或——-</li>\n<li>居由：——:</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">| 标题1           |      标题2      |           标题3 |</span><br><span class=\"line\">| :-------------- | :-------------: | --------------: |</span><br><span class=\"line\">| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |</span><br><span class=\"line\">| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">标题1</th>\n<th style=\"text-align:center\">标题2</th>\n<th style=\"text-align:right\">标题3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">居左测试文本1.1</td>\n<td style=\"text-align:center\">居中测试文本2.1</td>\n<td style=\"text-align:right\">居右测试文本3.1</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">居左测试文本1.2</td>\n<td style=\"text-align:center\">居中测试文本2.2</td>\n<td style=\"text-align:right\">居右测试文本3.2</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"6-段落和换行\"><a href=\"#6-段落和换行\" class=\"headerlink\" title=\"6.段落和换行\"></a>6.段落和换行</h3><h4 id=\"6-1首行缩进方式\"><a href=\"#6-1首行缩进方式\" class=\"headerlink\" title=\"6.1首行缩进方式\"></a>6.1首行缩进方式</h4><ul>\n<li><code>&amp;emsp;</code>中文空格</li>\n<li><code>&amp;ensp;</code>半中文空格</li>\n<li><code>&amp;nbsp;</code>英文空格</li>\n<li><code> </code>输入法切换到全角双击空格</li>\n</ul>\n<h4 id=\"6-2换行\"><a href=\"#6-2换行\" class=\"headerlink\" title=\"6.2换行\"></a>6.2换行</h4><ul>\n<li><code>` </code> `换行处连续打两个空格</li>\n<li>换行处使用<code>&lt;br&gt;</code>进行换行</li>\n</ul>\n<h4 id=\"6-3空行\"><a href=\"#6-3空行\" class=\"headerlink\" title=\"6.3空行\"></a>6.3空行</h4><ul>\n<li><code>` </code> ` 空行处连续打两个空格</li>\n<li>换行处使用<code>&lt;br&gt;</code>进行空行</li>\n</ul>\n<h3 id=\"6-引用和代码块\"><a href=\"#6-引用和代码块\" class=\"headerlink\" title=\"6.引用和代码块\"></a>6.引用和代码块</h3><h4 id=\"6-1引用\"><a href=\"#6-1引用\" class=\"headerlink\" title=\"6.1引用\"></a>6.1引用</h4><p>若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"quote\">&gt; 引用1</span></span><br><span class=\"line\"><span class=\"quote\">&gt; &gt; 引用1.1</span></span><br><span class=\"line\"><span class=\"quote\">&gt; &gt; 引用1.2</span></span><br><span class=\"line\"><span class=\"quote\">&gt; 引用2</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>引用1</p>\n<blockquote>\n<p>引用1.1</p>\n<p>引用1.2</p>\n</blockquote>\n<p>引用2</p>\n</blockquote>\n<h4 id=\"6-2代码块\"><a href=\"#6-2代码块\" class=\"headerlink\" title=\"6.2代码块\"></a>6.2代码块</h4><p>代码前后添加<figure class=\"highlight plain\"><figcaption><span>`表示代码块。</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```markdown</span><br><span class=\"line\">​```Python</span><br><span class=\"line\">print(&apos;代码块&apos;)</span><br><span class=\"line\">​</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight clean\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```python</span><br><span class=\"line\">print（<span class=\"string\">'代码块'</span>）</span><br></pre></td></tr></table></figure>\n<h3 id=\"7-链接\"><a href=\"#7-链接\" class=\"headerlink\" title=\"7.链接\"></a>7.链接</h3><h4 id=\"7-1图片链接\"><a href=\"#7-1图片链接\" class=\"headerlink\" title=\"7.1图片链接\"></a>7.1图片链接</h4><p>采用<code>![]()</code>来表示图片链接。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">![<span class=\"string\">图片名称</span>](<span class=\"link\">链接地址</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/18/Markdown写作教程/图片02.png\" alt=\"图片名称\"></p>\n<h4 id=\"7-2文字链接\"><a href=\"#7-2文字链接\" class=\"headerlink\" title=\"7.2文字链接\"></a>7.2文字链接</h4><p>采用<code>[]()</code>表示文字链接。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"string\">链接名称</span>](<span class=\"link\">链接地址</span>)</span><br></pre></td></tr></table></figure>\n<p><a href=\"weizhixiaoyi.com\">文字链接</a></p>\n<h4 id=\"7-3参考链接\"><a href=\"#7-3参考链接\" class=\"headerlink\" title=\"7.3参考链接\"></a>7.3参考链接</h4><p>采用<code>[ ]:</code>表示参考链接，注意符号后有空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"symbol\"> </span>]: <span class=\"link\">url title</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"8-分割线\"><a href=\"#8-分割线\" class=\"headerlink\" title=\"8.分割线\"></a>8.分割线</h3><p>上下文无关时可使用分割符进行分开。</p>\n<ul>\n<li>连续多个<code>-</code>  (&gt;=3)</li>\n<li>连续多个<code>*</code> （&gt;=3）</li>\n<li>连续多个下划线<code>_</code> （&gt;=3）</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---分割线</span><br><span class=\"line\"><span class=\"emphasis\">***</span>分割线</span><br><span class=\"line\"><span class=\"emphasis\">___</span>分割线</span><br></pre></td></tr></table></figure>\n<hr>\n<hr>\n<hr>\n<h3 id=\"9-脚注和注释\"><a href=\"#9-脚注和注释\" class=\"headerlink\" title=\"9.脚注和注释\"></a>9.脚注和注释</h3><h4 id=\"9-1脚注\"><a href=\"#9-1脚注\" class=\"headerlink\" title=\"9.1脚注\"></a>9.1脚注</h4><p>采用`<sup><a href=\"#fn_\" id=\"reffn_\"></a></sup>:表示脚注，注意空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"symbol\">^</span>]: <span class=\"link\">脚注</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"9-2注释\"><a href=\"#9-2注释\" class=\"headerlink\" title=\"9.2注释\"></a>9.2注释</h4><p>采用<code>&lt;!----&gt;</code>表示注释.</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"comment\">&lt;!--注释--&gt;</span></span></span><br></pre></td></tr></table></figure>\n<!--注释-->\n<h3 id=\"11-转义\"><a href=\"#11-转义\" class=\"headerlink\" title=\"11.转义\"></a>11.转义</h3><p>Markdown通过反斜杠<code>\\</code>来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\反斜线</span><br><span class=\"line\">\\`反引号</span><br><span class=\"line\">\\*星号</span><br><span class=\"line\">\\_下划线</span><br><span class=\"line\">\\&#123;&#125;花括号</span><br><span class=\"line\">\\[]方括号</span><br><span class=\"line\">\\()括弧</span><br><span class=\"line\">\\#井字号</span><br><span class=\"line\">\\+加号</span><br><span class=\"line\">\\-减号</span><br><span class=\"line\">\\.英文句点</span><br><span class=\"line\">\\!感叹号</span><br></pre></td></tr></table></figure>\n<p>\\\\反斜线<br>`反引号<br>*星号<br>_下划线<br>\\{}花括号<br>[]方括号<br>()括弧<br>#井字号<br>+加号<br>-减号<br>.英文句点<br>!感叹号</p>\n<h3 id=\"12-目录\"><a href=\"#12-目录\" class=\"headerlink\" title=\"12.目录\"></a>12.目录</h3><p>采用<code>[TOC]</code>来生成文章目录。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[TOC]</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/18/Markdown写作教程/图片03.png\" alt=\"图片03\"></p>\n<hr>\n<h3 id=\"13-推广\"><a href=\"#13-推广\" class=\"headerlink\" title=\"13.推广\"></a>13.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/18/Markdown写作教程/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://weizhixiaoyi.com/2018/03/16/Mac+Hexo+GitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/\">博客搭建教程</a>写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。</p>\n<h3 id=\"1-为什么选择Markdown\"><a href=\"#1-为什么选择Markdown\" class=\"headerlink\" title=\"1.为什么选择Markdown\"></a>1.为什么选择Markdown</h3><p>首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用<strong>印象笔记</strong>至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。</p>\n<h3 id=\"2-标题\"><a href=\"#2-标题\" class=\"headerlink\" title=\"2.标题\"></a>2.标题</h3><p>标题通过<code>#</code>的个数进行区分，Markdown共支持6级标题。</p>\n<p><img src=\"/2018/03/18/Markdown写作教程/图片01.png\" alt=\"标题\"></p>\n<h3 id=\"3-字体设置\"><a href=\"#3-字体设置\" class=\"headerlink\" title=\"3.字体设置\"></a>3.字体设置</h3><h4 id=\"3-1粗体\"><a href=\"#3-1粗体\" class=\"headerlink\" title=\"3.1粗体\"></a>3.1粗体</h4><p>文字前后加<code>**</code>来表示粗体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">**粗体**</span></span><br></pre></td></tr></table></figure>\n<p><strong>粗体</strong></p>\n<h4 id=\"3-2斜体\"><a href=\"#3-2斜体\" class=\"headerlink\" title=\"3.2斜体\"></a>3.2斜体</h4><p>文字前后加<code>*</code>来表示斜体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"emphasis\">*斜体*</span></span><br></pre></td></tr></table></figure>\n<p><em>斜体</em></p>\n<h4 id=\"3-3粗斜体\"><a href=\"#3-3粗斜体\" class=\"headerlink\" title=\"3.3粗斜体\"></a>3.3粗斜体</h4><p>文字前后加<code>***</code>来表示粗斜体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">***粗斜体**</span>*</span><br></pre></td></tr></table></figure>\n<p><strong><em>粗斜体</em></strong></p>\n<h4 id=\"3-4下划线\"><a href=\"#3-4下划线\" class=\"headerlink\" title=\"3.4下划线\"></a>3.4下划线</h4><p>文字前后加<code>&lt;u&gt;</code> <code>&lt;/u&gt;</code>来表示下划线。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">u</span>&gt;</span></span>下滑线<span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">u</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p><u>下划线</u></p>\n<h4 id=\"3-5删除线\"><a href=\"#3-5删除线\" class=\"headerlink\" title=\"3.5删除线\"></a>3.5删除线</h4><p>文字前后加<code>~~</code>来表示删除线。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~~删除线~~</span><br></pre></td></tr></table></figure>\n<p><del>删除线</del></p>\n<h4 id=\"3-6标记\"><a href=\"#3-6标记\" class=\"headerlink\" title=\"3.6标记\"></a>3.6标记</h4><p>文字前后加<code>` </code>来表示标记，该符号位于Esc键下面。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"code\">`标记`</span></span><br></pre></td></tr></table></figure>\n<p><code>标记</code></p>\n<h4 id=\"3-7Html标签\"><a href=\"#3-7Html标签\" class=\"headerlink\" title=\"3.7Html标签\"></a>3.7Html标签</h4><figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">font</span> <span class=\"attr\">face</span>=<span class=\"string\">\"微软雅黑\"</span> <span class=\"attr\">color</span>=<span class=\"string\">\"red\"</span> <span class=\"attr\">size</span>=<span class=\"string\">\"6\"</span>&gt;</span></span>字体及字体颜色和大小<span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">font</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<font face=\"微软雅黑\" color=\"red\" size=\"3\">字体及字体颜色和大小</font>\n\n<h3 id=\"4-列表\"><a href=\"#4-列表\" class=\"headerlink\" title=\"4.列表\"></a>4.列表</h3><h4 id=\"4-1有序列表\"><a href=\"#4-1有序列表\" class=\"headerlink\" title=\"4.1有序列表\"></a>4.1有序列表</h4><p>采用<code>1.</code> 后加空格形式表示有序列表。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">1. </span>有序列表1</span><br><span class=\"line\"><span class=\"bullet\">2. </span>有序列表2</span><br><span class=\"line\"><span class=\"bullet\">3. </span>有序列表3</span><br></pre></td></tr></table></figure>\n<ol>\n<li>有序列表1</li>\n<li>有序列表2</li>\n<li>有序列表3</li>\n</ol>\n<h4 id=\"4-2无序列表\"><a href=\"#4-2无序列表\" class=\"headerlink\" title=\"4.2无序列表\"></a>4.2无序列表</h4><p>采用<code>+</code> <code>-</code> <code>*</code> <code>=</code>符号表示无序列表，支持多级嵌套。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">+ </span>有序列表1</span><br><span class=\"line\"><span class=\"bullet\">+ </span>+ 有序列表1.1</span><br><span class=\"line\"><span class=\"bullet\">+ </span>+ 有序列表1.2</span><br><span class=\"line\"><span class=\"bullet\">+ </span>有序列表2</span><br><span class=\"line\"><span class=\"bullet\">+ </span>有序列表3</span><br></pre></td></tr></table></figure>\n<ul>\n<li>无序列表1<ul>\n<li>无序列表1.1</li>\n<li>无序列表1.2</li>\n</ul>\n</li>\n<li>无序列表2</li>\n<li>无序列表3</li>\n</ul>\n<h4 id=\"4-3未完成列表\"><a href=\"#4-3未完成列表\" class=\"headerlink\" title=\"4.3未完成列表\"></a>4.3未完成列表</h4><p>采用<code>- []</code>表示未完成任务，各符号间均有空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务1</span><br><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务2</span><br><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务3</span><br></pre></td></tr></table></figure>\n<ul>\n<li>[ ] 未完成任务1</li>\n<li>[ ] 未完成任务2</li>\n<li>[ ] 未完成任务3</li>\n</ul>\n<h4 id=\"4-4已完成任务\"><a href=\"#4-4已完成任务\" class=\"headerlink\" title=\"4.4已完成任务\"></a>4.4已完成任务</h4><p>采用<code>- [x]</code>表示已完成任务，各符号间均有空格。同时可直接在未完成任务间<code>打勾</code>来转换成已完成任务。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务1</span><br><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务2</span><br><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务3</span><br></pre></td></tr></table></figure>\n<ul>\n<li>[ ] 已完成任务1</li>\n<li>[ ] 已完成任务2</li>\n<li>[ ] 已完成任务3</li>\n</ul>\n<h3 id=\"5-表格\"><a href=\"#5-表格\" class=\"headerlink\" title=\"5.表格\"></a>5.表格</h3><p>表格对齐方式</p>\n<ul>\n<li>居左：:——</li>\n<li>居中：:——:或——-</li>\n<li>居由：——:</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">| 标题1           |      标题2      |           标题3 |</span><br><span class=\"line\">| :-------------- | :-------------: | --------------: |</span><br><span class=\"line\">| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |</span><br><span class=\"line\">| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">标题1</th>\n<th style=\"text-align:center\">标题2</th>\n<th style=\"text-align:right\">标题3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">居左测试文本1.1</td>\n<td style=\"text-align:center\">居中测试文本2.1</td>\n<td style=\"text-align:right\">居右测试文本3.1</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">居左测试文本1.2</td>\n<td style=\"text-align:center\">居中测试文本2.2</td>\n<td style=\"text-align:right\">居右测试文本3.2</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"6-段落和换行\"><a href=\"#6-段落和换行\" class=\"headerlink\" title=\"6.段落和换行\"></a>6.段落和换行</h3><h4 id=\"6-1首行缩进方式\"><a href=\"#6-1首行缩进方式\" class=\"headerlink\" title=\"6.1首行缩进方式\"></a>6.1首行缩进方式</h4><ul>\n<li><code>&amp;emsp;</code>中文空格</li>\n<li><code>&amp;ensp;</code>半中文空格</li>\n<li><code>&amp;nbsp;</code>英文空格</li>\n<li><code> </code>输入法切换到全角双击空格</li>\n</ul>\n<h4 id=\"6-2换行\"><a href=\"#6-2换行\" class=\"headerlink\" title=\"6.2换行\"></a>6.2换行</h4><ul>\n<li><code>` </code> `换行处连续打两个空格</li>\n<li>换行处使用<code>&lt;br&gt;</code>进行换行</li>\n</ul>\n<h4 id=\"6-3空行\"><a href=\"#6-3空行\" class=\"headerlink\" title=\"6.3空行\"></a>6.3空行</h4><ul>\n<li><code>` </code> ` 空行处连续打两个空格</li>\n<li>换行处使用<code>&lt;br&gt;</code>进行空行</li>\n</ul>\n<h3 id=\"6-引用和代码块\"><a href=\"#6-引用和代码块\" class=\"headerlink\" title=\"6.引用和代码块\"></a>6.引用和代码块</h3><h4 id=\"6-1引用\"><a href=\"#6-1引用\" class=\"headerlink\" title=\"6.1引用\"></a>6.1引用</h4><p>若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"quote\">&gt; 引用1</span></span><br><span class=\"line\"><span class=\"quote\">&gt; &gt; 引用1.1</span></span><br><span class=\"line\"><span class=\"quote\">&gt; &gt; 引用1.2</span></span><br><span class=\"line\"><span class=\"quote\">&gt; 引用2</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>引用1</p>\n<blockquote>\n<p>引用1.1</p>\n<p>引用1.2</p>\n</blockquote>\n<p>引用2</p>\n</blockquote>\n<h4 id=\"6-2代码块\"><a href=\"#6-2代码块\" class=\"headerlink\" title=\"6.2代码块\"></a>6.2代码块</h4><p>代码前后添加<figure class=\"highlight plain\"><figcaption><span>`表示代码块。</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```markdown</span><br><span class=\"line\">​```Python</span><br><span class=\"line\">print(&apos;代码块&apos;)</span><br><span class=\"line\">​</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight clean\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```python</span><br><span class=\"line\">print（<span class=\"string\">'代码块'</span>）</span><br></pre></td></tr></table></figure>\n<h3 id=\"7-链接\"><a href=\"#7-链接\" class=\"headerlink\" title=\"7.链接\"></a>7.链接</h3><h4 id=\"7-1图片链接\"><a href=\"#7-1图片链接\" class=\"headerlink\" title=\"7.1图片链接\"></a>7.1图片链接</h4><p>采用<code>![]()</code>来表示图片链接。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">![<span class=\"string\">图片名称</span>](<span class=\"link\">链接地址</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/18/Markdown写作教程/图片02.png\" alt=\"图片名称\"></p>\n<h4 id=\"7-2文字链接\"><a href=\"#7-2文字链接\" class=\"headerlink\" title=\"7.2文字链接\"></a>7.2文字链接</h4><p>采用<code>[]()</code>表示文字链接。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"string\">链接名称</span>](<span class=\"link\">链接地址</span>)</span><br></pre></td></tr></table></figure>\n<p><a href=\"weizhixiaoyi.com\">文字链接</a></p>\n<h4 id=\"7-3参考链接\"><a href=\"#7-3参考链接\" class=\"headerlink\" title=\"7.3参考链接\"></a>7.3参考链接</h4><p>采用<code>[ ]:</code>表示参考链接，注意符号后有空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"symbol\"> </span>]: <span class=\"link\">url title</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"8-分割线\"><a href=\"#8-分割线\" class=\"headerlink\" title=\"8.分割线\"></a>8.分割线</h3><p>上下文无关时可使用分割符进行分开。</p>\n<ul>\n<li>连续多个<code>-</code>  (&gt;=3)</li>\n<li>连续多个<code>*</code> （&gt;=3）</li>\n<li>连续多个下划线<code>_</code> （&gt;=3）</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---分割线</span><br><span class=\"line\"><span class=\"emphasis\">***</span>分割线</span><br><span class=\"line\"><span class=\"emphasis\">___</span>分割线</span><br></pre></td></tr></table></figure>\n<hr>\n<hr>\n<hr>\n<h3 id=\"9-脚注和注释\"><a href=\"#9-脚注和注释\" class=\"headerlink\" title=\"9.脚注和注释\"></a>9.脚注和注释</h3><h4 id=\"9-1脚注\"><a href=\"#9-1脚注\" class=\"headerlink\" title=\"9.1脚注\"></a>9.1脚注</h4><p>采用`<sup><a href=\"#fn_\" id=\"reffn_\"></a></sup>:表示脚注，注意空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"symbol\">^</span>]: <span class=\"link\">脚注</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"9-2注释\"><a href=\"#9-2注释\" class=\"headerlink\" title=\"9.2注释\"></a>9.2注释</h4><p>采用<code>&lt;!----&gt;</code>表示注释.</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"comment\">&lt;!--注释--&gt;</span></span></span><br></pre></td></tr></table></figure>\n<!--注释-->\n<h3 id=\"11-转义\"><a href=\"#11-转义\" class=\"headerlink\" title=\"11.转义\"></a>11.转义</h3><p>Markdown通过反斜杠<code>\\</code>来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\反斜线</span><br><span class=\"line\">\\`反引号</span><br><span class=\"line\">\\*星号</span><br><span class=\"line\">\\_下划线</span><br><span class=\"line\">\\&#123;&#125;花括号</span><br><span class=\"line\">\\[]方括号</span><br><span class=\"line\">\\()括弧</span><br><span class=\"line\">\\#井字号</span><br><span class=\"line\">\\+加号</span><br><span class=\"line\">\\-减号</span><br><span class=\"line\">\\.英文句点</span><br><span class=\"line\">\\!感叹号</span><br></pre></td></tr></table></figure>\n<p>\\\\反斜线<br>`反引号<br>*星号<br>_下划线<br>\\{}花括号<br>[]方括号<br>()括弧<br>#井字号<br>+加号<br>-减号<br>.英文句点<br>!感叹号</p>\n<h3 id=\"12-目录\"><a href=\"#12-目录\" class=\"headerlink\" title=\"12.目录\"></a>12.目录</h3><p>采用<code>[TOC]</code>来生成文章目录。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[TOC]</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/18/Markdown写作教程/图片03.png\" alt=\"图片03\"></p>\n<hr>\n<h3 id=\"13-推广\"><a href=\"#13-推广\" class=\"headerlink\" title=\"13.推广\"></a>13.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/18/Markdown写作教程/推广.png\" alt=\"推广\"></p>\n"},{"title":"Python之MatPlotLib使用教程","date":"2018-03-14T03:43:07.000Z","toc":true,"comments":1,"_content":"### 1.Matplotlib简介\n\n 1. Matplotlib是非常强大的python画图工具\n 2. Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 \n\n### 2.Matplotlib安装\n```\npip3 install matplotlib#python3\n```\n### 3.Matplotlib引入\n```\nimport matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas\n```\n### 4.Matplotlib基本应用\n```\nx=np.linspace(-1,1,50)#定义x数据范围\ny1=2*x+1#定义y数据范围\ny2=x**2\nplt.figure()#定义一个图像窗口\nplt.plot(x,y)#plot()画出曲线\nplt.show()#显示图像\n```\n![图片01](Python之MatPlotLib使用教程/01.png)\n#### 4.1figure图像\nmatplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。\n```\nx=np.linspace(-3,3,50)#50为生成的样本数\ny1=2*x+1\ny2=x**2\nplt.figure(num=1,figsize=(8,5))#定义编号为1 大小为(8,5)\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')#颜色为红色，线宽度为2，线风格为--\nplt.plot(x,y2)#进行画图\nplt.show()#显示图\n```\n![图片02](Python之MatPlotLib使用教程/02.png)\n#### 4.2设置坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='-')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(\"I'm x\")\nplt.ylabel(\"I'm y\")\nplt.show()\n```\n![图片03](Python之MatPlotLib使用教程/03.png)\n自定义坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='-')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(\"I'm x\")\nplt.ylabel(\"I'm y\")\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nprint(new_ticks)\n#[-1.   -0.25  0.5   1.25  2.  ]\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nplt.show()\n```\n![图片04](Python之MatPlotLib使用教程/04.png)\n设置边框属性\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nax=plt.gca()#gca=get current axis\nax.spines['right'].set_color('none')#边框属性设置为none 不显示\nax.spines['top'].set_color('none')\nplt.show()\n```\n![图片05](Python之MatPlotLib使用教程/05.png)\n调整移动坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nax=plt.gca()#gca=get current axis\nax.spines['right'].set_color('none')#边框属性设置为none 不显示\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none\nax.spines['bottom'].set_position(('data', 0))#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))#坐标中心点在(0,0)位置\nplt.show()\n```\n![这里写图片描述](Python之MatPlotLib使用教程/06.png)\n#### 4.3添加图例\nmatplotlib中legend图例帮助我们展示数据对应的图像名称。\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\n\nl1,=plt.plot(x,y1,color='red',linewidth=2,linestyle='--',label='linear line')\nl2,=plt.plot(x,y2,label='square line')#进行画图\nplt.legend(loc='best')#显示在最好的位置\nplt.show()#显示图\n```\n![图片07](Python之MatPlotLib使用教程/07.png)\n调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数\n```\nplt.legend(handles=[l1, l2], labels=['up', 'down'],  loc='best')\n#loc有很多参数 其中best自分配最佳位置\n'''\n 'best' : 0,          \n 'upper right'  : 1,\n 'upper left'   : 2,\n 'lower left'   : 3,\n 'lower right'  : 4,\n 'right'        : 5,\n 'center left'  : 6,\n 'center right' : 7,\n 'lower center' : 8,\n 'upper center' : 9,\n 'center'       : 10,\n '''\n```\n#### 4.4标注\n\n```\nx=np.linspace(-3,3,50)\ny = 2*x + 1\nplt.figure(num=1, figsize=(8, 5))\nplt.plot(x, y,)\n\n#移动坐标轴\nax = plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data', 0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data', 0))\n\n#标注信息\nx0=1\ny0=2*x0+1\nplt.scatter(x0,y0,s=50,color='b')\nplt.plot([x0,x0],[y0,0],'k--',lw=2.5)#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细\n#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置\nplt.annotate(r'$2x0+1=%s$' % y0, xy=(x0, y0), xycoords='data', xytext=(+30, -30),\n             textcoords='offset points', fontsize=16,\n             arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=.2\"))\n#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             \nplt.text(-3.7, 3, r'$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$',\n         fontdict={'size': 16, 'color': 'r'})\nplt.show()\n```\n![图片08](Python之MatPlotLib使用教程/08.png)\n#### 4.5能见度调整\n```\nx=np.linspace(-3, 3, 50)\ny=0.1*x\nplt.figure()\nplt.plot(x, y, linewidth=10, zorder=1)\nplt.ylim(-2, 2)\n\n#移动坐标轴\nax = plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data', 0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data', 0))\n\n#label.set_fontsize(12)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序\nfor label in ax.get_xticklabels() + ax.get_yticklabels():\n    label.set_fontsize(12)\n    label.set_bbox(dict(facecolor='red', edgecolor='None', alpha=0.7, zorder=2))\nplt.show()\n```\n![图片09](Python之MatPlotLib使用教程/09.png)\n### 5.画图种类\n#### 5.1Scatter散点图\n```\nn=1024\nX=np.random.normal(0,1,n)#每一个点的X值\nY=np.random.normal(0,1,n)#每一个点的Y值\nT=np.arctan2(Y,X)#arctan2返回给定的X和Y值的反正切值\n#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴\nplt.scatter(X,Y,s=75,c=T,alpha=0.5)\nplt.xlim(-1.5,1.5)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.5,1.5)\nplt.yticks(())#忽略yticks\nplt.show()\n```\n\n![图片10](Python之MatPlotLib使用教程/10.png)\n#### 5.2条形图\n```\n#基本图形\nn=12\nX=np.arange(n)\nY1=(1-X/float(n))*np.random.uniform(0.5,1,n)\nY2=(1-X/float(n))*np.random.uniform(0.5,1,n)\nplt.bar(X,+Y1,facecolor='#9999ff',edgecolor='white')\nplt.bar(X,-Y2,facecolor='#ff9999',edgecolor='white')\n\n#标记值\nfor x,y in zip(X,Y1):#zip表示可以传递两个值\n    plt.text(x+0.4,y+0.05,'%.2f'%y,ha='center',va='bottom')#ha表示横向对齐 bottom表示向下对齐\nfor x,y in zip(X,Y2):\n    plt.text(x+0.4,-y-0.05,'%.2f'%y,ha='center',va='top')\nplt.xlim(-0.5,n)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.25,1.25)\nplt.yticks(())#忽略yticks\nplt.show()\n```\n![图片11](Python之MatPlotLib使用教程/11.png)\n#### 5.3等高线图\n\n```\nn=256\nx=np.linspace(-3,3,n)\ny=np.linspace(-3,3,n)\nX,Y=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵\n#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中\ndef f(x,y):\n    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)\nplt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map\n#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5\nC=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)\n#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10\nplt.clabel(C,inline=True,fontsize=10)\nplt.xticks(())#隐藏坐标轴\nplt.yticks(())\nplt.show()\n```\n![图片12](Python之MatPlotLib使用教程/12.png)\n#### 5.4Image图片\n利用matplotlib打印出图像\n```\na = np.array([0.313660827978, 0.365348418405, 0.423733120134,\n              0.365348418405, 0.439599930621, 0.525083754405,\n              0.423733120134, 0.525083754405, 0.651536351379]).reshape(3,3)\n#origin='lower'代表的就是选择的原点位置\nplt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color map\nplt.colorbar(shrink=.92)#右边颜色说明 shrink参数是将图片长度变为原来的92%\nplt.xticks(())\nplt.yticks(())\nplt.show()              \n```\n![图片13](Python之MatPlotLib使用教程/13.png)\n出图方式 此处采用内插法中的nearest-neighbor\n![图片14](Python之MatPlotLib使用教程/14.png)\n#### 5.53D图像\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D\nfig=plt.figure()#定义图像窗口\nax=Axes3D(fig)#在窗口上添加3D坐标轴\n#将X和Y值编织成栅格\nX=np.arange(-4,4,0.25)\nY=np.arange(-4,4,0.25)\nX,Y=np.meshgrid(X,Y)\nR=np.sqrt(X**2+Y**2)\nZ=np.sin(R)#高度值\n#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度\nax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图\n#添加XY平面等高线 投影到z平面\nax.contourf(X,Y,Z,zdir='z',offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置\nax.set_zlim(-2,2)\nplt.show()\n```\n![图片15](Python之MatPlotLib使用教程/15.png)\n\n### 6.多图合并显示\n\n#### 6.1Subplot多合一显示\n\n均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。\n\n```python\nplt.figure()\nplt.subplot(2,1,1)#表示整个图像分割成2行2列，当前位置为1\nplt.plot([0,1],[0,1])#横坐标变化为[0,1] 竖坐标变化为[0,2]\n\nplt.subplot(2,3,4)\nplt.plot([0,1],[0,2])\n\nplt.subplot(2,3,5)\nplt.plot([0,1],[0,3])\n\nplt.subplot(2,3,6)\nplt.plot([0,1],[0,4])\nplt.show()\n```\n\n![图片16](Python之MatPlotLib使用教程/图片16.png)\n\n不均匀图中图\n\n```python\nplt.figure()\nplt.subplot(2,1,1)#将整个窗口分割成2行1列，当前位置表示第一个图\nplt.plot([0,1],[0,1])#横坐标变化为[0,1],竖坐标变化为[0,1]\n\nplt.subplot(2,3,4)#将整个窗口分割成2行3列，当前位置为4\nplt.plot([0,1],[0,2])\n\nplt.subplot(2,3,5)\nplt.plot([0,1],[0,3])\n\nplt.subplot(2,3,6)\nplt.plot([0,1],[0,4])\nplt.show()\n```\n\n![图片17](Python之MatPlotLib使用教程/图片17.png)\n\n#### 6.2SubPlot分格显示\n\n方法一\n\n```python\nimport matplotlib.gridspec as gridspec#引入新模块\nplt.figure()\n'''\n使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1\n'''\nax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3)  # stands for axes\nax1.plot([1, 2], [1, 2])\nax1.set_title('ax1_title')#设置图的标题\n\n#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2\nax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)\n\n#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2\nax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\n#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1\nax4 = plt.subplot2grid((3, 3), (2, 0))\nax4.scatter([1, 2], [2, 2])\nax4.set_xlabel('ax4_x')\nax4.set_ylabel('ax4_y')\nax5 = plt.subplot2grid((3, 3), (2, 1))\n```\n\n![图像18](Python之MatPlotLib使用教程/图像18.png)\n\n方法二\n\n```Python\nplt.figure()\ngs = gridspec.GridSpec(3, 3)#将图像分割成3行3列\nax6 = plt.subplot(gs[0, :])#gs[0:1]表示图占第0行和所有列\nax7 = plt.subplot(gs[1, :2])#gs[1,:2]表示图占第1行和第二列前的所有列\nax8 = plt.subplot(gs[1:, 2])\nax9 = plt.subplot(gs[-1, 0])\nax10 = plt.subplot(gs[-1, -2])#gs[-1.-2]表示这个图占倒数第1行和倒数第2行\nplt.show()\n```\n\n![图像19](Python之MatPlotLib使用教程/图像19.png)\n\n方法三\n\n```Python\n'''\n建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114\n'''\nf, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, sharex=True, sharey=True)\nax11.scatter([1,2], [1,2])ax11.scatter 坐标范围x为[1,2]，y为[1,2]\nplt.tight_layout()#表示紧凑显示图像\nplt.show()\n```\n\n![图像21](Python之MatPlotLib使用教程/图像20.png)\n\n#### 6.3图中图\n\n```Python\nfig=plt.figure()\n#创建数据\nx=[1,2,3,4,5,6,7]\ny=[1,3,4,2,5,8,6]\n\n#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。\nleft, bottom, width, height = 0.1, 0.1, 0.8, 0.8\nax1 = fig.add_axes([left, bottom, width, height])  # main axes\nax1.plot(x, y, 'r')#绘制大图，颜色为red\nax1.set_xlabel('x')#横坐标名称为x\nax1.set_ylabel('y')\nax1.set_title('title')#图名称为title\n\n#绘制小图，注意坐标系位置和大小的改变\nax2 = fig.add_axes([0.2, 0.6, 0.25, 0.25])\nax2.plot(y, x, 'b')#颜色为buue\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nax2.set_title('title inside 1')\n\n#绘制第二个小兔\nplt.axes([0.6, 0.2, 0.25, 0.25])\nplt.plot(y[::-1], x, 'g')#将y进行逆序\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('title inside 2')\nplt.show()\n```\n\n![图像21](Python之MatPlotLib使用教程/图像21.png)\n\n#### 6.4次坐标轴\n\n```Python\nx=np.arange(0,10,0.1)\ny1=0.5*x**2\ny2=-1*y1\nfig, ax1 = plt.subplots()\n\nax2 = ax1.twinx()#镜像显示\nax1.plot(x, y1, 'g-')\nax2.plot(x, y2, 'b-')\n\nax1.set_xlabel('X data')\nax1.set_ylabel('Y1 data', color='g')#第一个y坐标轴\nax2.set_ylabel('Y2 data', color='b')#第二个y坐标轴\nplt.show()\n```\n\n![图像22](Python之MatPlotLib使用教程/图像22.png)\n\n### 7.动画\n\n```Python\nfrom matplotlib import animation#引入新模块\nfig,ax=plt.subplots()\nx=np.arange(0,2*np.pi,0.01)#数据为0~2PI范围内的正弦曲线\nline,=ax.plot(x,np.sin(x))# line表示列表\n\n#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧\ndef animate(i):\n    line.set_ydata(np.sin(x+i/100))\n    return line,\n\n#构造开始帧函数init\ndef init():\n    line.set_ydata(np.sin(x))\n    return line,\n\n# frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 \n# blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。\nani=animation.FuncAnimation(fig=fig,func=animate,frames=200,init_func=init,interval=20,blit=False)\nplt.show()\n```\n\n![图像23](Python之MatPlotLib使用教程/图像23.png)\n\n**MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。**\n\n----------\n\n\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n","source":"_posts/Python之MatPlotLib使用教程.md","raw":"---\ntitle: Python之MatPlotLib使用教程\ndate: 2018-03-14 11:43:07\ntags: python\ntoc: true\ncategories: Python库\ncomments: true\n---\n### 1.Matplotlib简介\n\n 1. Matplotlib是非常强大的python画图工具\n 2. Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 \n\n### 2.Matplotlib安装\n```\npip3 install matplotlib#python3\n```\n### 3.Matplotlib引入\n```\nimport matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas\n```\n### 4.Matplotlib基本应用\n```\nx=np.linspace(-1,1,50)#定义x数据范围\ny1=2*x+1#定义y数据范围\ny2=x**2\nplt.figure()#定义一个图像窗口\nplt.plot(x,y)#plot()画出曲线\nplt.show()#显示图像\n```\n![图片01](Python之MatPlotLib使用教程/01.png)\n#### 4.1figure图像\nmatplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。\n```\nx=np.linspace(-3,3,50)#50为生成的样本数\ny1=2*x+1\ny2=x**2\nplt.figure(num=1,figsize=(8,5))#定义编号为1 大小为(8,5)\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')#颜色为红色，线宽度为2，线风格为--\nplt.plot(x,y2)#进行画图\nplt.show()#显示图\n```\n![图片02](Python之MatPlotLib使用教程/02.png)\n#### 4.2设置坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='-')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(\"I'm x\")\nplt.ylabel(\"I'm y\")\nplt.show()\n```\n![图片03](Python之MatPlotLib使用教程/03.png)\n自定义坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='-')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(\"I'm x\")\nplt.ylabel(\"I'm y\")\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nprint(new_ticks)\n#[-1.   -0.25  0.5   1.25  2.  ]\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nplt.show()\n```\n![图片04](Python之MatPlotLib使用教程/04.png)\n设置边框属性\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nax=plt.gca()#gca=get current axis\nax.spines['right'].set_color('none')#边框属性设置为none 不显示\nax.spines['top'].set_color('none')\nplt.show()\n```\n![图片05](Python之MatPlotLib使用教程/05.png)\n调整移动坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nax=plt.gca()#gca=get current axis\nax.spines['right'].set_color('none')#边框属性设置为none 不显示\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none\nax.spines['bottom'].set_position(('data', 0))#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))#坐标中心点在(0,0)位置\nplt.show()\n```\n![这里写图片描述](Python之MatPlotLib使用教程/06.png)\n#### 4.3添加图例\nmatplotlib中legend图例帮助我们展示数据对应的图像名称。\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\n\nl1,=plt.plot(x,y1,color='red',linewidth=2,linestyle='--',label='linear line')\nl2,=plt.plot(x,y2,label='square line')#进行画图\nplt.legend(loc='best')#显示在最好的位置\nplt.show()#显示图\n```\n![图片07](Python之MatPlotLib使用教程/07.png)\n调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数\n```\nplt.legend(handles=[l1, l2], labels=['up', 'down'],  loc='best')\n#loc有很多参数 其中best自分配最佳位置\n'''\n 'best' : 0,          \n 'upper right'  : 1,\n 'upper left'   : 2,\n 'lower left'   : 3,\n 'lower right'  : 4,\n 'right'        : 5,\n 'center left'  : 6,\n 'center right' : 7,\n 'lower center' : 8,\n 'upper center' : 9,\n 'center'       : 10,\n '''\n```\n#### 4.4标注\n\n```\nx=np.linspace(-3,3,50)\ny = 2*x + 1\nplt.figure(num=1, figsize=(8, 5))\nplt.plot(x, y,)\n\n#移动坐标轴\nax = plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data', 0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data', 0))\n\n#标注信息\nx0=1\ny0=2*x0+1\nplt.scatter(x0,y0,s=50,color='b')\nplt.plot([x0,x0],[y0,0],'k--',lw=2.5)#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细\n#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置\nplt.annotate(r'$2x0+1=%s$' % y0, xy=(x0, y0), xycoords='data', xytext=(+30, -30),\n             textcoords='offset points', fontsize=16,\n             arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=.2\"))\n#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             \nplt.text(-3.7, 3, r'$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$',\n         fontdict={'size': 16, 'color': 'r'})\nplt.show()\n```\n![图片08](Python之MatPlotLib使用教程/08.png)\n#### 4.5能见度调整\n```\nx=np.linspace(-3, 3, 50)\ny=0.1*x\nplt.figure()\nplt.plot(x, y, linewidth=10, zorder=1)\nplt.ylim(-2, 2)\n\n#移动坐标轴\nax = plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data', 0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data', 0))\n\n#label.set_fontsize(12)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序\nfor label in ax.get_xticklabels() + ax.get_yticklabels():\n    label.set_fontsize(12)\n    label.set_bbox(dict(facecolor='red', edgecolor='None', alpha=0.7, zorder=2))\nplt.show()\n```\n![图片09](Python之MatPlotLib使用教程/09.png)\n### 5.画图种类\n#### 5.1Scatter散点图\n```\nn=1024\nX=np.random.normal(0,1,n)#每一个点的X值\nY=np.random.normal(0,1,n)#每一个点的Y值\nT=np.arctan2(Y,X)#arctan2返回给定的X和Y值的反正切值\n#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴\nplt.scatter(X,Y,s=75,c=T,alpha=0.5)\nplt.xlim(-1.5,1.5)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.5,1.5)\nplt.yticks(())#忽略yticks\nplt.show()\n```\n\n![图片10](Python之MatPlotLib使用教程/10.png)\n#### 5.2条形图\n```\n#基本图形\nn=12\nX=np.arange(n)\nY1=(1-X/float(n))*np.random.uniform(0.5,1,n)\nY2=(1-X/float(n))*np.random.uniform(0.5,1,n)\nplt.bar(X,+Y1,facecolor='#9999ff',edgecolor='white')\nplt.bar(X,-Y2,facecolor='#ff9999',edgecolor='white')\n\n#标记值\nfor x,y in zip(X,Y1):#zip表示可以传递两个值\n    plt.text(x+0.4,y+0.05,'%.2f'%y,ha='center',va='bottom')#ha表示横向对齐 bottom表示向下对齐\nfor x,y in zip(X,Y2):\n    plt.text(x+0.4,-y-0.05,'%.2f'%y,ha='center',va='top')\nplt.xlim(-0.5,n)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.25,1.25)\nplt.yticks(())#忽略yticks\nplt.show()\n```\n![图片11](Python之MatPlotLib使用教程/11.png)\n#### 5.3等高线图\n\n```\nn=256\nx=np.linspace(-3,3,n)\ny=np.linspace(-3,3,n)\nX,Y=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵\n#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中\ndef f(x,y):\n    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)\nplt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map\n#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5\nC=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)\n#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10\nplt.clabel(C,inline=True,fontsize=10)\nplt.xticks(())#隐藏坐标轴\nplt.yticks(())\nplt.show()\n```\n![图片12](Python之MatPlotLib使用教程/12.png)\n#### 5.4Image图片\n利用matplotlib打印出图像\n```\na = np.array([0.313660827978, 0.365348418405, 0.423733120134,\n              0.365348418405, 0.439599930621, 0.525083754405,\n              0.423733120134, 0.525083754405, 0.651536351379]).reshape(3,3)\n#origin='lower'代表的就是选择的原点位置\nplt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color map\nplt.colorbar(shrink=.92)#右边颜色说明 shrink参数是将图片长度变为原来的92%\nplt.xticks(())\nplt.yticks(())\nplt.show()              \n```\n![图片13](Python之MatPlotLib使用教程/13.png)\n出图方式 此处采用内插法中的nearest-neighbor\n![图片14](Python之MatPlotLib使用教程/14.png)\n#### 5.53D图像\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D\nfig=plt.figure()#定义图像窗口\nax=Axes3D(fig)#在窗口上添加3D坐标轴\n#将X和Y值编织成栅格\nX=np.arange(-4,4,0.25)\nY=np.arange(-4,4,0.25)\nX,Y=np.meshgrid(X,Y)\nR=np.sqrt(X**2+Y**2)\nZ=np.sin(R)#高度值\n#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度\nax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图\n#添加XY平面等高线 投影到z平面\nax.contourf(X,Y,Z,zdir='z',offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置\nax.set_zlim(-2,2)\nplt.show()\n```\n![图片15](Python之MatPlotLib使用教程/15.png)\n\n### 6.多图合并显示\n\n#### 6.1Subplot多合一显示\n\n均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。\n\n```python\nplt.figure()\nplt.subplot(2,1,1)#表示整个图像分割成2行2列，当前位置为1\nplt.plot([0,1],[0,1])#横坐标变化为[0,1] 竖坐标变化为[0,2]\n\nplt.subplot(2,3,4)\nplt.plot([0,1],[0,2])\n\nplt.subplot(2,3,5)\nplt.plot([0,1],[0,3])\n\nplt.subplot(2,3,6)\nplt.plot([0,1],[0,4])\nplt.show()\n```\n\n![图片16](Python之MatPlotLib使用教程/图片16.png)\n\n不均匀图中图\n\n```python\nplt.figure()\nplt.subplot(2,1,1)#将整个窗口分割成2行1列，当前位置表示第一个图\nplt.plot([0,1],[0,1])#横坐标变化为[0,1],竖坐标变化为[0,1]\n\nplt.subplot(2,3,4)#将整个窗口分割成2行3列，当前位置为4\nplt.plot([0,1],[0,2])\n\nplt.subplot(2,3,5)\nplt.plot([0,1],[0,3])\n\nplt.subplot(2,3,6)\nplt.plot([0,1],[0,4])\nplt.show()\n```\n\n![图片17](Python之MatPlotLib使用教程/图片17.png)\n\n#### 6.2SubPlot分格显示\n\n方法一\n\n```python\nimport matplotlib.gridspec as gridspec#引入新模块\nplt.figure()\n'''\n使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1\n'''\nax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3)  # stands for axes\nax1.plot([1, 2], [1, 2])\nax1.set_title('ax1_title')#设置图的标题\n\n#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2\nax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)\n\n#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2\nax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\n#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1\nax4 = plt.subplot2grid((3, 3), (2, 0))\nax4.scatter([1, 2], [2, 2])\nax4.set_xlabel('ax4_x')\nax4.set_ylabel('ax4_y')\nax5 = plt.subplot2grid((3, 3), (2, 1))\n```\n\n![图像18](Python之MatPlotLib使用教程/图像18.png)\n\n方法二\n\n```Python\nplt.figure()\ngs = gridspec.GridSpec(3, 3)#将图像分割成3行3列\nax6 = plt.subplot(gs[0, :])#gs[0:1]表示图占第0行和所有列\nax7 = plt.subplot(gs[1, :2])#gs[1,:2]表示图占第1行和第二列前的所有列\nax8 = plt.subplot(gs[1:, 2])\nax9 = plt.subplot(gs[-1, 0])\nax10 = plt.subplot(gs[-1, -2])#gs[-1.-2]表示这个图占倒数第1行和倒数第2行\nplt.show()\n```\n\n![图像19](Python之MatPlotLib使用教程/图像19.png)\n\n方法三\n\n```Python\n'''\n建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114\n'''\nf, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, sharex=True, sharey=True)\nax11.scatter([1,2], [1,2])ax11.scatter 坐标范围x为[1,2]，y为[1,2]\nplt.tight_layout()#表示紧凑显示图像\nplt.show()\n```\n\n![图像21](Python之MatPlotLib使用教程/图像20.png)\n\n#### 6.3图中图\n\n```Python\nfig=plt.figure()\n#创建数据\nx=[1,2,3,4,5,6,7]\ny=[1,3,4,2,5,8,6]\n\n#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。\nleft, bottom, width, height = 0.1, 0.1, 0.8, 0.8\nax1 = fig.add_axes([left, bottom, width, height])  # main axes\nax1.plot(x, y, 'r')#绘制大图，颜色为red\nax1.set_xlabel('x')#横坐标名称为x\nax1.set_ylabel('y')\nax1.set_title('title')#图名称为title\n\n#绘制小图，注意坐标系位置和大小的改变\nax2 = fig.add_axes([0.2, 0.6, 0.25, 0.25])\nax2.plot(y, x, 'b')#颜色为buue\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nax2.set_title('title inside 1')\n\n#绘制第二个小兔\nplt.axes([0.6, 0.2, 0.25, 0.25])\nplt.plot(y[::-1], x, 'g')#将y进行逆序\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('title inside 2')\nplt.show()\n```\n\n![图像21](Python之MatPlotLib使用教程/图像21.png)\n\n#### 6.4次坐标轴\n\n```Python\nx=np.arange(0,10,0.1)\ny1=0.5*x**2\ny2=-1*y1\nfig, ax1 = plt.subplots()\n\nax2 = ax1.twinx()#镜像显示\nax1.plot(x, y1, 'g-')\nax2.plot(x, y2, 'b-')\n\nax1.set_xlabel('X data')\nax1.set_ylabel('Y1 data', color='g')#第一个y坐标轴\nax2.set_ylabel('Y2 data', color='b')#第二个y坐标轴\nplt.show()\n```\n\n![图像22](Python之MatPlotLib使用教程/图像22.png)\n\n### 7.动画\n\n```Python\nfrom matplotlib import animation#引入新模块\nfig,ax=plt.subplots()\nx=np.arange(0,2*np.pi,0.01)#数据为0~2PI范围内的正弦曲线\nline,=ax.plot(x,np.sin(x))# line表示列表\n\n#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧\ndef animate(i):\n    line.set_ydata(np.sin(x+i/100))\n    return line,\n\n#构造开始帧函数init\ndef init():\n    line.set_ydata(np.sin(x))\n    return line,\n\n# frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 \n# blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。\nani=animation.FuncAnimation(fig=fig,func=animate,frames=200,init_func=init,interval=20,blit=False)\nplt.show()\n```\n\n![图像23](Python之MatPlotLib使用教程/图像23.png)\n\n**MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。**\n\n----------\n\n\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n","slug":"Python之MatPlotLib使用教程","published":1,"updated":"2018-06-26T07:00:44.699Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3a0008jiz5akpjqz46","content":"<h3 id=\"1-Matplotlib简介\"><a href=\"#1-Matplotlib简介\" class=\"headerlink\" title=\"1.Matplotlib简介\"></a>1.Matplotlib简介</h3><ol>\n<li>Matplotlib是非常强大的python画图工具</li>\n<li>Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 </li>\n</ol>\n<h3 id=\"2-Matplotlib安装\"><a href=\"#2-Matplotlib安装\" class=\"headerlink\" title=\"2.Matplotlib安装\"></a>2.Matplotlib安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 <span class=\"keyword\">install</span> matplotlib<span class=\"comment\">#python3</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Matplotlib引入\"><a href=\"#3-Matplotlib引入\" class=\"headerlink\" title=\"3.Matplotlib引入\"></a>3.Matplotlib引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt<span class=\"comment\">#为方便简介为plt</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np<span class=\"comment\">#画图过程中会使用numpy</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd<span class=\"comment\">#画图过程中会使用pandas</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Matplotlib基本应用\"><a href=\"#4-Matplotlib基本应用\" class=\"headerlink\" title=\"4.Matplotlib基本应用\"></a>4.Matplotlib基本应用</h3><figure class=\"highlight makefile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-1,1,50)<span class=\"comment\">#定义x数据范围</span></span><br><span class=\"line\">y1=2*x+1<span class=\"comment\">#定义y数据范围</span></span><br><span class=\"line\">y2=x**2</span><br><span class=\"line\">plt.figure()<span class=\"comment\">#定义一个图像窗口</span></span><br><span class=\"line\">plt.plot(x,y)<span class=\"comment\">#plot()画出曲线</span></span><br><span class=\"line\">plt.show()<span class=\"comment\">#显示图像</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/01.png\" alt=\"图片01\"></p>\n<h4 id=\"4-1figure图像\"><a href=\"#4-1figure图像\" class=\"headerlink\" title=\"4.1figure图像\"></a>4.1figure图像</h4><p>matplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)#<span class=\"number\">50</span>为生成的样本数</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">1</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))#定义编号为<span class=\"number\">1</span> 大小为(<span class=\"number\">8</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='--')#颜色为红色，线宽度为<span class=\"number\">2</span>，线风格为--</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.show()#显示图</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/02.png\" alt=\"图片02\"></p>\n<h4 id=\"4-2设置坐标轴\"><a href=\"#4-2设置坐标轴\" class=\"headerlink\" title=\"4.2设置坐标轴\"></a>4.2设置坐标轴</h4><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='-')</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"I'm x\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"I'm y\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/03.png\" alt=\"图片03\"><br>自定义坐标轴<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='-')</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"I'm x\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"I'm y\"</span>)</span><br><span class=\"line\">new_ticks=np.linspace(<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)#小标从<span class=\"number\">-1</span>到<span class=\"number\">2</span>分为<span class=\"number\">5</span>个单位</span><br><span class=\"line\">print(new_ticks)</span><br><span class=\"line\">#[<span class=\"number\">-1.</span>   <span class=\"number\">-0.25</span>  <span class=\"number\">0.5</span>   <span class=\"number\">1.25</span>  <span class=\"number\">2.</span>  ]</span><br><span class=\"line\">plt.xticks(new_ticks)#进行替换新下标</span><br><span class=\"line\">plt.yticks([<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/04.png\" alt=\"图片04\"><br>设置边框属性<br><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"number\">2</span>,linestyle=<span class=\"string\">'--'</span>)</span><br><span class=\"line\">plt.plot(x,y2)<span class=\"comment\">#进行画图</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(-<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)<span class=\"comment\">#小标从-1到2分为5个单位</span></span><br><span class=\"line\">plt.xticks(new_ticks)<span class=\"comment\">#进行替换新下标</span></span><br><span class=\"line\">plt.yticks([-<span class=\"number\">2</span>,-<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [<span class=\"string\">r'$really\\ bad$'</span>,<span class=\"string\">'$bad$'</span>,<span class=\"string\">'$well$'</span>,<span class=\"string\">'$really\\ well$'</span>])</span><br><span class=\"line\">ax=plt.gca()<span class=\"comment\">#gca=get current axis</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)<span class=\"comment\">#边框属性设置为none 不显示</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/05.png\" alt=\"图片05\"><br>调整移动坐标轴<br><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"number\">2</span>,linestyle=<span class=\"string\">'--'</span>)</span><br><span class=\"line\">plt.plot(x,y2)<span class=\"comment\">#进行画图</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(-<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)<span class=\"comment\">#小标从-1到2分为5个单位</span></span><br><span class=\"line\">plt.xticks(new_ticks)<span class=\"comment\">#进行替换新下标</span></span><br><span class=\"line\">plt.yticks([-<span class=\"number\">2</span>,-<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [<span class=\"string\">r'$really\\ bad$'</span>,<span class=\"string\">'$bad$'</span>,<span class=\"string\">'$well$'</span>,<span class=\"string\">'$really\\ well$'</span>])</span><br><span class=\"line\">ax=plt.gca()<span class=\"comment\">#gca=get current axis</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)<span class=\"comment\">#边框属性设置为none 不显示</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)<span class=\"comment\">#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))<span class=\"comment\">#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data</span></span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))<span class=\"comment\">#坐标中心点在(0,0)位置</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/06.png\" alt=\"这里写图片描述\"></p>\n<h4 id=\"4-3添加图例\"><a href=\"#4-3添加图例\" class=\"headerlink\" title=\"4.3添加图例\"></a>4.3添加图例</h4><p>matplotlib中legend图例帮助我们展示数据对应的图像名称。<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)#小标从<span class=\"number\">-1</span>到<span class=\"number\">2</span>分为<span class=\"number\">5</span>个单位</span><br><span class=\"line\">plt.xticks(new_ticks)#进行替换新下标</span><br><span class=\"line\">plt.yticks([<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])</span><br><span class=\"line\"></span><br><span class=\"line\">l1,=plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='--',label='linear line')</span><br><span class=\"line\">l2,=plt.plot(x,y2,label='square line')#进行画图</span><br><span class=\"line\">plt.legend(loc='best')#显示在最好的位置</span><br><span class=\"line\">plt.show()#显示图</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/07.png\" alt=\"图片07\"><br>调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数<br><figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.legend(handles=[l1, l2], labels=[<span class=\"symbol\">'up</span>', <span class=\"symbol\">'down</span>'],  loc=<span class=\"symbol\">'best</span>')</span><br><span class=\"line\">#loc有很多参数 其中best自分配最佳位置</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"> <span class=\"symbol\">'best</span>' : 0,          </span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> right'  : 1,</span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> left'   : 2,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> left'   : 3,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> right'  : 4,</span><br><span class=\"line\"> <span class=\"symbol\">'right</span>'        : 5,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span> left'  : 6,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span> right' : 7,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> center' : 8,</span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> center' : 9,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span>'       : 10,</span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-4标注\"><a href=\"#4-4标注\" class=\"headerlink\" title=\"4.4标注\"></a>4.4标注</h4><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y = <span class=\"number\">2</span>*x + <span class=\"number\">1</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">1</span>, figsize=(<span class=\"number\">8</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x, y,)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#移动坐标轴</span></span><br><span class=\"line\">ax = plt.gca()</span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#标注信息</span></span><br><span class=\"line\">x0=<span class=\"number\">1</span></span><br><span class=\"line\">y0=<span class=\"number\">2</span>*x0+<span class=\"number\">1</span></span><br><span class=\"line\">plt.scatter(x0,y0,s=<span class=\"number\">50</span>,color=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">plt.plot([x0,x0],[y0,<span class=\"number\">0</span>],<span class=\"string\">'k--'</span>,lw=<span class=\"number\">2.5</span>)<span class=\"comment\">#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细</span></span><br><span class=\"line\"><span class=\"comment\">#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置</span></span><br><span class=\"line\">plt.annotate(<span class=\"string\">r'$2x0+1=%s$'</span> % y0, xy=(x0, y0), xycoords=<span class=\"string\">'data'</span>, xytext=(+<span class=\"number\">30</span>, -<span class=\"number\">30</span>),</span><br><span class=\"line\">             textcoords=<span class=\"string\">'offset points'</span>, fontsize=<span class=\"number\">16</span>,</span><br><span class=\"line\">             arrowprops=dict(arrowstyle=<span class=\"string\">'-&gt;'</span>, connectionstyle=<span class=\"string\">\"arc3,rad=.2\"</span>))</span><br><span class=\"line\"><span class=\"comment\">#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             </span></span><br><span class=\"line\">plt.text(-<span class=\"number\">3.7</span>, <span class=\"number\">3</span>, <span class=\"string\">r'$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$'</span>,</span><br><span class=\"line\">         fontdict=&#123;<span class=\"string\">'size'</span>: <span class=\"number\">16</span>, <span class=\"string\">'color'</span>: <span class=\"string\">'r'</span>&#125;)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/08.png\" alt=\"图片08\"></p>\n<h4 id=\"4-5能见度调整\"><a href=\"#4-5能见度调整\" class=\"headerlink\" title=\"4.5能见度调整\"></a>4.5能见度调整</h4><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">50</span>)</span><br><span class=\"line\">y=<span class=\"number\">0.1</span>*x</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(x, y, linewidth=<span class=\"number\">10</span>, zorder=<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">#移动坐标轴</span><br><span class=\"line\">ax = plt.gca()</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.xaxis</span><span class=\"selector-class\">.set_ticks_position</span>(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax<span class=\"selector-class\">.yaxis</span><span class=\"selector-class\">.set_ticks_position</span>(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-id\">#label</span>.set_fontsize(<span class=\"number\">12</span>)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序</span><br><span class=\"line\"><span class=\"keyword\">for</span> <span class=\"selector-tag\">label</span> <span class=\"keyword\">in</span> ax.get_xticklabels() + ax.get_yticklabels():</span><br><span class=\"line\">    <span class=\"selector-tag\">label</span>.set_fontsize(<span class=\"number\">12</span>)</span><br><span class=\"line\">    <span class=\"selector-tag\">label</span>.set_bbox(dict(facecolor=<span class=\"string\">'red'</span>, edgecolor=<span class=\"string\">'None'</span>, alpha=<span class=\"number\">0.7</span>, zorder=<span class=\"number\">2</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/09.png\" alt=\"图片09\"></p>\n<h3 id=\"5-画图种类\"><a href=\"#5-画图种类\" class=\"headerlink\" title=\"5.画图种类\"></a>5.画图种类</h3><h4 id=\"5-1Scatter散点图\"><a href=\"#5-1Scatter散点图\" class=\"headerlink\" title=\"5.1Scatter散点图\"></a>5.1Scatter散点图</h4><figure class=\"highlight makefile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n=1024</span><br><span class=\"line\">X=np.random.normal(0,1,n)<span class=\"comment\">#每一个点的X值</span></span><br><span class=\"line\">Y=np.random.normal(0,1,n)<span class=\"comment\">#每一个点的Y值</span></span><br><span class=\"line\">T=np.arctan2(Y,X)<span class=\"comment\">#arctan2返回给定的X和Y值的反正切值</span></span><br><span class=\"line\"><span class=\"comment\">#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴</span></span><br><span class=\"line\">plt.scatter(X,Y,s=75,c=T,alpha=0.5)</span><br><span class=\"line\">plt.xlim(-1.5,1.5)</span><br><span class=\"line\">plt.xticks(())<span class=\"comment\">#忽略xticks</span></span><br><span class=\"line\">plt.ylim(-1.5,1.5)</span><br><span class=\"line\">plt.yticks(())<span class=\"comment\">#忽略yticks</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/10.png\" alt=\"图片10\"></p>\n<h4 id=\"5-2条形图\"><a href=\"#5-2条形图\" class=\"headerlink\" title=\"5.2条形图\"></a>5.2条形图</h4><figure class=\"highlight livecodeserver\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#基本图形</span></span><br><span class=\"line\">n=<span class=\"number\">12</span></span><br><span class=\"line\">X=np.arange(n)</span><br><span class=\"line\">Y1=(<span class=\"number\">1</span>-X/float(n))*np.<span class=\"built_in\">random</span>.uniform(<span class=\"number\">0.5</span>,<span class=\"number\">1</span>,n)</span><br><span class=\"line\">Y2=(<span class=\"number\">1</span>-X/float(n))*np.<span class=\"built_in\">random</span>.uniform(<span class=\"number\">0.5</span>,<span class=\"number\">1</span>,n)</span><br><span class=\"line\">plt.bar(X,+Y1,facecolor=<span class=\"string\">'#9999ff'</span>,edgecolor=<span class=\"string\">'white'</span>)</span><br><span class=\"line\">plt.bar(X,-Y2,facecolor=<span class=\"string\">'#ff9999'</span>,edgecolor=<span class=\"string\">'white'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#标记值</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x,y <span class=\"keyword\">in</span> zip(X,Y1):<span class=\"comment\">#zip表示可以传递两个值</span></span><br><span class=\"line\">    plt.<span class=\"keyword\">text</span>(x+<span class=\"number\">0.4</span>,y+<span class=\"number\">0.05</span>,<span class=\"string\">'%.2f'</span>%y,ha=<span class=\"string\">'center'</span>,va=<span class=\"string\">'bottom'</span>)<span class=\"comment\">#ha表示横向对齐 bottom表示向下对齐</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x,y <span class=\"keyword\">in</span> zip(X,Y2):</span><br><span class=\"line\">    plt.<span class=\"keyword\">text</span>(x+<span class=\"number\">0.4</span>,-y<span class=\"number\">-0.05</span>,<span class=\"string\">'%.2f'</span>%y,ha=<span class=\"string\">'center'</span>,va=<span class=\"string\">'top'</span>)</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-0.5</span>,n)</span><br><span class=\"line\">plt.xticks(())<span class=\"comment\">#忽略xticks</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">-1.25</span>,<span class=\"number\">1.25</span>)</span><br><span class=\"line\">plt.yticks(())<span class=\"comment\">#忽略yticks</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/11.png\" alt=\"图片11\"></p>\n<h4 id=\"5-3等高线图\"><a href=\"#5-3等高线图\" class=\"headerlink\" title=\"5.3等高线图\"></a>5.3等高线图</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">n</span>=256</span><br><span class=\"line\"><span class=\"attribute\">x</span>=np.linspace(-3,3,n)</span><br><span class=\"line\"><span class=\"attribute\">y</span>=np.linspace(-3,3,n)</span><br><span class=\"line\">X,<span class=\"attribute\">Y</span>=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵</span><br><span class=\"line\"><span class=\"comment\">#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中</span></span><br><span class=\"line\">def f(x,y):</span><br><span class=\"line\">    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)</span><br><span class=\"line\">plt.contourf(X,Y,f(X,Y),8,<span class=\"attribute\">alpha</span>=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map</span><br><span class=\"line\"><span class=\"comment\">#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5</span></span><br><span class=\"line\"><span class=\"attribute\">C</span>=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)</span><br><span class=\"line\"><span class=\"comment\">#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10</span></span><br><span class=\"line\">plt.clabel(C,<span class=\"attribute\">inline</span>=<span class=\"literal\">True</span>,fontsize=10)</span><br><span class=\"line\">plt.xticks(())#隐藏坐标轴</span><br><span class=\"line\">plt.yticks(())</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/12.png\" alt=\"图片12\"></p>\n<h4 id=\"5-4Image图片\"><a href=\"#5-4Image图片\" class=\"headerlink\" title=\"5.4Image图片\"></a>5.4Image图片</h4><p>利用matplotlib打印出图像<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = np.array([<span class=\"number\">0.313660827978</span>, <span class=\"number\">0.365348418405</span>, <span class=\"number\">0.423733120134</span>,</span><br><span class=\"line\">              <span class=\"number\">0.365348418405</span>, <span class=\"number\">0.439599930621</span>, <span class=\"number\">0.525083754405</span>,</span><br><span class=\"line\">              <span class=\"number\">0.423733120134</span>, <span class=\"number\">0.525083754405</span>, <span class=\"number\">0.651536351379</span>]).reshape(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">#origin='lower'代表的就是选择的原点位置</span><br><span class=\"line\">plt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color map</span><br><span class=\"line\">plt.colorbar(shrink=<span class=\"number\">.92</span>)#右边颜色说明 shrink参数是将图片长度变为原来的<span class=\"number\">92</span>%</span><br><span class=\"line\">plt.xticks(())</span><br><span class=\"line\">plt.yticks(())</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/13.png\" alt=\"图片13\"><br>出图方式 此处采用内插法中的nearest-neighbor<br><img src=\"/2018/03/14/Python之MatPlotLib使用教程/14.png\" alt=\"图片14\"></p>\n<h4 id=\"5-53D图像\"><a href=\"#5-53D图像\" class=\"headerlink\" title=\"5.53D图像\"></a>5.53D图像</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D</span><br><span class=\"line\"><span class=\"attribute\">fig</span>=plt.figure()#定义图像窗口</span><br><span class=\"line\"><span class=\"attribute\">ax</span>=Axes3D(fig)#在窗口上添加3D坐标轴</span><br><span class=\"line\"><span class=\"comment\">#将X和Y值编织成栅格</span></span><br><span class=\"line\"><span class=\"attribute\">X</span>=np.arange(-4,4,0.25)</span><br><span class=\"line\"><span class=\"attribute\">Y</span>=np.arange(-4,4,0.25)</span><br><span class=\"line\">X,<span class=\"attribute\">Y</span>=np.meshgrid(X,Y)</span><br><span class=\"line\"><span class=\"attribute\">R</span>=np.sqrt(X**2+Y**2)</span><br><span class=\"line\"><span class=\"attribute\">Z</span>=np.sin(R)#高度值</span><br><span class=\"line\"><span class=\"comment\">#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度</span></span><br><span class=\"line\">ax.plot_surface(X,Y,Z,<span class=\"attribute\">rstride</span>=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图</span><br><span class=\"line\"><span class=\"comment\">#添加XY平面等高线 投影到z平面</span></span><br><span class=\"line\">ax.contourf(X,Y,Z,<span class=\"attribute\">zdir</span>=<span class=\"string\">'z'</span>,offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置</span><br><span class=\"line\">ax.set_zlim(-2,2)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/15.png\" alt=\"图片15\"></p>\n<h3 id=\"6-多图合并显示\"><a href=\"#6-多图合并显示\" class=\"headerlink\" title=\"6.多图合并显示\"></a>6.多图合并显示</h3><h4 id=\"6-1Subplot多合一显示\"><a href=\"#6-1Subplot多合一显示\" class=\"headerlink\" title=\"6.1Subplot多合一显示\"></a>6.1Subplot多合一显示</h4><p>均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>)<span class=\"comment\">#表示整个图像分割成2行2列，当前位置为1</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>])<span class=\"comment\">#横坐标变化为[0,1] 竖坐标变化为[0,2]</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图片16.png\" alt=\"图片16\"></p>\n<p>不均匀图中图</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>)<span class=\"comment\">#将整个窗口分割成2行1列，当前位置表示第一个图</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>])<span class=\"comment\">#横坐标变化为[0,1],竖坐标变化为[0,1]</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)<span class=\"comment\">#将整个窗口分割成2行3列，当前位置为4</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图片17.png\" alt=\"图片17\"></p>\n<h4 id=\"6-2SubPlot分格显示\"><a href=\"#6-2SubPlot分格显示\" class=\"headerlink\" title=\"6.2SubPlot分格显示\"></a>6.2SubPlot分格显示</h4><p>方法一</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.gridspec <span class=\"keyword\">as</span> gridspec<span class=\"comment\">#引入新模块</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">ax1 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">0</span>, <span class=\"number\">0</span>), colspan=<span class=\"number\">3</span>)  <span class=\"comment\"># stands for axes</span></span><br><span class=\"line\">ax1.plot([<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">'ax1_title'</span>)<span class=\"comment\">#设置图的标题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2</span></span><br><span class=\"line\">ax2 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">1</span>, <span class=\"number\">0</span>), colspan=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2</span></span><br><span class=\"line\">ax3 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">1</span>, <span class=\"number\">2</span>), rowspan=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1</span></span><br><span class=\"line\">ax4 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">2</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax4.scatter([<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax4.set_xlabel(<span class=\"string\">'ax4_x'</span>)</span><br><span class=\"line\">ax4.set_ylabel(<span class=\"string\">'ax4_y'</span>)</span><br><span class=\"line\">ax5 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像18.png\" alt=\"图像18\"></p>\n<p>方法二</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">gs = gridspec.GridSpec(<span class=\"number\">3</span>, <span class=\"number\">3</span>)<span class=\"comment\">#将图像分割成3行3列</span></span><br><span class=\"line\">ax6 = plt.subplot(gs[<span class=\"number\">0</span>, :])<span class=\"comment\">#gs[0:1]表示图占第0行和所有列</span></span><br><span class=\"line\">ax7 = plt.subplot(gs[<span class=\"number\">1</span>, :<span class=\"number\">2</span>])<span class=\"comment\">#gs[1,:2]表示图占第1行和第二列前的所有列</span></span><br><span class=\"line\">ax8 = plt.subplot(gs[<span class=\"number\">1</span>:, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax9 = plt.subplot(gs[<span class=\"number\">-1</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">ax10 = plt.subplot(gs[<span class=\"number\">-1</span>, <span class=\"number\">-2</span>])<span class=\"comment\">#gs[-1.-2]表示这个图占倒数第1行和倒数第2行</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像19.png\" alt=\"图像19\"></p>\n<p>方法三</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">f, ((ax11, ax12), (ax13, ax14)) = plt.subplots(<span class=\"number\">2</span>, <span class=\"number\">2</span>, sharex=<span class=\"keyword\">True</span>, sharey=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">ax11.scatter([<span class=\"number\">1</span>,<span class=\"number\">2</span>], [<span class=\"number\">1</span>,<span class=\"number\">2</span>])ax11.scatter 坐标范围x为[<span class=\"number\">1</span>,<span class=\"number\">2</span>]，y为[<span class=\"number\">1</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\">plt.tight_layout()<span class=\"comment\">#表示紧凑显示图像</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像20.png\" alt=\"图像21\"></p>\n<h4 id=\"6-3图中图\"><a href=\"#6-3图中图\" class=\"headerlink\" title=\"6.3图中图\"></a>6.3图中图</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig=plt.figure()</span><br><span class=\"line\"><span class=\"comment\">#创建数据</span></span><br><span class=\"line\">x=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>]</span><br><span class=\"line\">y=[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>,<span class=\"number\">8</span>,<span class=\"number\">6</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。</span></span><br><span class=\"line\">left, bottom, width, height = <span class=\"number\">0.1</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.8</span></span><br><span class=\"line\">ax1 = fig.add_axes([left, bottom, width, height])  <span class=\"comment\"># main axes</span></span><br><span class=\"line\">ax1.plot(x, y, <span class=\"string\">'r'</span>)<span class=\"comment\">#绘制大图，颜色为red</span></span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">'x'</span>)<span class=\"comment\">#横坐标名称为x</span></span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">'title'</span>)<span class=\"comment\">#图名称为title</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制小图，注意坐标系位置和大小的改变</span></span><br><span class=\"line\">ax2 = fig.add_axes([<span class=\"number\">0.2</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>])</span><br><span class=\"line\">ax2.plot(y, x, <span class=\"string\">'b'</span>)<span class=\"comment\">#颜色为buue</span></span><br><span class=\"line\">ax2.set_xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">ax2.set_title(<span class=\"string\">'title inside 1'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制第二个小兔</span></span><br><span class=\"line\">plt.axes([<span class=\"number\">0.6</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>])</span><br><span class=\"line\">plt.plot(y[::<span class=\"number\">-1</span>], x, <span class=\"string\">'g'</span>)<span class=\"comment\">#将y进行逆序</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'title inside 2'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像21.png\" alt=\"图像21\"></p>\n<h4 id=\"6-4次坐标轴\"><a href=\"#6-4次坐标轴\" class=\"headerlink\" title=\"6.4次坐标轴\"></a>6.4次坐标轴</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">10</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">y1=<span class=\"number\">0.5</span>*x**<span class=\"number\">2</span></span><br><span class=\"line\">y2=<span class=\"number\">-1</span>*y1</span><br><span class=\"line\">fig, ax1 = plt.subplots()</span><br><span class=\"line\"></span><br><span class=\"line\">ax2 = ax1.twinx()<span class=\"comment\">#镜像显示</span></span><br><span class=\"line\">ax1.plot(x, y1, <span class=\"string\">'g-'</span>)</span><br><span class=\"line\">ax2.plot(x, y2, <span class=\"string\">'b-'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">'X data'</span>)</span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">'Y1 data'</span>, color=<span class=\"string\">'g'</span>)<span class=\"comment\">#第一个y坐标轴</span></span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">'Y2 data'</span>, color=<span class=\"string\">'b'</span>)<span class=\"comment\">#第二个y坐标轴</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像22.png\" alt=\"图像22\"></p>\n<h3 id=\"7-动画\"><a href=\"#7-动画\" class=\"headerlink\" title=\"7.动画\"></a>7.动画</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> animation<span class=\"comment\">#引入新模块</span></span><br><span class=\"line\">fig,ax=plt.subplots()</span><br><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">2</span>*np.pi,<span class=\"number\">0.01</span>)<span class=\"comment\">#数据为0~2PI范围内的正弦曲线</span></span><br><span class=\"line\">line,=ax.plot(x,np.sin(x))<span class=\"comment\"># line表示列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">animate</span><span class=\"params\">(i)</span>:</span></span><br><span class=\"line\">    line.set_ydata(np.sin(x+i/<span class=\"number\">100</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> line,</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#构造开始帧函数init</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    line.set_ydata(np.sin(x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> line,</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 </span></span><br><span class=\"line\"><span class=\"comment\"># blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。</span></span><br><span class=\"line\">ani=animation.FuncAnimation(fig=fig,func=animate,frames=<span class=\"number\">200</span>,init_func=init,interval=<span class=\"number\">20</span>,blit=<span class=\"keyword\">False</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像23.png\" alt=\"图像23\"></p>\n<p><strong>MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。</strong></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Matplotlib简介\"><a href=\"#1-Matplotlib简介\" class=\"headerlink\" title=\"1.Matplotlib简介\"></a>1.Matplotlib简介</h3><ol>\n<li>Matplotlib是非常强大的python画图工具</li>\n<li>Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 </li>\n</ol>\n<h3 id=\"2-Matplotlib安装\"><a href=\"#2-Matplotlib安装\" class=\"headerlink\" title=\"2.Matplotlib安装\"></a>2.Matplotlib安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 <span class=\"keyword\">install</span> matplotlib<span class=\"comment\">#python3</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Matplotlib引入\"><a href=\"#3-Matplotlib引入\" class=\"headerlink\" title=\"3.Matplotlib引入\"></a>3.Matplotlib引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt<span class=\"comment\">#为方便简介为plt</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np<span class=\"comment\">#画图过程中会使用numpy</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd<span class=\"comment\">#画图过程中会使用pandas</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Matplotlib基本应用\"><a href=\"#4-Matplotlib基本应用\" class=\"headerlink\" title=\"4.Matplotlib基本应用\"></a>4.Matplotlib基本应用</h3><figure class=\"highlight makefile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-1,1,50)<span class=\"comment\">#定义x数据范围</span></span><br><span class=\"line\">y1=2*x+1<span class=\"comment\">#定义y数据范围</span></span><br><span class=\"line\">y2=x**2</span><br><span class=\"line\">plt.figure()<span class=\"comment\">#定义一个图像窗口</span></span><br><span class=\"line\">plt.plot(x,y)<span class=\"comment\">#plot()画出曲线</span></span><br><span class=\"line\">plt.show()<span class=\"comment\">#显示图像</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/01.png\" alt=\"图片01\"></p>\n<h4 id=\"4-1figure图像\"><a href=\"#4-1figure图像\" class=\"headerlink\" title=\"4.1figure图像\"></a>4.1figure图像</h4><p>matplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)#<span class=\"number\">50</span>为生成的样本数</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">1</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))#定义编号为<span class=\"number\">1</span> 大小为(<span class=\"number\">8</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='--')#颜色为红色，线宽度为<span class=\"number\">2</span>，线风格为--</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.show()#显示图</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/02.png\" alt=\"图片02\"></p>\n<h4 id=\"4-2设置坐标轴\"><a href=\"#4-2设置坐标轴\" class=\"headerlink\" title=\"4.2设置坐标轴\"></a>4.2设置坐标轴</h4><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='-')</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"I'm x\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"I'm y\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/03.png\" alt=\"图片03\"><br>自定义坐标轴<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='-')</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"I'm x\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"I'm y\"</span>)</span><br><span class=\"line\">new_ticks=np.linspace(<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)#小标从<span class=\"number\">-1</span>到<span class=\"number\">2</span>分为<span class=\"number\">5</span>个单位</span><br><span class=\"line\">print(new_ticks)</span><br><span class=\"line\">#[<span class=\"number\">-1.</span>   <span class=\"number\">-0.25</span>  <span class=\"number\">0.5</span>   <span class=\"number\">1.25</span>  <span class=\"number\">2.</span>  ]</span><br><span class=\"line\">plt.xticks(new_ticks)#进行替换新下标</span><br><span class=\"line\">plt.yticks([<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/04.png\" alt=\"图片04\"><br>设置边框属性<br><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"number\">2</span>,linestyle=<span class=\"string\">'--'</span>)</span><br><span class=\"line\">plt.plot(x,y2)<span class=\"comment\">#进行画图</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(-<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)<span class=\"comment\">#小标从-1到2分为5个单位</span></span><br><span class=\"line\">plt.xticks(new_ticks)<span class=\"comment\">#进行替换新下标</span></span><br><span class=\"line\">plt.yticks([-<span class=\"number\">2</span>,-<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [<span class=\"string\">r'$really\\ bad$'</span>,<span class=\"string\">'$bad$'</span>,<span class=\"string\">'$well$'</span>,<span class=\"string\">'$really\\ well$'</span>])</span><br><span class=\"line\">ax=plt.gca()<span class=\"comment\">#gca=get current axis</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)<span class=\"comment\">#边框属性设置为none 不显示</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/05.png\" alt=\"图片05\"><br>调整移动坐标轴<br><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"number\">2</span>,linestyle=<span class=\"string\">'--'</span>)</span><br><span class=\"line\">plt.plot(x,y2)<span class=\"comment\">#进行画图</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(-<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)<span class=\"comment\">#小标从-1到2分为5个单位</span></span><br><span class=\"line\">plt.xticks(new_ticks)<span class=\"comment\">#进行替换新下标</span></span><br><span class=\"line\">plt.yticks([-<span class=\"number\">2</span>,-<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [<span class=\"string\">r'$really\\ bad$'</span>,<span class=\"string\">'$bad$'</span>,<span class=\"string\">'$well$'</span>,<span class=\"string\">'$really\\ well$'</span>])</span><br><span class=\"line\">ax=plt.gca()<span class=\"comment\">#gca=get current axis</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)<span class=\"comment\">#边框属性设置为none 不显示</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)<span class=\"comment\">#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))<span class=\"comment\">#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data</span></span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))<span class=\"comment\">#坐标中心点在(0,0)位置</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/06.png\" alt=\"这里写图片描述\"></p>\n<h4 id=\"4-3添加图例\"><a href=\"#4-3添加图例\" class=\"headerlink\" title=\"4.3添加图例\"></a>4.3添加图例</h4><p>matplotlib中legend图例帮助我们展示数据对应的图像名称。<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)#小标从<span class=\"number\">-1</span>到<span class=\"number\">2</span>分为<span class=\"number\">5</span>个单位</span><br><span class=\"line\">plt.xticks(new_ticks)#进行替换新下标</span><br><span class=\"line\">plt.yticks([<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])</span><br><span class=\"line\"></span><br><span class=\"line\">l1,=plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='--',label='linear line')</span><br><span class=\"line\">l2,=plt.plot(x,y2,label='square line')#进行画图</span><br><span class=\"line\">plt.legend(loc='best')#显示在最好的位置</span><br><span class=\"line\">plt.show()#显示图</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/07.png\" alt=\"图片07\"><br>调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数<br><figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.legend(handles=[l1, l2], labels=[<span class=\"symbol\">'up</span>', <span class=\"symbol\">'down</span>'],  loc=<span class=\"symbol\">'best</span>')</span><br><span class=\"line\">#loc有很多参数 其中best自分配最佳位置</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"> <span class=\"symbol\">'best</span>' : 0,          </span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> right'  : 1,</span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> left'   : 2,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> left'   : 3,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> right'  : 4,</span><br><span class=\"line\"> <span class=\"symbol\">'right</span>'        : 5,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span> left'  : 6,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span> right' : 7,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> center' : 8,</span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> center' : 9,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span>'       : 10,</span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-4标注\"><a href=\"#4-4标注\" class=\"headerlink\" title=\"4.4标注\"></a>4.4标注</h4><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y = <span class=\"number\">2</span>*x + <span class=\"number\">1</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">1</span>, figsize=(<span class=\"number\">8</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x, y,)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#移动坐标轴</span></span><br><span class=\"line\">ax = plt.gca()</span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#标注信息</span></span><br><span class=\"line\">x0=<span class=\"number\">1</span></span><br><span class=\"line\">y0=<span class=\"number\">2</span>*x0+<span class=\"number\">1</span></span><br><span class=\"line\">plt.scatter(x0,y0,s=<span class=\"number\">50</span>,color=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">plt.plot([x0,x0],[y0,<span class=\"number\">0</span>],<span class=\"string\">'k--'</span>,lw=<span class=\"number\">2.5</span>)<span class=\"comment\">#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细</span></span><br><span class=\"line\"><span class=\"comment\">#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置</span></span><br><span class=\"line\">plt.annotate(<span class=\"string\">r'$2x0+1=%s$'</span> % y0, xy=(x0, y0), xycoords=<span class=\"string\">'data'</span>, xytext=(+<span class=\"number\">30</span>, -<span class=\"number\">30</span>),</span><br><span class=\"line\">             textcoords=<span class=\"string\">'offset points'</span>, fontsize=<span class=\"number\">16</span>,</span><br><span class=\"line\">             arrowprops=dict(arrowstyle=<span class=\"string\">'-&gt;'</span>, connectionstyle=<span class=\"string\">\"arc3,rad=.2\"</span>))</span><br><span class=\"line\"><span class=\"comment\">#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             </span></span><br><span class=\"line\">plt.text(-<span class=\"number\">3.7</span>, <span class=\"number\">3</span>, <span class=\"string\">r'$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$'</span>,</span><br><span class=\"line\">         fontdict=&#123;<span class=\"string\">'size'</span>: <span class=\"number\">16</span>, <span class=\"string\">'color'</span>: <span class=\"string\">'r'</span>&#125;)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/08.png\" alt=\"图片08\"></p>\n<h4 id=\"4-5能见度调整\"><a href=\"#4-5能见度调整\" class=\"headerlink\" title=\"4.5能见度调整\"></a>4.5能见度调整</h4><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">50</span>)</span><br><span class=\"line\">y=<span class=\"number\">0.1</span>*x</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(x, y, linewidth=<span class=\"number\">10</span>, zorder=<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">#移动坐标轴</span><br><span class=\"line\">ax = plt.gca()</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.xaxis</span><span class=\"selector-class\">.set_ticks_position</span>(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax<span class=\"selector-class\">.yaxis</span><span class=\"selector-class\">.set_ticks_position</span>(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-id\">#label</span>.set_fontsize(<span class=\"number\">12</span>)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序</span><br><span class=\"line\"><span class=\"keyword\">for</span> <span class=\"selector-tag\">label</span> <span class=\"keyword\">in</span> ax.get_xticklabels() + ax.get_yticklabels():</span><br><span class=\"line\">    <span class=\"selector-tag\">label</span>.set_fontsize(<span class=\"number\">12</span>)</span><br><span class=\"line\">    <span class=\"selector-tag\">label</span>.set_bbox(dict(facecolor=<span class=\"string\">'red'</span>, edgecolor=<span class=\"string\">'None'</span>, alpha=<span class=\"number\">0.7</span>, zorder=<span class=\"number\">2</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/09.png\" alt=\"图片09\"></p>\n<h3 id=\"5-画图种类\"><a href=\"#5-画图种类\" class=\"headerlink\" title=\"5.画图种类\"></a>5.画图种类</h3><h4 id=\"5-1Scatter散点图\"><a href=\"#5-1Scatter散点图\" class=\"headerlink\" title=\"5.1Scatter散点图\"></a>5.1Scatter散点图</h4><figure class=\"highlight makefile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n=1024</span><br><span class=\"line\">X=np.random.normal(0,1,n)<span class=\"comment\">#每一个点的X值</span></span><br><span class=\"line\">Y=np.random.normal(0,1,n)<span class=\"comment\">#每一个点的Y值</span></span><br><span class=\"line\">T=np.arctan2(Y,X)<span class=\"comment\">#arctan2返回给定的X和Y值的反正切值</span></span><br><span class=\"line\"><span class=\"comment\">#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴</span></span><br><span class=\"line\">plt.scatter(X,Y,s=75,c=T,alpha=0.5)</span><br><span class=\"line\">plt.xlim(-1.5,1.5)</span><br><span class=\"line\">plt.xticks(())<span class=\"comment\">#忽略xticks</span></span><br><span class=\"line\">plt.ylim(-1.5,1.5)</span><br><span class=\"line\">plt.yticks(())<span class=\"comment\">#忽略yticks</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/10.png\" alt=\"图片10\"></p>\n<h4 id=\"5-2条形图\"><a href=\"#5-2条形图\" class=\"headerlink\" title=\"5.2条形图\"></a>5.2条形图</h4><figure class=\"highlight livecodeserver\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#基本图形</span></span><br><span class=\"line\">n=<span class=\"number\">12</span></span><br><span class=\"line\">X=np.arange(n)</span><br><span class=\"line\">Y1=(<span class=\"number\">1</span>-X/float(n))*np.<span class=\"built_in\">random</span>.uniform(<span class=\"number\">0.5</span>,<span class=\"number\">1</span>,n)</span><br><span class=\"line\">Y2=(<span class=\"number\">1</span>-X/float(n))*np.<span class=\"built_in\">random</span>.uniform(<span class=\"number\">0.5</span>,<span class=\"number\">1</span>,n)</span><br><span class=\"line\">plt.bar(X,+Y1,facecolor=<span class=\"string\">'#9999ff'</span>,edgecolor=<span class=\"string\">'white'</span>)</span><br><span class=\"line\">plt.bar(X,-Y2,facecolor=<span class=\"string\">'#ff9999'</span>,edgecolor=<span class=\"string\">'white'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#标记值</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x,y <span class=\"keyword\">in</span> zip(X,Y1):<span class=\"comment\">#zip表示可以传递两个值</span></span><br><span class=\"line\">    plt.<span class=\"keyword\">text</span>(x+<span class=\"number\">0.4</span>,y+<span class=\"number\">0.05</span>,<span class=\"string\">'%.2f'</span>%y,ha=<span class=\"string\">'center'</span>,va=<span class=\"string\">'bottom'</span>)<span class=\"comment\">#ha表示横向对齐 bottom表示向下对齐</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x,y <span class=\"keyword\">in</span> zip(X,Y2):</span><br><span class=\"line\">    plt.<span class=\"keyword\">text</span>(x+<span class=\"number\">0.4</span>,-y<span class=\"number\">-0.05</span>,<span class=\"string\">'%.2f'</span>%y,ha=<span class=\"string\">'center'</span>,va=<span class=\"string\">'top'</span>)</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-0.5</span>,n)</span><br><span class=\"line\">plt.xticks(())<span class=\"comment\">#忽略xticks</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">-1.25</span>,<span class=\"number\">1.25</span>)</span><br><span class=\"line\">plt.yticks(())<span class=\"comment\">#忽略yticks</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/11.png\" alt=\"图片11\"></p>\n<h4 id=\"5-3等高线图\"><a href=\"#5-3等高线图\" class=\"headerlink\" title=\"5.3等高线图\"></a>5.3等高线图</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">n</span>=256</span><br><span class=\"line\"><span class=\"attribute\">x</span>=np.linspace(-3,3,n)</span><br><span class=\"line\"><span class=\"attribute\">y</span>=np.linspace(-3,3,n)</span><br><span class=\"line\">X,<span class=\"attribute\">Y</span>=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵</span><br><span class=\"line\"><span class=\"comment\">#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中</span></span><br><span class=\"line\">def f(x,y):</span><br><span class=\"line\">    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)</span><br><span class=\"line\">plt.contourf(X,Y,f(X,Y),8,<span class=\"attribute\">alpha</span>=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map</span><br><span class=\"line\"><span class=\"comment\">#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5</span></span><br><span class=\"line\"><span class=\"attribute\">C</span>=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)</span><br><span class=\"line\"><span class=\"comment\">#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10</span></span><br><span class=\"line\">plt.clabel(C,<span class=\"attribute\">inline</span>=<span class=\"literal\">True</span>,fontsize=10)</span><br><span class=\"line\">plt.xticks(())#隐藏坐标轴</span><br><span class=\"line\">plt.yticks(())</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/12.png\" alt=\"图片12\"></p>\n<h4 id=\"5-4Image图片\"><a href=\"#5-4Image图片\" class=\"headerlink\" title=\"5.4Image图片\"></a>5.4Image图片</h4><p>利用matplotlib打印出图像<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = np.array([<span class=\"number\">0.313660827978</span>, <span class=\"number\">0.365348418405</span>, <span class=\"number\">0.423733120134</span>,</span><br><span class=\"line\">              <span class=\"number\">0.365348418405</span>, <span class=\"number\">0.439599930621</span>, <span class=\"number\">0.525083754405</span>,</span><br><span class=\"line\">              <span class=\"number\">0.423733120134</span>, <span class=\"number\">0.525083754405</span>, <span class=\"number\">0.651536351379</span>]).reshape(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">#origin='lower'代表的就是选择的原点位置</span><br><span class=\"line\">plt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color map</span><br><span class=\"line\">plt.colorbar(shrink=<span class=\"number\">.92</span>)#右边颜色说明 shrink参数是将图片长度变为原来的<span class=\"number\">92</span>%</span><br><span class=\"line\">plt.xticks(())</span><br><span class=\"line\">plt.yticks(())</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/13.png\" alt=\"图片13\"><br>出图方式 此处采用内插法中的nearest-neighbor<br><img src=\"/2018/03/14/Python之MatPlotLib使用教程/14.png\" alt=\"图片14\"></p>\n<h4 id=\"5-53D图像\"><a href=\"#5-53D图像\" class=\"headerlink\" title=\"5.53D图像\"></a>5.53D图像</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D</span><br><span class=\"line\"><span class=\"attribute\">fig</span>=plt.figure()#定义图像窗口</span><br><span class=\"line\"><span class=\"attribute\">ax</span>=Axes3D(fig)#在窗口上添加3D坐标轴</span><br><span class=\"line\"><span class=\"comment\">#将X和Y值编织成栅格</span></span><br><span class=\"line\"><span class=\"attribute\">X</span>=np.arange(-4,4,0.25)</span><br><span class=\"line\"><span class=\"attribute\">Y</span>=np.arange(-4,4,0.25)</span><br><span class=\"line\">X,<span class=\"attribute\">Y</span>=np.meshgrid(X,Y)</span><br><span class=\"line\"><span class=\"attribute\">R</span>=np.sqrt(X**2+Y**2)</span><br><span class=\"line\"><span class=\"attribute\">Z</span>=np.sin(R)#高度值</span><br><span class=\"line\"><span class=\"comment\">#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度</span></span><br><span class=\"line\">ax.plot_surface(X,Y,Z,<span class=\"attribute\">rstride</span>=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图</span><br><span class=\"line\"><span class=\"comment\">#添加XY平面等高线 投影到z平面</span></span><br><span class=\"line\">ax.contourf(X,Y,Z,<span class=\"attribute\">zdir</span>=<span class=\"string\">'z'</span>,offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置</span><br><span class=\"line\">ax.set_zlim(-2,2)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/15.png\" alt=\"图片15\"></p>\n<h3 id=\"6-多图合并显示\"><a href=\"#6-多图合并显示\" class=\"headerlink\" title=\"6.多图合并显示\"></a>6.多图合并显示</h3><h4 id=\"6-1Subplot多合一显示\"><a href=\"#6-1Subplot多合一显示\" class=\"headerlink\" title=\"6.1Subplot多合一显示\"></a>6.1Subplot多合一显示</h4><p>均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>)<span class=\"comment\">#表示整个图像分割成2行2列，当前位置为1</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>])<span class=\"comment\">#横坐标变化为[0,1] 竖坐标变化为[0,2]</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图片16.png\" alt=\"图片16\"></p>\n<p>不均匀图中图</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>)<span class=\"comment\">#将整个窗口分割成2行1列，当前位置表示第一个图</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>])<span class=\"comment\">#横坐标变化为[0,1],竖坐标变化为[0,1]</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)<span class=\"comment\">#将整个窗口分割成2行3列，当前位置为4</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图片17.png\" alt=\"图片17\"></p>\n<h4 id=\"6-2SubPlot分格显示\"><a href=\"#6-2SubPlot分格显示\" class=\"headerlink\" title=\"6.2SubPlot分格显示\"></a>6.2SubPlot分格显示</h4><p>方法一</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.gridspec <span class=\"keyword\">as</span> gridspec<span class=\"comment\">#引入新模块</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">ax1 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">0</span>, <span class=\"number\">0</span>), colspan=<span class=\"number\">3</span>)  <span class=\"comment\"># stands for axes</span></span><br><span class=\"line\">ax1.plot([<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">'ax1_title'</span>)<span class=\"comment\">#设置图的标题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2</span></span><br><span class=\"line\">ax2 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">1</span>, <span class=\"number\">0</span>), colspan=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2</span></span><br><span class=\"line\">ax3 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">1</span>, <span class=\"number\">2</span>), rowspan=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1</span></span><br><span class=\"line\">ax4 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">2</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax4.scatter([<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax4.set_xlabel(<span class=\"string\">'ax4_x'</span>)</span><br><span class=\"line\">ax4.set_ylabel(<span class=\"string\">'ax4_y'</span>)</span><br><span class=\"line\">ax5 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像18.png\" alt=\"图像18\"></p>\n<p>方法二</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">gs = gridspec.GridSpec(<span class=\"number\">3</span>, <span class=\"number\">3</span>)<span class=\"comment\">#将图像分割成3行3列</span></span><br><span class=\"line\">ax6 = plt.subplot(gs[<span class=\"number\">0</span>, :])<span class=\"comment\">#gs[0:1]表示图占第0行和所有列</span></span><br><span class=\"line\">ax7 = plt.subplot(gs[<span class=\"number\">1</span>, :<span class=\"number\">2</span>])<span class=\"comment\">#gs[1,:2]表示图占第1行和第二列前的所有列</span></span><br><span class=\"line\">ax8 = plt.subplot(gs[<span class=\"number\">1</span>:, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax9 = plt.subplot(gs[<span class=\"number\">-1</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">ax10 = plt.subplot(gs[<span class=\"number\">-1</span>, <span class=\"number\">-2</span>])<span class=\"comment\">#gs[-1.-2]表示这个图占倒数第1行和倒数第2行</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像19.png\" alt=\"图像19\"></p>\n<p>方法三</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">f, ((ax11, ax12), (ax13, ax14)) = plt.subplots(<span class=\"number\">2</span>, <span class=\"number\">2</span>, sharex=<span class=\"keyword\">True</span>, sharey=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">ax11.scatter([<span class=\"number\">1</span>,<span class=\"number\">2</span>], [<span class=\"number\">1</span>,<span class=\"number\">2</span>])ax11.scatter 坐标范围x为[<span class=\"number\">1</span>,<span class=\"number\">2</span>]，y为[<span class=\"number\">1</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\">plt.tight_layout()<span class=\"comment\">#表示紧凑显示图像</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像20.png\" alt=\"图像21\"></p>\n<h4 id=\"6-3图中图\"><a href=\"#6-3图中图\" class=\"headerlink\" title=\"6.3图中图\"></a>6.3图中图</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig=plt.figure()</span><br><span class=\"line\"><span class=\"comment\">#创建数据</span></span><br><span class=\"line\">x=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>]</span><br><span class=\"line\">y=[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>,<span class=\"number\">8</span>,<span class=\"number\">6</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。</span></span><br><span class=\"line\">left, bottom, width, height = <span class=\"number\">0.1</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.8</span></span><br><span class=\"line\">ax1 = fig.add_axes([left, bottom, width, height])  <span class=\"comment\"># main axes</span></span><br><span class=\"line\">ax1.plot(x, y, <span class=\"string\">'r'</span>)<span class=\"comment\">#绘制大图，颜色为red</span></span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">'x'</span>)<span class=\"comment\">#横坐标名称为x</span></span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">'title'</span>)<span class=\"comment\">#图名称为title</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制小图，注意坐标系位置和大小的改变</span></span><br><span class=\"line\">ax2 = fig.add_axes([<span class=\"number\">0.2</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>])</span><br><span class=\"line\">ax2.plot(y, x, <span class=\"string\">'b'</span>)<span class=\"comment\">#颜色为buue</span></span><br><span class=\"line\">ax2.set_xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">ax2.set_title(<span class=\"string\">'title inside 1'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制第二个小兔</span></span><br><span class=\"line\">plt.axes([<span class=\"number\">0.6</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>])</span><br><span class=\"line\">plt.plot(y[::<span class=\"number\">-1</span>], x, <span class=\"string\">'g'</span>)<span class=\"comment\">#将y进行逆序</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'title inside 2'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像21.png\" alt=\"图像21\"></p>\n<h4 id=\"6-4次坐标轴\"><a href=\"#6-4次坐标轴\" class=\"headerlink\" title=\"6.4次坐标轴\"></a>6.4次坐标轴</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">10</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">y1=<span class=\"number\">0.5</span>*x**<span class=\"number\">2</span></span><br><span class=\"line\">y2=<span class=\"number\">-1</span>*y1</span><br><span class=\"line\">fig, ax1 = plt.subplots()</span><br><span class=\"line\"></span><br><span class=\"line\">ax2 = ax1.twinx()<span class=\"comment\">#镜像显示</span></span><br><span class=\"line\">ax1.plot(x, y1, <span class=\"string\">'g-'</span>)</span><br><span class=\"line\">ax2.plot(x, y2, <span class=\"string\">'b-'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">'X data'</span>)</span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">'Y1 data'</span>, color=<span class=\"string\">'g'</span>)<span class=\"comment\">#第一个y坐标轴</span></span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">'Y2 data'</span>, color=<span class=\"string\">'b'</span>)<span class=\"comment\">#第二个y坐标轴</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像22.png\" alt=\"图像22\"></p>\n<h3 id=\"7-动画\"><a href=\"#7-动画\" class=\"headerlink\" title=\"7.动画\"></a>7.动画</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> animation<span class=\"comment\">#引入新模块</span></span><br><span class=\"line\">fig,ax=plt.subplots()</span><br><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">2</span>*np.pi,<span class=\"number\">0.01</span>)<span class=\"comment\">#数据为0~2PI范围内的正弦曲线</span></span><br><span class=\"line\">line,=ax.plot(x,np.sin(x))<span class=\"comment\"># line表示列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">animate</span><span class=\"params\">(i)</span>:</span></span><br><span class=\"line\">    line.set_ydata(np.sin(x+i/<span class=\"number\">100</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> line,</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#构造开始帧函数init</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    line.set_ydata(np.sin(x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> line,</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 </span></span><br><span class=\"line\"><span class=\"comment\"># blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。</span></span><br><span class=\"line\">ani=animation.FuncAnimation(fig=fig,func=animate,frames=<span class=\"number\">200</span>,init_func=init,interval=<span class=\"number\">20</span>,blit=<span class=\"keyword\">False</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/14/Python之MatPlotLib使用教程/图像23.png\" alt=\"图像23\"></p>\n<p><strong>MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。</strong></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"Python之NumPy使用教程","date":"2018-03-13T09:35:25.000Z","toc":true,"comments":1,"_content":"### 1.NumPy概述\nNumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：\n 1. 强大的N维数组对象Array\n 2. 成熟的函数库\n 3. 用于集成C/C++和Fortran代码的工具\n 4. 实用的线性代数、傅立叶变换和随机生成函数\n\n### 2.NumPy安装\n```\npip install numpy或pip3 install numpy\n```\n### 3.NumPy引入\n```\nimport numpy as np#为了方便实用numpy 采用np简写\n```\n### 4.NumPy方法\n```\narray=np.array([[1,2,3],[4,5,6]])#将列表转换为矩阵 并转换为int类型\nprint(array)\n'''\n[[1 2 3]\n [4 5 6]]\n '''\n```\n#### 4.1NumPy属性\n```\nprint('array of dim:',array.ndim)#矩阵的维度\n#array of dim:2\nprint('array of shape',array.shape)#矩阵的行数和列数\n#array of shape:(2,3)\nprint('number of size:',array.size)#元素的个数\n#number of size:6\n```\n#### 4.2NumPy创建Array\n\n - array:创建数组\n - dtype:指定数据类型\n - zeros:创建数据全为0\n - ones:创建数据全为1\n - empty:创建数据接近0\n - arange:指定范围内创建数据\n - linspace创建线段\n\n创建数组\n```\na=np.array([1,2,3])\nprint(a)\n#[1,2,3]\n```\n指定数据dtype\n```\na=np.array([1,2,3],dtype=np.int)#指定为int类型\nprint(a.dtype)\n#int 64\nb=np.array([1,2,3],dtype=np.float)#指定为float类型\nprint(b.dtype)\n#float 64\n```\n创建特定数据\n```\na=np.array([[1,2,3],[4,5,6]])#矩阵 2行3列\nprint(a)\n'''\n[[1 2 3]\n [4 5 6]]\n '''\n```\n创建全0数组\n```\na=np.zeros((2,3))#数据全0 2行3列\nprint(a)\n'''\n[[0 0 0]\n [0 0 0]]\n '''\n```\n创建全1数组 指定特定类型dtype\n```\na=np.zeros((2,3),dtype=np.int)#数据全1 2行3列 同时指定类型\nprint(a)\n'''\n[[1 1 1]\n [1 1 1]]\n '''\n```\n创建全空数组 每个值接近0\n```\na=np.empty(2,3)#数据全为empty 3行4列\nprint(a)\n'''\n[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]\n [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]\n '''\n```\n用array创建连续数组\n```\na=np.arange(1,10,2)#1到10的数据 2步长\nprint(a)\n#[1 3 5 7 9]\n```\n用reshape改变数据形状\n```\na=np.arange(6).reshape(2,3)\nprint(a)\n'''\n[[0 1 2]\n [3 4 5]]\n '''\n```\n用linspace创建线段形数据\n```\na=np.linspace(1,10,20)#开始端1 结束端5 分割成10个数据 生成线段\nprint(a)\n'''\n[ 1.          1.44444444  1.88888889  2.33333333  2.77777778  3.22222222\n  3.66666667  4.11111111  4.55555556  5.        ]\n  '''\n```\n#### 4.3NumPy基础运算\n基础运算之加、减、三角函数等\n```\na=np.array([10,20,30,40])\nb=np.arange(4) #array[0,1,2,3]\n\nc=a+b#加法运算\nprint(c)\n#[10,21,32,43]\n\nc=a-b#减法运算\nprint(c)\n#[10.19,28,37]\n\nc=10*np.sin(a)#三角函数运算\n#[-5.44021111,  9.12945251, -9.88031624,  7.4511316 ]\n\nprint(b<3)#逻辑判断\n#[ True  True  True False]\n\nd=np.random.random((2,3))#随机生成2行3列的矩阵\nprint(d)\n'''\n[[ 0.21116981  0.0804489   0.51855475]\n [ 0.38359164  0.55852973  0.73218811]]\n'''\nprint(np.sum(d))#元素求和\n#2.48448292958\nprint(np.max(d))#元素求最大值\n#0.732188108709\nprint(np.min(d))#元素求最小值\n#0.0804488978886\n```\n多维矩阵运算\n```\na=np.array([[1,1],[0,1]])\nb=np.arange(4).reshape((2,2))\n\nc=np.dot(a,b)#或c=a.dot(b)矩阵运算\nprint(c)\n'''\n[[2 4]\n [2 3]]\n '''\n```\n对行或列执行查找运算\n```\na=np.array([[1,2],[3,4]])\nprint(a)\n'''\n[[1,2]\n [3,4]]\n '''\nprint(np.max(a,axis=0))#axis=0时是对列进行操作\n#[3,4]\nprint(np.min(a,axis=1))#axis=1是对行进行操作\n#[1,3]\n```\n矩阵索引操作\n```\nA=np.arange(2,14).reshape(3,4)\nprint(A)\n'''\n[[2,3,4,5]\n [6,7,8,9]\n [10,11,12,13]]\n '''\nprint(np.argmax(A))#矩阵中最大元素的索引\n#11\nprint(np.argmin(A))#矩阵中最小元素的索引\n#0\nprint(np.mean(A))#或者np.average(A)求解矩阵均值\n#7.5\nprint(np.cumsum(A))#矩阵累加函数\n#[2 5 9 14 20 27 35 44 54 65 77 90]\nprint(np.diff(A))#矩阵累差函数\n'''\n[[1 1 1]\n [1 1 1]\n [1 1 1]]\n '''\nprint(np.nonzero(A))#将非0元素的行与列坐标分割开来\n#(array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]))\n```\n矩阵排序、转置、替换操作\n```\nA=np.arange(14,2,-1).reshape((3,4))\nprint(A)\n'''\n[[14 13 12 11]\n [10  9  8  7]\n [ 6  5  4  3]]\n '''\nprint(np.sort(A))#排序\n'''\n[[11 12 13 14]\n [ 7  8  9 10]\n [ 3  4  5  6]]\n '''\n\nprint(np.transpose(A))\n'''\n[[14 10  6]\n [13  9  5]\n [12  8  4]\n [11  7  3]]\n '''\n\nprint(np.clip(A,5,9))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换\n'''\n[[9 9 9 9]\n [9 9 8 7]\n [6 5 5 5]]\n '''\n```\n### 5.索引\n一维索引\n```\nA=np.arange(0,12)\nprint(A)\n#[ 0  1  2  3  4  5  6  7  8  9 10 11]\nprint(A[1])#一维索引\n#1\n\nA=np.arange(0,12).reshape((3,4))\nprint(A[0])\n#[0,1,2,3]\n```\n二维索引\n```\nA=np.arange(0,12).reshape((3,4))\nprint(A)\n'''\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n '''\nprint(A[1][1])#或者A[1,1]\n#5\nprint(A[1,1:3])#切片处理\n#[5,6]\n\nfor row in A:\n    print(A)\n'''\n[0 1 2 3]\n[4 5 6 7]\n[ 8  9 10 11]\n '''\nfor col in A:\n    print(col)\n'''\n[0 4 8]\n[1 5 9]\n[ 2  6 10]\n[ 3  7 11]\n '''\n\nfor item in A.flat:\n    print(item)\n'''\n0\n1\n...\n10\n11\n'''\n```\n### 6.NumPy之Array合并\n```\nA=np.array([1,1,1])\nB=np.array([2,2,2])\nprint(np.vstack((A,B)))#上下合并\n'''\n[[1 1 1]\n [2 2 2]]\n '''\nprint(np.hstack((A,B)))#左右合并\n#[1 1 1 2 2 2]\n```\n增加维度\n```\nA=np.array([1,1,1])\nprint(A.shape)\n#(3,)\nprint(A[np.newaxis,:])\n#[[1 1 1]]\nprint(A[np.newaxis,:].shape)#newaxis增加维度\n#(1,3)\n\nprint(A[:,np.newaxis])\n'''\n[[1]\n [1]\n [1]]\n '''\nprint(A[:,np.newaxis].shape)\n#（3,1）\n```\n多矩阵合并\n```\nA = np.array([1,1,1])[:,np.newaxis]\nB = np.array([2,2,2])[:,np.newaxis]\nprint(np.concatenate((A,B,B,A),axis=0))#0表示上下合并\n'''\n[[1]\n [1]\n [1]\n [2]\n [2]\n [2]\n [2]\n [2]\n [2]\n [1]\n [1]\n [1]]\n '''\nprint(np.concatenate((A,B,B,A),axis=1))#1表示左右合并\n'''\n[[1 2 2 1]\n [1 2 2 1]\n [1 2 2 1]]\n '''\n```\n### 7.NumPy分割\n```\nA=np.arange(12).reshape((3,4))\nprint(A)\n'''\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n '''\nprint(np.split(A,3,axis=0))#横向分割成3部分 或者np.vsplit(A,3)\n#[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]\n\nprint(np.split(A,2,axis=1))#竖向分割成2部分 或者np.hsplit(A,2)\n'''\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2,  3],\n       [ 6,  7],\n       [10, 11]])]\n '''\n \nprint(np.array_split(A,3,axis=1))#不等量分割成3部分\n'''\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2],\n       [ 6],\n       [10]]), array([[ 3],\n       [ 7],\n       [11]])]\n'''  \n```\n### 8.NumPy中copy和deep copy\n'='赋值方式会带有关联性\n```\na=np.arange(4)\nprint(a)\n#[1 2 3 4]\nb=a\nc=a\nd=b\nprint(b is a)\n#True\nprint(c is a)\n#True\nprint(d is a)\n#True\n\nb[0]=5#改变b的值，a,c,d同样会进行改变\nprint(a)\n#[5 2 3 4]\n```\n'copy()'赋值方式没有关联性\n```\na=np.arange(4)#deep copy\nprint(a)\n#[0 1 2 3]\nb=a.copy()\na[0]=5\nprint(b)#值并不发生改变\n#[0 1 2 3]\n```\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/Python之NumPy使用教程.md","raw":"---\ntitle: Python之NumPy使用教程\ndate: 2018-03-13 17:35:25\ntags: python\ntoc: true\ncategories: Python库\ncomments: true\n---\n### 1.NumPy概述\nNumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：\n 1. 强大的N维数组对象Array\n 2. 成熟的函数库\n 3. 用于集成C/C++和Fortran代码的工具\n 4. 实用的线性代数、傅立叶变换和随机生成函数\n\n### 2.NumPy安装\n```\npip install numpy或pip3 install numpy\n```\n### 3.NumPy引入\n```\nimport numpy as np#为了方便实用numpy 采用np简写\n```\n### 4.NumPy方法\n```\narray=np.array([[1,2,3],[4,5,6]])#将列表转换为矩阵 并转换为int类型\nprint(array)\n'''\n[[1 2 3]\n [4 5 6]]\n '''\n```\n#### 4.1NumPy属性\n```\nprint('array of dim:',array.ndim)#矩阵的维度\n#array of dim:2\nprint('array of shape',array.shape)#矩阵的行数和列数\n#array of shape:(2,3)\nprint('number of size:',array.size)#元素的个数\n#number of size:6\n```\n#### 4.2NumPy创建Array\n\n - array:创建数组\n - dtype:指定数据类型\n - zeros:创建数据全为0\n - ones:创建数据全为1\n - empty:创建数据接近0\n - arange:指定范围内创建数据\n - linspace创建线段\n\n创建数组\n```\na=np.array([1,2,3])\nprint(a)\n#[1,2,3]\n```\n指定数据dtype\n```\na=np.array([1,2,3],dtype=np.int)#指定为int类型\nprint(a.dtype)\n#int 64\nb=np.array([1,2,3],dtype=np.float)#指定为float类型\nprint(b.dtype)\n#float 64\n```\n创建特定数据\n```\na=np.array([[1,2,3],[4,5,6]])#矩阵 2行3列\nprint(a)\n'''\n[[1 2 3]\n [4 5 6]]\n '''\n```\n创建全0数组\n```\na=np.zeros((2,3))#数据全0 2行3列\nprint(a)\n'''\n[[0 0 0]\n [0 0 0]]\n '''\n```\n创建全1数组 指定特定类型dtype\n```\na=np.zeros((2,3),dtype=np.int)#数据全1 2行3列 同时指定类型\nprint(a)\n'''\n[[1 1 1]\n [1 1 1]]\n '''\n```\n创建全空数组 每个值接近0\n```\na=np.empty(2,3)#数据全为empty 3行4列\nprint(a)\n'''\n[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]\n [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]\n '''\n```\n用array创建连续数组\n```\na=np.arange(1,10,2)#1到10的数据 2步长\nprint(a)\n#[1 3 5 7 9]\n```\n用reshape改变数据形状\n```\na=np.arange(6).reshape(2,3)\nprint(a)\n'''\n[[0 1 2]\n [3 4 5]]\n '''\n```\n用linspace创建线段形数据\n```\na=np.linspace(1,10,20)#开始端1 结束端5 分割成10个数据 生成线段\nprint(a)\n'''\n[ 1.          1.44444444  1.88888889  2.33333333  2.77777778  3.22222222\n  3.66666667  4.11111111  4.55555556  5.        ]\n  '''\n```\n#### 4.3NumPy基础运算\n基础运算之加、减、三角函数等\n```\na=np.array([10,20,30,40])\nb=np.arange(4) #array[0,1,2,3]\n\nc=a+b#加法运算\nprint(c)\n#[10,21,32,43]\n\nc=a-b#减法运算\nprint(c)\n#[10.19,28,37]\n\nc=10*np.sin(a)#三角函数运算\n#[-5.44021111,  9.12945251, -9.88031624,  7.4511316 ]\n\nprint(b<3)#逻辑判断\n#[ True  True  True False]\n\nd=np.random.random((2,3))#随机生成2行3列的矩阵\nprint(d)\n'''\n[[ 0.21116981  0.0804489   0.51855475]\n [ 0.38359164  0.55852973  0.73218811]]\n'''\nprint(np.sum(d))#元素求和\n#2.48448292958\nprint(np.max(d))#元素求最大值\n#0.732188108709\nprint(np.min(d))#元素求最小值\n#0.0804488978886\n```\n多维矩阵运算\n```\na=np.array([[1,1],[0,1]])\nb=np.arange(4).reshape((2,2))\n\nc=np.dot(a,b)#或c=a.dot(b)矩阵运算\nprint(c)\n'''\n[[2 4]\n [2 3]]\n '''\n```\n对行或列执行查找运算\n```\na=np.array([[1,2],[3,4]])\nprint(a)\n'''\n[[1,2]\n [3,4]]\n '''\nprint(np.max(a,axis=0))#axis=0时是对列进行操作\n#[3,4]\nprint(np.min(a,axis=1))#axis=1是对行进行操作\n#[1,3]\n```\n矩阵索引操作\n```\nA=np.arange(2,14).reshape(3,4)\nprint(A)\n'''\n[[2,3,4,5]\n [6,7,8,9]\n [10,11,12,13]]\n '''\nprint(np.argmax(A))#矩阵中最大元素的索引\n#11\nprint(np.argmin(A))#矩阵中最小元素的索引\n#0\nprint(np.mean(A))#或者np.average(A)求解矩阵均值\n#7.5\nprint(np.cumsum(A))#矩阵累加函数\n#[2 5 9 14 20 27 35 44 54 65 77 90]\nprint(np.diff(A))#矩阵累差函数\n'''\n[[1 1 1]\n [1 1 1]\n [1 1 1]]\n '''\nprint(np.nonzero(A))#将非0元素的行与列坐标分割开来\n#(array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]))\n```\n矩阵排序、转置、替换操作\n```\nA=np.arange(14,2,-1).reshape((3,4))\nprint(A)\n'''\n[[14 13 12 11]\n [10  9  8  7]\n [ 6  5  4  3]]\n '''\nprint(np.sort(A))#排序\n'''\n[[11 12 13 14]\n [ 7  8  9 10]\n [ 3  4  5  6]]\n '''\n\nprint(np.transpose(A))\n'''\n[[14 10  6]\n [13  9  5]\n [12  8  4]\n [11  7  3]]\n '''\n\nprint(np.clip(A,5,9))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换\n'''\n[[9 9 9 9]\n [9 9 8 7]\n [6 5 5 5]]\n '''\n```\n### 5.索引\n一维索引\n```\nA=np.arange(0,12)\nprint(A)\n#[ 0  1  2  3  4  5  6  7  8  9 10 11]\nprint(A[1])#一维索引\n#1\n\nA=np.arange(0,12).reshape((3,4))\nprint(A[0])\n#[0,1,2,3]\n```\n二维索引\n```\nA=np.arange(0,12).reshape((3,4))\nprint(A)\n'''\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n '''\nprint(A[1][1])#或者A[1,1]\n#5\nprint(A[1,1:3])#切片处理\n#[5,6]\n\nfor row in A:\n    print(A)\n'''\n[0 1 2 3]\n[4 5 6 7]\n[ 8  9 10 11]\n '''\nfor col in A:\n    print(col)\n'''\n[0 4 8]\n[1 5 9]\n[ 2  6 10]\n[ 3  7 11]\n '''\n\nfor item in A.flat:\n    print(item)\n'''\n0\n1\n...\n10\n11\n'''\n```\n### 6.NumPy之Array合并\n```\nA=np.array([1,1,1])\nB=np.array([2,2,2])\nprint(np.vstack((A,B)))#上下合并\n'''\n[[1 1 1]\n [2 2 2]]\n '''\nprint(np.hstack((A,B)))#左右合并\n#[1 1 1 2 2 2]\n```\n增加维度\n```\nA=np.array([1,1,1])\nprint(A.shape)\n#(3,)\nprint(A[np.newaxis,:])\n#[[1 1 1]]\nprint(A[np.newaxis,:].shape)#newaxis增加维度\n#(1,3)\n\nprint(A[:,np.newaxis])\n'''\n[[1]\n [1]\n [1]]\n '''\nprint(A[:,np.newaxis].shape)\n#（3,1）\n```\n多矩阵合并\n```\nA = np.array([1,1,1])[:,np.newaxis]\nB = np.array([2,2,2])[:,np.newaxis]\nprint(np.concatenate((A,B,B,A),axis=0))#0表示上下合并\n'''\n[[1]\n [1]\n [1]\n [2]\n [2]\n [2]\n [2]\n [2]\n [2]\n [1]\n [1]\n [1]]\n '''\nprint(np.concatenate((A,B,B,A),axis=1))#1表示左右合并\n'''\n[[1 2 2 1]\n [1 2 2 1]\n [1 2 2 1]]\n '''\n```\n### 7.NumPy分割\n```\nA=np.arange(12).reshape((3,4))\nprint(A)\n'''\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n '''\nprint(np.split(A,3,axis=0))#横向分割成3部分 或者np.vsplit(A,3)\n#[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]\n\nprint(np.split(A,2,axis=1))#竖向分割成2部分 或者np.hsplit(A,2)\n'''\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2,  3],\n       [ 6,  7],\n       [10, 11]])]\n '''\n \nprint(np.array_split(A,3,axis=1))#不等量分割成3部分\n'''\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2],\n       [ 6],\n       [10]]), array([[ 3],\n       [ 7],\n       [11]])]\n'''  \n```\n### 8.NumPy中copy和deep copy\n'='赋值方式会带有关联性\n```\na=np.arange(4)\nprint(a)\n#[1 2 3 4]\nb=a\nc=a\nd=b\nprint(b is a)\n#True\nprint(c is a)\n#True\nprint(d is a)\n#True\n\nb[0]=5#改变b的值，a,c,d同样会进行改变\nprint(a)\n#[5 2 3 4]\n```\n'copy()'赋值方式没有关联性\n```\na=np.arange(4)#deep copy\nprint(a)\n#[0 1 2 3]\nb=a.copy()\na[0]=5\nprint(b)#值并不发生改变\n#[0 1 2 3]\n```\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"Python之NumPy使用教程","published":1,"updated":"2018-06-26T06:55:37.248Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3c0009jiz59ivzbrai","content":"<h3 id=\"1-NumPy概述\"><a href=\"#1-NumPy概述\" class=\"headerlink\" title=\"1.NumPy概述\"></a>1.NumPy概述</h3><p>NumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：</p>\n<ol>\n<li>强大的N维数组对象Array</li>\n<li>成熟的函数库</li>\n<li>用于集成C/C++和Fortran代码的工具</li>\n<li>实用的线性代数、傅立叶变换和随机生成函数</li>\n</ol>\n<h3 id=\"2-NumPy安装\"><a href=\"#2-NumPy安装\" class=\"headerlink\" title=\"2.NumPy安装\"></a>2.NumPy安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip <span class=\"keyword\">install</span> numpy或pip3 <span class=\"keyword\">install</span> numpy</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-NumPy引入\"><a href=\"#3-NumPy引入\" class=\"headerlink\" title=\"3.NumPy引入\"></a>3.NumPy引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np<span class=\"comment\">#为了方便实用numpy 采用np简写</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-NumPy方法\"><a href=\"#4-NumPy方法\" class=\"headerlink\" title=\"4.NumPy方法\"></a>4.NumPy方法</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])#将列表转换为矩阵 并转换为int类型</span><br><span class=\"line\">print(array)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\"> [<span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure>\n<h4 id=\"4-1NumPy属性\"><a href=\"#4-1NumPy属性\" class=\"headerlink\" title=\"4.1NumPy属性\"></a>4.1NumPy属性</h4><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'array of dim:'</span>,array.ndim)</span></span>#矩阵的维度</span><br><span class=\"line\"><span class=\"selector-id\">#array</span> of dim:<span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'array of shape'</span>,array.shape)</span></span>#矩阵的行数和列数</span><br><span class=\"line\"><span class=\"selector-id\">#array</span> of shape:(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'number of size:'</span>,array.size)</span></span>#元素的个数</span><br><span class=\"line\"><span class=\"selector-id\">#number</span> of size:<span class=\"number\">6</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2NumPy创建Array\"><a href=\"#4-2NumPy创建Array\" class=\"headerlink\" title=\"4.2NumPy创建Array\"></a>4.2NumPy创建Array</h4><ul>\n<li>array:创建数组</li>\n<li>dtype:指定数据类型</li>\n<li>zeros:创建数据全为0</li>\n<li>ones:创建数据全为1</li>\n<li>empty:创建数据接近0</li>\n<li>arange:指定范围内创建数据</li>\n<li>linspace创建线段</li>\n</ul>\n<p>创建数组<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>指定数据dtype<br><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">a</span>=np.array([1,2,3],<span class=\"attribute\">dtype</span>=np.int)#指定为int类型</span><br><span class=\"line\"><span class=\"builtin-name\">print</span>(a.dtype)</span><br><span class=\"line\"><span class=\"comment\">#int 64</span></span><br><span class=\"line\"><span class=\"attribute\">b</span>=np.array([1,2,3],<span class=\"attribute\">dtype</span>=np.float)#指定为float类型</span><br><span class=\"line\"><span class=\"builtin-name\">print</span>(b.dtype)</span><br><span class=\"line\"><span class=\"comment\">#float 64</span></span><br></pre></td></tr></table></figure></p>\n<p>创建特定数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])#矩阵 <span class=\"number\">2</span>行<span class=\"number\">3</span>列</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\"> [<span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>创建全0数组<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.zeros((<span class=\"number\">2</span>,<span class=\"number\">3</span>))<span class=\"comment\">#数据全0 2行3列</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[0 0 0]</span></span><br><span class=\"line\"><span class=\"string\"> [0 0 0]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>创建全1数组 指定特定类型dtype<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.zeros((<span class=\"number\">2</span>,<span class=\"number\">3</span>),dtype=np.int)<span class=\"comment\">#数据全1 2行3列 同时指定类型</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1 1 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 1 1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>创建全空数组 每个值接近0<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>)<span class=\"comment\">#数据全为empty 3行4列</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]</span></span><br><span class=\"line\"><span class=\"string\"> [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>用array创建连续数组<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">2</span>)#<span class=\"number\">1</span>到<span class=\"number\">10</span>的数据 <span class=\"number\">2</span>步长</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">5</span> <span class=\"number\">7</span> <span class=\"number\">9</span>]</span><br></pre></td></tr></table></figure></p>\n<p>用reshape改变数据形状<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">6</span>).reshape(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[0 1 2]</span></span><br><span class=\"line\"><span class=\"string\"> [3 4 5]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>用linspace创建线段形数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.linspace(<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">20</span>)#开始端<span class=\"number\">1</span> 结束端<span class=\"number\">5</span> 分割成<span class=\"number\">10</span>个数据 生成线段</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[ <span class=\"number\">1.</span>          <span class=\"number\">1.44444444</span>  <span class=\"number\">1.88888889</span>  <span class=\"number\">2.33333333</span>  <span class=\"number\">2.77777778</span>  <span class=\"number\">3.22222222</span></span><br><span class=\"line\">  <span class=\"number\">3.66666667</span>  <span class=\"number\">4.11111111</span>  <span class=\"number\">4.55555556</span>  <span class=\"number\">5.</span>        ]</span><br><span class=\"line\">  '''</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-3NumPy基础运算\"><a href=\"#4-3NumPy基础运算\" class=\"headerlink\" title=\"4.3NumPy基础运算\"></a>4.3NumPy基础运算</h4><p>基础运算之加、减、三角函数等<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([<span class=\"number\">10</span>,<span class=\"number\">20</span>,<span class=\"number\">30</span>,<span class=\"number\">40</span>])</span><br><span class=\"line\">b=np.arange(<span class=\"number\">4</span>) #array[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=a+b#加法运算</span><br><span class=\"line\">print(c)</span><br><span class=\"line\">#[<span class=\"number\">10</span>,<span class=\"number\">21</span>,<span class=\"number\">32</span>,<span class=\"number\">43</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=a-b#减法运算</span><br><span class=\"line\">print(c)</span><br><span class=\"line\">#[<span class=\"number\">10.19</span>,<span class=\"number\">28</span>,<span class=\"number\">37</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=<span class=\"number\">10</span>*np.sin(a)#三角函数运算</span><br><span class=\"line\">#[<span class=\"number\">-5.44021111</span>,  <span class=\"number\">9.12945251</span>, <span class=\"number\">-9.88031624</span>,  <span class=\"number\">7.4511316</span> ]</span><br><span class=\"line\"></span><br><span class=\"line\">print(b&lt;<span class=\"number\">3</span>)#逻辑判断</span><br><span class=\"line\">#[ True  True  True False]</span><br><span class=\"line\"></span><br><span class=\"line\">d=np.random.random((<span class=\"number\">2</span>,<span class=\"number\">3</span>))#随机生成<span class=\"number\">2</span>行<span class=\"number\">3</span>列的矩阵</span><br><span class=\"line\">print(d)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[ <span class=\"number\">0.21116981</span>  <span class=\"number\">0.0804489</span>   <span class=\"number\">0.51855475</span>]</span><br><span class=\"line\"> [ <span class=\"number\">0.38359164</span>  <span class=\"number\">0.55852973</span>  <span class=\"number\">0.73218811</span>]]</span><br><span class=\"line\">'''</span><br><span class=\"line\">print(np.sum(d))#元素求和</span><br><span class=\"line\">#<span class=\"number\">2.48448292958</span></span><br><span class=\"line\">print(np.max(d))#元素求最大值</span><br><span class=\"line\">#<span class=\"number\">0.732188108709</span></span><br><span class=\"line\">print(np.min(d))#元素求最小值</span><br><span class=\"line\">#<span class=\"number\">0.0804488978886</span></span><br></pre></td></tr></table></figure></p>\n<p>多维矩阵运算<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>]])</span><br><span class=\"line\">b=np.arange(<span class=\"number\">4</span>).reshape((<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">c=np.dot(a,b)<span class=\"comment\">#或c=a.dot(b)矩阵运算</span></span><br><span class=\"line\">print(c)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[2 4]</span></span><br><span class=\"line\"><span class=\"string\"> [2 3]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>对行或列执行查找运算<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>],[<span class=\"number\">3</span>,<span class=\"number\">4</span>]])</span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1,2]</span></span><br><span class=\"line\"><span class=\"string\"> [3,4]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(np.max(a,axis=<span class=\"number\">0</span>))<span class=\"comment\">#axis=0时是对列进行操作</span></span><br><span class=\"line\"><span class=\"comment\">#[3,4]</span></span><br><span class=\"line\">print(np.min(a,axis=<span class=\"number\">1</span>))<span class=\"comment\">#axis=1是对行进行操作</span></span><br><span class=\"line\"><span class=\"comment\">#[1,3]</span></span><br></pre></td></tr></table></figure></p>\n<p>矩阵索引操作<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">2</span>,<span class=\"number\">14</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]</span><br><span class=\"line\"> [<span class=\"number\">10</span>,<span class=\"number\">11</span>,<span class=\"number\">12</span>,<span class=\"number\">13</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.argmax(A))#矩阵中最大元素的索引</span><br><span class=\"line\">#<span class=\"number\">11</span></span><br><span class=\"line\">print(np.argmin(A))#矩阵中最小元素的索引</span><br><span class=\"line\">#<span class=\"number\">0</span></span><br><span class=\"line\">print(np.mean(A))#或者np.average(A)求解矩阵均值</span><br><span class=\"line\">#<span class=\"number\">7.5</span></span><br><span class=\"line\">print(np.cumsum(A))#矩阵累加函数</span><br><span class=\"line\">#[<span class=\"number\">2</span> <span class=\"number\">5</span> <span class=\"number\">9</span> <span class=\"number\">14</span> <span class=\"number\">20</span> <span class=\"number\">27</span> <span class=\"number\">35</span> <span class=\"number\">44</span> <span class=\"number\">54</span> <span class=\"number\">65</span> <span class=\"number\">77</span> <span class=\"number\">90</span>]</span><br><span class=\"line\">print(np.diff(A))#矩阵累差函数</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.nonzero(A))#将非<span class=\"number\">0</span>元素的行与列坐标分割开来</span><br><span class=\"line\">#(array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]), array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]))</span><br></pre></td></tr></table></figure></p>\n<p>矩阵排序、转置、替换操作<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">14</span>,<span class=\"number\">2</span>,<span class=\"number\">-1</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">14</span> <span class=\"number\">13</span> <span class=\"number\">12</span> <span class=\"number\">11</span>]</span><br><span class=\"line\"> [<span class=\"number\">10</span>  <span class=\"number\">9</span>  <span class=\"number\">8</span>  <span class=\"number\">7</span>]</span><br><span class=\"line\"> [ <span class=\"number\">6</span>  <span class=\"number\">5</span>  <span class=\"number\">4</span>  <span class=\"number\">3</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.sort(A))#排序</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">11</span> <span class=\"number\">12</span> <span class=\"number\">13</span> <span class=\"number\">14</span>]</span><br><span class=\"line\"> [ <span class=\"number\">7</span>  <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span>]</span><br><span class=\"line\"> [ <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.transpose(A))</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">14</span> <span class=\"number\">10</span>  <span class=\"number\">6</span>]</span><br><span class=\"line\"> [<span class=\"number\">13</span>  <span class=\"number\">9</span>  <span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">12</span>  <span class=\"number\">8</span>  <span class=\"number\">4</span>]</span><br><span class=\"line\"> [<span class=\"number\">11</span>  <span class=\"number\">7</span>  <span class=\"number\">3</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.clip(A,<span class=\"number\">5</span>,<span class=\"number\">9</span>))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">9</span>]</span><br><span class=\"line\"> [<span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">8</span> <span class=\"number\">7</span>]</span><br><span class=\"line\"> [<span class=\"number\">6</span> <span class=\"number\">5</span> <span class=\"number\">5</span> <span class=\"number\">5</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-索引\"><a href=\"#5-索引\" class=\"headerlink\" title=\"5.索引\"></a>5.索引</h3><p>一维索引<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>)</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">#[ <span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>  <span class=\"number\">7</span>  <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]</span><br><span class=\"line\">print(A[<span class=\"number\">1</span>])#一维索引</span><br><span class=\"line\">#<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A[<span class=\"number\">0</span>])</span><br><span class=\"line\">#[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>二维索引<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[ 0  1  2  3]</span></span><br><span class=\"line\"><span class=\"string\"> [ 4  5  6  7]</span></span><br><span class=\"line\"><span class=\"string\"> [ 8  9 10 11]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(A[<span class=\"number\">1</span>][<span class=\"number\">1</span>])<span class=\"comment\">#或者A[1,1]</span></span><br><span class=\"line\"><span class=\"comment\">#5</span></span><br><span class=\"line\">print(A[<span class=\"number\">1</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>])<span class=\"comment\">#切片处理</span></span><br><span class=\"line\"><span class=\"comment\">#[5,6]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">    print(A)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 1 2 3]</span></span><br><span class=\"line\"><span class=\"string\">[4 5 6 7]</span></span><br><span class=\"line\"><span class=\"string\">[ 8  9 10 11]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> col <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">    print(col)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 4 8]</span></span><br><span class=\"line\"><span class=\"string\">[1 5 9]</span></span><br><span class=\"line\"><span class=\"string\">[ 2  6 10]</span></span><br><span class=\"line\"><span class=\"string\">[ 3  7 11]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> A.flat:</span><br><span class=\"line\">    print(item)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">10</span></span><br><span class=\"line\"><span class=\"string\">11</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-NumPy之Array合并\"><a href=\"#6-NumPy之Array合并\" class=\"headerlink\" title=\"6.NumPy之Array合并\"></a>6.NumPy之Array合并</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">B=np.array([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\">print(np.vstack((A,B)))#上下合并</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.hstack((A,B)))#左右合并</span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n<p>增加维度<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">print(A.shape)</span><br><span class=\"line\"><span class=\"comment\">#(3,)</span></span><br><span class=\"line\">print(A[np.newaxis,:])</span><br><span class=\"line\"><span class=\"comment\">#[[1 1 1]]</span></span><br><span class=\"line\">print(A[np.newaxis,:].shape)<span class=\"comment\">#newaxis增加维度</span></span><br><span class=\"line\"><span class=\"comment\">#(1,3)</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(A[:,np.newaxis])</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(A[:,np.newaxis].shape)</span><br><span class=\"line\"><span class=\"comment\">#（3,1）</span></span><br></pre></td></tr></table></figure></p>\n<p>多矩阵合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])[:,np.newaxis]</span><br><span class=\"line\">B = np.array([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])[:,np.newaxis]</span><br><span class=\"line\">print(np.concatenate((A,B,B,A),axis=<span class=\"number\">0</span>))<span class=\"comment\">#0表示上下合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(np.concatenate((A,B,B,A),axis=<span class=\"number\">1</span>))<span class=\"comment\">#1表示左右合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1 2 2 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 2 2 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 2 2 1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-NumPy分割\"><a href=\"#7-NumPy分割\" class=\"headerlink\" title=\"7.NumPy分割\"></a>7.NumPy分割</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[ <span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>]</span><br><span class=\"line\"> [ <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>  <span class=\"number\">7</span>]</span><br><span class=\"line\"> [ <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.split(A,<span class=\"number\">3</span>,axis=<span class=\"number\">0</span>))#横向分割成<span class=\"number\">3</span>部分 或者np.vsplit(A,<span class=\"number\">3</span>)</span><br><span class=\"line\">#[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]]), array([[<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>]]), array([[ <span class=\"number\">8</span>,  <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>]])]</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.split(A,<span class=\"number\">2</span>,axis=<span class=\"number\">1</span>))#竖向分割成<span class=\"number\">2</span>部分 或者np.hsplit(A,<span class=\"number\">2</span>)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">       [<span class=\"number\">8</span>, <span class=\"number\">9</span>]]), array([[ <span class=\"number\">2</span>,  <span class=\"number\">3</span>],</span><br><span class=\"line\">       [ <span class=\"number\">6</span>,  <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">10</span>, <span class=\"number\">11</span>]])]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"> </span><br><span class=\"line\">print(np.array_split(A,<span class=\"number\">3</span>,axis=<span class=\"number\">1</span>))#不等量分割成<span class=\"number\">3</span>部分</span><br><span class=\"line\">'''</span><br><span class=\"line\">[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">       [<span class=\"number\">8</span>, <span class=\"number\">9</span>]]), array([[ <span class=\"number\">2</span>],</span><br><span class=\"line\">       [ <span class=\"number\">6</span>],</span><br><span class=\"line\">       [<span class=\"number\">10</span>]]), array([[ <span class=\"number\">3</span>],</span><br><span class=\"line\">       [ <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">11</span>]])]</span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-NumPy中copy和deep-copy\"><a href=\"#8-NumPy中copy和deep-copy\" class=\"headerlink\" title=\"8.NumPy中copy和deep copy\"></a>8.NumPy中copy和deep copy</h3><p>‘=’赋值方式会带有关联性<br><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(a)</span></span></span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br><span class=\"line\">b=a</span><br><span class=\"line\">c=a</span><br><span class=\"line\">d=b</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(b is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(c is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(d is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">b</span>[<span class=\"number\">0</span>]=<span class=\"number\">5</span>#改变b的值，<span class=\"selector-tag\">a</span>,c,d同样会进行改变</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(a)</span></span></span><br><span class=\"line\">#[<span class=\"number\">5</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br></pre></td></tr></table></figure></p>\n<p>‘copy()’赋值方式没有关联性<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">4</span>)#deep copy</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\">b=a.copy()</span><br><span class=\"line\">a[<span class=\"number\">0</span>]=<span class=\"number\">5</span></span><br><span class=\"line\">print(b)#值并不发生改变</span><br><span class=\"line\">#[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-NumPy概述\"><a href=\"#1-NumPy概述\" class=\"headerlink\" title=\"1.NumPy概述\"></a>1.NumPy概述</h3><p>NumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：</p>\n<ol>\n<li>强大的N维数组对象Array</li>\n<li>成熟的函数库</li>\n<li>用于集成C/C++和Fortran代码的工具</li>\n<li>实用的线性代数、傅立叶变换和随机生成函数</li>\n</ol>\n<h3 id=\"2-NumPy安装\"><a href=\"#2-NumPy安装\" class=\"headerlink\" title=\"2.NumPy安装\"></a>2.NumPy安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip <span class=\"keyword\">install</span> numpy或pip3 <span class=\"keyword\">install</span> numpy</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-NumPy引入\"><a href=\"#3-NumPy引入\" class=\"headerlink\" title=\"3.NumPy引入\"></a>3.NumPy引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np<span class=\"comment\">#为了方便实用numpy 采用np简写</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-NumPy方法\"><a href=\"#4-NumPy方法\" class=\"headerlink\" title=\"4.NumPy方法\"></a>4.NumPy方法</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])#将列表转换为矩阵 并转换为int类型</span><br><span class=\"line\">print(array)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\"> [<span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure>\n<h4 id=\"4-1NumPy属性\"><a href=\"#4-1NumPy属性\" class=\"headerlink\" title=\"4.1NumPy属性\"></a>4.1NumPy属性</h4><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'array of dim:'</span>,array.ndim)</span></span>#矩阵的维度</span><br><span class=\"line\"><span class=\"selector-id\">#array</span> of dim:<span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'array of shape'</span>,array.shape)</span></span>#矩阵的行数和列数</span><br><span class=\"line\"><span class=\"selector-id\">#array</span> of shape:(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'number of size:'</span>,array.size)</span></span>#元素的个数</span><br><span class=\"line\"><span class=\"selector-id\">#number</span> of size:<span class=\"number\">6</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2NumPy创建Array\"><a href=\"#4-2NumPy创建Array\" class=\"headerlink\" title=\"4.2NumPy创建Array\"></a>4.2NumPy创建Array</h4><ul>\n<li>array:创建数组</li>\n<li>dtype:指定数据类型</li>\n<li>zeros:创建数据全为0</li>\n<li>ones:创建数据全为1</li>\n<li>empty:创建数据接近0</li>\n<li>arange:指定范围内创建数据</li>\n<li>linspace创建线段</li>\n</ul>\n<p>创建数组<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>指定数据dtype<br><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">a</span>=np.array([1,2,3],<span class=\"attribute\">dtype</span>=np.int)#指定为int类型</span><br><span class=\"line\"><span class=\"builtin-name\">print</span>(a.dtype)</span><br><span class=\"line\"><span class=\"comment\">#int 64</span></span><br><span class=\"line\"><span class=\"attribute\">b</span>=np.array([1,2,3],<span class=\"attribute\">dtype</span>=np.float)#指定为float类型</span><br><span class=\"line\"><span class=\"builtin-name\">print</span>(b.dtype)</span><br><span class=\"line\"><span class=\"comment\">#float 64</span></span><br></pre></td></tr></table></figure></p>\n<p>创建特定数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])#矩阵 <span class=\"number\">2</span>行<span class=\"number\">3</span>列</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\"> [<span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>创建全0数组<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.zeros((<span class=\"number\">2</span>,<span class=\"number\">3</span>))<span class=\"comment\">#数据全0 2行3列</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[0 0 0]</span></span><br><span class=\"line\"><span class=\"string\"> [0 0 0]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>创建全1数组 指定特定类型dtype<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.zeros((<span class=\"number\">2</span>,<span class=\"number\">3</span>),dtype=np.int)<span class=\"comment\">#数据全1 2行3列 同时指定类型</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1 1 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 1 1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>创建全空数组 每个值接近0<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>)<span class=\"comment\">#数据全为empty 3行4列</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]</span></span><br><span class=\"line\"><span class=\"string\"> [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>用array创建连续数组<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">2</span>)#<span class=\"number\">1</span>到<span class=\"number\">10</span>的数据 <span class=\"number\">2</span>步长</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">5</span> <span class=\"number\">7</span> <span class=\"number\">9</span>]</span><br></pre></td></tr></table></figure></p>\n<p>用reshape改变数据形状<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">6</span>).reshape(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[0 1 2]</span></span><br><span class=\"line\"><span class=\"string\"> [3 4 5]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>用linspace创建线段形数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.linspace(<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">20</span>)#开始端<span class=\"number\">1</span> 结束端<span class=\"number\">5</span> 分割成<span class=\"number\">10</span>个数据 生成线段</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[ <span class=\"number\">1.</span>          <span class=\"number\">1.44444444</span>  <span class=\"number\">1.88888889</span>  <span class=\"number\">2.33333333</span>  <span class=\"number\">2.77777778</span>  <span class=\"number\">3.22222222</span></span><br><span class=\"line\">  <span class=\"number\">3.66666667</span>  <span class=\"number\">4.11111111</span>  <span class=\"number\">4.55555556</span>  <span class=\"number\">5.</span>        ]</span><br><span class=\"line\">  '''</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-3NumPy基础运算\"><a href=\"#4-3NumPy基础运算\" class=\"headerlink\" title=\"4.3NumPy基础运算\"></a>4.3NumPy基础运算</h4><p>基础运算之加、减、三角函数等<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([<span class=\"number\">10</span>,<span class=\"number\">20</span>,<span class=\"number\">30</span>,<span class=\"number\">40</span>])</span><br><span class=\"line\">b=np.arange(<span class=\"number\">4</span>) #array[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=a+b#加法运算</span><br><span class=\"line\">print(c)</span><br><span class=\"line\">#[<span class=\"number\">10</span>,<span class=\"number\">21</span>,<span class=\"number\">32</span>,<span class=\"number\">43</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=a-b#减法运算</span><br><span class=\"line\">print(c)</span><br><span class=\"line\">#[<span class=\"number\">10.19</span>,<span class=\"number\">28</span>,<span class=\"number\">37</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=<span class=\"number\">10</span>*np.sin(a)#三角函数运算</span><br><span class=\"line\">#[<span class=\"number\">-5.44021111</span>,  <span class=\"number\">9.12945251</span>, <span class=\"number\">-9.88031624</span>,  <span class=\"number\">7.4511316</span> ]</span><br><span class=\"line\"></span><br><span class=\"line\">print(b&lt;<span class=\"number\">3</span>)#逻辑判断</span><br><span class=\"line\">#[ True  True  True False]</span><br><span class=\"line\"></span><br><span class=\"line\">d=np.random.random((<span class=\"number\">2</span>,<span class=\"number\">3</span>))#随机生成<span class=\"number\">2</span>行<span class=\"number\">3</span>列的矩阵</span><br><span class=\"line\">print(d)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[ <span class=\"number\">0.21116981</span>  <span class=\"number\">0.0804489</span>   <span class=\"number\">0.51855475</span>]</span><br><span class=\"line\"> [ <span class=\"number\">0.38359164</span>  <span class=\"number\">0.55852973</span>  <span class=\"number\">0.73218811</span>]]</span><br><span class=\"line\">'''</span><br><span class=\"line\">print(np.sum(d))#元素求和</span><br><span class=\"line\">#<span class=\"number\">2.48448292958</span></span><br><span class=\"line\">print(np.max(d))#元素求最大值</span><br><span class=\"line\">#<span class=\"number\">0.732188108709</span></span><br><span class=\"line\">print(np.min(d))#元素求最小值</span><br><span class=\"line\">#<span class=\"number\">0.0804488978886</span></span><br></pre></td></tr></table></figure></p>\n<p>多维矩阵运算<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>]])</span><br><span class=\"line\">b=np.arange(<span class=\"number\">4</span>).reshape((<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">c=np.dot(a,b)<span class=\"comment\">#或c=a.dot(b)矩阵运算</span></span><br><span class=\"line\">print(c)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[2 4]</span></span><br><span class=\"line\"><span class=\"string\"> [2 3]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>对行或列执行查找运算<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>],[<span class=\"number\">3</span>,<span class=\"number\">4</span>]])</span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1,2]</span></span><br><span class=\"line\"><span class=\"string\"> [3,4]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(np.max(a,axis=<span class=\"number\">0</span>))<span class=\"comment\">#axis=0时是对列进行操作</span></span><br><span class=\"line\"><span class=\"comment\">#[3,4]</span></span><br><span class=\"line\">print(np.min(a,axis=<span class=\"number\">1</span>))<span class=\"comment\">#axis=1是对行进行操作</span></span><br><span class=\"line\"><span class=\"comment\">#[1,3]</span></span><br></pre></td></tr></table></figure></p>\n<p>矩阵索引操作<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">2</span>,<span class=\"number\">14</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]</span><br><span class=\"line\"> [<span class=\"number\">10</span>,<span class=\"number\">11</span>,<span class=\"number\">12</span>,<span class=\"number\">13</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.argmax(A))#矩阵中最大元素的索引</span><br><span class=\"line\">#<span class=\"number\">11</span></span><br><span class=\"line\">print(np.argmin(A))#矩阵中最小元素的索引</span><br><span class=\"line\">#<span class=\"number\">0</span></span><br><span class=\"line\">print(np.mean(A))#或者np.average(A)求解矩阵均值</span><br><span class=\"line\">#<span class=\"number\">7.5</span></span><br><span class=\"line\">print(np.cumsum(A))#矩阵累加函数</span><br><span class=\"line\">#[<span class=\"number\">2</span> <span class=\"number\">5</span> <span class=\"number\">9</span> <span class=\"number\">14</span> <span class=\"number\">20</span> <span class=\"number\">27</span> <span class=\"number\">35</span> <span class=\"number\">44</span> <span class=\"number\">54</span> <span class=\"number\">65</span> <span class=\"number\">77</span> <span class=\"number\">90</span>]</span><br><span class=\"line\">print(np.diff(A))#矩阵累差函数</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.nonzero(A))#将非<span class=\"number\">0</span>元素的行与列坐标分割开来</span><br><span class=\"line\">#(array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]), array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]))</span><br></pre></td></tr></table></figure></p>\n<p>矩阵排序、转置、替换操作<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">14</span>,<span class=\"number\">2</span>,<span class=\"number\">-1</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">14</span> <span class=\"number\">13</span> <span class=\"number\">12</span> <span class=\"number\">11</span>]</span><br><span class=\"line\"> [<span class=\"number\">10</span>  <span class=\"number\">9</span>  <span class=\"number\">8</span>  <span class=\"number\">7</span>]</span><br><span class=\"line\"> [ <span class=\"number\">6</span>  <span class=\"number\">5</span>  <span class=\"number\">4</span>  <span class=\"number\">3</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.sort(A))#排序</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">11</span> <span class=\"number\">12</span> <span class=\"number\">13</span> <span class=\"number\">14</span>]</span><br><span class=\"line\"> [ <span class=\"number\">7</span>  <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span>]</span><br><span class=\"line\"> [ <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.transpose(A))</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">14</span> <span class=\"number\">10</span>  <span class=\"number\">6</span>]</span><br><span class=\"line\"> [<span class=\"number\">13</span>  <span class=\"number\">9</span>  <span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">12</span>  <span class=\"number\">8</span>  <span class=\"number\">4</span>]</span><br><span class=\"line\"> [<span class=\"number\">11</span>  <span class=\"number\">7</span>  <span class=\"number\">3</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.clip(A,<span class=\"number\">5</span>,<span class=\"number\">9</span>))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">9</span>]</span><br><span class=\"line\"> [<span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">8</span> <span class=\"number\">7</span>]</span><br><span class=\"line\"> [<span class=\"number\">6</span> <span class=\"number\">5</span> <span class=\"number\">5</span> <span class=\"number\">5</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-索引\"><a href=\"#5-索引\" class=\"headerlink\" title=\"5.索引\"></a>5.索引</h3><p>一维索引<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>)</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">#[ <span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>  <span class=\"number\">7</span>  <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]</span><br><span class=\"line\">print(A[<span class=\"number\">1</span>])#一维索引</span><br><span class=\"line\">#<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A[<span class=\"number\">0</span>])</span><br><span class=\"line\">#[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>二维索引<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[ 0  1  2  3]</span></span><br><span class=\"line\"><span class=\"string\"> [ 4  5  6  7]</span></span><br><span class=\"line\"><span class=\"string\"> [ 8  9 10 11]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(A[<span class=\"number\">1</span>][<span class=\"number\">1</span>])<span class=\"comment\">#或者A[1,1]</span></span><br><span class=\"line\"><span class=\"comment\">#5</span></span><br><span class=\"line\">print(A[<span class=\"number\">1</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>])<span class=\"comment\">#切片处理</span></span><br><span class=\"line\"><span class=\"comment\">#[5,6]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">    print(A)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 1 2 3]</span></span><br><span class=\"line\"><span class=\"string\">[4 5 6 7]</span></span><br><span class=\"line\"><span class=\"string\">[ 8  9 10 11]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> col <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">    print(col)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 4 8]</span></span><br><span class=\"line\"><span class=\"string\">[1 5 9]</span></span><br><span class=\"line\"><span class=\"string\">[ 2  6 10]</span></span><br><span class=\"line\"><span class=\"string\">[ 3  7 11]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> A.flat:</span><br><span class=\"line\">    print(item)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">10</span></span><br><span class=\"line\"><span class=\"string\">11</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-NumPy之Array合并\"><a href=\"#6-NumPy之Array合并\" class=\"headerlink\" title=\"6.NumPy之Array合并\"></a>6.NumPy之Array合并</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">B=np.array([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\">print(np.vstack((A,B)))#上下合并</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.hstack((A,B)))#左右合并</span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n<p>增加维度<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">print(A.shape)</span><br><span class=\"line\"><span class=\"comment\">#(3,)</span></span><br><span class=\"line\">print(A[np.newaxis,:])</span><br><span class=\"line\"><span class=\"comment\">#[[1 1 1]]</span></span><br><span class=\"line\">print(A[np.newaxis,:].shape)<span class=\"comment\">#newaxis增加维度</span></span><br><span class=\"line\"><span class=\"comment\">#(1,3)</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(A[:,np.newaxis])</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(A[:,np.newaxis].shape)</span><br><span class=\"line\"><span class=\"comment\">#（3,1）</span></span><br></pre></td></tr></table></figure></p>\n<p>多矩阵合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])[:,np.newaxis]</span><br><span class=\"line\">B = np.array([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])[:,np.newaxis]</span><br><span class=\"line\">print(np.concatenate((A,B,B,A),axis=<span class=\"number\">0</span>))<span class=\"comment\">#0表示上下合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(np.concatenate((A,B,B,A),axis=<span class=\"number\">1</span>))<span class=\"comment\">#1表示左右合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1 2 2 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 2 2 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 2 2 1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-NumPy分割\"><a href=\"#7-NumPy分割\" class=\"headerlink\" title=\"7.NumPy分割\"></a>7.NumPy分割</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[ <span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>]</span><br><span class=\"line\"> [ <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>  <span class=\"number\">7</span>]</span><br><span class=\"line\"> [ <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.split(A,<span class=\"number\">3</span>,axis=<span class=\"number\">0</span>))#横向分割成<span class=\"number\">3</span>部分 或者np.vsplit(A,<span class=\"number\">3</span>)</span><br><span class=\"line\">#[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]]), array([[<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>]]), array([[ <span class=\"number\">8</span>,  <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>]])]</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.split(A,<span class=\"number\">2</span>,axis=<span class=\"number\">1</span>))#竖向分割成<span class=\"number\">2</span>部分 或者np.hsplit(A,<span class=\"number\">2</span>)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">       [<span class=\"number\">8</span>, <span class=\"number\">9</span>]]), array([[ <span class=\"number\">2</span>,  <span class=\"number\">3</span>],</span><br><span class=\"line\">       [ <span class=\"number\">6</span>,  <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">10</span>, <span class=\"number\">11</span>]])]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"> </span><br><span class=\"line\">print(np.array_split(A,<span class=\"number\">3</span>,axis=<span class=\"number\">1</span>))#不等量分割成<span class=\"number\">3</span>部分</span><br><span class=\"line\">'''</span><br><span class=\"line\">[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">       [<span class=\"number\">8</span>, <span class=\"number\">9</span>]]), array([[ <span class=\"number\">2</span>],</span><br><span class=\"line\">       [ <span class=\"number\">6</span>],</span><br><span class=\"line\">       [<span class=\"number\">10</span>]]), array([[ <span class=\"number\">3</span>],</span><br><span class=\"line\">       [ <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">11</span>]])]</span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-NumPy中copy和deep-copy\"><a href=\"#8-NumPy中copy和deep-copy\" class=\"headerlink\" title=\"8.NumPy中copy和deep copy\"></a>8.NumPy中copy和deep copy</h3><p>‘=’赋值方式会带有关联性<br><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(a)</span></span></span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br><span class=\"line\">b=a</span><br><span class=\"line\">c=a</span><br><span class=\"line\">d=b</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(b is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(c is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(d is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">b</span>[<span class=\"number\">0</span>]=<span class=\"number\">5</span>#改变b的值，<span class=\"selector-tag\">a</span>,c,d同样会进行改变</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(a)</span></span></span><br><span class=\"line\">#[<span class=\"number\">5</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br></pre></td></tr></table></figure></p>\n<p>‘copy()’赋值方式没有关联性<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">4</span>)#deep copy</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\">b=a.copy()</span><br><span class=\"line\">a[<span class=\"number\">0</span>]=<span class=\"number\">5</span></span><br><span class=\"line\">print(b)#值并不发生改变</span><br><span class=\"line\">#[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"Out of Gas and Driving on E","date":"2018-06-25T07:04:22.000Z","comments":1,"_content":"\n> 美国数学建模参赛论文(获得一等奖)\n\n### 1.Summary\n\nWith the rapid increase of electric vehicle, the development mode of charging station has attracted all circles attention. In this paper, we aim to construct a model to find a more reasonable network of charging stations. \n\nFirst, according the time series prediction model of the three indexes, the estimated electric vehicle proportion reached 80% in 2034. Therefore, Tesla is on track to allow a complete switch to all-electric in the US. The queuing theory model is applied to obtain the peak flow rate of charging station, and the quantity planning model of charging pile is established. There are 67,000 super charging stations and 114,000 destination charging piles. With the asymmetric Nash negotiation model, considering the regional feature factors, the share of urban, suburban and rural charging stations was 60.55%, 27.81% and 11.64%, with the asymmetric Nash negotiation model. \n\nSecond , for super charging pile location problem based on the minimum circle and Voronoi map algorithm of maximum coverage model, combined with site location model in the Floyd the short-circuit algorithm, using the Python programming for Ireland charging pile distribution throughout the country. Then, according to the operation analysis model of charging pile, the proposal of investment charging pile is put forward.Finally, we analyze the elasticity coefficient relationship between the growth rate of automobile and GDP growth rate, and introduce Bass model to propose the Irish electric vehicle timeline. \n\nThird, it proposed to revise the cosine similarity function to determine whether network of charging stations still adapt to other countries. Then we use the characteristic factors of each region to establish the DAGSVM multi-category classification system.Finally, we give development and investment advice to different categories of countries. \n\nFourth, we analyzed the impact of emerging technologies on the development model of electric vehicles from factors such as technical economy, environmental pollution, infrastructure and geographical conditions. \n\nFinally, we construct the sensitivity analysis of the prediction model and use the ROC curve to test the reliability of the DAGSVM classification model.The analysis shows that the model is feasible and reasonable which can accommodate to various situations \n\nKey words:Time Series Model, Asymmetrical Nash Negotiation Model, Maximum Coverage Model, DAGSVM Classifier. \n\n![0002](Out-of-Gas-and-Driving-on-E/0002.jpg)\n![0003](Out-of-Gas-and-Driving-on-E/0003.jpg)\n![0004](Out-of-Gas-and-Driving-on-E/0004.jpg)\n![0005](Out-of-Gas-and-Driving-on-E/0005.jpg)\n![0006](Out-of-Gas-and-Driving-on-E/0006.jpg)\n![0007](Out-of-Gas-and-Driving-on-E/0007.jpg)\n![0008](Out-of-Gas-and-Driving-on-E/0008.jpg)\n![0009](Out-of-Gas-and-Driving-on-E/0009.jpg)\n![0010](Out-of-Gas-and-Driving-on-E/0010.jpg)\n![0011](Out-of-Gas-and-Driving-on-E/0011.jpg)\n![0012](Out-of-Gas-and-Driving-on-E/0012.jpg)\n![0013](Out-of-Gas-and-Driving-on-E/0013.jpg)\n![0014](Out-of-Gas-and-Driving-on-E/0014.jpg)\n![0015](Out-of-Gas-and-Driving-on-E/0015.jpg)\n![0016](Out-of-Gas-and-Driving-on-E/0016.jpg)\n![0017](Out-of-Gas-and-Driving-on-E/0017.jpg)\n![0018](Out-of-Gas-and-Driving-on-E/0018.jpg)\n![0019](Out-of-Gas-and-Driving-on-E/0019.jpg)\n![0020](Out-of-Gas-and-Driving-on-E/0020.jpg)\n![0021](Out-of-Gas-and-Driving-on-E/0021.jpg)\n![0022](Out-of-Gas-and-Driving-on-E/0022.jpg)\n![0023](Out-of-Gas-and-Driving-on-E/0023.jpg)","source":"_posts/Out-of-Gas-and-Driving-on-E.md","raw":"---\ntitle: Out of Gas and Driving on E\ndate: 2018-06-25 15:04:22\ntags: [数学建模]\ncategories: 比赛\ncomments: true\n---\n\n> 美国数学建模参赛论文(获得一等奖)\n\n### 1.Summary\n\nWith the rapid increase of electric vehicle, the development mode of charging station has attracted all circles attention. In this paper, we aim to construct a model to find a more reasonable network of charging stations. \n\nFirst, according the time series prediction model of the three indexes, the estimated electric vehicle proportion reached 80% in 2034. Therefore, Tesla is on track to allow a complete switch to all-electric in the US. The queuing theory model is applied to obtain the peak flow rate of charging station, and the quantity planning model of charging pile is established. There are 67,000 super charging stations and 114,000 destination charging piles. With the asymmetric Nash negotiation model, considering the regional feature factors, the share of urban, suburban and rural charging stations was 60.55%, 27.81% and 11.64%, with the asymmetric Nash negotiation model. \n\nSecond , for super charging pile location problem based on the minimum circle and Voronoi map algorithm of maximum coverage model, combined with site location model in the Floyd the short-circuit algorithm, using the Python programming for Ireland charging pile distribution throughout the country. Then, according to the operation analysis model of charging pile, the proposal of investment charging pile is put forward.Finally, we analyze the elasticity coefficient relationship between the growth rate of automobile and GDP growth rate, and introduce Bass model to propose the Irish electric vehicle timeline. \n\nThird, it proposed to revise the cosine similarity function to determine whether network of charging stations still adapt to other countries. Then we use the characteristic factors of each region to establish the DAGSVM multi-category classification system.Finally, we give development and investment advice to different categories of countries. \n\nFourth, we analyzed the impact of emerging technologies on the development model of electric vehicles from factors such as technical economy, environmental pollution, infrastructure and geographical conditions. \n\nFinally, we construct the sensitivity analysis of the prediction model and use the ROC curve to test the reliability of the DAGSVM classification model.The analysis shows that the model is feasible and reasonable which can accommodate to various situations \n\nKey words:Time Series Model, Asymmetrical Nash Negotiation Model, Maximum Coverage Model, DAGSVM Classifier. \n\n![0002](Out-of-Gas-and-Driving-on-E/0002.jpg)\n![0003](Out-of-Gas-and-Driving-on-E/0003.jpg)\n![0004](Out-of-Gas-and-Driving-on-E/0004.jpg)\n![0005](Out-of-Gas-and-Driving-on-E/0005.jpg)\n![0006](Out-of-Gas-and-Driving-on-E/0006.jpg)\n![0007](Out-of-Gas-and-Driving-on-E/0007.jpg)\n![0008](Out-of-Gas-and-Driving-on-E/0008.jpg)\n![0009](Out-of-Gas-and-Driving-on-E/0009.jpg)\n![0010](Out-of-Gas-and-Driving-on-E/0010.jpg)\n![0011](Out-of-Gas-and-Driving-on-E/0011.jpg)\n![0012](Out-of-Gas-and-Driving-on-E/0012.jpg)\n![0013](Out-of-Gas-and-Driving-on-E/0013.jpg)\n![0014](Out-of-Gas-and-Driving-on-E/0014.jpg)\n![0015](Out-of-Gas-and-Driving-on-E/0015.jpg)\n![0016](Out-of-Gas-and-Driving-on-E/0016.jpg)\n![0017](Out-of-Gas-and-Driving-on-E/0017.jpg)\n![0018](Out-of-Gas-and-Driving-on-E/0018.jpg)\n![0019](Out-of-Gas-and-Driving-on-E/0019.jpg)\n![0020](Out-of-Gas-and-Driving-on-E/0020.jpg)\n![0021](Out-of-Gas-and-Driving-on-E/0021.jpg)\n![0022](Out-of-Gas-and-Driving-on-E/0022.jpg)\n![0023](Out-of-Gas-and-Driving-on-E/0023.jpg)","slug":"Out-of-Gas-and-Driving-on-E","published":1,"updated":"2018-06-26T03:06:53.132Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3e000djiz5pfkh6p5j","content":"<blockquote>\n<p>美国数学建模参赛论文(获得一等奖)</p>\n</blockquote>\n<h3 id=\"1-Summary\"><a href=\"#1-Summary\" class=\"headerlink\" title=\"1.Summary\"></a>1.Summary</h3><p>With the rapid increase of electric vehicle, the development mode of charging station has attracted all circles attention. In this paper, we aim to construct a model to find a more reasonable network of charging stations. </p>\n<p>First, according the time series prediction model of the three indexes, the estimated electric vehicle proportion reached 80% in 2034. Therefore, Tesla is on track to allow a complete switch to all-electric in the US. The queuing theory model is applied to obtain the peak flow rate of charging station, and the quantity planning model of charging pile is established. There are 67,000 super charging stations and 114,000 destination charging piles. With the asymmetric Nash negotiation model, considering the regional feature factors, the share of urban, suburban and rural charging stations was 60.55%, 27.81% and 11.64%, with the asymmetric Nash negotiation model. </p>\n<p>Second , for super charging pile location problem based on the minimum circle and Voronoi map algorithm of maximum coverage model, combined with site location model in the Floyd the short-circuit algorithm, using the Python programming for Ireland charging pile distribution throughout the country. Then, according to the operation analysis model of charging pile, the proposal of investment charging pile is put forward.Finally, we analyze the elasticity coefficient relationship between the growth rate of automobile and GDP growth rate, and introduce Bass model to propose the Irish electric vehicle timeline. </p>\n<p>Third, it proposed to revise the cosine similarity function to determine whether network of charging stations still adapt to other countries. Then we use the characteristic factors of each region to establish the DAGSVM multi-category classification system.Finally, we give development and investment advice to different categories of countries. </p>\n<p>Fourth, we analyzed the impact of emerging technologies on the development model of electric vehicles from factors such as technical economy, environmental pollution, infrastructure and geographical conditions. </p>\n<p>Finally, we construct the sensitivity analysis of the prediction model and use the ROC curve to test the reliability of the DAGSVM classification model.The analysis shows that the model is feasible and reasonable which can accommodate to various situations </p>\n<p>Key words:Time Series Model, Asymmetrical Nash Negotiation Model, Maximum Coverage Model, DAGSVM Classifier. </p>\n<p><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0002.jpg\" alt=\"0002\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0003.jpg\" alt=\"0003\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0004.jpg\" alt=\"0004\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0005.jpg\" alt=\"0005\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0006.jpg\" alt=\"0006\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0007.jpg\" alt=\"0007\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0008.jpg\" alt=\"0008\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0009.jpg\" alt=\"0009\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0010.jpg\" alt=\"0010\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0011.jpg\" alt=\"0011\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0012.jpg\" alt=\"0012\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0013.jpg\" alt=\"0013\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0014.jpg\" alt=\"0014\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0015.jpg\" alt=\"0015\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0016.jpg\" alt=\"0016\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0017.jpg\" alt=\"0017\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0018.jpg\" alt=\"0018\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0019.jpg\" alt=\"0019\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0020.jpg\" alt=\"0020\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0021.jpg\" alt=\"0021\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0022.jpg\" alt=\"0022\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0023.jpg\" alt=\"0023\"></p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>美国数学建模参赛论文(获得一等奖)</p>\n</blockquote>\n<h3 id=\"1-Summary\"><a href=\"#1-Summary\" class=\"headerlink\" title=\"1.Summary\"></a>1.Summary</h3><p>With the rapid increase of electric vehicle, the development mode of charging station has attracted all circles attention. In this paper, we aim to construct a model to find a more reasonable network of charging stations. </p>\n<p>First, according the time series prediction model of the three indexes, the estimated electric vehicle proportion reached 80% in 2034. Therefore, Tesla is on track to allow a complete switch to all-electric in the US. The queuing theory model is applied to obtain the peak flow rate of charging station, and the quantity planning model of charging pile is established. There are 67,000 super charging stations and 114,000 destination charging piles. With the asymmetric Nash negotiation model, considering the regional feature factors, the share of urban, suburban and rural charging stations was 60.55%, 27.81% and 11.64%, with the asymmetric Nash negotiation model. </p>\n<p>Second , for super charging pile location problem based on the minimum circle and Voronoi map algorithm of maximum coverage model, combined with site location model in the Floyd the short-circuit algorithm, using the Python programming for Ireland charging pile distribution throughout the country. Then, according to the operation analysis model of charging pile, the proposal of investment charging pile is put forward.Finally, we analyze the elasticity coefficient relationship between the growth rate of automobile and GDP growth rate, and introduce Bass model to propose the Irish electric vehicle timeline. </p>\n<p>Third, it proposed to revise the cosine similarity function to determine whether network of charging stations still adapt to other countries. Then we use the characteristic factors of each region to establish the DAGSVM multi-category classification system.Finally, we give development and investment advice to different categories of countries. </p>\n<p>Fourth, we analyzed the impact of emerging technologies on the development model of electric vehicles from factors such as technical economy, environmental pollution, infrastructure and geographical conditions. </p>\n<p>Finally, we construct the sensitivity analysis of the prediction model and use the ROC curve to test the reliability of the DAGSVM classification model.The analysis shows that the model is feasible and reasonable which can accommodate to various situations </p>\n<p>Key words:Time Series Model, Asymmetrical Nash Negotiation Model, Maximum Coverage Model, DAGSVM Classifier. </p>\n<p><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0002.jpg\" alt=\"0002\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0003.jpg\" alt=\"0003\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0004.jpg\" alt=\"0004\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0005.jpg\" alt=\"0005\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0006.jpg\" alt=\"0006\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0007.jpg\" alt=\"0007\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0008.jpg\" alt=\"0008\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0009.jpg\" alt=\"0009\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0010.jpg\" alt=\"0010\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0011.jpg\" alt=\"0011\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0012.jpg\" alt=\"0012\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0013.jpg\" alt=\"0013\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0014.jpg\" alt=\"0014\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0015.jpg\" alt=\"0015\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0016.jpg\" alt=\"0016\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0017.jpg\" alt=\"0017\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0018.jpg\" alt=\"0018\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0019.jpg\" alt=\"0019\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0020.jpg\" alt=\"0020\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0021.jpg\" alt=\"0021\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0022.jpg\" alt=\"0022\"><br><img src=\"/2018/06/25/Out-of-Gas-and-Driving-on-E/0023.jpg\" alt=\"0023\"></p>\n"},{"title":"Python之Pandas使用教程","date":"2018-03-12T05:18:48.000Z","toc":true,"comments":1,"_content":"### 1.Pandas概述\n\n  1. Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。\n  2. Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。\n  3. Pandas提供大量能使我们快速便捷地处理数据的函数和方法。\n  4. Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。   \n\n\n### 2.Pandas安装\n\n\n\n```\npip3 install pandas\n```\n### 3.Pandas引入\n```\nimport pandas as pd#为了方便实用pandas 采用pd简写\n```\n### 4.Pandas数据结构\n#### 4.1Series\n```\nimport numpy as np\nimport pandas as pd\ns=pd.Series([1,2,3,np.nan,5,6])\nprint(s)#索引在左边 值在右边\n'''\n0    1.0\n1    2.0\n2    3.0\n3    NaN\n4    5.0\n5    6.0\ndtype: float64\n '''\n```\n#### 4.2DataFrame\nDataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。\n```\ndates=pd.date_range('20180310',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置\nprint(df)#输出6行4列的表格\n'''\n                   A         B         C         D\n2018-03-10 -0.092889 -0.503172  0.692763 -1.261313\n2018-03-11 -0.895628 -2.300249 -1.098069  0.468986\n2018-03-12  0.084732 -1.275078  1.638007 -0.291145\n2018-03-13 -0.561528  0.431088  0.430414  1.065939\n2018-03-14  1.485434 -0.341404  0.267613 -1.493366\n2018-03-15 -1.671474  0.110933  1.688264 -0.910599\n  '''\nprint(df['B'])\n'''\n2018-03-10   -0.927291\n2018-03-11   -0.406842\n2018-03-12   -0.088316\n2018-03-13   -1.631055\n2018-03-14   -0.929926\n2018-03-15   -0.010904\nFreq: D, Name: B, dtype: float64\n '''\n\n#创建特定数据的DataFrame\ndf_1=pd.DataFrame({'A' : 1.,\n                    'B' : pd.Timestamp('20180310'),\n                    'C' : pd.Series(1,index=list(range(4)),dtype='float32'),\n                    'D' : np.array([3] * 4,dtype='int32'),\n                    'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]),\n                    'F' : 'foo'\n                    })\nprint(df_1)\n'''\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n'''\nprint(df_1.dtypes)\n'''\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n'''\nprint(df_1.index)#行的序号\n#Int64Index([0, 1, 2, 3], dtype='int64')\nprint(df_1.columns)#列的序号名字\n#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')\nprint(df_1.values)#把每个值进行打印出来\n'''\n[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']]\n '''\nprint(df_1.describe())#数字总结\n'''\n         A    C    D\ncount  4.0  4.0  4.0\nmean   1.0  1.0  3.0\nstd    0.0  0.0  0.0\nmin    1.0  1.0  3.0\n25%    1.0  1.0  3.0\n50%    1.0  1.0  3.0\n75%    1.0  1.0  3.0\nmax    1.0  1.0  3.0\n'''\nprint(df_1.T)#翻转数据\n'''\n                     0                    1                    2  \\\nA                    1                    1                    1   \nB  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   \nC                    1                    1                    1   \nD                    3                    3                    3   \nE                 test                train                 test   \nF                  foo                  foo                  foo   \n\n                     3  \nA                    1  \nB  2018-03-10 00:00:00  \nC                    1  \nD                    3  \nE                train  \nF                  foo  \n'''\nprint(df_1.sort_index(axis=1, ascending=False))#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示\n'''\n     F      E  D    C          B    A\n0  foo   test  3  1.0 2018-03-10  1.0\n1  foo  train  3  1.0 2018-03-10  1.0\n2  foo   test  3  1.0 2018-03-10  1.0\n3  foo  train  3  1.0 2018-03-10  1.0\n'''\nprint(df_1.sort_values(by='E'))#按值进行排序\n'''\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n'''\n```\n###5.Pandas选择数据\n```\ndates=pd.date_range('20180310',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置\nprint(df)\n'''\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n2018-03-15 -0.188331 -0.578581 -0.845854 -0.056373\n '''\nprint(df['A'])#或者df.A 选择某列\n'''\n2018-03-10   -0.520509\n2018-03-11    0.332656\n2018-03-12    0.499960\n2018-03-13    0.540385\n2018-03-14    0.191962\n2018-03-15   -0.188331\n'''\n```\n切片选择\n```\nprint(df[0:3], df['20180310':'20180314'])#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择\n'''\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465                    \n                  A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n '''\n```\n根据标签loc-行标签进行选择数据\n```\nprint(df.loc['20180312', ['A','B']])#按照行标签进行选择 精确选择\n '''\nA    0.499960\nB    1.576897\nName: 2018-03-12 00:00:00, dtype: float64\n'''\n```\n根据序列iloc-行号进行选择数据\n```\nprint(df.iloc[3, 1])#输出第三行第一列的数据\n#0.427336827399\n\nprint(df.iloc[3:5,0:2])#进行切片选择\n '''\n                   A         B\n2018-03-13  0.540385  0.427337\n2018-03-14  0.191962  1.237843\n '''\n\nprint(df.iloc[[1,2,4],[0,2]])#进行不连续筛选\n'''\n                   A         C\n2018-03-11  0.332656  0.382384\n2018-03-12  0.499960  2.128730\n2018-03-14  0.191962  1.903370\n '''\n```\n根据混合的两种ix\n```\nprint(df.ix[:3, ['A', 'C']])\n'''\n                   A         C\n2018-03-10 -0.919275 -1.356037\n2018-03-11  0.010171 -0.380010\n2018-03-12  0.285251 -1.174265\n '''\n```\n\n根据判断筛选\n```\nprint(df[df.A > 0])#筛选出df.A大于0的元素 布尔条件筛选\n'''\n                   A         B         C         D\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n '''\n```\n### 6.Pandas设置数据\n根据loc和iloc设置\n```\ndates = pd.date_range('20180310', periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])\nprint(df)\n'''\n             A   B     C   D\n2018-03-10   0   1     2   3\n2018-03-11   4   5     6   7\n2018-03-12   8   9  1111  11\n2018-03-13  12  13    14  15\n2018-03-14  16  17    18  19\n2018-03-15  20  21    22  23\n'''\n\ndf.iloc[2,2] = 999#单点设置\ndf.loc['2018-03-13', 'D'] = 999\nprint(df)\n'''\n            A   B    C    D\n2018-03-10  0   1    2    3\n2018-03-11  0   5    6    7\n2018-03-12  0   9  999   11\n2018-03-13  0  13   14  999\n2018-03-14  0  17   18   19\n2018-03-15  0  21   22   23\n'''\n```\n根据条件设置\n```\ndf[df.A>0]=999#将df.A大于0的值改变\nprint(df)\n'''\n              A   B    C    D\n2018-03-10    0   1    2    3\n2018-03-11  999   5    6    7\n2018-03-12  999   9  999   11\n2018-03-13  999  13   14  999\n2018-03-14  999  17   18   19\n2018-03-15  999  21   22   23\n '''\n```\n根据行或列设置\n```\ndf['F']=np.nan\nprint(df)\n'''\n              A   B    C   D\n2018-03-10    0   1    2 NaN\n2018-03-11  999   5    6 NaN\n2018-03-12  999   9  999 NaN\n2018-03-13  999  13   14 NaN\n2018-03-14  999  17   18 NaN\n2018-03-15  999  21   22 NaN\n '''\n```\n添加数据\n```\ndf['E']  = pd.Series([1,2,3,4,5,6], index=pd.date_range('20180313', periods=6))#增加一列\nprint(df)\n'''\n              A   B    C   D    E\n2018-03-10    0   1    2 NaN  NaN\n2018-03-11  999   5    6 NaN  NaN\n2018-03-12  999   9  999 NaN  NaN\n2018-03-13  999  13   14 NaN  1.0\n2018-03-14  999  17   18 NaN  2.0\n2018-03-15  999  21   22 NaN  3.0\n'''\n```\n### 7.Pandas处理丢失数据\n处理数据中NaN数据\n```\ndates = pd.date_range('20180310', periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])\ndf.iloc[0,1]=np.nan\ndf.iloc[1,2]=np.nan\nprint(df)\n'''\n             A     B     C   D\n2018-03-10   0   NaN   2.0   3\n2018-03-11   4   5.0   NaN   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n'''\n```\n使用dropna（）函数去掉NaN的行或列\n```\nprint(df.dropna(axis=0,how='any'#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop\n'''\n             A     B     C   D\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n '''\n```\n使用fillna（）函数替换NaN值 \n```\nprint(df.fillna(value=0))#将NaN值替换为0\n'''\n             A     B     C   D\n2018-03-10   0   0.0   2.0   3\n2018-03-11   4   5.0   0.0   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n '''\n```\n使用isnull()函数判断数据是否丢失\n```\nprint(pd.isnull(df))#矩阵用布尔来进行表示 是nan为ture 不是nan为false\n'''\n                A      B      C      D\n2018-03-10  False   True  False  False\n2018-03-11  False  False   True  False\n2018-03-12  False  False  False  False\n2018-03-13  False  False  False  False\n2018-03-14  False  False  False  False\n2018-03-15  False  False  False  False\n '''\nprint(np.any(df.isnull()))#判断数据中是否会存在NaN值\n#True\n```\n### 8.Pandas导入导出\npandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看[官方资料](http://pandas.pydata.org/pandas-docs/stable/io.html)\n```\ndata=pd.read_csv('test1.csv')#读取csv文件\ndata.to_pickle('test2.pickle')#将资料存取成pickle文件 \n#其他文件导入导出方式相同\n```\n### 9.Pandas合并数据\naxis合并方向\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ndf3 = pd.DataFrame(np.ones((3,4))*2, columns=['a','b','c','d'])\nres = pd.concat([df1, df2, df3], axis=0, ignore_index=True)#0表示竖项合并 1表示横项合并 ingnore_index重置序列index index变为0 1 2 3 4 5 6 7 8\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n6  2.0  2.0  2.0  2.0\n7  2.0  2.0  2.0  2.0\n8  2.0  2.0  2.0  2.0\n '''\n```\njoin合并方式\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'], index=[1,2,3])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['b','c','d', 'e'], index=[2,3,4])\nprint(df1)\n'''\n     a    b    c    d\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n '''\nprint(df2)\n'''\n     b    c    d    e\n2  1.0  1.0  1.0  1.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n '''\nres=pd.concat([df1,df2],axis=1,join='outer')#行往外进行合并\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n4  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\n '''\n\nres=pd.concat([df1,df2],axis=1,join='outer')#行相同的进行合并\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n'''\n\nres=pd.concat([df1,df2],axis=1,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n'''\n```\nappend添加数据\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ndf3 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ns1 = pd.Series([1,2,3,4], index=['a','b','c','d'])\n\nres=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n'''\n\nres=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  2.0  3.0  4.0\n'''\n```\n### 10.Pandas合并merge\n依据一组key合并\n```\nleft = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                     'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3']})\nprint(left)\n'''\n    A   B key\n0  A0  B0  K0\n1  A1  B1  K1\n2  A2  B2  K2\n3  A3  B3  K3\n'''\nright = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                      'C': ['C0', 'C1', 'C2',  'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']})\nprint(right)\n'''\n    C   D key\n0  C0  D0  K0\n1  C1  D1  K1\n2  C2  D2  K2\n3  C3  D3  K3\n'''\nres=pd.merge(left,right,on='key')\nprint(res)\n'''\n    A   B key   C   D\n0  A0  B0  K0  C0  D0\n1  A1  B1  K1  C1  D1\n2  A2  B2  K2  C2  D2\n3  A3  B3  K3  C3  D3\n'''\n```\n依据两组key合并\n```\nleft = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n                             'key2': ['K0', 'K1', 'K0', 'K1'],\n                             'A': ['A0', 'A1', 'A2', 'A3'],\n                             'B': ['B0', 'B1', 'B2', 'B3']})\nprint(left)\n'''\n    A   B key1 key2\n0  A0  B0   K0   K0\n1  A1  B1   K0   K1\n2  A2  B2   K1   K0\n3  A3  B3   K2   K1\n '''\nright = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n                              'key2': ['K0', 'K0', 'K0', 'K0'],\n                              'C': ['C0', 'C1', 'C2', 'C3'],\n                              'D': ['D0', 'D1', 'D2', 'D3']})\nprint(right)\n'''\n    C   D key1 key2\n0  C0  D0   K0   K0\n1  C1  D1   K1   K0\n2  C2  D2   K1   K0\n3  C3  D3   K2   K0\n '''\n\nres=pd.merge(left,right,on=['key1','key2'],how='inner')#内联合并\nprint(res)\n'''\n    A   B key1 key2   C   D\n0  A0  B0   K0   K0  C0  D0\n1  A2  B2   K1   K0  C1  D1\n2  A2  B2   K1   K0  C2  D2\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='outer')#外联合并\nprint(res)\n'''\n     A    B key1 key2    C    D\n0   A0   B0   K0   K0   C0   D0\n1   A1   B1   K0   K1  NaN  NaN\n2   A2   B2   K1   K0   C1   D1\n3   A2   B2   K1   K0   C2   D2\n4   A3   B3   K2   K1  NaN  NaN\n5  NaN  NaN   K2   K0   C3   D3\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='left')#左联合并\n'''\n    A   B key1 key2    C    D\n0  A0  B0   K0   K0   C0   D0\n1  A1  B1   K0   K1  NaN  NaN\n2  A2  B2   K1   K0   C1   D1\n3  A2  B2   K1   K0   C2   D2\n4  A3  B3   K2   K1  NaN  NaN\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='right')#右联合并\nprint(res)\n'''\n     A    B key1 key2   C   D\n0   A0   B0   K0   K0  C0  D0\n1   A2   B2   K1   K0  C1  D1\n2   A2   B2   K1   K0  C2  D2\n3  NaN  NaN   K2   K0  C3  D3\n'''\n```\nIndicator合并\n```\ndf1 = pd.DataFrame({'col1':[0,1], 'col_left':['a','b']})\nprint(df1)\n'''\n   col1 col_left\n0     0        a\n1     1        b\n '''\ndf2 = pd.DataFrame({'col1':[1,2,2],'col_right':[2,2,2]})\nprint(df2)\n'''\n   col1  col_right\n0     1          2\n1     2          2\n2     2          2\n '''\n\nres=pd.merge(df1,df2,on='col1',how='outer',indicator=True)#依据col1进行合并 并启用indicator=True输出每项合并方式\nprint(res)\n'''\n   col1 col_left  col_right      _merge\n0     0        a        NaN   left_only\n1     1        b        2.0        both\n2     2      NaN        2.0  right_only\n3     2      NaN        2.0  right_only\n'''\n\nres = pd.merge(df1, df2, on='col1', how='outer', indicator='indicator_column')#自定义indicator column名称\nprint(res)\n'''\n   col1 col_left  col_right indicator_column\n0     0        a        NaN        left_only\n1     1        b        2.0             both\n2     2      NaN        2.0       right_only\n3     2      NaN        2.0       right_only\n'''\n```\n依据index合并\n```\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n                                  'B': ['B0', 'B1', 'B2']},\n                                  index=['K0', 'K1', 'K2'])\nprint(left)\n'''\n     A   B\nK0  A0  B0\nK1  A1  B1\nK2  A2  B2\n '''\nright = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n                                     'D': ['D0', 'D2', 'D3']},\n                                      index=['K0', 'K2', 'K3'])\nprint(right)\n'''\n     C   D\nK0  C0  D0\nK2  C2  D2\nK3  C3  D3\n'''\n\nres=pd.merge(left,right,left_index=True,right_index=True,how='outer')#根据index索引进行合并 并选择外联合并\nprint(res)\n'''\n      A    B    C    D\nK0   A0   B0   C0   D0\nK1   A1   B1  NaN  NaN\nK2   A2   B2   C2   D2\nK3  NaN  NaN   C3   D3\n'''\n\nres=pd.merge(left,right,left_index=True,right_index=True,how='inner')\nprint(res)\n'''\n     A   B   C   D\nK0  A0  B0  C0  D0\nK2  A2  B2  C2  D2\n'''\n```\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/Python之Pandas使用教程.md","raw":"---\ntitle: Python之Pandas使用教程\ndate: 2018-03-12 13:18:48\ntags: python\ntoc: true\ncategories: Python库\ncomments: true\n---\n### 1.Pandas概述\n\n  1. Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。\n  2. Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。\n  3. Pandas提供大量能使我们快速便捷地处理数据的函数和方法。\n  4. Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。   \n\n\n### 2.Pandas安装\n\n\n\n```\npip3 install pandas\n```\n### 3.Pandas引入\n```\nimport pandas as pd#为了方便实用pandas 采用pd简写\n```\n### 4.Pandas数据结构\n#### 4.1Series\n```\nimport numpy as np\nimport pandas as pd\ns=pd.Series([1,2,3,np.nan,5,6])\nprint(s)#索引在左边 值在右边\n'''\n0    1.0\n1    2.0\n2    3.0\n3    NaN\n4    5.0\n5    6.0\ndtype: float64\n '''\n```\n#### 4.2DataFrame\nDataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。\n```\ndates=pd.date_range('20180310',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置\nprint(df)#输出6行4列的表格\n'''\n                   A         B         C         D\n2018-03-10 -0.092889 -0.503172  0.692763 -1.261313\n2018-03-11 -0.895628 -2.300249 -1.098069  0.468986\n2018-03-12  0.084732 -1.275078  1.638007 -0.291145\n2018-03-13 -0.561528  0.431088  0.430414  1.065939\n2018-03-14  1.485434 -0.341404  0.267613 -1.493366\n2018-03-15 -1.671474  0.110933  1.688264 -0.910599\n  '''\nprint(df['B'])\n'''\n2018-03-10   -0.927291\n2018-03-11   -0.406842\n2018-03-12   -0.088316\n2018-03-13   -1.631055\n2018-03-14   -0.929926\n2018-03-15   -0.010904\nFreq: D, Name: B, dtype: float64\n '''\n\n#创建特定数据的DataFrame\ndf_1=pd.DataFrame({'A' : 1.,\n                    'B' : pd.Timestamp('20180310'),\n                    'C' : pd.Series(1,index=list(range(4)),dtype='float32'),\n                    'D' : np.array([3] * 4,dtype='int32'),\n                    'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]),\n                    'F' : 'foo'\n                    })\nprint(df_1)\n'''\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n'''\nprint(df_1.dtypes)\n'''\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n'''\nprint(df_1.index)#行的序号\n#Int64Index([0, 1, 2, 3], dtype='int64')\nprint(df_1.columns)#列的序号名字\n#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')\nprint(df_1.values)#把每个值进行打印出来\n'''\n[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']]\n '''\nprint(df_1.describe())#数字总结\n'''\n         A    C    D\ncount  4.0  4.0  4.0\nmean   1.0  1.0  3.0\nstd    0.0  0.0  0.0\nmin    1.0  1.0  3.0\n25%    1.0  1.0  3.0\n50%    1.0  1.0  3.0\n75%    1.0  1.0  3.0\nmax    1.0  1.0  3.0\n'''\nprint(df_1.T)#翻转数据\n'''\n                     0                    1                    2  \\\nA                    1                    1                    1   \nB  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   \nC                    1                    1                    1   \nD                    3                    3                    3   \nE                 test                train                 test   \nF                  foo                  foo                  foo   \n\n                     3  \nA                    1  \nB  2018-03-10 00:00:00  \nC                    1  \nD                    3  \nE                train  \nF                  foo  \n'''\nprint(df_1.sort_index(axis=1, ascending=False))#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示\n'''\n     F      E  D    C          B    A\n0  foo   test  3  1.0 2018-03-10  1.0\n1  foo  train  3  1.0 2018-03-10  1.0\n2  foo   test  3  1.0 2018-03-10  1.0\n3  foo  train  3  1.0 2018-03-10  1.0\n'''\nprint(df_1.sort_values(by='E'))#按值进行排序\n'''\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n'''\n```\n###5.Pandas选择数据\n```\ndates=pd.date_range('20180310',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置\nprint(df)\n'''\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n2018-03-15 -0.188331 -0.578581 -0.845854 -0.056373\n '''\nprint(df['A'])#或者df.A 选择某列\n'''\n2018-03-10   -0.520509\n2018-03-11    0.332656\n2018-03-12    0.499960\n2018-03-13    0.540385\n2018-03-14    0.191962\n2018-03-15   -0.188331\n'''\n```\n切片选择\n```\nprint(df[0:3], df['20180310':'20180314'])#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择\n'''\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465                    \n                  A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n '''\n```\n根据标签loc-行标签进行选择数据\n```\nprint(df.loc['20180312', ['A','B']])#按照行标签进行选择 精确选择\n '''\nA    0.499960\nB    1.576897\nName: 2018-03-12 00:00:00, dtype: float64\n'''\n```\n根据序列iloc-行号进行选择数据\n```\nprint(df.iloc[3, 1])#输出第三行第一列的数据\n#0.427336827399\n\nprint(df.iloc[3:5,0:2])#进行切片选择\n '''\n                   A         B\n2018-03-13  0.540385  0.427337\n2018-03-14  0.191962  1.237843\n '''\n\nprint(df.iloc[[1,2,4],[0,2]])#进行不连续筛选\n'''\n                   A         C\n2018-03-11  0.332656  0.382384\n2018-03-12  0.499960  2.128730\n2018-03-14  0.191962  1.903370\n '''\n```\n根据混合的两种ix\n```\nprint(df.ix[:3, ['A', 'C']])\n'''\n                   A         C\n2018-03-10 -0.919275 -1.356037\n2018-03-11  0.010171 -0.380010\n2018-03-12  0.285251 -1.174265\n '''\n```\n\n根据判断筛选\n```\nprint(df[df.A > 0])#筛选出df.A大于0的元素 布尔条件筛选\n'''\n                   A         B         C         D\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n '''\n```\n### 6.Pandas设置数据\n根据loc和iloc设置\n```\ndates = pd.date_range('20180310', periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])\nprint(df)\n'''\n             A   B     C   D\n2018-03-10   0   1     2   3\n2018-03-11   4   5     6   7\n2018-03-12   8   9  1111  11\n2018-03-13  12  13    14  15\n2018-03-14  16  17    18  19\n2018-03-15  20  21    22  23\n'''\n\ndf.iloc[2,2] = 999#单点设置\ndf.loc['2018-03-13', 'D'] = 999\nprint(df)\n'''\n            A   B    C    D\n2018-03-10  0   1    2    3\n2018-03-11  0   5    6    7\n2018-03-12  0   9  999   11\n2018-03-13  0  13   14  999\n2018-03-14  0  17   18   19\n2018-03-15  0  21   22   23\n'''\n```\n根据条件设置\n```\ndf[df.A>0]=999#将df.A大于0的值改变\nprint(df)\n'''\n              A   B    C    D\n2018-03-10    0   1    2    3\n2018-03-11  999   5    6    7\n2018-03-12  999   9  999   11\n2018-03-13  999  13   14  999\n2018-03-14  999  17   18   19\n2018-03-15  999  21   22   23\n '''\n```\n根据行或列设置\n```\ndf['F']=np.nan\nprint(df)\n'''\n              A   B    C   D\n2018-03-10    0   1    2 NaN\n2018-03-11  999   5    6 NaN\n2018-03-12  999   9  999 NaN\n2018-03-13  999  13   14 NaN\n2018-03-14  999  17   18 NaN\n2018-03-15  999  21   22 NaN\n '''\n```\n添加数据\n```\ndf['E']  = pd.Series([1,2,3,4,5,6], index=pd.date_range('20180313', periods=6))#增加一列\nprint(df)\n'''\n              A   B    C   D    E\n2018-03-10    0   1    2 NaN  NaN\n2018-03-11  999   5    6 NaN  NaN\n2018-03-12  999   9  999 NaN  NaN\n2018-03-13  999  13   14 NaN  1.0\n2018-03-14  999  17   18 NaN  2.0\n2018-03-15  999  21   22 NaN  3.0\n'''\n```\n### 7.Pandas处理丢失数据\n处理数据中NaN数据\n```\ndates = pd.date_range('20180310', periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])\ndf.iloc[0,1]=np.nan\ndf.iloc[1,2]=np.nan\nprint(df)\n'''\n             A     B     C   D\n2018-03-10   0   NaN   2.0   3\n2018-03-11   4   5.0   NaN   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n'''\n```\n使用dropna（）函数去掉NaN的行或列\n```\nprint(df.dropna(axis=0,how='any'#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop\n'''\n             A     B     C   D\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n '''\n```\n使用fillna（）函数替换NaN值 \n```\nprint(df.fillna(value=0))#将NaN值替换为0\n'''\n             A     B     C   D\n2018-03-10   0   0.0   2.0   3\n2018-03-11   4   5.0   0.0   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n '''\n```\n使用isnull()函数判断数据是否丢失\n```\nprint(pd.isnull(df))#矩阵用布尔来进行表示 是nan为ture 不是nan为false\n'''\n                A      B      C      D\n2018-03-10  False   True  False  False\n2018-03-11  False  False   True  False\n2018-03-12  False  False  False  False\n2018-03-13  False  False  False  False\n2018-03-14  False  False  False  False\n2018-03-15  False  False  False  False\n '''\nprint(np.any(df.isnull()))#判断数据中是否会存在NaN值\n#True\n```\n### 8.Pandas导入导出\npandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看[官方资料](http://pandas.pydata.org/pandas-docs/stable/io.html)\n```\ndata=pd.read_csv('test1.csv')#读取csv文件\ndata.to_pickle('test2.pickle')#将资料存取成pickle文件 \n#其他文件导入导出方式相同\n```\n### 9.Pandas合并数据\naxis合并方向\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ndf3 = pd.DataFrame(np.ones((3,4))*2, columns=['a','b','c','d'])\nres = pd.concat([df1, df2, df3], axis=0, ignore_index=True)#0表示竖项合并 1表示横项合并 ingnore_index重置序列index index变为0 1 2 3 4 5 6 7 8\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n6  2.0  2.0  2.0  2.0\n7  2.0  2.0  2.0  2.0\n8  2.0  2.0  2.0  2.0\n '''\n```\njoin合并方式\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'], index=[1,2,3])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['b','c','d', 'e'], index=[2,3,4])\nprint(df1)\n'''\n     a    b    c    d\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n '''\nprint(df2)\n'''\n     b    c    d    e\n2  1.0  1.0  1.0  1.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n '''\nres=pd.concat([df1,df2],axis=1,join='outer')#行往外进行合并\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n4  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\n '''\n\nres=pd.concat([df1,df2],axis=1,join='outer')#行相同的进行合并\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n'''\n\nres=pd.concat([df1,df2],axis=1,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n'''\n```\nappend添加数据\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ndf3 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ns1 = pd.Series([1,2,3,4], index=['a','b','c','d'])\n\nres=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n'''\n\nres=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  2.0  3.0  4.0\n'''\n```\n### 10.Pandas合并merge\n依据一组key合并\n```\nleft = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                     'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3']})\nprint(left)\n'''\n    A   B key\n0  A0  B0  K0\n1  A1  B1  K1\n2  A2  B2  K2\n3  A3  B3  K3\n'''\nright = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                      'C': ['C0', 'C1', 'C2',  'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']})\nprint(right)\n'''\n    C   D key\n0  C0  D0  K0\n1  C1  D1  K1\n2  C2  D2  K2\n3  C3  D3  K3\n'''\nres=pd.merge(left,right,on='key')\nprint(res)\n'''\n    A   B key   C   D\n0  A0  B0  K0  C0  D0\n1  A1  B1  K1  C1  D1\n2  A2  B2  K2  C2  D2\n3  A3  B3  K3  C3  D3\n'''\n```\n依据两组key合并\n```\nleft = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n                             'key2': ['K0', 'K1', 'K0', 'K1'],\n                             'A': ['A0', 'A1', 'A2', 'A3'],\n                             'B': ['B0', 'B1', 'B2', 'B3']})\nprint(left)\n'''\n    A   B key1 key2\n0  A0  B0   K0   K0\n1  A1  B1   K0   K1\n2  A2  B2   K1   K0\n3  A3  B3   K2   K1\n '''\nright = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n                              'key2': ['K0', 'K0', 'K0', 'K0'],\n                              'C': ['C0', 'C1', 'C2', 'C3'],\n                              'D': ['D0', 'D1', 'D2', 'D3']})\nprint(right)\n'''\n    C   D key1 key2\n0  C0  D0   K0   K0\n1  C1  D1   K1   K0\n2  C2  D2   K1   K0\n3  C3  D3   K2   K0\n '''\n\nres=pd.merge(left,right,on=['key1','key2'],how='inner')#内联合并\nprint(res)\n'''\n    A   B key1 key2   C   D\n0  A0  B0   K0   K0  C0  D0\n1  A2  B2   K1   K0  C1  D1\n2  A2  B2   K1   K0  C2  D2\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='outer')#外联合并\nprint(res)\n'''\n     A    B key1 key2    C    D\n0   A0   B0   K0   K0   C0   D0\n1   A1   B1   K0   K1  NaN  NaN\n2   A2   B2   K1   K0   C1   D1\n3   A2   B2   K1   K0   C2   D2\n4   A3   B3   K2   K1  NaN  NaN\n5  NaN  NaN   K2   K0   C3   D3\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='left')#左联合并\n'''\n    A   B key1 key2    C    D\n0  A0  B0   K0   K0   C0   D0\n1  A1  B1   K0   K1  NaN  NaN\n2  A2  B2   K1   K0   C1   D1\n3  A2  B2   K1   K0   C2   D2\n4  A3  B3   K2   K1  NaN  NaN\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='right')#右联合并\nprint(res)\n'''\n     A    B key1 key2   C   D\n0   A0   B0   K0   K0  C0  D0\n1   A2   B2   K1   K0  C1  D1\n2   A2   B2   K1   K0  C2  D2\n3  NaN  NaN   K2   K0  C3  D3\n'''\n```\nIndicator合并\n```\ndf1 = pd.DataFrame({'col1':[0,1], 'col_left':['a','b']})\nprint(df1)\n'''\n   col1 col_left\n0     0        a\n1     1        b\n '''\ndf2 = pd.DataFrame({'col1':[1,2,2],'col_right':[2,2,2]})\nprint(df2)\n'''\n   col1  col_right\n0     1          2\n1     2          2\n2     2          2\n '''\n\nres=pd.merge(df1,df2,on='col1',how='outer',indicator=True)#依据col1进行合并 并启用indicator=True输出每项合并方式\nprint(res)\n'''\n   col1 col_left  col_right      _merge\n0     0        a        NaN   left_only\n1     1        b        2.0        both\n2     2      NaN        2.0  right_only\n3     2      NaN        2.0  right_only\n'''\n\nres = pd.merge(df1, df2, on='col1', how='outer', indicator='indicator_column')#自定义indicator column名称\nprint(res)\n'''\n   col1 col_left  col_right indicator_column\n0     0        a        NaN        left_only\n1     1        b        2.0             both\n2     2      NaN        2.0       right_only\n3     2      NaN        2.0       right_only\n'''\n```\n依据index合并\n```\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n                                  'B': ['B0', 'B1', 'B2']},\n                                  index=['K0', 'K1', 'K2'])\nprint(left)\n'''\n     A   B\nK0  A0  B0\nK1  A1  B1\nK2  A2  B2\n '''\nright = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n                                     'D': ['D0', 'D2', 'D3']},\n                                      index=['K0', 'K2', 'K3'])\nprint(right)\n'''\n     C   D\nK0  C0  D0\nK2  C2  D2\nK3  C3  D3\n'''\n\nres=pd.merge(left,right,left_index=True,right_index=True,how='outer')#根据index索引进行合并 并选择外联合并\nprint(res)\n'''\n      A    B    C    D\nK0   A0   B0   C0   D0\nK1   A1   B1  NaN  NaN\nK2   A2   B2   C2   D2\nK3  NaN  NaN   C3   D3\n'''\n\nres=pd.merge(left,right,left_index=True,right_index=True,how='inner')\nprint(res)\n'''\n     A   B   C   D\nK0  A0  B0  C0  D0\nK2  A2  B2  C2  D2\n'''\n```\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"Python之Pandas使用教程","published":1,"updated":"2018-06-26T06:54:48.386Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3g000ejiz5uve4wie1","content":"<h3 id=\"1-Pandas概述\"><a href=\"#1-Pandas概述\" class=\"headerlink\" title=\"1.Pandas概述\"></a>1.Pandas概述</h3><ol>\n<li>Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。</li>\n<li>Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。</li>\n<li>Pandas提供大量能使我们快速便捷地处理数据的函数和方法。</li>\n<li>Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。   </li>\n</ol>\n<h3 id=\"2-Pandas安装\"><a href=\"#2-Pandas安装\" class=\"headerlink\" title=\"2.Pandas安装\"></a>2.Pandas安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 <span class=\"keyword\">install</span> pandas</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Pandas引入\"><a href=\"#3-Pandas引入\" class=\"headerlink\" title=\"3.Pandas引入\"></a>3.Pandas引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd<span class=\"comment\">#为了方便实用pandas 采用pd简写</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Pandas数据结构\"><a href=\"#4-Pandas数据结构\" class=\"headerlink\" title=\"4.Pandas数据结构\"></a>4.Pandas数据结构</h3><h4 id=\"4-1Series\"><a href=\"#4-1Series\" class=\"headerlink\" title=\"4.1Series\"></a>4.1Series</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">s=pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,np.nan,<span class=\"number\">5</span>,<span class=\"number\">6</span>])</span><br><span class=\"line\">print(s)<span class=\"comment\">#索引在左边 值在右边</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">0    1.0</span></span><br><span class=\"line\"><span class=\"string\">1    2.0</span></span><br><span class=\"line\"><span class=\"string\">2    3.0</span></span><br><span class=\"line\"><span class=\"string\">3    NaN</span></span><br><span class=\"line\"><span class=\"string\">4    5.0</span></span><br><span class=\"line\"><span class=\"string\">5    6.0</span></span><br><span class=\"line\"><span class=\"string\">dtype: float64</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2DataFrame\"><a href=\"#4-2DataFrame\" class=\"headerlink\" title=\"4.2DataFrame\"></a>4.2DataFrame</h4><p>DataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates=pd.date_range(<span class=\"string\">'20180310'</span>,periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">6</span>,<span class=\"number\">4</span>), index=dates, columns=[<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>,<span class=\"string\">'C'</span>,<span class=\"string\">'D'</span>])<span class=\"comment\">#生成6行4列位置</span></span><br><span class=\"line\">print(df)<span class=\"comment\">#输出6行4列的表格</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         B         C         D</span></span><br><span class=\"line\"><span class=\"string\">2018-03-10 -0.092889 -0.503172  0.692763 -1.261313</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11 -0.895628 -2.300249 -1.098069  0.468986</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12  0.084732 -1.275078  1.638007 -0.291145</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13 -0.561528  0.431088  0.430414  1.065939</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  1.485434 -0.341404  0.267613 -1.493366</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15 -1.671474  0.110933  1.688264 -0.910599</span></span><br><span class=\"line\"><span class=\"string\">  '''</span></span><br><span class=\"line\">print(df[<span class=\"string\">'B'</span>])</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">2018-03-10   -0.927291</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11   -0.406842</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12   -0.088316</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13   -1.631055</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14   -0.929926</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15   -0.010904</span></span><br><span class=\"line\"><span class=\"string\">Freq: D, Name: B, dtype: float64</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#创建特定数据的DataFrame</span></span><br><span class=\"line\">df_1=pd.DataFrame(&#123;<span class=\"string\">'A'</span> : <span class=\"number\">1.</span>,</span><br><span class=\"line\">                    <span class=\"string\">'B'</span> : pd.Timestamp(<span class=\"string\">'20180310'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'C'</span> : pd.Series(<span class=\"number\">1</span>,index=list(range(<span class=\"number\">4</span>)),dtype=<span class=\"string\">'float32'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'D'</span> : np.array([<span class=\"number\">3</span>] * <span class=\"number\">4</span>,dtype=<span class=\"string\">'int32'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'E'</span> : pd.Categorical([<span class=\"string\">\"test\"</span>,<span class=\"string\">\"train\"</span>,<span class=\"string\">\"test\"</span>,<span class=\"string\">\"train\"</span>]),</span><br><span class=\"line\">                    <span class=\"string\">'F'</span> : <span class=\"string\">'foo'</span></span><br><span class=\"line\">                    &#125;)</span><br><span class=\"line\">print(df_1)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A          B    C  D      E    F</span></span><br><span class=\"line\"><span class=\"string\">0  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">1  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">2  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">3  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.dtypes)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">A           float64</span></span><br><span class=\"line\"><span class=\"string\">B    datetime64[ns]</span></span><br><span class=\"line\"><span class=\"string\">C           float32</span></span><br><span class=\"line\"><span class=\"string\">D             int32</span></span><br><span class=\"line\"><span class=\"string\">E          category</span></span><br><span class=\"line\"><span class=\"string\">F            object</span></span><br><span class=\"line\"><span class=\"string\">dtype: object</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.index)<span class=\"comment\">#行的序号</span></span><br><span class=\"line\"><span class=\"comment\">#Int64Index([0, 1, 2, 3], dtype='int64')</span></span><br><span class=\"line\">print(df_1.columns)<span class=\"comment\">#列的序号名字</span></span><br><span class=\"line\"><span class=\"comment\">#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')</span></span><br><span class=\"line\">print(df_1.values)<span class=\"comment\">#把每个值进行打印出来</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(df_1.describe())<span class=\"comment\">#数字总结</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">         A    C    D</span></span><br><span class=\"line\"><span class=\"string\">count  4.0  4.0  4.0</span></span><br><span class=\"line\"><span class=\"string\">mean   1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">std    0.0  0.0  0.0</span></span><br><span class=\"line\"><span class=\"string\">min    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">25%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">50%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">75%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">max    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.T)<span class=\"comment\">#翻转数据</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                     0                    1                    2  \\</span></span><br><span class=\"line\"><span class=\"string\">A                    1                    1                    1   </span></span><br><span class=\"line\"><span class=\"string\">B  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   </span></span><br><span class=\"line\"><span class=\"string\">C                    1                    1                    1   </span></span><br><span class=\"line\"><span class=\"string\">D                    3                    3                    3   </span></span><br><span class=\"line\"><span class=\"string\">E                 test                train                 test   </span></span><br><span class=\"line\"><span class=\"string\">F                  foo                  foo                  foo   </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">                     3  </span></span><br><span class=\"line\"><span class=\"string\">A                    1  </span></span><br><span class=\"line\"><span class=\"string\">B  2018-03-10 00:00:00  </span></span><br><span class=\"line\"><span class=\"string\">C                    1  </span></span><br><span class=\"line\"><span class=\"string\">D                    3  </span></span><br><span class=\"line\"><span class=\"string\">E                train  </span></span><br><span class=\"line\"><span class=\"string\">F                  foo  </span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.sort_index(axis=<span class=\"number\">1</span>, ascending=<span class=\"keyword\">False</span>))<span class=\"comment\">#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     F      E  D    C          B    A</span></span><br><span class=\"line\"><span class=\"string\">0  foo   test  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">1  foo  train  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">2  foo   test  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">3  foo  train  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.sort_values(by=<span class=\"string\">'E'</span>))<span class=\"comment\">#按值进行排序</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A          B    C  D      E    F</span></span><br><span class=\"line\"><span class=\"string\">0  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">2  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">1  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">3  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-Pandas选择数据\"><a href=\"#5-Pandas选择数据\" class=\"headerlink\" title=\"5.Pandas选择数据\"></a>5.Pandas选择数据</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates=pd.date_range('<span class=\"number\">20180310</span>',periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">6</span>,<span class=\"number\">4</span>), index=dates, columns=['A','B','C','D'])#生成<span class=\"number\">6</span>行<span class=\"number\">4</span>列位置</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span> <span class=\"number\">-0.520509</span> <span class=\"number\">-0.136602</span> <span class=\"number\">-0.516984</span>  <span class=\"number\">1.357505</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0.332656</span> <span class=\"number\">-0.094633</span>  <span class=\"number\">0.382384</span> <span class=\"number\">-0.914339</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0.499960</span>  <span class=\"number\">1.576897</span>  <span class=\"number\">2.128730</span>  <span class=\"number\">2.197465</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0.540385</span>  <span class=\"number\">0.427337</span> <span class=\"number\">-0.591381</span>  <span class=\"number\">0.126503</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0.191962</span>  <span class=\"number\">1.237843</span>  <span class=\"number\">1.903370</span>  <span class=\"number\">2.155366</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span> <span class=\"number\">-0.188331</span> <span class=\"number\">-0.578581</span> <span class=\"number\">-0.845854</span> <span class=\"number\">-0.056373</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(df['A'])#或者df.A 选择某列</span><br><span class=\"line\">'''</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">-0.520509</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>    <span class=\"number\">0.332656</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>    <span class=\"number\">0.499960</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>    <span class=\"number\">0.540385</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>    <span class=\"number\">0.191962</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>   <span class=\"number\">-0.188331</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure>\n<p>切片选择<br><figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df[<span class=\"number\">0</span>:<span class=\"number\">3</span>], df['<span class=\"number\">20180310</span>':'<span class=\"number\">20180314</span>'])<span class=\"meta\">#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择</span></span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">520509</span> -0.<span class=\"number\">136602</span> -0.<span class=\"number\">516984</span>  1.<span class=\"number\">357505</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">332656</span> -0.<span class=\"number\">094633</span>  0.<span class=\"number\">382384</span> -0.<span class=\"number\">914339</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">499960</span>  1.<span class=\"number\">576897</span>  2.<span class=\"number\">128730</span>  2.<span class=\"number\">197465</span>                    </span><br><span class=\"line\">                  A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">520509</span> -0.<span class=\"number\">136602</span> -0.<span class=\"number\">516984</span>  1.<span class=\"number\">357505</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">332656</span> -0.<span class=\"number\">094633</span>  0.<span class=\"number\">382384</span> -0.<span class=\"number\">914339</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">499960</span>  1.<span class=\"number\">576897</span>  2.<span class=\"number\">128730</span>  2.<span class=\"number\">197465</span></span><br><span class=\"line\"><span class=\"number\">2018-03-13</span>  0.<span class=\"number\">540385</span>  0.<span class=\"number\">427337</span> -0.<span class=\"number\">591381</span>  0.<span class=\"number\">126503</span></span><br><span class=\"line\"><span class=\"number\">2018-03-14</span>  0.<span class=\"number\">191962</span>  1.<span class=\"number\">237843</span>  1.<span class=\"number\">903370</span>  2.<span class=\"number\">155366</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据标签loc-行标签进行选择数据<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.loc[<span class=\"string\">'20180312'</span>, [<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>]])<span class=\"comment\">#按照行标签进行选择 精确选择</span></span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">A    0.499960</span></span><br><span class=\"line\"><span class=\"string\">B    1.576897</span></span><br><span class=\"line\"><span class=\"string\">Name: 2018-03-12 00:00:00, dtype: float64</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>根据序列iloc-行号进行选择数据<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.iloc[<span class=\"number\">3</span>, <span class=\"number\">1</span>])<span class=\"comment\">#输出第三行第一列的数据</span></span><br><span class=\"line\"><span class=\"comment\">#0.427336827399</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(df.iloc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,<span class=\"number\">0</span>:<span class=\"number\">2</span>])<span class=\"comment\">#进行切片选择</span></span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         B</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13  0.540385  0.427337</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  0.191962  1.237843</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(df.iloc[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>]])<span class=\"comment\">#进行不连续筛选</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         C</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11  0.332656  0.382384</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12  0.499960  2.128730</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  0.191962  1.903370</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>根据混合的两种ix<br><figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.ix[:<span class=\"number\">3</span>, ['A', 'C']])</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         C</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">919275</span> -1.<span class=\"number\">356037</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">010171</span> -0.<span class=\"number\">380010</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">285251</span> -1.<span class=\"number\">174265</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据判断筛选<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df[df.A &gt; <span class=\"number\">0</span>])#筛选出df.A大于<span class=\"number\">0</span>的元素 布尔条件筛选</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0.332656</span> <span class=\"number\">-0.094633</span>  <span class=\"number\">0.382384</span> <span class=\"number\">-0.914339</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0.499960</span>  <span class=\"number\">1.576897</span>  <span class=\"number\">2.128730</span>  <span class=\"number\">2.197465</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0.540385</span>  <span class=\"number\">0.427337</span> <span class=\"number\">-0.591381</span>  <span class=\"number\">0.126503</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0.191962</span>  <span class=\"number\">1.237843</span>  <span class=\"number\">1.903370</span>  <span class=\"number\">2.155366</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-Pandas设置数据\"><a href=\"#6-Pandas设置数据\" class=\"headerlink\" title=\"6.Pandas设置数据\"></a>6.Pandas设置数据</h3><p>根据loc和iloc设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates = pd.date_range('<span class=\"number\">20180310</span>', periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.arange(<span class=\"number\">24</span>).reshape((<span class=\"number\">6</span>,<span class=\"number\">4</span>)), index=dates, columns=['A', 'B', 'C', 'D'])</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">             A   B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   <span class=\"number\">1</span>     <span class=\"number\">2</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5</span>     <span class=\"number\">6</span>   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9</span>  <span class=\"number\">1111</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13</span>    <span class=\"number\">14</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17</span>    <span class=\"number\">18</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21</span>    <span class=\"number\">22</span>  <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">df.iloc[<span class=\"number\">2</span>,<span class=\"number\">2</span>] = <span class=\"number\">999</span>#单点设置</span><br><span class=\"line\">df.loc['<span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>', 'D'] = <span class=\"number\">999</span></span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">            A   B    C    D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>  <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span>    <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span>   <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span>  <span class=\"number\">999</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span>   <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">0</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span>   <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>根据条件设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df.A&gt;<span class=\"number\">0</span>]=<span class=\"number\">999</span>#将df.A大于<span class=\"number\">0</span>的值改变</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C    D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span>    <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span>   <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span>  <span class=\"number\">999</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span>   <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span>   <span class=\"number\">23</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据行或列设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df['F']=np.nan</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span> NaN</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>添加数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df['E']  = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>], index=pd.date_range('<span class=\"number\">20180313</span>', periods=<span class=\"number\">6</span>))#增加一列</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C   D    E</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span> NaN  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span> NaN  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span> NaN  <span class=\"number\">3.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-Pandas处理丢失数据\"><a href=\"#7-Pandas处理丢失数据\" class=\"headerlink\" title=\"7.Pandas处理丢失数据\"></a>7.Pandas处理丢失数据</h3><p>处理数据中NaN数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates = pd.date_range('<span class=\"number\">20180310</span>', periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.arange(<span class=\"number\">24</span>).reshape((<span class=\"number\">6</span>,<span class=\"number\">4</span>)), index=dates, columns=['A', 'B', 'C', 'D'])</span><br><span class=\"line\">df.iloc[<span class=\"number\">0</span>,<span class=\"number\">1</span>]=np.nan</span><br><span class=\"line\">df.iloc[<span class=\"number\">1</span>,<span class=\"number\">2</span>]=np.nan</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">             A     B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   NaN   <span class=\"number\">2.0</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5.0</span>   NaN   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9.0</span>  <span class=\"number\">10.0</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13.0</span>  <span class=\"number\">14.0</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17.0</span>  <span class=\"number\">18.0</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21.0</span>  <span class=\"number\">22.0</span>  <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>使用dropna（）函数去掉NaN的行或列<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.dropna(axis=<span class=\"number\">0</span>,how=<span class=\"string\">'any'</span><span class=\"comment\">#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">             A     B     C   D</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12   8   9.0  10.0  11</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13  12  13.0  14.0  15</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  16  17.0  18.0  19</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15  20  21.0  22.0  23</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>使用fillna（）函数替换NaN值<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.fillna(value=<span class=\"number\">0</span>))#将NaN值替换为<span class=\"number\">0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\">             A     B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   <span class=\"number\">0.0</span>   <span class=\"number\">2.0</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5.0</span>   <span class=\"number\">0.0</span>   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9.0</span>  <span class=\"number\">10.0</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13.0</span>  <span class=\"number\">14.0</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17.0</span>  <span class=\"number\">18.0</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21.0</span>  <span class=\"number\">22.0</span>  <span class=\"number\">23</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>使用isnull()函数判断数据是否丢失<br><figure class=\"highlight vbnet\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(pd.isnull(df))<span class=\"meta\">#矩阵用布尔来进行表示 是nan为ture 不是nan为false</span></span><br><span class=\"line\"><span class=\"comment\"><span class=\"doctag\">'''</span></span></span><br><span class=\"line\">                A      B      C      D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>  <span class=\"literal\">False</span>   <span class=\"literal\">True</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>   <span class=\"literal\">True</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"> <span class=\"comment\"><span class=\"doctag\">'''</span></span></span><br><span class=\"line\">print(np.any(df.isnull()))<span class=\"meta\">#判断数据中是否会存在NaN值</span></span><br><span class=\"line\"><span class=\"meta\">#True</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"8-Pandas导入导出\"><a href=\"#8-Pandas导入导出\" class=\"headerlink\" title=\"8.Pandas导入导出\"></a>8.Pandas导入导出</h3><p>pandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看<a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html\" target=\"_blank\" rel=\"noopener\">官方资料</a><br><figure class=\"highlight haskell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">data</span>=pd.read_csv('<span class=\"title\">test1</span>.<span class=\"title\">csv'</span>)#读取csv文件</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">data</span>.to_pickle('<span class=\"title\">test2</span>.<span class=\"title\">pickle'</span>)#将资料存取成pickle文件 </span></span><br><span class=\"line\"><span class=\"meta\">#其他文件导入导出方式相同</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"9-Pandas合并数据\"><a href=\"#9-Pandas合并数据\" class=\"headerlink\" title=\"9.Pandas合并数据\"></a>9.Pandas合并数据</h3><p>axis合并方向<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df3 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">2</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">res = pd.concat([df1, df2, df3], axis=<span class=\"number\">0</span>, ignore_index=True)#<span class=\"number\">0</span>表示竖项合并 <span class=\"number\">1</span>表示横项合并 ingnore_index重置序列index index变为<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span> <span class=\"number\">7</span> <span class=\"number\">8</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">6</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">7</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">8</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>join合并方式<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'], index=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['b','c','d', 'e'], index=[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     b    c    d    e</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join='outer')#行往外进行合并</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  NaN  NaN  NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  NaN  NaN  NaN  NaN  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join='outer')#行相同的进行合并</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  NaN  NaN  NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>append添加数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df3 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">s1 = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>], index=['a','b','c','d'])</span><br><span class=\"line\"></span><br><span class=\"line\">res=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">res=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">3.0</span>  <span class=\"number\">4.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"10-Pandas合并merge\"><a href=\"#10-Pandas合并merge\" class=\"headerlink\" title=\"10.Pandas合并merge\"></a>10.Pandas合并merge</h3><p>依据一组key合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'key'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>],</span><br><span class=\"line\">                     <span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>, <span class=\"string\">'A3'</span>],</span><br><span class=\"line\">                     <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>, <span class=\"string\">'B3'</span>]&#125;)</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0  K0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1  K1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2  K2</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3  K3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'key'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>],</span><br><span class=\"line\">                      <span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C1'</span>, <span class=\"string\">'C2'</span>,  <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                      <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D1'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;)</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    C   D key</span></span><br><span class=\"line\"><span class=\"string\">0  C0  D0  K0</span></span><br><span class=\"line\"><span class=\"string\">1  C1  D1  K1</span></span><br><span class=\"line\"><span class=\"string\">2  C2  D2  K2</span></span><br><span class=\"line\"><span class=\"string\">3  C3  D3  K3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">res=pd.merge(left,right,on=<span class=\"string\">'key'</span>)</span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key   C   D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0  K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1  K1  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2  K2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3  K3  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>依据两组key合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'key1'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'key2'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>, <span class=\"string\">'A3'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>, <span class=\"string\">'B3'</span>]&#125;)</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1   K0   K1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3   K2   K1</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'key1'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'key2'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C1'</span>, <span class=\"string\">'C2'</span>, <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D1'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;)</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    C   D key1 key2</span></span><br><span class=\"line\"><span class=\"string\">0  C0  D0   K0   K0</span></span><br><span class=\"line\"><span class=\"string\">1  C1  D1   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">2  C2  D2   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">3  C3  D3   K2   K0</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'inner'</span>)<span class=\"comment\">#内联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2   C   D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1  A2  B2   K1   K0  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'outer'</span>)<span class=\"comment\">#外联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A    B key1 key2    C    D</span></span><br><span class=\"line\"><span class=\"string\">0   A0   B0   K0   K0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">1   A1   B1   K0   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">2   A2   B2   K1   K0   C1   D1</span></span><br><span class=\"line\"><span class=\"string\">3   A2   B2   K1   K0   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">4   A3   B3   K2   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">5  NaN  NaN   K2   K0   C3   D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'left'</span>)<span class=\"comment\">#左联合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2    C    D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1   K0   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0   C1   D1</span></span><br><span class=\"line\"><span class=\"string\">3  A2  B2   K1   K0   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">4  A3  B3   K2   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'right'</span>)<span class=\"comment\">#右联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A    B key1 key2   C   D</span></span><br><span class=\"line\"><span class=\"string\">0   A0   B0   K0   K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1   A2   B2   K1   K0  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2   A2   B2   K1   K0  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">3  NaN  NaN   K2   K0  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>Indicator合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(&#123;<span class=\"string\">'col1'</span>:[<span class=\"number\">0</span>,<span class=\"number\">1</span>], <span class=\"string\">'col_left'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>]&#125;)</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left</span></span><br><span class=\"line\"><span class=\"string\">0     0        a</span></span><br><span class=\"line\"><span class=\"string\">1     1        b</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">df2 = pd.DataFrame(&#123;<span class=\"string\">'col1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>],<span class=\"string\">'col_right'</span>:[<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>]&#125;)</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1  col_right</span></span><br><span class=\"line\"><span class=\"string\">0     1          2</span></span><br><span class=\"line\"><span class=\"string\">1     2          2</span></span><br><span class=\"line\"><span class=\"string\">2     2          2</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(df1,df2,on=<span class=\"string\">'col1'</span>,how=<span class=\"string\">'outer'</span>,indicator=<span class=\"keyword\">True</span>)<span class=\"comment\">#依据col1进行合并 并启用indicator=True输出每项合并方式</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left  col_right      _merge</span></span><br><span class=\"line\"><span class=\"string\">0     0        a        NaN   left_only</span></span><br><span class=\"line\"><span class=\"string\">1     1        b        2.0        both</span></span><br><span class=\"line\"><span class=\"string\">2     2      NaN        2.0  right_only</span></span><br><span class=\"line\"><span class=\"string\">3     2      NaN        2.0  right_only</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res = pd.merge(df1, df2, on=<span class=\"string\">'col1'</span>, how=<span class=\"string\">'outer'</span>, indicator=<span class=\"string\">'indicator_column'</span>)<span class=\"comment\">#自定义indicator column名称</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left  col_right indicator_column</span></span><br><span class=\"line\"><span class=\"string\">0     0        a        NaN        left_only</span></span><br><span class=\"line\"><span class=\"string\">1     1        b        2.0             both</span></span><br><span class=\"line\"><span class=\"string\">2     2      NaN        2.0       right_only</span></span><br><span class=\"line\"><span class=\"string\">3     2      NaN        2.0       right_only</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>依据index合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>],</span><br><span class=\"line\">                                  <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>]&#125;,</span><br><span class=\"line\">                                  index=[<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>])</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A   B</span></span><br><span class=\"line\"><span class=\"string\">K0  A0  B0</span></span><br><span class=\"line\"><span class=\"string\">K1  A1  B1</span></span><br><span class=\"line\"><span class=\"string\">K2  A2  B2</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C2'</span>, <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                                     <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;,</span><br><span class=\"line\">                                      index=[<span class=\"string\">'K0'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>])</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     C   D</span></span><br><span class=\"line\"><span class=\"string\">K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">K2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">K3  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,left_index=<span class=\"keyword\">True</span>,right_index=<span class=\"keyword\">True</span>,how=<span class=\"string\">'outer'</span>)<span class=\"comment\">#根据index索引进行合并 并选择外联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">      A    B    C    D</span></span><br><span class=\"line\"><span class=\"string\">K0   A0   B0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">K1   A1   B1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">K2   A2   B2   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">K3  NaN  NaN   C3   D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,left_index=<span class=\"keyword\">True</span>,right_index=<span class=\"keyword\">True</span>,how=<span class=\"string\">'inner'</span>)</span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A   B   C   D</span></span><br><span class=\"line\"><span class=\"string\">K0  A0  B0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">K2  A2  B2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Pandas概述\"><a href=\"#1-Pandas概述\" class=\"headerlink\" title=\"1.Pandas概述\"></a>1.Pandas概述</h3><ol>\n<li>Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。</li>\n<li>Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。</li>\n<li>Pandas提供大量能使我们快速便捷地处理数据的函数和方法。</li>\n<li>Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。   </li>\n</ol>\n<h3 id=\"2-Pandas安装\"><a href=\"#2-Pandas安装\" class=\"headerlink\" title=\"2.Pandas安装\"></a>2.Pandas安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 <span class=\"keyword\">install</span> pandas</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Pandas引入\"><a href=\"#3-Pandas引入\" class=\"headerlink\" title=\"3.Pandas引入\"></a>3.Pandas引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd<span class=\"comment\">#为了方便实用pandas 采用pd简写</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Pandas数据结构\"><a href=\"#4-Pandas数据结构\" class=\"headerlink\" title=\"4.Pandas数据结构\"></a>4.Pandas数据结构</h3><h4 id=\"4-1Series\"><a href=\"#4-1Series\" class=\"headerlink\" title=\"4.1Series\"></a>4.1Series</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">s=pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,np.nan,<span class=\"number\">5</span>,<span class=\"number\">6</span>])</span><br><span class=\"line\">print(s)<span class=\"comment\">#索引在左边 值在右边</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">0    1.0</span></span><br><span class=\"line\"><span class=\"string\">1    2.0</span></span><br><span class=\"line\"><span class=\"string\">2    3.0</span></span><br><span class=\"line\"><span class=\"string\">3    NaN</span></span><br><span class=\"line\"><span class=\"string\">4    5.0</span></span><br><span class=\"line\"><span class=\"string\">5    6.0</span></span><br><span class=\"line\"><span class=\"string\">dtype: float64</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2DataFrame\"><a href=\"#4-2DataFrame\" class=\"headerlink\" title=\"4.2DataFrame\"></a>4.2DataFrame</h4><p>DataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates=pd.date_range(<span class=\"string\">'20180310'</span>,periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">6</span>,<span class=\"number\">4</span>), index=dates, columns=[<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>,<span class=\"string\">'C'</span>,<span class=\"string\">'D'</span>])<span class=\"comment\">#生成6行4列位置</span></span><br><span class=\"line\">print(df)<span class=\"comment\">#输出6行4列的表格</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         B         C         D</span></span><br><span class=\"line\"><span class=\"string\">2018-03-10 -0.092889 -0.503172  0.692763 -1.261313</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11 -0.895628 -2.300249 -1.098069  0.468986</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12  0.084732 -1.275078  1.638007 -0.291145</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13 -0.561528  0.431088  0.430414  1.065939</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  1.485434 -0.341404  0.267613 -1.493366</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15 -1.671474  0.110933  1.688264 -0.910599</span></span><br><span class=\"line\"><span class=\"string\">  '''</span></span><br><span class=\"line\">print(df[<span class=\"string\">'B'</span>])</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">2018-03-10   -0.927291</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11   -0.406842</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12   -0.088316</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13   -1.631055</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14   -0.929926</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15   -0.010904</span></span><br><span class=\"line\"><span class=\"string\">Freq: D, Name: B, dtype: float64</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#创建特定数据的DataFrame</span></span><br><span class=\"line\">df_1=pd.DataFrame(&#123;<span class=\"string\">'A'</span> : <span class=\"number\">1.</span>,</span><br><span class=\"line\">                    <span class=\"string\">'B'</span> : pd.Timestamp(<span class=\"string\">'20180310'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'C'</span> : pd.Series(<span class=\"number\">1</span>,index=list(range(<span class=\"number\">4</span>)),dtype=<span class=\"string\">'float32'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'D'</span> : np.array([<span class=\"number\">3</span>] * <span class=\"number\">4</span>,dtype=<span class=\"string\">'int32'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'E'</span> : pd.Categorical([<span class=\"string\">\"test\"</span>,<span class=\"string\">\"train\"</span>,<span class=\"string\">\"test\"</span>,<span class=\"string\">\"train\"</span>]),</span><br><span class=\"line\">                    <span class=\"string\">'F'</span> : <span class=\"string\">'foo'</span></span><br><span class=\"line\">                    &#125;)</span><br><span class=\"line\">print(df_1)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A          B    C  D      E    F</span></span><br><span class=\"line\"><span class=\"string\">0  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">1  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">2  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">3  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.dtypes)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">A           float64</span></span><br><span class=\"line\"><span class=\"string\">B    datetime64[ns]</span></span><br><span class=\"line\"><span class=\"string\">C           float32</span></span><br><span class=\"line\"><span class=\"string\">D             int32</span></span><br><span class=\"line\"><span class=\"string\">E          category</span></span><br><span class=\"line\"><span class=\"string\">F            object</span></span><br><span class=\"line\"><span class=\"string\">dtype: object</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.index)<span class=\"comment\">#行的序号</span></span><br><span class=\"line\"><span class=\"comment\">#Int64Index([0, 1, 2, 3], dtype='int64')</span></span><br><span class=\"line\">print(df_1.columns)<span class=\"comment\">#列的序号名字</span></span><br><span class=\"line\"><span class=\"comment\">#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')</span></span><br><span class=\"line\">print(df_1.values)<span class=\"comment\">#把每个值进行打印出来</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(df_1.describe())<span class=\"comment\">#数字总结</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">         A    C    D</span></span><br><span class=\"line\"><span class=\"string\">count  4.0  4.0  4.0</span></span><br><span class=\"line\"><span class=\"string\">mean   1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">std    0.0  0.0  0.0</span></span><br><span class=\"line\"><span class=\"string\">min    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">25%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">50%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">75%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">max    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.T)<span class=\"comment\">#翻转数据</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                     0                    1                    2  \\</span></span><br><span class=\"line\"><span class=\"string\">A                    1                    1                    1   </span></span><br><span class=\"line\"><span class=\"string\">B  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   </span></span><br><span class=\"line\"><span class=\"string\">C                    1                    1                    1   </span></span><br><span class=\"line\"><span class=\"string\">D                    3                    3                    3   </span></span><br><span class=\"line\"><span class=\"string\">E                 test                train                 test   </span></span><br><span class=\"line\"><span class=\"string\">F                  foo                  foo                  foo   </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">                     3  </span></span><br><span class=\"line\"><span class=\"string\">A                    1  </span></span><br><span class=\"line\"><span class=\"string\">B  2018-03-10 00:00:00  </span></span><br><span class=\"line\"><span class=\"string\">C                    1  </span></span><br><span class=\"line\"><span class=\"string\">D                    3  </span></span><br><span class=\"line\"><span class=\"string\">E                train  </span></span><br><span class=\"line\"><span class=\"string\">F                  foo  </span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.sort_index(axis=<span class=\"number\">1</span>, ascending=<span class=\"keyword\">False</span>))<span class=\"comment\">#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     F      E  D    C          B    A</span></span><br><span class=\"line\"><span class=\"string\">0  foo   test  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">1  foo  train  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">2  foo   test  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">3  foo  train  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.sort_values(by=<span class=\"string\">'E'</span>))<span class=\"comment\">#按值进行排序</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A          B    C  D      E    F</span></span><br><span class=\"line\"><span class=\"string\">0  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">2  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">1  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">3  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-Pandas选择数据\"><a href=\"#5-Pandas选择数据\" class=\"headerlink\" title=\"5.Pandas选择数据\"></a>5.Pandas选择数据</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates=pd.date_range('<span class=\"number\">20180310</span>',periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">6</span>,<span class=\"number\">4</span>), index=dates, columns=['A','B','C','D'])#生成<span class=\"number\">6</span>行<span class=\"number\">4</span>列位置</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span> <span class=\"number\">-0.520509</span> <span class=\"number\">-0.136602</span> <span class=\"number\">-0.516984</span>  <span class=\"number\">1.357505</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0.332656</span> <span class=\"number\">-0.094633</span>  <span class=\"number\">0.382384</span> <span class=\"number\">-0.914339</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0.499960</span>  <span class=\"number\">1.576897</span>  <span class=\"number\">2.128730</span>  <span class=\"number\">2.197465</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0.540385</span>  <span class=\"number\">0.427337</span> <span class=\"number\">-0.591381</span>  <span class=\"number\">0.126503</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0.191962</span>  <span class=\"number\">1.237843</span>  <span class=\"number\">1.903370</span>  <span class=\"number\">2.155366</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span> <span class=\"number\">-0.188331</span> <span class=\"number\">-0.578581</span> <span class=\"number\">-0.845854</span> <span class=\"number\">-0.056373</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(df['A'])#或者df.A 选择某列</span><br><span class=\"line\">'''</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">-0.520509</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>    <span class=\"number\">0.332656</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>    <span class=\"number\">0.499960</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>    <span class=\"number\">0.540385</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>    <span class=\"number\">0.191962</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>   <span class=\"number\">-0.188331</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure>\n<p>切片选择<br><figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df[<span class=\"number\">0</span>:<span class=\"number\">3</span>], df['<span class=\"number\">20180310</span>':'<span class=\"number\">20180314</span>'])<span class=\"meta\">#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择</span></span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">520509</span> -0.<span class=\"number\">136602</span> -0.<span class=\"number\">516984</span>  1.<span class=\"number\">357505</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">332656</span> -0.<span class=\"number\">094633</span>  0.<span class=\"number\">382384</span> -0.<span class=\"number\">914339</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">499960</span>  1.<span class=\"number\">576897</span>  2.<span class=\"number\">128730</span>  2.<span class=\"number\">197465</span>                    </span><br><span class=\"line\">                  A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">520509</span> -0.<span class=\"number\">136602</span> -0.<span class=\"number\">516984</span>  1.<span class=\"number\">357505</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">332656</span> -0.<span class=\"number\">094633</span>  0.<span class=\"number\">382384</span> -0.<span class=\"number\">914339</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">499960</span>  1.<span class=\"number\">576897</span>  2.<span class=\"number\">128730</span>  2.<span class=\"number\">197465</span></span><br><span class=\"line\"><span class=\"number\">2018-03-13</span>  0.<span class=\"number\">540385</span>  0.<span class=\"number\">427337</span> -0.<span class=\"number\">591381</span>  0.<span class=\"number\">126503</span></span><br><span class=\"line\"><span class=\"number\">2018-03-14</span>  0.<span class=\"number\">191962</span>  1.<span class=\"number\">237843</span>  1.<span class=\"number\">903370</span>  2.<span class=\"number\">155366</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据标签loc-行标签进行选择数据<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.loc[<span class=\"string\">'20180312'</span>, [<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>]])<span class=\"comment\">#按照行标签进行选择 精确选择</span></span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">A    0.499960</span></span><br><span class=\"line\"><span class=\"string\">B    1.576897</span></span><br><span class=\"line\"><span class=\"string\">Name: 2018-03-12 00:00:00, dtype: float64</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>根据序列iloc-行号进行选择数据<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.iloc[<span class=\"number\">3</span>, <span class=\"number\">1</span>])<span class=\"comment\">#输出第三行第一列的数据</span></span><br><span class=\"line\"><span class=\"comment\">#0.427336827399</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(df.iloc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,<span class=\"number\">0</span>:<span class=\"number\">2</span>])<span class=\"comment\">#进行切片选择</span></span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         B</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13  0.540385  0.427337</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  0.191962  1.237843</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(df.iloc[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>]])<span class=\"comment\">#进行不连续筛选</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         C</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11  0.332656  0.382384</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12  0.499960  2.128730</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  0.191962  1.903370</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>根据混合的两种ix<br><figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.ix[:<span class=\"number\">3</span>, ['A', 'C']])</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         C</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">919275</span> -1.<span class=\"number\">356037</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">010171</span> -0.<span class=\"number\">380010</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">285251</span> -1.<span class=\"number\">174265</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据判断筛选<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df[df.A &gt; <span class=\"number\">0</span>])#筛选出df.A大于<span class=\"number\">0</span>的元素 布尔条件筛选</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0.332656</span> <span class=\"number\">-0.094633</span>  <span class=\"number\">0.382384</span> <span class=\"number\">-0.914339</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0.499960</span>  <span class=\"number\">1.576897</span>  <span class=\"number\">2.128730</span>  <span class=\"number\">2.197465</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0.540385</span>  <span class=\"number\">0.427337</span> <span class=\"number\">-0.591381</span>  <span class=\"number\">0.126503</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0.191962</span>  <span class=\"number\">1.237843</span>  <span class=\"number\">1.903370</span>  <span class=\"number\">2.155366</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-Pandas设置数据\"><a href=\"#6-Pandas设置数据\" class=\"headerlink\" title=\"6.Pandas设置数据\"></a>6.Pandas设置数据</h3><p>根据loc和iloc设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates = pd.date_range('<span class=\"number\">20180310</span>', periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.arange(<span class=\"number\">24</span>).reshape((<span class=\"number\">6</span>,<span class=\"number\">4</span>)), index=dates, columns=['A', 'B', 'C', 'D'])</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">             A   B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   <span class=\"number\">1</span>     <span class=\"number\">2</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5</span>     <span class=\"number\">6</span>   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9</span>  <span class=\"number\">1111</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13</span>    <span class=\"number\">14</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17</span>    <span class=\"number\">18</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21</span>    <span class=\"number\">22</span>  <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">df.iloc[<span class=\"number\">2</span>,<span class=\"number\">2</span>] = <span class=\"number\">999</span>#单点设置</span><br><span class=\"line\">df.loc['<span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>', 'D'] = <span class=\"number\">999</span></span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">            A   B    C    D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>  <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span>    <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span>   <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span>  <span class=\"number\">999</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span>   <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">0</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span>   <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>根据条件设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df.A&gt;<span class=\"number\">0</span>]=<span class=\"number\">999</span>#将df.A大于<span class=\"number\">0</span>的值改变</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C    D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span>    <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span>   <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span>  <span class=\"number\">999</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span>   <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span>   <span class=\"number\">23</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据行或列设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df['F']=np.nan</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span> NaN</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>添加数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df['E']  = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>], index=pd.date_range('<span class=\"number\">20180313</span>', periods=<span class=\"number\">6</span>))#增加一列</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C   D    E</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span> NaN  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span> NaN  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span> NaN  <span class=\"number\">3.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-Pandas处理丢失数据\"><a href=\"#7-Pandas处理丢失数据\" class=\"headerlink\" title=\"7.Pandas处理丢失数据\"></a>7.Pandas处理丢失数据</h3><p>处理数据中NaN数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates = pd.date_range('<span class=\"number\">20180310</span>', periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.arange(<span class=\"number\">24</span>).reshape((<span class=\"number\">6</span>,<span class=\"number\">4</span>)), index=dates, columns=['A', 'B', 'C', 'D'])</span><br><span class=\"line\">df.iloc[<span class=\"number\">0</span>,<span class=\"number\">1</span>]=np.nan</span><br><span class=\"line\">df.iloc[<span class=\"number\">1</span>,<span class=\"number\">2</span>]=np.nan</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">             A     B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   NaN   <span class=\"number\">2.0</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5.0</span>   NaN   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9.0</span>  <span class=\"number\">10.0</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13.0</span>  <span class=\"number\">14.0</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17.0</span>  <span class=\"number\">18.0</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21.0</span>  <span class=\"number\">22.0</span>  <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>使用dropna（）函数去掉NaN的行或列<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.dropna(axis=<span class=\"number\">0</span>,how=<span class=\"string\">'any'</span><span class=\"comment\">#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">             A     B     C   D</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12   8   9.0  10.0  11</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13  12  13.0  14.0  15</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  16  17.0  18.0  19</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15  20  21.0  22.0  23</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>使用fillna（）函数替换NaN值<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.fillna(value=<span class=\"number\">0</span>))#将NaN值替换为<span class=\"number\">0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\">             A     B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   <span class=\"number\">0.0</span>   <span class=\"number\">2.0</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5.0</span>   <span class=\"number\">0.0</span>   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9.0</span>  <span class=\"number\">10.0</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13.0</span>  <span class=\"number\">14.0</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17.0</span>  <span class=\"number\">18.0</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21.0</span>  <span class=\"number\">22.0</span>  <span class=\"number\">23</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>使用isnull()函数判断数据是否丢失<br><figure class=\"highlight vbnet\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(pd.isnull(df))<span class=\"meta\">#矩阵用布尔来进行表示 是nan为ture 不是nan为false</span></span><br><span class=\"line\"><span class=\"comment\"><span class=\"doctag\">'''</span></span></span><br><span class=\"line\">                A      B      C      D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>  <span class=\"literal\">False</span>   <span class=\"literal\">True</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>   <span class=\"literal\">True</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"> <span class=\"comment\"><span class=\"doctag\">'''</span></span></span><br><span class=\"line\">print(np.any(df.isnull()))<span class=\"meta\">#判断数据中是否会存在NaN值</span></span><br><span class=\"line\"><span class=\"meta\">#True</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"8-Pandas导入导出\"><a href=\"#8-Pandas导入导出\" class=\"headerlink\" title=\"8.Pandas导入导出\"></a>8.Pandas导入导出</h3><p>pandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看<a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html\" target=\"_blank\" rel=\"noopener\">官方资料</a><br><figure class=\"highlight haskell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">data</span>=pd.read_csv('<span class=\"title\">test1</span>.<span class=\"title\">csv'</span>)#读取csv文件</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">data</span>.to_pickle('<span class=\"title\">test2</span>.<span class=\"title\">pickle'</span>)#将资料存取成pickle文件 </span></span><br><span class=\"line\"><span class=\"meta\">#其他文件导入导出方式相同</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"9-Pandas合并数据\"><a href=\"#9-Pandas合并数据\" class=\"headerlink\" title=\"9.Pandas合并数据\"></a>9.Pandas合并数据</h3><p>axis合并方向<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df3 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">2</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">res = pd.concat([df1, df2, df3], axis=<span class=\"number\">0</span>, ignore_index=True)#<span class=\"number\">0</span>表示竖项合并 <span class=\"number\">1</span>表示横项合并 ingnore_index重置序列index index变为<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span> <span class=\"number\">7</span> <span class=\"number\">8</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">6</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">7</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">8</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>join合并方式<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'], index=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['b','c','d', 'e'], index=[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     b    c    d    e</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join='outer')#行往外进行合并</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  NaN  NaN  NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  NaN  NaN  NaN  NaN  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join='outer')#行相同的进行合并</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  NaN  NaN  NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>append添加数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df3 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">s1 = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>], index=['a','b','c','d'])</span><br><span class=\"line\"></span><br><span class=\"line\">res=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">res=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">3.0</span>  <span class=\"number\">4.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"10-Pandas合并merge\"><a href=\"#10-Pandas合并merge\" class=\"headerlink\" title=\"10.Pandas合并merge\"></a>10.Pandas合并merge</h3><p>依据一组key合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'key'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>],</span><br><span class=\"line\">                     <span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>, <span class=\"string\">'A3'</span>],</span><br><span class=\"line\">                     <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>, <span class=\"string\">'B3'</span>]&#125;)</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0  K0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1  K1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2  K2</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3  K3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'key'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>],</span><br><span class=\"line\">                      <span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C1'</span>, <span class=\"string\">'C2'</span>,  <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                      <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D1'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;)</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    C   D key</span></span><br><span class=\"line\"><span class=\"string\">0  C0  D0  K0</span></span><br><span class=\"line\"><span class=\"string\">1  C1  D1  K1</span></span><br><span class=\"line\"><span class=\"string\">2  C2  D2  K2</span></span><br><span class=\"line\"><span class=\"string\">3  C3  D3  K3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">res=pd.merge(left,right,on=<span class=\"string\">'key'</span>)</span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key   C   D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0  K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1  K1  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2  K2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3  K3  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>依据两组key合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'key1'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'key2'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>, <span class=\"string\">'A3'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>, <span class=\"string\">'B3'</span>]&#125;)</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1   K0   K1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3   K2   K1</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'key1'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'key2'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C1'</span>, <span class=\"string\">'C2'</span>, <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D1'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;)</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    C   D key1 key2</span></span><br><span class=\"line\"><span class=\"string\">0  C0  D0   K0   K0</span></span><br><span class=\"line\"><span class=\"string\">1  C1  D1   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">2  C2  D2   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">3  C3  D3   K2   K0</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'inner'</span>)<span class=\"comment\">#内联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2   C   D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1  A2  B2   K1   K0  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'outer'</span>)<span class=\"comment\">#外联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A    B key1 key2    C    D</span></span><br><span class=\"line\"><span class=\"string\">0   A0   B0   K0   K0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">1   A1   B1   K0   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">2   A2   B2   K1   K0   C1   D1</span></span><br><span class=\"line\"><span class=\"string\">3   A2   B2   K1   K0   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">4   A3   B3   K2   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">5  NaN  NaN   K2   K0   C3   D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'left'</span>)<span class=\"comment\">#左联合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2    C    D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1   K0   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0   C1   D1</span></span><br><span class=\"line\"><span class=\"string\">3  A2  B2   K1   K0   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">4  A3  B3   K2   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'right'</span>)<span class=\"comment\">#右联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A    B key1 key2   C   D</span></span><br><span class=\"line\"><span class=\"string\">0   A0   B0   K0   K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1   A2   B2   K1   K0  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2   A2   B2   K1   K0  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">3  NaN  NaN   K2   K0  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>Indicator合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(&#123;<span class=\"string\">'col1'</span>:[<span class=\"number\">0</span>,<span class=\"number\">1</span>], <span class=\"string\">'col_left'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>]&#125;)</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left</span></span><br><span class=\"line\"><span class=\"string\">0     0        a</span></span><br><span class=\"line\"><span class=\"string\">1     1        b</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">df2 = pd.DataFrame(&#123;<span class=\"string\">'col1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>],<span class=\"string\">'col_right'</span>:[<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>]&#125;)</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1  col_right</span></span><br><span class=\"line\"><span class=\"string\">0     1          2</span></span><br><span class=\"line\"><span class=\"string\">1     2          2</span></span><br><span class=\"line\"><span class=\"string\">2     2          2</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(df1,df2,on=<span class=\"string\">'col1'</span>,how=<span class=\"string\">'outer'</span>,indicator=<span class=\"keyword\">True</span>)<span class=\"comment\">#依据col1进行合并 并启用indicator=True输出每项合并方式</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left  col_right      _merge</span></span><br><span class=\"line\"><span class=\"string\">0     0        a        NaN   left_only</span></span><br><span class=\"line\"><span class=\"string\">1     1        b        2.0        both</span></span><br><span class=\"line\"><span class=\"string\">2     2      NaN        2.0  right_only</span></span><br><span class=\"line\"><span class=\"string\">3     2      NaN        2.0  right_only</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res = pd.merge(df1, df2, on=<span class=\"string\">'col1'</span>, how=<span class=\"string\">'outer'</span>, indicator=<span class=\"string\">'indicator_column'</span>)<span class=\"comment\">#自定义indicator column名称</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left  col_right indicator_column</span></span><br><span class=\"line\"><span class=\"string\">0     0        a        NaN        left_only</span></span><br><span class=\"line\"><span class=\"string\">1     1        b        2.0             both</span></span><br><span class=\"line\"><span class=\"string\">2     2      NaN        2.0       right_only</span></span><br><span class=\"line\"><span class=\"string\">3     2      NaN        2.0       right_only</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>依据index合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>],</span><br><span class=\"line\">                                  <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>]&#125;,</span><br><span class=\"line\">                                  index=[<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>])</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A   B</span></span><br><span class=\"line\"><span class=\"string\">K0  A0  B0</span></span><br><span class=\"line\"><span class=\"string\">K1  A1  B1</span></span><br><span class=\"line\"><span class=\"string\">K2  A2  B2</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C2'</span>, <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                                     <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;,</span><br><span class=\"line\">                                      index=[<span class=\"string\">'K0'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>])</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     C   D</span></span><br><span class=\"line\"><span class=\"string\">K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">K2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">K3  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,left_index=<span class=\"keyword\">True</span>,right_index=<span class=\"keyword\">True</span>,how=<span class=\"string\">'outer'</span>)<span class=\"comment\">#根据index索引进行合并 并选择外联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">      A    B    C    D</span></span><br><span class=\"line\"><span class=\"string\">K0   A0   B0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">K1   A1   B1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">K2   A2   B2   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">K3  NaN  NaN   C3   D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,left_index=<span class=\"keyword\">True</span>,right_index=<span class=\"keyword\">True</span>,how=<span class=\"string\">'inner'</span>)</span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A   B   C   D</span></span><br><span class=\"line\"><span class=\"string\">K0  A0  B0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">K2  A2  B2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"Python之Sklearn使用教程","date":"2018-04-15T03:43:18.000Z","_content":"\n### 1.Sklearn简介\n\nScikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：\n\n+ 简单高效的数据挖掘和数据分析工具\n\n\n+ 让每个人能够在复杂环境中重复使用\n+ 建立NumPy、Scipy、MatPlotLib之上\n\n![Python之Sklearn使用教程图片01](Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png)\n\n### 2.Sklearn安装\n\nSklearn安装要求`Python(>=2.7 or >=3.3)`、`NumPy (>= 1.8.2)`、`SciPy (>= 0.13.3)`。如果已经安装NumPy和SciPy，安装scikit-learn可以使用`pip install -U scikit-learn`。\n\n### 3.Sklearn通用学习模式\n\nSklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，`4.Sklearn datasets`中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过`MatPlotLib`等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。\n\n```python\nfrom sklearn import datasets#引入数据集,sklearn包含众多数据集\nfrom sklearn.model_selection import train_test_split#将数据分为测试集和训练集\nfrom sklearn.neighbors import KNeighborsClassifier#利用邻近点方式训练数据\n\n###引入数据###\niris=datasets.load_iris()#引入iris鸢尾花数据,iris数据包含4个特征变量\niris_X=iris.data#特征变量\niris_y=iris.target#目标值\nX_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)#利用train_test_split进行将训练集和测试集进行分开，test_size占30%\nprint(y_train)#我们看到训练数据的特征值分为3类\n'''\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n '''\n\n###训练数据###\nknn=KNeighborsClassifier()#引入训练方法\nknn.fit(X_train,y_train)#进行填充测试数据进行训练\n\n###预测数据###\nprint(knn.predict(X_test))#预测特征值\n'''\n[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n 1 2 1 0 0 1 0 2]\n'''\nprint(y_test)#真实特征值\n'''\n[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n 1 2 1 0 0 1 0 2]\n'''\n```\n\n### 4.Sklearn datasets\n\nSklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的`load_iris`数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过`load_sample_images()`来引入图片。\n\n![Python之Sklearn使用教程图片02](Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png)\n\n除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。\n\n```python\nfrom sklearn import datasets#引入数据集\n#构造的各种参数可以根据自己需要调整\nX,y=datasets.make_regression(n_samples=100,n_features=1,n_targets=1,noise=1)\n\n###绘制构造的数据###\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.scatter(X,y)\nplt.show()\n```\n\n![Python之Sklearn使用教程03](Python之Sklearn使用教程/Python之Sklearn使用教程03.png)\n\n### 5.Sklearn Model的属性和功能\n\n数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数`y=0.3x+1`，我们可通过`_coef`得到模型的系数为0.3，通过`_intercept`得到模型的截距为1。\n\n```Python\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression#引入线性回归模型\n\n###引入数据###\nload_data=datasets.load_boston()\ndata_X=load_data.data\ndata_y=load_data.target\nprint(data_X.shape)\n#(506, 13)data_X共13个特征变量\n\n###训练数据###\nmodel=LinearRegression()\nmodel.fit(data_X,data_y)\nmodel.predict(data_X[:4,:])#预测前4个数据\n\n###属性和功能###\nprint(model.coef_)\n'''\n[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00\n  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00\n   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03\n  -5.25466633e-01]\n'''\nprint(model.intercept_)\n#36.4911032804\nprint(model.get_params())#得到模型的参数\n#{'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True}\nprint(model.score(data_X,data_y))#对训练情况进行打分\n#0.740607742865\n```\n\n### 6.Sklearn数据预处理\n\n数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。\n\n例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。\n\n```python\nfrom sklearn import preprocessing\nimport numpy as np\na=np.array([[10,2.7,3.6],\n            [-100,5,-2],\n            [120,20,40]],dtype=np.float64)\nprint(a)\nprint(preprocessing.scale(a))#将值的相差度减小\n'''\n[[  10.     2.7    3.6]\n [-100.     5.    -2. ]\n [ 120.    20.    40\n[[ 0.         -0.85170713 -0.55138018]\n [-1.22474487 -0.55187146 -0.852133  ]\n [ 1.22474487  1.40357859  1.40351318]]\n'''\n```\n\n我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为`0.511111111111`，预处理后模型评分为`0.933333333333`，可以看到预处理对模型评分有很大程度的提升。\n\n```Python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets.samples_generator import make_classification\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\n\n###生成的数据如下图所示###\nplt.figure\nX,y=make_classification(n_samples=300,n_features=2,n_redundant=0,n_informative=2,             random_state=22,n_clusters_per_class=1,scale=100)\nplt.scatter(X[:,0],X[:,1],c=y)\nplt.show()\n\n###利用minmax方式对数据进行规范化###\nX=preprocessing.minmax_scale(X)#feature_range=(-1,1)可设置重置范围\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\nclf=SVC()\nclf.fit(X_train,y_train)\nprint(clf.score(X_test,y_test))\n#0.933333333333\n#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升\n```\n\n![Python之Sklearn使用教程04](Python之Sklearn使用教程/Python之Sklearn使用教程04.png)\n\n### 7.交叉验证\n\n交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。\n\n机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：**训练集、验证集和测试集**。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。\n\n以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。\n\n![Python之Sklearn使用教程05](Python之Sklearn使用教程/Python之Sklearn使用教程05.png)\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n###引入数据###\niris=load_iris()\nX=iris.data\ny=iris.target\n\n###训练数据###\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n#引入交叉验证,数据分为5组进行训练\nfrom sklearn.model_selection import cross_val_score\nknn=KNeighborsClassifier(n_neighbors=5)#选择邻近的5个点\nscores=cross_val_score(knn,X,y,cv=5,scoring='accuracy')#评分方式为accuracy\nprint(scores)#每组的评分结果\n#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据\nprint(scores.mean())#平均评分结果\n#0.973333333333\n```\n\n那么是否**n_neighbor=5**便是最好呢，我们来调整参数来看模型最终训练分数。\n\n```Python\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score#引入交叉验证\nimport  matplotlib.pyplot as plt\n###引入数据###\niris=datasets.load_iris()\nX=iris.data\ny=iris.target\n###设置n_neighbors的值为1到30,通过绘图来看训练分数###\nk_range=range(1,31)\nk_score=[]\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    scores=cross_val_score(knn,X,y,cv=10,scoring='accuracy')#for classfication\n    k_score.append(loss.mean())\nplt.figure()\nplt.plot(k_range,k_score)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('CrossValidation accuracy')\nplt.show()\n#K过大会带来过拟合问题,我们可以选择12-18之间的值\n```\n\n我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择`2-fold Cross Validation`,`Leave-One-Out Cross Validation`等方法来分割数据，比较不同方法和参数得到最优结果。\n\n![Python之Sklearn使用教程06](Python之Sklearn使用教程/Python之Sklearn使用教程06.png)\n\n我们将上述代码中的循环部分改变一下，评分函数改为`neg_mean_squared_error`，便得到对于不同参数时的损失函数。\n\n```\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    loss=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# for regression\n    k_score.append(loss.mean())\n```\n\n![Python之Sklearn使用教程07](Python之Sklearn使用教程/Python之Sklearn使用教程07.png)\n\n### 8.过拟合问题\n\n什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。\n\n![Python之Sklearn使用教程08](Python之Sklearn使用教程/Python之Sklearn使用教程08.png)\n\n我们先举例如何辨别**overfitting**问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。\n\n```Python\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#引入数据\ndigits=load_digits()\nX=digits.data\ny=digits.target\n\n#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下\ntrain_size,train_loss,test_loss=learning_curve(\n    SVC(gamma=0.1),X,y,cv=10,scoring='neg_mean_squared_error',\n    train_sizes=[0.1,0.25,0.5,0.75,1]\n)\ntrain_loss_mean=-np.mean(train_loss,axis=1)\ntest_loss_mean=-np.mean(test_loss,axis=1)\n\nplt.figure()\n#将每一步进行打印出来\nplt.plot(train_size,train_loss_mean,'o-',color='r',label='Training')\nplt.plot(train_size,test_loss_mean,'o-',color='g',label='Cross-validation')\nplt.legend('best')\nplt.show()\n```\n\n![Python之Sklearn使用教程09](Python之Sklearn使用教程/Python之Sklearn使用教程09.png)\n\n如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。\n\n![Python之Sklearn使用教程10](Python之Sklearn使用教程/Python之Sklearn使用教程10.png)\n\n\n\n下面我们通过修改gamma参数来修正过拟合问题。\n\n```Python\nfrom sklearn.model_selection import  validation_curve#将learning_curve改为validation_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n#引入数据\ndigits=load_digits()\nX=digits.data\ny=digits.target\n\n#改变param来观察Loss函数情况\nparam_range=np.logspace(-6,-2.3,5)\ntrain_loss,test_loss=validation_curve(\n    SVC(),X,y,param_name='gamma',param_range=param_range,cv=10,\n    scoring='neg_mean_squared_error'\n)\ntrain_loss_mean=-np.mean(train_loss,axis=1)\ntest_loss_mean=-np.mean(test_loss,axis=1)\n\nplt.figure()\nplt.plot(param_range,train_loss_mean,'o-',color='r',label='Training')\nplt.plot(param_range,test_loss_mean,'o-',color='g',label='Cross-validation')\nplt.xlabel('gamma')\nplt.ylabel('loss')\nplt.legend(loc='best')\nplt.show()\n```\n\n通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。\n\n![Python之Sklearn使用教程11.png](Python之Sklearn使用教程/Python之Sklearn使用教程11.png)\n\n### 9.保存模型\n\n我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。\n\n```python\nfrom sklearn import svm\nfrom sklearn import datasets\n\n#引入和训练数据\niris=datasets.load_iris()\nX,y=iris.data,iris.target\nclf=svm.SVC()\nclf.fit(X,y)\n\n#引入sklearn中自带的保存模块\nfrom sklearn.externals import joblib\n#保存model\njoblib.dump(clf,'sklearn_save/clf.pkl')\n\n#重新加载model，只有保存一次后才能加载model\nclf3=joblib.load('sklearn_save/clf.pkl')\nprint(clf3.predict(X[0:1]))\n#存放model能够更快的获得以前的结果\n```\n\n### 10.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![Python之Sklearn使用教程推广](Python之Sklearn使用教程/Python之Sklearn使用教程推广.png)","source":"_posts/Python之Sklearn使用教程.md","raw":"---\ntitle: Python之Sklearn使用教程\ndate: 2018-04-15 11:43:18\ntags: [Python,机器学习]\ncategories: Python库\n---\n\n### 1.Sklearn简介\n\nScikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：\n\n+ 简单高效的数据挖掘和数据分析工具\n\n\n+ 让每个人能够在复杂环境中重复使用\n+ 建立NumPy、Scipy、MatPlotLib之上\n\n![Python之Sklearn使用教程图片01](Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png)\n\n### 2.Sklearn安装\n\nSklearn安装要求`Python(>=2.7 or >=3.3)`、`NumPy (>= 1.8.2)`、`SciPy (>= 0.13.3)`。如果已经安装NumPy和SciPy，安装scikit-learn可以使用`pip install -U scikit-learn`。\n\n### 3.Sklearn通用学习模式\n\nSklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，`4.Sklearn datasets`中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过`MatPlotLib`等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。\n\n```python\nfrom sklearn import datasets#引入数据集,sklearn包含众多数据集\nfrom sklearn.model_selection import train_test_split#将数据分为测试集和训练集\nfrom sklearn.neighbors import KNeighborsClassifier#利用邻近点方式训练数据\n\n###引入数据###\niris=datasets.load_iris()#引入iris鸢尾花数据,iris数据包含4个特征变量\niris_X=iris.data#特征变量\niris_y=iris.target#目标值\nX_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)#利用train_test_split进行将训练集和测试集进行分开，test_size占30%\nprint(y_train)#我们看到训练数据的特征值分为3类\n'''\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n '''\n\n###训练数据###\nknn=KNeighborsClassifier()#引入训练方法\nknn.fit(X_train,y_train)#进行填充测试数据进行训练\n\n###预测数据###\nprint(knn.predict(X_test))#预测特征值\n'''\n[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n 1 2 1 0 0 1 0 2]\n'''\nprint(y_test)#真实特征值\n'''\n[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n 1 2 1 0 0 1 0 2]\n'''\n```\n\n### 4.Sklearn datasets\n\nSklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的`load_iris`数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过`load_sample_images()`来引入图片。\n\n![Python之Sklearn使用教程图片02](Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png)\n\n除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。\n\n```python\nfrom sklearn import datasets#引入数据集\n#构造的各种参数可以根据自己需要调整\nX,y=datasets.make_regression(n_samples=100,n_features=1,n_targets=1,noise=1)\n\n###绘制构造的数据###\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.scatter(X,y)\nplt.show()\n```\n\n![Python之Sklearn使用教程03](Python之Sklearn使用教程/Python之Sklearn使用教程03.png)\n\n### 5.Sklearn Model的属性和功能\n\n数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数`y=0.3x+1`，我们可通过`_coef`得到模型的系数为0.3，通过`_intercept`得到模型的截距为1。\n\n```Python\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression#引入线性回归模型\n\n###引入数据###\nload_data=datasets.load_boston()\ndata_X=load_data.data\ndata_y=load_data.target\nprint(data_X.shape)\n#(506, 13)data_X共13个特征变量\n\n###训练数据###\nmodel=LinearRegression()\nmodel.fit(data_X,data_y)\nmodel.predict(data_X[:4,:])#预测前4个数据\n\n###属性和功能###\nprint(model.coef_)\n'''\n[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00\n  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00\n   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03\n  -5.25466633e-01]\n'''\nprint(model.intercept_)\n#36.4911032804\nprint(model.get_params())#得到模型的参数\n#{'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True}\nprint(model.score(data_X,data_y))#对训练情况进行打分\n#0.740607742865\n```\n\n### 6.Sklearn数据预处理\n\n数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。\n\n例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。\n\n```python\nfrom sklearn import preprocessing\nimport numpy as np\na=np.array([[10,2.7,3.6],\n            [-100,5,-2],\n            [120,20,40]],dtype=np.float64)\nprint(a)\nprint(preprocessing.scale(a))#将值的相差度减小\n'''\n[[  10.     2.7    3.6]\n [-100.     5.    -2. ]\n [ 120.    20.    40\n[[ 0.         -0.85170713 -0.55138018]\n [-1.22474487 -0.55187146 -0.852133  ]\n [ 1.22474487  1.40357859  1.40351318]]\n'''\n```\n\n我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为`0.511111111111`，预处理后模型评分为`0.933333333333`，可以看到预处理对模型评分有很大程度的提升。\n\n```Python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets.samples_generator import make_classification\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\n\n###生成的数据如下图所示###\nplt.figure\nX,y=make_classification(n_samples=300,n_features=2,n_redundant=0,n_informative=2,             random_state=22,n_clusters_per_class=1,scale=100)\nplt.scatter(X[:,0],X[:,1],c=y)\nplt.show()\n\n###利用minmax方式对数据进行规范化###\nX=preprocessing.minmax_scale(X)#feature_range=(-1,1)可设置重置范围\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\nclf=SVC()\nclf.fit(X_train,y_train)\nprint(clf.score(X_test,y_test))\n#0.933333333333\n#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升\n```\n\n![Python之Sklearn使用教程04](Python之Sklearn使用教程/Python之Sklearn使用教程04.png)\n\n### 7.交叉验证\n\n交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。\n\n机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：**训练集、验证集和测试集**。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。\n\n以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。\n\n![Python之Sklearn使用教程05](Python之Sklearn使用教程/Python之Sklearn使用教程05.png)\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n###引入数据###\niris=load_iris()\nX=iris.data\ny=iris.target\n\n###训练数据###\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n#引入交叉验证,数据分为5组进行训练\nfrom sklearn.model_selection import cross_val_score\nknn=KNeighborsClassifier(n_neighbors=5)#选择邻近的5个点\nscores=cross_val_score(knn,X,y,cv=5,scoring='accuracy')#评分方式为accuracy\nprint(scores)#每组的评分结果\n#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据\nprint(scores.mean())#平均评分结果\n#0.973333333333\n```\n\n那么是否**n_neighbor=5**便是最好呢，我们来调整参数来看模型最终训练分数。\n\n```Python\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score#引入交叉验证\nimport  matplotlib.pyplot as plt\n###引入数据###\niris=datasets.load_iris()\nX=iris.data\ny=iris.target\n###设置n_neighbors的值为1到30,通过绘图来看训练分数###\nk_range=range(1,31)\nk_score=[]\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    scores=cross_val_score(knn,X,y,cv=10,scoring='accuracy')#for classfication\n    k_score.append(loss.mean())\nplt.figure()\nplt.plot(k_range,k_score)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('CrossValidation accuracy')\nplt.show()\n#K过大会带来过拟合问题,我们可以选择12-18之间的值\n```\n\n我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择`2-fold Cross Validation`,`Leave-One-Out Cross Validation`等方法来分割数据，比较不同方法和参数得到最优结果。\n\n![Python之Sklearn使用教程06](Python之Sklearn使用教程/Python之Sklearn使用教程06.png)\n\n我们将上述代码中的循环部分改变一下，评分函数改为`neg_mean_squared_error`，便得到对于不同参数时的损失函数。\n\n```\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    loss=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# for regression\n    k_score.append(loss.mean())\n```\n\n![Python之Sklearn使用教程07](Python之Sklearn使用教程/Python之Sklearn使用教程07.png)\n\n### 8.过拟合问题\n\n什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。\n\n![Python之Sklearn使用教程08](Python之Sklearn使用教程/Python之Sklearn使用教程08.png)\n\n我们先举例如何辨别**overfitting**问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。\n\n```Python\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#引入数据\ndigits=load_digits()\nX=digits.data\ny=digits.target\n\n#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下\ntrain_size,train_loss,test_loss=learning_curve(\n    SVC(gamma=0.1),X,y,cv=10,scoring='neg_mean_squared_error',\n    train_sizes=[0.1,0.25,0.5,0.75,1]\n)\ntrain_loss_mean=-np.mean(train_loss,axis=1)\ntest_loss_mean=-np.mean(test_loss,axis=1)\n\nplt.figure()\n#将每一步进行打印出来\nplt.plot(train_size,train_loss_mean,'o-',color='r',label='Training')\nplt.plot(train_size,test_loss_mean,'o-',color='g',label='Cross-validation')\nplt.legend('best')\nplt.show()\n```\n\n![Python之Sklearn使用教程09](Python之Sklearn使用教程/Python之Sklearn使用教程09.png)\n\n如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。\n\n![Python之Sklearn使用教程10](Python之Sklearn使用教程/Python之Sklearn使用教程10.png)\n\n\n\n下面我们通过修改gamma参数来修正过拟合问题。\n\n```Python\nfrom sklearn.model_selection import  validation_curve#将learning_curve改为validation_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n#引入数据\ndigits=load_digits()\nX=digits.data\ny=digits.target\n\n#改变param来观察Loss函数情况\nparam_range=np.logspace(-6,-2.3,5)\ntrain_loss,test_loss=validation_curve(\n    SVC(),X,y,param_name='gamma',param_range=param_range,cv=10,\n    scoring='neg_mean_squared_error'\n)\ntrain_loss_mean=-np.mean(train_loss,axis=1)\ntest_loss_mean=-np.mean(test_loss,axis=1)\n\nplt.figure()\nplt.plot(param_range,train_loss_mean,'o-',color='r',label='Training')\nplt.plot(param_range,test_loss_mean,'o-',color='g',label='Cross-validation')\nplt.xlabel('gamma')\nplt.ylabel('loss')\nplt.legend(loc='best')\nplt.show()\n```\n\n通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。\n\n![Python之Sklearn使用教程11.png](Python之Sklearn使用教程/Python之Sklearn使用教程11.png)\n\n### 9.保存模型\n\n我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。\n\n```python\nfrom sklearn import svm\nfrom sklearn import datasets\n\n#引入和训练数据\niris=datasets.load_iris()\nX,y=iris.data,iris.target\nclf=svm.SVC()\nclf.fit(X,y)\n\n#引入sklearn中自带的保存模块\nfrom sklearn.externals import joblib\n#保存model\njoblib.dump(clf,'sklearn_save/clf.pkl')\n\n#重新加载model，只有保存一次后才能加载model\nclf3=joblib.load('sklearn_save/clf.pkl')\nprint(clf3.predict(X[0:1]))\n#存放model能够更快的获得以前的结果\n```\n\n### 10.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![Python之Sklearn使用教程推广](Python之Sklearn使用教程/Python之Sklearn使用教程推广.png)","slug":"Python之Sklearn使用教程","published":1,"updated":"2018-06-07T17:52:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjktv5d3i000ijiz58kkvlylp","content":"<h3 id=\"1-Sklearn简介\"><a href=\"#1-Sklearn简介\" class=\"headerlink\" title=\"1.Sklearn简介\"></a>1.Sklearn简介</h3><p>Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：</p>\n<ul>\n<li>简单高效的数据挖掘和数据分析工具</li>\n</ul>\n<ul>\n<li>让每个人能够在复杂环境中重复使用</li>\n<li>建立NumPy、Scipy、MatPlotLib之上</li>\n</ul>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png\" alt=\"Python之Sklearn使用教程图片01\"></p>\n<h3 id=\"2-Sklearn安装\"><a href=\"#2-Sklearn安装\" class=\"headerlink\" title=\"2.Sklearn安装\"></a>2.Sklearn安装</h3><p>Sklearn安装要求<code>Python(&gt;=2.7 or &gt;=3.3)</code>、<code>NumPy (&gt;= 1.8.2)</code>、<code>SciPy (&gt;= 0.13.3)</code>。如果已经安装NumPy和SciPy，安装scikit-learn可以使用<code>pip install -U scikit-learn</code>。</p>\n<h3 id=\"3-Sklearn通用学习模式\"><a href=\"#3-Sklearn通用学习模式\" class=\"headerlink\" title=\"3.Sklearn通用学习模式\"></a>3.Sklearn通用学习模式</h3><p>Sklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，<code>4.Sklearn datasets</code>中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过<code>MatPlotLib</code>等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets<span class=\"comment\">#引入数据集,sklearn包含众多数据集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split<span class=\"comment\">#将数据分为测试集和训练集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier<span class=\"comment\">#利用邻近点方式训练数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=datasets.load_iris()<span class=\"comment\">#引入iris鸢尾花数据,iris数据包含4个特征变量</span></span><br><span class=\"line\">iris_X=iris.data<span class=\"comment\">#特征变量</span></span><br><span class=\"line\">iris_y=iris.target<span class=\"comment\">#目标值</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=<span class=\"number\">0.3</span>)<span class=\"comment\">#利用train_test_split进行将训练集和测试集进行分开，test_size占30%</span></span><br><span class=\"line\">print(y_train)<span class=\"comment\">#我们看到训练数据的特征值分为3类</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span></span><br><span class=\"line\"><span class=\"string\"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span></span><br><span class=\"line\"><span class=\"string\"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class=\"line\"><span class=\"string\"> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class=\"line\"><span class=\"string\"> 2 2]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">knn=KNeighborsClassifier()<span class=\"comment\">#引入训练方法</span></span><br><span class=\"line\">knn.fit(X_train,y_train)<span class=\"comment\">#进行填充测试数据进行训练</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###预测数据###</span></span><br><span class=\"line\">print(knn.predict(X_test))<span class=\"comment\">#预测特征值</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0</span></span><br><span class=\"line\"><span class=\"string\"> 1 2 1 0 0 1 0 2]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(y_test)<span class=\"comment\">#真实特征值</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0</span></span><br><span class=\"line\"><span class=\"string\"> 1 2 1 0 0 1 0 2]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Sklearn-datasets\"><a href=\"#4-Sklearn-datasets\" class=\"headerlink\" title=\"4.Sklearn datasets\"></a>4.Sklearn datasets</h3><p>Sklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的<code>load_iris</code>数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过<code>load_sample_images()</code>来引入图片。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png\" alt=\"Python之Sklearn使用教程图片02\"></p>\n<p>除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets<span class=\"comment\">#引入数据集</span></span><br><span class=\"line\"><span class=\"comment\">#构造的各种参数可以根据自己需要调整</span></span><br><span class=\"line\">X,y=datasets.make_regression(n_samples=<span class=\"number\">100</span>,n_features=<span class=\"number\">1</span>,n_targets=<span class=\"number\">1</span>,noise=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###绘制构造的数据###</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X,y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程03.png\" alt=\"Python之Sklearn使用教程03\"></p>\n<h3 id=\"5-Sklearn-Model的属性和功能\"><a href=\"#5-Sklearn-Model的属性和功能\" class=\"headerlink\" title=\"5.Sklearn Model的属性和功能\"></a>5.Sklearn Model的属性和功能</h3><p>数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数<code>y=0.3x+1</code>，我们可通过<code>_coef</code>得到模型的系数为0.3，通过<code>_intercept</code>得到模型的截距为1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression<span class=\"comment\">#引入线性回归模型</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">load_data=datasets.load_boston()</span><br><span class=\"line\">data_X=load_data.data</span><br><span class=\"line\">data_y=load_data.target</span><br><span class=\"line\">print(data_X.shape)</span><br><span class=\"line\"><span class=\"comment\">#(506, 13)data_X共13个特征变量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">model=LinearRegression()</span><br><span class=\"line\">model.fit(data_X,data_y)</span><br><span class=\"line\">model.predict(data_X[:<span class=\"number\">4</span>,:])<span class=\"comment\">#预测前4个数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###属性和功能###</span></span><br><span class=\"line\">print(model.coef_)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00</span></span><br><span class=\"line\"><span class=\"string\">  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00</span></span><br><span class=\"line\"><span class=\"string\">   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03</span></span><br><span class=\"line\"><span class=\"string\">  -5.25466633e-01]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(model.intercept_)</span><br><span class=\"line\"><span class=\"comment\">#36.4911032804</span></span><br><span class=\"line\">print(model.get_params())<span class=\"comment\">#得到模型的参数</span></span><br><span class=\"line\"><span class=\"comment\">#&#123;'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True&#125;</span></span><br><span class=\"line\">print(model.score(data_X,data_y))<span class=\"comment\">#对训练情况进行打分</span></span><br><span class=\"line\"><span class=\"comment\">#0.740607742865</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"6-Sklearn数据预处理\"><a href=\"#6-Sklearn数据预处理\" class=\"headerlink\" title=\"6.Sklearn数据预处理\"></a>6.Sklearn数据预处理</h3><p>数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。</p>\n<p>例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a=np.array([[<span class=\"number\">10</span>,<span class=\"number\">2.7</span>,<span class=\"number\">3.6</span>],</span><br><span class=\"line\">            [<span class=\"number\">-100</span>,<span class=\"number\">5</span>,<span class=\"number\">-2</span>],</span><br><span class=\"line\">            [<span class=\"number\">120</span>,<span class=\"number\">20</span>,<span class=\"number\">40</span>]],dtype=np.float64)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(preprocessing.scale(a))<span class=\"comment\">#将值的相差度减小</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[  10.     2.7    3.6]</span></span><br><span class=\"line\"><span class=\"string\"> [-100.     5.    -2. ]</span></span><br><span class=\"line\"><span class=\"string\"> [ 120.    20.    40</span></span><br><span class=\"line\"><span class=\"string\">[[ 0.         -0.85170713 -0.55138018]</span></span><br><span class=\"line\"><span class=\"string\"> [-1.22474487 -0.55187146 -0.852133  ]</span></span><br><span class=\"line\"><span class=\"string\"> [ 1.22474487  1.40357859  1.40351318]]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure>\n<p>我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为<code>0.511111111111</code>，预处理后模型评分为<code>0.933333333333</code>，可以看到预处理对模型评分有很大程度的提升。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###生成的数据如下图所示###</span></span><br><span class=\"line\">plt.figure</span><br><span class=\"line\">X,y=make_classification(n_samples=<span class=\"number\">300</span>,n_features=<span class=\"number\">2</span>,n_redundant=<span class=\"number\">0</span>,n_informative=<span class=\"number\">2</span>,             random_state=<span class=\"number\">22</span>,n_clusters_per_class=<span class=\"number\">1</span>,scale=<span class=\"number\">100</span>)</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],c=y)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###利用minmax方式对数据进行规范化###</span></span><br><span class=\"line\">X=preprocessing.minmax_scale(X)<span class=\"comment\">#feature_range=(-1,1)可设置重置范围</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\">clf=SVC()</span><br><span class=\"line\">clf.fit(X_train,y_train)</span><br><span class=\"line\">print(clf.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\">#0.933333333333</span></span><br><span class=\"line\"><span class=\"comment\">#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程04.png\" alt=\"Python之Sklearn使用教程04\"></p>\n<h3 id=\"7-交叉验证\"><a href=\"#7-交叉验证\" class=\"headerlink\" title=\"7.交叉验证\"></a>7.交叉验证</h3><p>交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。</p>\n<p>机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：<strong>训练集、验证集和测试集</strong>。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。</p>\n<p>以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程05.png\" alt=\"Python之Sklearn使用教程05\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\"><span class=\"comment\">#引入交叉验证,数据分为5组进行训练</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score</span><br><span class=\"line\">knn=KNeighborsClassifier(n_neighbors=<span class=\"number\">5</span>)<span class=\"comment\">#选择邻近的5个点</span></span><br><span class=\"line\">scores=cross_val_score(knn,X,y,cv=<span class=\"number\">5</span>,scoring=<span class=\"string\">'accuracy'</span>)<span class=\"comment\">#评分方式为accuracy</span></span><br><span class=\"line\">print(scores)<span class=\"comment\">#每组的评分结果</span></span><br><span class=\"line\"><span class=\"comment\">#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据</span></span><br><span class=\"line\">print(scores.mean())<span class=\"comment\">#平均评分结果</span></span><br><span class=\"line\"><span class=\"comment\">#0.973333333333</span></span><br></pre></td></tr></table></figure>\n<p>那么是否<strong>n_neighbor=5</strong>便是最好呢，我们来调整参数来看模型最终训练分数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score<span class=\"comment\">#引入交叉验证</span></span><br><span class=\"line\"><span class=\"keyword\">import</span>  matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=datasets.load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"><span class=\"comment\">###设置n_neighbors的值为1到30,通过绘图来看训练分数###</span></span><br><span class=\"line\">k_range=range(<span class=\"number\">1</span>,<span class=\"number\">31</span>)</span><br><span class=\"line\">k_score=[]</span><br><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_range:</span><br><span class=\"line\">    knn=KNeighborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    scores=cross_val_score(knn,X,y,cv=<span class=\"number\">10</span>,scoring=<span class=\"string\">'accuracy'</span>)<span class=\"comment\">#for classfication</span></span><br><span class=\"line\">    k_score.append(loss.mean())</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(k_range,k_score)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'Value of k for KNN'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'CrossValidation accuracy'</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"><span class=\"comment\">#K过大会带来过拟合问题,我们可以选择12-18之间的值</span></span><br></pre></td></tr></table></figure>\n<p>我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择<code>2-fold Cross Validation</code>,<code>Leave-One-Out Cross Validation</code>等方法来分割数据，比较不同方法和参数得到最优结果。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程06.png\" alt=\"Python之Sklearn使用教程06\"></p>\n<p>我们将上述代码中的循环部分改变一下，评分函数改为<code>neg_mean_squared_error</code>，便得到对于不同参数时的损失函数。</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_range:</span><br><span class=\"line\">    <span class=\"attribute\">knn</span>=KNeighborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    <span class=\"attribute\">loss</span>=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# <span class=\"keyword\">for</span> regression</span><br><span class=\"line\">    k_score.append(loss.mean())</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程07.png\" alt=\"Python之Sklearn使用教程07\"></p>\n<h3 id=\"8-过拟合问题\"><a href=\"#8-过拟合问题\" class=\"headerlink\" title=\"8.过拟合问题\"></a>8.过拟合问题</h3><p>什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程08.png\" alt=\"Python之Sklearn使用教程08\"></p>\n<p>我们先举例如何辨别<strong>overfitting</strong>问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> learning_curve</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">digits=load_digits()</span><br><span class=\"line\">X=digits.data</span><br><span class=\"line\">y=digits.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下</span></span><br><span class=\"line\">train_size,train_loss,test_loss=learning_curve(</span><br><span class=\"line\">    SVC(gamma=<span class=\"number\">0.1</span>),X,y,cv=<span class=\"number\">10</span>,scoring=<span class=\"string\">'neg_mean_squared_error'</span>,</span><br><span class=\"line\">    train_sizes=[<span class=\"number\">0.1</span>,<span class=\"number\">0.25</span>,<span class=\"number\">0.5</span>,<span class=\"number\">0.75</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">)</span><br><span class=\"line\">train_loss_mean=-np.mean(train_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">test_loss_mean=-np.mean(test_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"><span class=\"comment\">#将每一步进行打印出来</span></span><br><span class=\"line\">plt.plot(train_size,train_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'r'</span>,label=<span class=\"string\">'Training'</span>)</span><br><span class=\"line\">plt.plot(train_size,test_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'g'</span>,label=<span class=\"string\">'Cross-validation'</span>)</span><br><span class=\"line\">plt.legend(<span class=\"string\">'best'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程09.png\" alt=\"Python之Sklearn使用教程09\"></p>\n<p>如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程10.png\" alt=\"Python之Sklearn使用教程10\"></p>\n<p>下面我们通过修改gamma参数来修正过拟合问题。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span>  validation_curve<span class=\"comment\">#将learning_curve改为validation_curve</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">digits=load_digits()</span><br><span class=\"line\">X=digits.data</span><br><span class=\"line\">y=digits.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#改变param来观察Loss函数情况</span></span><br><span class=\"line\">param_range=np.logspace(<span class=\"number\">-6</span>,<span class=\"number\">-2.3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">train_loss,test_loss=validation_curve(</span><br><span class=\"line\">    SVC(),X,y,param_name=<span class=\"string\">'gamma'</span>,param_range=param_range,cv=<span class=\"number\">10</span>,</span><br><span class=\"line\">    scoring=<span class=\"string\">'neg_mean_squared_error'</span></span><br><span class=\"line\">)</span><br><span class=\"line\">train_loss_mean=-np.mean(train_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">test_loss_mean=-np.mean(test_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(param_range,train_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'r'</span>,label=<span class=\"string\">'Training'</span>)</span><br><span class=\"line\">plt.plot(param_range,test_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'g'</span>,label=<span class=\"string\">'Cross-validation'</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'gamma'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'loss'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">'best'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程11.png\" alt=\"Python之Sklearn使用教程11.png\"></p>\n<h3 id=\"9-保存模型\"><a href=\"#9-保存模型\" class=\"headerlink\" title=\"9.保存模型\"></a>9.保存模型</h3><p>我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> svm</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入和训练数据</span></span><br><span class=\"line\">iris=datasets.load_iris()</span><br><span class=\"line\">X,y=iris.data,iris.target</span><br><span class=\"line\">clf=svm.SVC()</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入sklearn中自带的保存模块</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.externals <span class=\"keyword\">import</span> joblib</span><br><span class=\"line\"><span class=\"comment\">#保存model</span></span><br><span class=\"line\">joblib.dump(clf,<span class=\"string\">'sklearn_save/clf.pkl'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#重新加载model，只有保存一次后才能加载model</span></span><br><span class=\"line\">clf3=joblib.load(<span class=\"string\">'sklearn_save/clf.pkl'</span>)</span><br><span class=\"line\">print(clf3.predict(X[<span class=\"number\">0</span>:<span class=\"number\">1</span>]))</span><br><span class=\"line\"><span class=\"comment\">#存放model能够更快的获得以前的结果</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"10-推广\"><a href=\"#10-推广\" class=\"headerlink\" title=\"10.推广\"></a>10.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程推广.png\" alt=\"Python之Sklearn使用教程推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Sklearn简介\"><a href=\"#1-Sklearn简介\" class=\"headerlink\" title=\"1.Sklearn简介\"></a>1.Sklearn简介</h3><p>Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：</p>\n<ul>\n<li>简单高效的数据挖掘和数据分析工具</li>\n</ul>\n<ul>\n<li>让每个人能够在复杂环境中重复使用</li>\n<li>建立NumPy、Scipy、MatPlotLib之上</li>\n</ul>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png\" alt=\"Python之Sklearn使用教程图片01\"></p>\n<h3 id=\"2-Sklearn安装\"><a href=\"#2-Sklearn安装\" class=\"headerlink\" title=\"2.Sklearn安装\"></a>2.Sklearn安装</h3><p>Sklearn安装要求<code>Python(&gt;=2.7 or &gt;=3.3)</code>、<code>NumPy (&gt;= 1.8.2)</code>、<code>SciPy (&gt;= 0.13.3)</code>。如果已经安装NumPy和SciPy，安装scikit-learn可以使用<code>pip install -U scikit-learn</code>。</p>\n<h3 id=\"3-Sklearn通用学习模式\"><a href=\"#3-Sklearn通用学习模式\" class=\"headerlink\" title=\"3.Sklearn通用学习模式\"></a>3.Sklearn通用学习模式</h3><p>Sklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，<code>4.Sklearn datasets</code>中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过<code>MatPlotLib</code>等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets<span class=\"comment\">#引入数据集,sklearn包含众多数据集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split<span class=\"comment\">#将数据分为测试集和训练集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier<span class=\"comment\">#利用邻近点方式训练数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=datasets.load_iris()<span class=\"comment\">#引入iris鸢尾花数据,iris数据包含4个特征变量</span></span><br><span class=\"line\">iris_X=iris.data<span class=\"comment\">#特征变量</span></span><br><span class=\"line\">iris_y=iris.target<span class=\"comment\">#目标值</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=<span class=\"number\">0.3</span>)<span class=\"comment\">#利用train_test_split进行将训练集和测试集进行分开，test_size占30%</span></span><br><span class=\"line\">print(y_train)<span class=\"comment\">#我们看到训练数据的特征值分为3类</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span></span><br><span class=\"line\"><span class=\"string\"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span></span><br><span class=\"line\"><span class=\"string\"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class=\"line\"><span class=\"string\"> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class=\"line\"><span class=\"string\"> 2 2]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">knn=KNeighborsClassifier()<span class=\"comment\">#引入训练方法</span></span><br><span class=\"line\">knn.fit(X_train,y_train)<span class=\"comment\">#进行填充测试数据进行训练</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###预测数据###</span></span><br><span class=\"line\">print(knn.predict(X_test))<span class=\"comment\">#预测特征值</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0</span></span><br><span class=\"line\"><span class=\"string\"> 1 2 1 0 0 1 0 2]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(y_test)<span class=\"comment\">#真实特征值</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0</span></span><br><span class=\"line\"><span class=\"string\"> 1 2 1 0 0 1 0 2]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Sklearn-datasets\"><a href=\"#4-Sklearn-datasets\" class=\"headerlink\" title=\"4.Sklearn datasets\"></a>4.Sklearn datasets</h3><p>Sklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的<code>load_iris</code>数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过<code>load_sample_images()</code>来引入图片。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png\" alt=\"Python之Sklearn使用教程图片02\"></p>\n<p>除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets<span class=\"comment\">#引入数据集</span></span><br><span class=\"line\"><span class=\"comment\">#构造的各种参数可以根据自己需要调整</span></span><br><span class=\"line\">X,y=datasets.make_regression(n_samples=<span class=\"number\">100</span>,n_features=<span class=\"number\">1</span>,n_targets=<span class=\"number\">1</span>,noise=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###绘制构造的数据###</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X,y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程03.png\" alt=\"Python之Sklearn使用教程03\"></p>\n<h3 id=\"5-Sklearn-Model的属性和功能\"><a href=\"#5-Sklearn-Model的属性和功能\" class=\"headerlink\" title=\"5.Sklearn Model的属性和功能\"></a>5.Sklearn Model的属性和功能</h3><p>数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数<code>y=0.3x+1</code>，我们可通过<code>_coef</code>得到模型的系数为0.3，通过<code>_intercept</code>得到模型的截距为1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression<span class=\"comment\">#引入线性回归模型</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">load_data=datasets.load_boston()</span><br><span class=\"line\">data_X=load_data.data</span><br><span class=\"line\">data_y=load_data.target</span><br><span class=\"line\">print(data_X.shape)</span><br><span class=\"line\"><span class=\"comment\">#(506, 13)data_X共13个特征变量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">model=LinearRegression()</span><br><span class=\"line\">model.fit(data_X,data_y)</span><br><span class=\"line\">model.predict(data_X[:<span class=\"number\">4</span>,:])<span class=\"comment\">#预测前4个数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###属性和功能###</span></span><br><span class=\"line\">print(model.coef_)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00</span></span><br><span class=\"line\"><span class=\"string\">  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00</span></span><br><span class=\"line\"><span class=\"string\">   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03</span></span><br><span class=\"line\"><span class=\"string\">  -5.25466633e-01]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(model.intercept_)</span><br><span class=\"line\"><span class=\"comment\">#36.4911032804</span></span><br><span class=\"line\">print(model.get_params())<span class=\"comment\">#得到模型的参数</span></span><br><span class=\"line\"><span class=\"comment\">#&#123;'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True&#125;</span></span><br><span class=\"line\">print(model.score(data_X,data_y))<span class=\"comment\">#对训练情况进行打分</span></span><br><span class=\"line\"><span class=\"comment\">#0.740607742865</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"6-Sklearn数据预处理\"><a href=\"#6-Sklearn数据预处理\" class=\"headerlink\" title=\"6.Sklearn数据预处理\"></a>6.Sklearn数据预处理</h3><p>数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。</p>\n<p>例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a=np.array([[<span class=\"number\">10</span>,<span class=\"number\">2.7</span>,<span class=\"number\">3.6</span>],</span><br><span class=\"line\">            [<span class=\"number\">-100</span>,<span class=\"number\">5</span>,<span class=\"number\">-2</span>],</span><br><span class=\"line\">            [<span class=\"number\">120</span>,<span class=\"number\">20</span>,<span class=\"number\">40</span>]],dtype=np.float64)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(preprocessing.scale(a))<span class=\"comment\">#将值的相差度减小</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[  10.     2.7    3.6]</span></span><br><span class=\"line\"><span class=\"string\"> [-100.     5.    -2. ]</span></span><br><span class=\"line\"><span class=\"string\"> [ 120.    20.    40</span></span><br><span class=\"line\"><span class=\"string\">[[ 0.         -0.85170713 -0.55138018]</span></span><br><span class=\"line\"><span class=\"string\"> [-1.22474487 -0.55187146 -0.852133  ]</span></span><br><span class=\"line\"><span class=\"string\"> [ 1.22474487  1.40357859  1.40351318]]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure>\n<p>我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为<code>0.511111111111</code>，预处理后模型评分为<code>0.933333333333</code>，可以看到预处理对模型评分有很大程度的提升。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###生成的数据如下图所示###</span></span><br><span class=\"line\">plt.figure</span><br><span class=\"line\">X,y=make_classification(n_samples=<span class=\"number\">300</span>,n_features=<span class=\"number\">2</span>,n_redundant=<span class=\"number\">0</span>,n_informative=<span class=\"number\">2</span>,             random_state=<span class=\"number\">22</span>,n_clusters_per_class=<span class=\"number\">1</span>,scale=<span class=\"number\">100</span>)</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],c=y)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###利用minmax方式对数据进行规范化###</span></span><br><span class=\"line\">X=preprocessing.minmax_scale(X)<span class=\"comment\">#feature_range=(-1,1)可设置重置范围</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\">clf=SVC()</span><br><span class=\"line\">clf.fit(X_train,y_train)</span><br><span class=\"line\">print(clf.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\">#0.933333333333</span></span><br><span class=\"line\"><span class=\"comment\">#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程04.png\" alt=\"Python之Sklearn使用教程04\"></p>\n<h3 id=\"7-交叉验证\"><a href=\"#7-交叉验证\" class=\"headerlink\" title=\"7.交叉验证\"></a>7.交叉验证</h3><p>交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。</p>\n<p>机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：<strong>训练集、验证集和测试集</strong>。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。</p>\n<p>以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程05.png\" alt=\"Python之Sklearn使用教程05\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\"><span class=\"comment\">#引入交叉验证,数据分为5组进行训练</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score</span><br><span class=\"line\">knn=KNeighborsClassifier(n_neighbors=<span class=\"number\">5</span>)<span class=\"comment\">#选择邻近的5个点</span></span><br><span class=\"line\">scores=cross_val_score(knn,X,y,cv=<span class=\"number\">5</span>,scoring=<span class=\"string\">'accuracy'</span>)<span class=\"comment\">#评分方式为accuracy</span></span><br><span class=\"line\">print(scores)<span class=\"comment\">#每组的评分结果</span></span><br><span class=\"line\"><span class=\"comment\">#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据</span></span><br><span class=\"line\">print(scores.mean())<span class=\"comment\">#平均评分结果</span></span><br><span class=\"line\"><span class=\"comment\">#0.973333333333</span></span><br></pre></td></tr></table></figure>\n<p>那么是否<strong>n_neighbor=5</strong>便是最好呢，我们来调整参数来看模型最终训练分数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score<span class=\"comment\">#引入交叉验证</span></span><br><span class=\"line\"><span class=\"keyword\">import</span>  matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=datasets.load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"><span class=\"comment\">###设置n_neighbors的值为1到30,通过绘图来看训练分数###</span></span><br><span class=\"line\">k_range=range(<span class=\"number\">1</span>,<span class=\"number\">31</span>)</span><br><span class=\"line\">k_score=[]</span><br><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_range:</span><br><span class=\"line\">    knn=KNeighborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    scores=cross_val_score(knn,X,y,cv=<span class=\"number\">10</span>,scoring=<span class=\"string\">'accuracy'</span>)<span class=\"comment\">#for classfication</span></span><br><span class=\"line\">    k_score.append(loss.mean())</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(k_range,k_score)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'Value of k for KNN'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'CrossValidation accuracy'</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"><span class=\"comment\">#K过大会带来过拟合问题,我们可以选择12-18之间的值</span></span><br></pre></td></tr></table></figure>\n<p>我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择<code>2-fold Cross Validation</code>,<code>Leave-One-Out Cross Validation</code>等方法来分割数据，比较不同方法和参数得到最优结果。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程06.png\" alt=\"Python之Sklearn使用教程06\"></p>\n<p>我们将上述代码中的循环部分改变一下，评分函数改为<code>neg_mean_squared_error</code>，便得到对于不同参数时的损失函数。</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_range:</span><br><span class=\"line\">    <span class=\"attribute\">knn</span>=KNeighborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    <span class=\"attribute\">loss</span>=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# <span class=\"keyword\">for</span> regression</span><br><span class=\"line\">    k_score.append(loss.mean())</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程07.png\" alt=\"Python之Sklearn使用教程07\"></p>\n<h3 id=\"8-过拟合问题\"><a href=\"#8-过拟合问题\" class=\"headerlink\" title=\"8.过拟合问题\"></a>8.过拟合问题</h3><p>什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程08.png\" alt=\"Python之Sklearn使用教程08\"></p>\n<p>我们先举例如何辨别<strong>overfitting</strong>问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> learning_curve</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">digits=load_digits()</span><br><span class=\"line\">X=digits.data</span><br><span class=\"line\">y=digits.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下</span></span><br><span class=\"line\">train_size,train_loss,test_loss=learning_curve(</span><br><span class=\"line\">    SVC(gamma=<span class=\"number\">0.1</span>),X,y,cv=<span class=\"number\">10</span>,scoring=<span class=\"string\">'neg_mean_squared_error'</span>,</span><br><span class=\"line\">    train_sizes=[<span class=\"number\">0.1</span>,<span class=\"number\">0.25</span>,<span class=\"number\">0.5</span>,<span class=\"number\">0.75</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">)</span><br><span class=\"line\">train_loss_mean=-np.mean(train_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">test_loss_mean=-np.mean(test_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"><span class=\"comment\">#将每一步进行打印出来</span></span><br><span class=\"line\">plt.plot(train_size,train_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'r'</span>,label=<span class=\"string\">'Training'</span>)</span><br><span class=\"line\">plt.plot(train_size,test_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'g'</span>,label=<span class=\"string\">'Cross-validation'</span>)</span><br><span class=\"line\">plt.legend(<span class=\"string\">'best'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程09.png\" alt=\"Python之Sklearn使用教程09\"></p>\n<p>如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程10.png\" alt=\"Python之Sklearn使用教程10\"></p>\n<p>下面我们通过修改gamma参数来修正过拟合问题。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span>  validation_curve<span class=\"comment\">#将learning_curve改为validation_curve</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">digits=load_digits()</span><br><span class=\"line\">X=digits.data</span><br><span class=\"line\">y=digits.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#改变param来观察Loss函数情况</span></span><br><span class=\"line\">param_range=np.logspace(<span class=\"number\">-6</span>,<span class=\"number\">-2.3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">train_loss,test_loss=validation_curve(</span><br><span class=\"line\">    SVC(),X,y,param_name=<span class=\"string\">'gamma'</span>,param_range=param_range,cv=<span class=\"number\">10</span>,</span><br><span class=\"line\">    scoring=<span class=\"string\">'neg_mean_squared_error'</span></span><br><span class=\"line\">)</span><br><span class=\"line\">train_loss_mean=-np.mean(train_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">test_loss_mean=-np.mean(test_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(param_range,train_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'r'</span>,label=<span class=\"string\">'Training'</span>)</span><br><span class=\"line\">plt.plot(param_range,test_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'g'</span>,label=<span class=\"string\">'Cross-validation'</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'gamma'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'loss'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">'best'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程11.png\" alt=\"Python之Sklearn使用教程11.png\"></p>\n<h3 id=\"9-保存模型\"><a href=\"#9-保存模型\" class=\"headerlink\" title=\"9.保存模型\"></a>9.保存模型</h3><p>我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> svm</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入和训练数据</span></span><br><span class=\"line\">iris=datasets.load_iris()</span><br><span class=\"line\">X,y=iris.data,iris.target</span><br><span class=\"line\">clf=svm.SVC()</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入sklearn中自带的保存模块</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.externals <span class=\"keyword\">import</span> joblib</span><br><span class=\"line\"><span class=\"comment\">#保存model</span></span><br><span class=\"line\">joblib.dump(clf,<span class=\"string\">'sklearn_save/clf.pkl'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#重新加载model，只有保存一次后才能加载model</span></span><br><span class=\"line\">clf3=joblib.load(<span class=\"string\">'sklearn_save/clf.pkl'</span>)</span><br><span class=\"line\">print(clf3.predict(X[<span class=\"number\">0</span>:<span class=\"number\">1</span>]))</span><br><span class=\"line\"><span class=\"comment\">#存放model能够更快的获得以前的结果</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"10-推广\"><a href=\"#10-推广\" class=\"headerlink\" title=\"10.推广\"></a>10.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程推广.png\" alt=\"Python之Sklearn使用教程推广\"></p>\n"},{"title":"智慧考古探测","date":"2018-03-20T03:05:56.000Z","comments":1,"_content":"\n> 模拟赛\n\n![0003](智慧考古探测/0003.jpg)\n![0004](智慧考古探测/0004.jpg)\n![0005](智慧考古探测/0005.jpg)\n![0006](智慧考古探测/0006.jpg)\n![0007](智慧考古探测/0007.jpg)\n![0008](智慧考古探测/0008.jpg)\n![0009](智慧考古探测/0009.jpg)\n![0010](智慧考古探测/0010.jpg)\n![0011](智慧考古探测/0011.jpg)\n![0012](智慧考古探测/0012.jpg)\n![0013](智慧考古探测/0013.jpg)\n![0014](智慧考古探测/0014.jpg)\n![0015](智慧考古探测/0015.jpg)\n![0016](智慧考古探测/0016.jpg)","source":"_posts/智慧考古探测.md","raw":"---\ntitle: 智慧考古探测\ndate: 2018-03-20 11:05:56\ntags: [数学建模]\ncategories: 比赛\ncomments: true\n---\n\n> 模拟赛\n\n![0003](智慧考古探测/0003.jpg)\n![0004](智慧考古探测/0004.jpg)\n![0005](智慧考古探测/0005.jpg)\n![0006](智慧考古探测/0006.jpg)\n![0007](智慧考古探测/0007.jpg)\n![0008](智慧考古探测/0008.jpg)\n![0009](智慧考古探测/0009.jpg)\n![0010](智慧考古探测/0010.jpg)\n![0011](智慧考古探测/0011.jpg)\n![0012](智慧考古探测/0012.jpg)\n![0013](智慧考古探测/0013.jpg)\n![0014](智慧考古探测/0014.jpg)\n![0015](智慧考古探测/0015.jpg)\n![0016](智慧考古探测/0016.jpg)","slug":"智慧考古探测","published":1,"updated":"2018-06-26T03:16:13.852Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3k000jjiz57cp6io50","content":"<blockquote>\n<p>模拟赛</p>\n</blockquote>\n<p><img src=\"/2018/03/20/智慧考古探测/0003.jpg\" alt=\"0003\"><br><img src=\"/2018/03/20/智慧考古探测/0004.jpg\" alt=\"0004\"><br><img src=\"/2018/03/20/智慧考古探测/0005.jpg\" alt=\"0005\"><br><img src=\"/2018/03/20/智慧考古探测/0006.jpg\" alt=\"0006\"><br><img src=\"/2018/03/20/智慧考古探测/0007.jpg\" alt=\"0007\"><br><img src=\"/2018/03/20/智慧考古探测/0008.jpg\" alt=\"0008\"><br><img src=\"/2018/03/20/智慧考古探测/0009.jpg\" alt=\"0009\"><br><img src=\"/2018/03/20/智慧考古探测/0010.jpg\" alt=\"0010\"><br><img src=\"/2018/03/20/智慧考古探测/0011.jpg\" alt=\"0011\"><br><img src=\"/2018/03/20/智慧考古探测/0012.jpg\" alt=\"0012\"><br><img src=\"/2018/03/20/智慧考古探测/0013.jpg\" alt=\"0013\"><br><img src=\"/2018/03/20/智慧考古探测/0014.jpg\" alt=\"0014\"><br><img src=\"/2018/03/20/智慧考古探测/0015.jpg\" alt=\"0015\"><br><img src=\"/2018/03/20/智慧考古探测/0016.jpg\" alt=\"0016\"></p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>模拟赛</p>\n</blockquote>\n<p><img src=\"/2018/03/20/智慧考古探测/0003.jpg\" alt=\"0003\"><br><img src=\"/2018/03/20/智慧考古探测/0004.jpg\" alt=\"0004\"><br><img src=\"/2018/03/20/智慧考古探测/0005.jpg\" alt=\"0005\"><br><img src=\"/2018/03/20/智慧考古探测/0006.jpg\" alt=\"0006\"><br><img src=\"/2018/03/20/智慧考古探测/0007.jpg\" alt=\"0007\"><br><img src=\"/2018/03/20/智慧考古探测/0008.jpg\" alt=\"0008\"><br><img src=\"/2018/03/20/智慧考古探测/0009.jpg\" alt=\"0009\"><br><img src=\"/2018/03/20/智慧考古探测/0010.jpg\" alt=\"0010\"><br><img src=\"/2018/03/20/智慧考古探测/0011.jpg\" alt=\"0011\"><br><img src=\"/2018/03/20/智慧考古探测/0012.jpg\" alt=\"0012\"><br><img src=\"/2018/03/20/智慧考古探测/0013.jpg\" alt=\"0013\"><br><img src=\"/2018/03/20/智慧考古探测/0014.jpg\" alt=\"0014\"><br><img src=\"/2018/03/20/智慧考古探测/0015.jpg\" alt=\"0015\"><br><img src=\"/2018/03/20/智慧考古探测/0016.jpg\" alt=\"0016\"></p>\n"},{"title":"基于google protobuf的gRPC实现(python版)","date":"2018-08-14T04:01:12.000Z","mathjax":false,"_content":"\n### 1.Protobuf简介\n\n**Protobuf(Google Protocol Buffers)**提供一种灵活、高效、自动化的机制，用于序列化结构数据。Protobuf仅需自定义一次所需要的数据格式，然后我们就可以使用Protobuf编译器自动生成各种语言的源码，方便我们读写自定义的格式化数据。另外Protobuf的使用与平台和语言无关，可以在不破坏原数据格式的基础上，扩展新的数据。\n\n我们可以将Protobuf与XML进行对比，但Protobuf更小、更快、更加简单。总结来说具有一下特点：\n+ 性能好、效率高。Protobuf作用与XML、json类似，但它是二进制格式，所以性能更好。但同时因为是二进制格式，所以缺点也就是可读性差。\n+ 代码生成机制，易于使用。\n+ 解析速度快。\n+ 支持多种语言，例C++、C#、Go、Java、Python等。\n+ 向前兼容，向后兼容。\n\n### 2.Protobuf安装\n\nMac用户可以使用brew进行安装，命令如下所示。\n\n> brew install protobuf\n\n如需要安装特定版本，可以先进行搜索有哪些版本，命令如下所示。搜索完成之后，采用上述brew安装方法，安装特定版本即可。\n\n> brew search protobuf\n\n安装完成之后，可以通过protoc \\-\\-version查看是否安装成功。\n\n> protoc \\-\\-version\n> libprotoc 3.6.0\n\n另外可以通过which protoc命令查看protoc安装所在的位置。\n\n> which protoc\n> /usr/local/bin/protoc\n\n### 3.Protobuf实例\n\n#### 3.1编译.proto文件\n\n首先我们需要创建一个以**.proto**结尾的文件，可以在其中定义**message**来指定所需要序列化的数据格式。每一个message都是一个小的信息逻辑单元，包含一系列的name-value值对。以官网上的示例，我们创建一个addressbook.proto文件，内容如下所示。\n\n```python\nsyntax = \"proto2\";\n\npackage tutorial;\n\nmessage Person {\n  required string name = 1;\n  required int32 id = 2;\n  optional string email = 3;\n\n  enum PhoneType {\n    MOBILE = 0;\n    HOME = 1;\n    WORK = 2;\n  }\n\n  message PhoneNumber {\n    required string number = 1;\n    optional PhoneType type = 2 [default = HOME];\n  }\n\n  repeated PhoneNumber phones = 4;\n}\n\nmessage AddressBook {\n  repeated Person people = 1;\n}\n```\n\n+ **syntax=”proto2”**代表版本，目前支持proto2和proto3，不写默认proto2。\n+ **package**类似于C++中的namespace概念。\n+ **message**是包含了各种类型字段的聚集，相当于struct，并且可以嵌套。\n+ proto3版本去掉了required和optional类型，保留了repeated(数组)。其中“＝1”，“＝2”表示每个元素的标识号，它会用在二进制编码中对域的标识，[1,15]之内的标志符在使用时占用一个字节，[16,2047]之内的标识号则占用2个字节，所以从最优化角度考虑，可以将[1,15]使用在一些较常用或repeated的元素上。同时为了考虑将来可能会增加新的标志符，我们要事先预留一些标志符。\n\n构建好addressbook.proto文件后，运行Protobuf编译器编译.proto文件，运行方法如下所示。其中-I表示.protoc所在的路径，\\-\\-python_out表示指定生成的目标文件存在的路径，最后的参数表示要编译的.proto文件。\n\n> protoc \\-I=\\$SRC\\_DIR \\-\\-python_out=\\$DST_DIR \\$SRC_DIR/addressbook.proto \n\n其中SRC_DIR为目录，如果处于当前目录的话，可通过如下所示命令来编译.proto文件。\n\n> protoc -I=. \\-\\-python_out=. addressbook.proto\n\n编译完成之后会生成addressbook_pb2.py文件，里面包含序列化和反序列化等方法。\n\n### 3.2序列化\n\n```python\nimport addressbook_pb2\nimport sys\n\ndef PromptForAddress(person):\n  person.id = int(raw_input(\"Enter person ID number: \"))\n  person.name = raw_input(\"Enter name: \")\n\n  email = raw_input(\"Enter email address (blank for none): \")\n  if email != \"\":\n    person.email = email\n\n  while True:\n    number = raw_input(\"Enter a phone number (or leave blank to finish): \")\n    if number == \"\":\n        break\n\n    phone_number = person.phones.add()\n    phone_number.number = number\n\n    type = raw_input(\"Is this a mobile, home, or work phone? \")\n    if type == \"mobile\":\n        phone_number.type = addressbook_pb2.Person.MOBILE\n    elif type == \"home\":\n        phone_number.type = addressbook_pb2.Person.HOME\n    elif type == \"work\":\n        phone_number.type = addressbook_pb2.Person.WORK\n    else:\n        print \"Unknown phone type; leaving as default value.\"\n\n\nif len(sys.argv) != 2:\n  print \"Usage:\", sys.argv[0], \"ADDRESS_BOOK_FILE\"\n  sys.exit(-1)\n\naddress_book = addressbook_pb2.AddressBook()\n\n# Read the existing address book.\ntry:\n  f = open(sys.argv[1], \"rb\")\n  address_book.ParseFromString(f.read())\n  f.close()\nexcept IOError:\n  print sys.argv[1] + \": Could not open file.  Creating a new one.\"\n\n# Add an address.\nPromptForAddress(address_book.people.add())\n\n# Write the new address book back to disk.\nf = open(sys.argv[1], \"wb\")\nf.write(address_book.SerializeToString())\nf.close()\n```\n\n创建add_person.py文件，代码如上所示，然后通过SerializeToString()方法来进行序列化addressbook.proto中所定义的信息。如果想要运行上述代码的话，我们首先需要创建一个输入文件，例如命名为input.txt，不需输入值。然后采用`python add_person input.txt`，便可进行序列化所输入的数据。如果运行`python add_person`的话，不指定输入文件，则会报错。\n\n> Enter person ID number: 1001\n> Enter name: 1001\n> Enter email address (blank for none): hello@email.com\n> Enter a phone number (or leave blank to finish): 10010\n> Is this a mobile, home, or work phone? work\n> Enter a phone number (or leave blank to finish): \n\n### 3.3反序列化\n\n```python\n#! /usr/bin/python\nimport addressbook_pb2\nimport sys\n\n# Iterates though all people in the AddressBook and prints info about them.\ndef ListPeople(address_book):\n  for person in address_book.people:\n    print \"Person ID:\", person.id\n    print \"  Name:\", person.name\n    if person.HasField('email'):\n      print \"  E-mail address:\", person.email\n\n    for phone_number in person.phones:\n      if phone_number.type == addressbook_pb2.Person.MOBILE:\n        print \"  Mobile phone #: \",\n      elif phone_number.type == addressbook_pb2.Person.HOME:\n        print \"  Home phone #: \",\n      elif phone_number.type == addressbook_pb2.Person.WORK:\n        print \"  Work phone #: \",\n      print phone_number.number\n\n# Main procedure:  Reads the entire address book from a file and prints all\n#   the information inside.\n\nif len(sys.argv) != 2:\n  print \"Usage:\", sys.argv[0], \"ADDRESS_BOOK_FILE\"\n  sys.exit(-1)\n\naddress_book = addressbook_pb2.AddressBook()\n\n# Read the existing address book.\nf = open(sys.argv[1], \"rb\")\naddress_book.ParseFromString(f.read())\nf.close()\n\nListPeople(address_book)\n```\n\n创建list_person.py文件来进行反序列化，代码如上所示。通过`python list_person.py input.txt`命令来执行上述代码，输出结果如下所示。\n\n> Person ID: 1001\n> Name: 1001\n> E-mail address: hello@email.com\n> Work phone #:  10010\n\n### 4.RPC简介\n\n这里引用知乎用户**用心阁**关于**谁能用通俗的语言解释一下什么是 RPC 框架？**的问题答案来解释什么是RPC。**RPC(Remote Procedure Call)**是指远程过程调用，也就是说两台服务器A、B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间上，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。如果需要实现RPC，那么需要解决如下几个问题。\n\n+ 通讯：主要是通过在客户端和服务器之间建立TCP连接，远程过程调用的所有交换的数据都在这个连接里传输。连接可以是按需连接，调用结束后就断掉，也可以是长连接，多个远程过程调用共享同一个连接。 \n+ 寻址：A服务器上的应用怎么告诉底层的RPC框架，如何连接到B服务器（如主机或IP地址）以及特定的端口，方法的名称名称是什么。\n+ 序列化：当A服务器上的应用发起远程过程调用时，方法的参数需要通过底层的网络协议，如TCP传递到B服务器。由于网络协议是基于二进制的，内存中的参数值要序列化成二进制的形式，也就是序列化（Serialize）或编组（marshal），通过寻址和传输将序列化的二进制发送给B服务器。 B服务器收到请求后，需要对参数进行反序列化，恢复为内存中的表达方式，然后找到对应的方法进行本地调用，然后得到返回值。 返回值还要发送回服务器A上的应用，也要经过序列化的方式发送，服务器A接到后，再反序列化，恢复为内存中的表达方式，交给A服务器上的应用 。\n\n![基于google-protobuf的gRPC实现-python版图片01](基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片01.png)\n\n总结来说，RPC提供一种透明调用机制让使用者不必显示区分本地调用还是远程调用。如上图所示，客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理`RpcProxy` 。代理封装调用信息并将调用转交给`RpcInvoker` 去实际执行。在客户端的`RpcInvoker` 通过连接器`RpcConnector` 去维持与服务端的通道`RpcChannel`，并使用`RpcProtocol` 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。RPC 服务端接收器 `RpcAcceptor` 接收客户端的调用请求，同样使用`RpcProtocol` 执行协议解码（decode）。解码后的调用信息传递给`RpcProcessor` 去控制处理调用过程，最后再委托调用给`RpcInvoker` 去实际执行并返回调用结果。\n\n### 5.基于google protobuf的gRPC实现\n\n我们可以利用protobuf实现序列化和反序列化，但如何实现RPC通信呢。为简单起见，我们先介绍gRPC，gRPC是google构建的RPC框架，这样我们就不再考虑如何写通信方法。\n\n#### 5.1gRPC安装\n\n首先安装gRPC，安装命令如下所示。\n\n> pip install grpcio\n\n然后安装protobuf相关的依赖库。\n\n> pip install protobuf\n\n然后安装python gRPC相关的protobuf相关文件。\n\n> pip install grpcio-tools\n\n#### 5.2gRPC实例\n\n创建三个文件夹，名称为example、server、client，里面内容如下所示，具体含义在后面解释。\n\n> + example\n>   - \\_\\_init\\_\\_.py\n>   - data.proto\n>   - data\\_pb2.py\n>   - data\\_pb2\\_grpc.py\n> + server\n>   - server.py\n> + client\n>   + client.py\n\n##### 5.2.1 example\n\nexample主要用于编写.proto文件并生成data接口，其中\\_\\_init\\_\\_.py的作用是方便其他文件夹引用example文件夹中文件，data.proto文件内容如下所示。\n\n```python\nsyntax=\"proto3\";\npackage example;\n\nmessage Data{\n    string text=1;\n}\n\nservice FormatData{\n    rpc DoFormat(Data) returns (Data) {}\n}\n```\n\n然后在example目录下利用下述命令生成data_pb2.py和data_pb2_grpc.py文件。data_pb2.py用于序列化信息，data_pb2_grpc.py用于通信。\n\n> python \\-m grpc\\_tools.protoc -I. \\-\\-python_out=. \\-\\-grpc_python_out=. ./data.proto\n\n##### 5.2.2 server\n\nserver为服务器端，server.py实现接受客户端发送的数据，并对数据进行处理后返回给客户端。FormatData的作用是将服务器端传过来的数据转换为大写，具体含义见相关代码和注释。\n\n```python\n#! /usr/bin/env python\n# -*- coding: utf-8 -*-\nimport grpc\nimport time\nfrom concurrent import futures #具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能\nfrom example import data_pb2\nfrom example import data_pb2_grpc\n\n_ONE_DAY_IN_SECONDS = 60*60*24\n_HOST='localhost'\n_PORT='8080'\n\nclass FormatData(data_pb2_grpc.FormatDataServicer):\n    def DoFormat(self,request,context):\n        str=request.text\n        return data_pb2.Data(text=str.upper())\n\ndef serve():\n    grpcServer=grpc.server(futures.ThreadPoolExecutor(max_workers=4))#最多有多少work并行执行任务\n    data_pb2_grpc.add_FormatDataServicer_to_server(FormatData(),grpcServer)# 添加函数方法和服务器，服务器端会进行反序列化。\n    grpcServer.add_insecure_port(_HOST+':'+_PORT) #建立服务器和端口\n    grpcServer.start()# 启动服务端\n    try:\n        while True:\n            time.sleep(_ONE_DAY_IN_SECONDS)\n    except KeyboardInterrupt:\n        grpcServer.stop(0)\n\n\nif __name__=='__main__':\n    serve()\n```\n\n##### 5.2.3 client\n\nclinet为客户端，client.py实现客户端发送数据，并接受server处理后返回的数据，具体含义见相关代码和注释。\n\n```python\n#! /usr/bin/env python\n# -*- coding: utf-8 -*-\nimport grpc\nfrom example import data_pb2,data_pb2_grpc\n\n_HOST='localhost'\n_PORT='8080'\n\ndef run():\n    conn=grpc.insecure_channel(_HOST+':'+_PORT)# 服务器信息\n    client=data_pb2_grpc.FormatDataStub(channel=conn) #客户端建立连接\n    for i in range(0,5):\n        respnse = client.DoFormat(data_pb2.Data(text='hello,world!'))  # 序列化数据传递过去\n        print(\"received: \" + respnse.text)\n\nif __name__=='__main__':\n    run()\n```\n\n接下来运行server.py来启动服务器，然后运行client.py便可以得到结果，可以看到所有数据均已大写。最后需要关闭服务器端，否则一直会处于运行状态。\n\n> received: HELLO,WORLD!\n> received: HELLO,WORLD!\n> received: HELLO,WORLD!\n> received: HELLO,WORLD!\n> received: HELLO,WORLD!\n\n### 6.基于google protobuf的RPC实现\n\n因为RPC需要我们实现通信，所以会有一定难度，代码量很大程度上也有增加，不方便在文中展现出来。所以我把代码放到了github上面，地址在[https://github.com/weizhixiaoyi/google-protobuf-service](https://github.com/weizhixiaoyi/google-protobuf-service)，有兴趣的可以看下。\n\n总的来说，protobuf RPC定义了一个抽象的RPC框架，RpcServiceStub和RpcService类是protobuf编译器根据proto定义生成的类，RpcService定义了服务端暴露给客户端的函数接口，具体实现需要用户自己继承这个类来实现。RpcServiceStub定义了服务端暴露函数的描述，并将客户端对RpcServiceStub中函数的调用统一转换到调用RpcChannel中的CallMethod方法，CallMethod通过RpcServiceStub传过来的函数描述符和函数参数对该次rpc调用进行encode，最终通过RpcConnecor发送给服务方。对方以客户端相反的过程最终调用RpcSerivice中定义的函数。\n\n![基于google-protobuf的gRPC实现-python版图片02](基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片02.png)\n\n事实上，protobuf rpc的框架只是RpcChannel中定义了空的CallMethod，所以具体怎样进行encode和调用RpcConnector都要自己实现。RpcConnector在protobuf中没有定义，所以这个完成由用户自己实现，它的作用就是收发rpc消息包。在服务端，RpcChannel通过调用RpcService中的CallMethod来具体调用RpcService中暴露给客户端的函数。\n\n**参考**\n\n> [用心阁-谁能用通俗的语言解释一下什么是 RPC 框架？](https://www.zhihu.com/question/25536695)\n>\n> [在于思考-python通过protobuf实现rpc](https://www.cnblogs.com/chengxuyuancc/p/5245749.html)\n\n### 7.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](基于google-protobuf的gRPC实现-python版/推广.png)\n\n","source":"_posts/基于google-protobuf的gRPC实现-python版.md","raw":"---\ntitle: 基于google protobuf的gRPC实现(python版)\ndate: 2018-08-14 12:01:12\ntags: [protobuf,RPC,gRPC]\ncategories: protobuf\nmathjax: False\n---\n\n### 1.Protobuf简介\n\n**Protobuf(Google Protocol Buffers)**提供一种灵活、高效、自动化的机制，用于序列化结构数据。Protobuf仅需自定义一次所需要的数据格式，然后我们就可以使用Protobuf编译器自动生成各种语言的源码，方便我们读写自定义的格式化数据。另外Protobuf的使用与平台和语言无关，可以在不破坏原数据格式的基础上，扩展新的数据。\n\n我们可以将Protobuf与XML进行对比，但Protobuf更小、更快、更加简单。总结来说具有一下特点：\n+ 性能好、效率高。Protobuf作用与XML、json类似，但它是二进制格式，所以性能更好。但同时因为是二进制格式，所以缺点也就是可读性差。\n+ 代码生成机制，易于使用。\n+ 解析速度快。\n+ 支持多种语言，例C++、C#、Go、Java、Python等。\n+ 向前兼容，向后兼容。\n\n### 2.Protobuf安装\n\nMac用户可以使用brew进行安装，命令如下所示。\n\n> brew install protobuf\n\n如需要安装特定版本，可以先进行搜索有哪些版本，命令如下所示。搜索完成之后，采用上述brew安装方法，安装特定版本即可。\n\n> brew search protobuf\n\n安装完成之后，可以通过protoc \\-\\-version查看是否安装成功。\n\n> protoc \\-\\-version\n> libprotoc 3.6.0\n\n另外可以通过which protoc命令查看protoc安装所在的位置。\n\n> which protoc\n> /usr/local/bin/protoc\n\n### 3.Protobuf实例\n\n#### 3.1编译.proto文件\n\n首先我们需要创建一个以**.proto**结尾的文件，可以在其中定义**message**来指定所需要序列化的数据格式。每一个message都是一个小的信息逻辑单元，包含一系列的name-value值对。以官网上的示例，我们创建一个addressbook.proto文件，内容如下所示。\n\n```python\nsyntax = \"proto2\";\n\npackage tutorial;\n\nmessage Person {\n  required string name = 1;\n  required int32 id = 2;\n  optional string email = 3;\n\n  enum PhoneType {\n    MOBILE = 0;\n    HOME = 1;\n    WORK = 2;\n  }\n\n  message PhoneNumber {\n    required string number = 1;\n    optional PhoneType type = 2 [default = HOME];\n  }\n\n  repeated PhoneNumber phones = 4;\n}\n\nmessage AddressBook {\n  repeated Person people = 1;\n}\n```\n\n+ **syntax=”proto2”**代表版本，目前支持proto2和proto3，不写默认proto2。\n+ **package**类似于C++中的namespace概念。\n+ **message**是包含了各种类型字段的聚集，相当于struct，并且可以嵌套。\n+ proto3版本去掉了required和optional类型，保留了repeated(数组)。其中“＝1”，“＝2”表示每个元素的标识号，它会用在二进制编码中对域的标识，[1,15]之内的标志符在使用时占用一个字节，[16,2047]之内的标识号则占用2个字节，所以从最优化角度考虑，可以将[1,15]使用在一些较常用或repeated的元素上。同时为了考虑将来可能会增加新的标志符，我们要事先预留一些标志符。\n\n构建好addressbook.proto文件后，运行Protobuf编译器编译.proto文件，运行方法如下所示。其中-I表示.protoc所在的路径，\\-\\-python_out表示指定生成的目标文件存在的路径，最后的参数表示要编译的.proto文件。\n\n> protoc \\-I=\\$SRC\\_DIR \\-\\-python_out=\\$DST_DIR \\$SRC_DIR/addressbook.proto \n\n其中SRC_DIR为目录，如果处于当前目录的话，可通过如下所示命令来编译.proto文件。\n\n> protoc -I=. \\-\\-python_out=. addressbook.proto\n\n编译完成之后会生成addressbook_pb2.py文件，里面包含序列化和反序列化等方法。\n\n### 3.2序列化\n\n```python\nimport addressbook_pb2\nimport sys\n\ndef PromptForAddress(person):\n  person.id = int(raw_input(\"Enter person ID number: \"))\n  person.name = raw_input(\"Enter name: \")\n\n  email = raw_input(\"Enter email address (blank for none): \")\n  if email != \"\":\n    person.email = email\n\n  while True:\n    number = raw_input(\"Enter a phone number (or leave blank to finish): \")\n    if number == \"\":\n        break\n\n    phone_number = person.phones.add()\n    phone_number.number = number\n\n    type = raw_input(\"Is this a mobile, home, or work phone? \")\n    if type == \"mobile\":\n        phone_number.type = addressbook_pb2.Person.MOBILE\n    elif type == \"home\":\n        phone_number.type = addressbook_pb2.Person.HOME\n    elif type == \"work\":\n        phone_number.type = addressbook_pb2.Person.WORK\n    else:\n        print \"Unknown phone type; leaving as default value.\"\n\n\nif len(sys.argv) != 2:\n  print \"Usage:\", sys.argv[0], \"ADDRESS_BOOK_FILE\"\n  sys.exit(-1)\n\naddress_book = addressbook_pb2.AddressBook()\n\n# Read the existing address book.\ntry:\n  f = open(sys.argv[1], \"rb\")\n  address_book.ParseFromString(f.read())\n  f.close()\nexcept IOError:\n  print sys.argv[1] + \": Could not open file.  Creating a new one.\"\n\n# Add an address.\nPromptForAddress(address_book.people.add())\n\n# Write the new address book back to disk.\nf = open(sys.argv[1], \"wb\")\nf.write(address_book.SerializeToString())\nf.close()\n```\n\n创建add_person.py文件，代码如上所示，然后通过SerializeToString()方法来进行序列化addressbook.proto中所定义的信息。如果想要运行上述代码的话，我们首先需要创建一个输入文件，例如命名为input.txt，不需输入值。然后采用`python add_person input.txt`，便可进行序列化所输入的数据。如果运行`python add_person`的话，不指定输入文件，则会报错。\n\n> Enter person ID number: 1001\n> Enter name: 1001\n> Enter email address (blank for none): hello@email.com\n> Enter a phone number (or leave blank to finish): 10010\n> Is this a mobile, home, or work phone? work\n> Enter a phone number (or leave blank to finish): \n\n### 3.3反序列化\n\n```python\n#! /usr/bin/python\nimport addressbook_pb2\nimport sys\n\n# Iterates though all people in the AddressBook and prints info about them.\ndef ListPeople(address_book):\n  for person in address_book.people:\n    print \"Person ID:\", person.id\n    print \"  Name:\", person.name\n    if person.HasField('email'):\n      print \"  E-mail address:\", person.email\n\n    for phone_number in person.phones:\n      if phone_number.type == addressbook_pb2.Person.MOBILE:\n        print \"  Mobile phone #: \",\n      elif phone_number.type == addressbook_pb2.Person.HOME:\n        print \"  Home phone #: \",\n      elif phone_number.type == addressbook_pb2.Person.WORK:\n        print \"  Work phone #: \",\n      print phone_number.number\n\n# Main procedure:  Reads the entire address book from a file and prints all\n#   the information inside.\n\nif len(sys.argv) != 2:\n  print \"Usage:\", sys.argv[0], \"ADDRESS_BOOK_FILE\"\n  sys.exit(-1)\n\naddress_book = addressbook_pb2.AddressBook()\n\n# Read the existing address book.\nf = open(sys.argv[1], \"rb\")\naddress_book.ParseFromString(f.read())\nf.close()\n\nListPeople(address_book)\n```\n\n创建list_person.py文件来进行反序列化，代码如上所示。通过`python list_person.py input.txt`命令来执行上述代码，输出结果如下所示。\n\n> Person ID: 1001\n> Name: 1001\n> E-mail address: hello@email.com\n> Work phone #:  10010\n\n### 4.RPC简介\n\n这里引用知乎用户**用心阁**关于**谁能用通俗的语言解释一下什么是 RPC 框架？**的问题答案来解释什么是RPC。**RPC(Remote Procedure Call)**是指远程过程调用，也就是说两台服务器A、B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间上，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。如果需要实现RPC，那么需要解决如下几个问题。\n\n+ 通讯：主要是通过在客户端和服务器之间建立TCP连接，远程过程调用的所有交换的数据都在这个连接里传输。连接可以是按需连接，调用结束后就断掉，也可以是长连接，多个远程过程调用共享同一个连接。 \n+ 寻址：A服务器上的应用怎么告诉底层的RPC框架，如何连接到B服务器（如主机或IP地址）以及特定的端口，方法的名称名称是什么。\n+ 序列化：当A服务器上的应用发起远程过程调用时，方法的参数需要通过底层的网络协议，如TCP传递到B服务器。由于网络协议是基于二进制的，内存中的参数值要序列化成二进制的形式，也就是序列化（Serialize）或编组（marshal），通过寻址和传输将序列化的二进制发送给B服务器。 B服务器收到请求后，需要对参数进行反序列化，恢复为内存中的表达方式，然后找到对应的方法进行本地调用，然后得到返回值。 返回值还要发送回服务器A上的应用，也要经过序列化的方式发送，服务器A接到后，再反序列化，恢复为内存中的表达方式，交给A服务器上的应用 。\n\n![基于google-protobuf的gRPC实现-python版图片01](基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片01.png)\n\n总结来说，RPC提供一种透明调用机制让使用者不必显示区分本地调用还是远程调用。如上图所示，客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理`RpcProxy` 。代理封装调用信息并将调用转交给`RpcInvoker` 去实际执行。在客户端的`RpcInvoker` 通过连接器`RpcConnector` 去维持与服务端的通道`RpcChannel`，并使用`RpcProtocol` 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。RPC 服务端接收器 `RpcAcceptor` 接收客户端的调用请求，同样使用`RpcProtocol` 执行协议解码（decode）。解码后的调用信息传递给`RpcProcessor` 去控制处理调用过程，最后再委托调用给`RpcInvoker` 去实际执行并返回调用结果。\n\n### 5.基于google protobuf的gRPC实现\n\n我们可以利用protobuf实现序列化和反序列化，但如何实现RPC通信呢。为简单起见，我们先介绍gRPC，gRPC是google构建的RPC框架，这样我们就不再考虑如何写通信方法。\n\n#### 5.1gRPC安装\n\n首先安装gRPC，安装命令如下所示。\n\n> pip install grpcio\n\n然后安装protobuf相关的依赖库。\n\n> pip install protobuf\n\n然后安装python gRPC相关的protobuf相关文件。\n\n> pip install grpcio-tools\n\n#### 5.2gRPC实例\n\n创建三个文件夹，名称为example、server、client，里面内容如下所示，具体含义在后面解释。\n\n> + example\n>   - \\_\\_init\\_\\_.py\n>   - data.proto\n>   - data\\_pb2.py\n>   - data\\_pb2\\_grpc.py\n> + server\n>   - server.py\n> + client\n>   + client.py\n\n##### 5.2.1 example\n\nexample主要用于编写.proto文件并生成data接口，其中\\_\\_init\\_\\_.py的作用是方便其他文件夹引用example文件夹中文件，data.proto文件内容如下所示。\n\n```python\nsyntax=\"proto3\";\npackage example;\n\nmessage Data{\n    string text=1;\n}\n\nservice FormatData{\n    rpc DoFormat(Data) returns (Data) {}\n}\n```\n\n然后在example目录下利用下述命令生成data_pb2.py和data_pb2_grpc.py文件。data_pb2.py用于序列化信息，data_pb2_grpc.py用于通信。\n\n> python \\-m grpc\\_tools.protoc -I. \\-\\-python_out=. \\-\\-grpc_python_out=. ./data.proto\n\n##### 5.2.2 server\n\nserver为服务器端，server.py实现接受客户端发送的数据，并对数据进行处理后返回给客户端。FormatData的作用是将服务器端传过来的数据转换为大写，具体含义见相关代码和注释。\n\n```python\n#! /usr/bin/env python\n# -*- coding: utf-8 -*-\nimport grpc\nimport time\nfrom concurrent import futures #具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能\nfrom example import data_pb2\nfrom example import data_pb2_grpc\n\n_ONE_DAY_IN_SECONDS = 60*60*24\n_HOST='localhost'\n_PORT='8080'\n\nclass FormatData(data_pb2_grpc.FormatDataServicer):\n    def DoFormat(self,request,context):\n        str=request.text\n        return data_pb2.Data(text=str.upper())\n\ndef serve():\n    grpcServer=grpc.server(futures.ThreadPoolExecutor(max_workers=4))#最多有多少work并行执行任务\n    data_pb2_grpc.add_FormatDataServicer_to_server(FormatData(),grpcServer)# 添加函数方法和服务器，服务器端会进行反序列化。\n    grpcServer.add_insecure_port(_HOST+':'+_PORT) #建立服务器和端口\n    grpcServer.start()# 启动服务端\n    try:\n        while True:\n            time.sleep(_ONE_DAY_IN_SECONDS)\n    except KeyboardInterrupt:\n        grpcServer.stop(0)\n\n\nif __name__=='__main__':\n    serve()\n```\n\n##### 5.2.3 client\n\nclinet为客户端，client.py实现客户端发送数据，并接受server处理后返回的数据，具体含义见相关代码和注释。\n\n```python\n#! /usr/bin/env python\n# -*- coding: utf-8 -*-\nimport grpc\nfrom example import data_pb2,data_pb2_grpc\n\n_HOST='localhost'\n_PORT='8080'\n\ndef run():\n    conn=grpc.insecure_channel(_HOST+':'+_PORT)# 服务器信息\n    client=data_pb2_grpc.FormatDataStub(channel=conn) #客户端建立连接\n    for i in range(0,5):\n        respnse = client.DoFormat(data_pb2.Data(text='hello,world!'))  # 序列化数据传递过去\n        print(\"received: \" + respnse.text)\n\nif __name__=='__main__':\n    run()\n```\n\n接下来运行server.py来启动服务器，然后运行client.py便可以得到结果，可以看到所有数据均已大写。最后需要关闭服务器端，否则一直会处于运行状态。\n\n> received: HELLO,WORLD!\n> received: HELLO,WORLD!\n> received: HELLO,WORLD!\n> received: HELLO,WORLD!\n> received: HELLO,WORLD!\n\n### 6.基于google protobuf的RPC实现\n\n因为RPC需要我们实现通信，所以会有一定难度，代码量很大程度上也有增加，不方便在文中展现出来。所以我把代码放到了github上面，地址在[https://github.com/weizhixiaoyi/google-protobuf-service](https://github.com/weizhixiaoyi/google-protobuf-service)，有兴趣的可以看下。\n\n总的来说，protobuf RPC定义了一个抽象的RPC框架，RpcServiceStub和RpcService类是protobuf编译器根据proto定义生成的类，RpcService定义了服务端暴露给客户端的函数接口，具体实现需要用户自己继承这个类来实现。RpcServiceStub定义了服务端暴露函数的描述，并将客户端对RpcServiceStub中函数的调用统一转换到调用RpcChannel中的CallMethod方法，CallMethod通过RpcServiceStub传过来的函数描述符和函数参数对该次rpc调用进行encode，最终通过RpcConnecor发送给服务方。对方以客户端相反的过程最终调用RpcSerivice中定义的函数。\n\n![基于google-protobuf的gRPC实现-python版图片02](基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片02.png)\n\n事实上，protobuf rpc的框架只是RpcChannel中定义了空的CallMethod，所以具体怎样进行encode和调用RpcConnector都要自己实现。RpcConnector在protobuf中没有定义，所以这个完成由用户自己实现，它的作用就是收发rpc消息包。在服务端，RpcChannel通过调用RpcService中的CallMethod来具体调用RpcService中暴露给客户端的函数。\n\n**参考**\n\n> [用心阁-谁能用通俗的语言解释一下什么是 RPC 框架？](https://www.zhihu.com/question/25536695)\n>\n> [在于思考-python通过protobuf实现rpc](https://www.cnblogs.com/chengxuyuancc/p/5245749.html)\n\n### 7.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](基于google-protobuf的gRPC实现-python版/推广.png)\n\n","slug":"基于google-protobuf的gRPC实现-python版","published":1,"updated":"2018-08-14T15:33:51.053Z","_id":"cjktv5d3n000njiz5j4jt0gka","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"1-Protobuf简介\"><a href=\"#1-Protobuf简介\" class=\"headerlink\" title=\"1.Protobuf简介\"></a>1.Protobuf简介</h3><p><strong>Protobuf(Google Protocol Buffers)</strong>提供一种灵活、高效、自动化的机制，用于序列化结构数据。Protobuf仅需自定义一次所需要的数据格式，然后我们就可以使用Protobuf编译器自动生成各种语言的源码，方便我们读写自定义的格式化数据。另外Protobuf的使用与平台和语言无关，可以在不破坏原数据格式的基础上，扩展新的数据。</p>\n<p>我们可以将Protobuf与XML进行对比，但Protobuf更小、更快、更加简单。总结来说具有一下特点：</p>\n<ul>\n<li>性能好、效率高。Protobuf作用与XML、json类似，但它是二进制格式，所以性能更好。但同时因为是二进制格式，所以缺点也就是可读性差。</li>\n<li>代码生成机制，易于使用。</li>\n<li>解析速度快。</li>\n<li>支持多种语言，例C++、C#、Go、Java、Python等。</li>\n<li>向前兼容，向后兼容。</li>\n</ul>\n<h3 id=\"2-Protobuf安装\"><a href=\"#2-Protobuf安装\" class=\"headerlink\" title=\"2.Protobuf安装\"></a>2.Protobuf安装</h3><p>Mac用户可以使用brew进行安装，命令如下所示。</p>\n<blockquote>\n<p>brew install protobuf</p>\n</blockquote>\n<p>如需要安装特定版本，可以先进行搜索有哪些版本，命令如下所示。搜索完成之后，采用上述brew安装方法，安装特定版本即可。</p>\n<blockquote>\n<p>brew search protobuf</p>\n</blockquote>\n<p>安装完成之后，可以通过protoc --version查看是否安装成功。</p>\n<blockquote>\n<p>protoc --version<br>libprotoc 3.6.0</p>\n</blockquote>\n<p>另外可以通过which protoc命令查看protoc安装所在的位置。</p>\n<blockquote>\n<p>which protoc<br>/usr/local/bin/protoc</p>\n</blockquote>\n<h3 id=\"3-Protobuf实例\"><a href=\"#3-Protobuf实例\" class=\"headerlink\" title=\"3.Protobuf实例\"></a>3.Protobuf实例</h3><h4 id=\"3-1编译-proto文件\"><a href=\"#3-1编译-proto文件\" class=\"headerlink\" title=\"3.1编译.proto文件\"></a>3.1编译.proto文件</h4><p>首先我们需要创建一个以<strong>.proto</strong>结尾的文件，可以在其中定义<strong>message</strong>来指定所需要序列化的数据格式。每一个message都是一个小的信息逻辑单元，包含一系列的name-value值对。以官网上的示例，我们创建一个addressbook.proto文件，内容如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">syntax = <span class=\"string\">\"proto2\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">package tutorial;</span><br><span class=\"line\"></span><br><span class=\"line\">message Person &#123;</span><br><span class=\"line\">  required string name = <span class=\"number\">1</span>;</span><br><span class=\"line\">  required int32 id = <span class=\"number\">2</span>;</span><br><span class=\"line\">  optional string email = <span class=\"number\">3</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  enum PhoneType &#123;</span><br><span class=\"line\">    MOBILE = <span class=\"number\">0</span>;</span><br><span class=\"line\">    HOME = <span class=\"number\">1</span>;</span><br><span class=\"line\">    WORK = <span class=\"number\">2</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  message PhoneNumber &#123;</span><br><span class=\"line\">    required string number = <span class=\"number\">1</span>;</span><br><span class=\"line\">    optional PhoneType type = <span class=\"number\">2</span> [default = HOME];</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  repeated PhoneNumber phones = <span class=\"number\">4</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">message AddressBook &#123;</span><br><span class=\"line\">  repeated Person people = <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>syntax=”proto2”</strong>代表版本，目前支持proto2和proto3，不写默认proto2。</li>\n<li><strong>package</strong>类似于C++中的namespace概念。</li>\n<li><strong>message</strong>是包含了各种类型字段的聚集，相当于struct，并且可以嵌套。</li>\n<li>proto3版本去掉了required和optional类型，保留了repeated(数组)。其中“＝1”，“＝2”表示每个元素的标识号，它会用在二进制编码中对域的标识，[1,15]之内的标志符在使用时占用一个字节，[16,2047]之内的标识号则占用2个字节，所以从最优化角度考虑，可以将[1,15]使用在一些较常用或repeated的元素上。同时为了考虑将来可能会增加新的标志符，我们要事先预留一些标志符。</li>\n</ul>\n<p>构建好addressbook.proto文件后，运行Protobuf编译器编译.proto文件，运行方法如下所示。其中-I表示.protoc所在的路径，--python_out表示指定生成的目标文件存在的路径，最后的参数表示要编译的.proto文件。</p>\n<blockquote>\n<p>protoc -I=$SRC_DIR --python_out=$DST_DIR $SRC_DIR/addressbook.proto </p>\n</blockquote>\n<p>其中SRC_DIR为目录，如果处于当前目录的话，可通过如下所示命令来编译.proto文件。</p>\n<blockquote>\n<p>protoc -I=. --python_out=. addressbook.proto</p>\n</blockquote>\n<p>编译完成之后会生成addressbook_pb2.py文件，里面包含序列化和反序列化等方法。</p>\n<h3 id=\"3-2序列化\"><a href=\"#3-2序列化\" class=\"headerlink\" title=\"3.2序列化\"></a>3.2序列化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> addressbook_pb2</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">PromptForAddress</span><span class=\"params\">(person)</span>:</span></span><br><span class=\"line\">  person.id = int(raw_input(<span class=\"string\">\"Enter person ID number: \"</span>))</span><br><span class=\"line\">  person.name = raw_input(<span class=\"string\">\"Enter name: \"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  email = raw_input(<span class=\"string\">\"Enter email address (blank for none): \"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">if</span> email != <span class=\"string\">\"\"</span>:</span><br><span class=\"line\">    person.email = email</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">    number = raw_input(<span class=\"string\">\"Enter a phone number (or leave blank to finish): \"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> number == <span class=\"string\">\"\"</span>:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    phone_number = person.phones.add()</span><br><span class=\"line\">    phone_number.number = number</span><br><span class=\"line\"></span><br><span class=\"line\">    type = raw_input(<span class=\"string\">\"Is this a mobile, home, or work phone? \"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> type == <span class=\"string\">\"mobile\"</span>:</span><br><span class=\"line\">        phone_number.type = addressbook_pb2.Person.MOBILE</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> type == <span class=\"string\">\"home\"</span>:</span><br><span class=\"line\">        phone_number.type = addressbook_pb2.Person.HOME</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> type == <span class=\"string\">\"work\"</span>:</span><br><span class=\"line\">        phone_number.type = addressbook_pb2.Person.WORK</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"Unknown phone type; leaving as default value.\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(sys.argv) != <span class=\"number\">2</span>:</span><br><span class=\"line\">  <span class=\"keyword\">print</span> <span class=\"string\">\"Usage:\"</span>, sys.argv[<span class=\"number\">0</span>], <span class=\"string\">\"ADDRESS_BOOK_FILE\"</span></span><br><span class=\"line\">  sys.exit(<span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">address_book = addressbook_pb2.AddressBook()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the existing address book.</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">  f = open(sys.argv[<span class=\"number\">1</span>], <span class=\"string\">\"rb\"</span>)</span><br><span class=\"line\">  address_book.ParseFromString(f.read())</span><br><span class=\"line\">  f.close()</span><br><span class=\"line\"><span class=\"keyword\">except</span> IOError:</span><br><span class=\"line\">  <span class=\"keyword\">print</span> sys.argv[<span class=\"number\">1</span>] + <span class=\"string\">\": Could not open file.  Creating a new one.\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Add an address.</span></span><br><span class=\"line\">PromptForAddress(address_book.people.add())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Write the new address book back to disk.</span></span><br><span class=\"line\">f = open(sys.argv[<span class=\"number\">1</span>], <span class=\"string\">\"wb\"</span>)</span><br><span class=\"line\">f.write(address_book.SerializeToString())</span><br><span class=\"line\">f.close()</span><br></pre></td></tr></table></figure>\n<p>创建add_person.py文件，代码如上所示，然后通过SerializeToString()方法来进行序列化addressbook.proto中所定义的信息。如果想要运行上述代码的话，我们首先需要创建一个输入文件，例如命名为input.txt，不需输入值。然后采用<code>python add_person input.txt</code>，便可进行序列化所输入的数据。如果运行<code>python add_person</code>的话，不指定输入文件，则会报错。</p>\n<blockquote>\n<p>Enter person ID number: 1001<br>Enter name: 1001<br>Enter email address (blank for none): hello@email.com<br>Enter a phone number (or leave blank to finish): 10010<br>Is this a mobile, home, or work phone? work<br>Enter a phone number (or leave blank to finish): </p>\n</blockquote>\n<h3 id=\"3-3反序列化\"><a href=\"#3-3反序列化\" class=\"headerlink\" title=\"3.3反序列化\"></a>3.3反序列化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#! /usr/bin/python</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> addressbook_pb2</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Iterates though all people in the AddressBook and prints info about them.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ListPeople</span><span class=\"params\">(address_book)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> person <span class=\"keyword\">in</span> address_book.people:</span><br><span class=\"line\">    <span class=\"keyword\">print</span> <span class=\"string\">\"Person ID:\"</span>, person.id</span><br><span class=\"line\">    <span class=\"keyword\">print</span> <span class=\"string\">\"  Name:\"</span>, person.name</span><br><span class=\"line\">    <span class=\"keyword\">if</span> person.HasField(<span class=\"string\">'email'</span>):</span><br><span class=\"line\">      <span class=\"keyword\">print</span> <span class=\"string\">\"  E-mail address:\"</span>, person.email</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> phone_number <span class=\"keyword\">in</span> person.phones:</span><br><span class=\"line\">      <span class=\"keyword\">if</span> phone_number.type == addressbook_pb2.Person.MOBILE:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"  Mobile phone #: \"</span>,</span><br><span class=\"line\">      <span class=\"keyword\">elif</span> phone_number.type == addressbook_pb2.Person.HOME:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"  Home phone #: \"</span>,</span><br><span class=\"line\">      <span class=\"keyword\">elif</span> phone_number.type == addressbook_pb2.Person.WORK:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"  Work phone #: \"</span>,</span><br><span class=\"line\">      <span class=\"keyword\">print</span> phone_number.number</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Main procedure:  Reads the entire address book from a file and prints all</span></span><br><span class=\"line\"><span class=\"comment\">#   the information inside.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(sys.argv) != <span class=\"number\">2</span>:</span><br><span class=\"line\">  <span class=\"keyword\">print</span> <span class=\"string\">\"Usage:\"</span>, sys.argv[<span class=\"number\">0</span>], <span class=\"string\">\"ADDRESS_BOOK_FILE\"</span></span><br><span class=\"line\">  sys.exit(<span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">address_book = addressbook_pb2.AddressBook()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the existing address book.</span></span><br><span class=\"line\">f = open(sys.argv[<span class=\"number\">1</span>], <span class=\"string\">\"rb\"</span>)</span><br><span class=\"line\">address_book.ParseFromString(f.read())</span><br><span class=\"line\">f.close()</span><br><span class=\"line\"></span><br><span class=\"line\">ListPeople(address_book)</span><br></pre></td></tr></table></figure>\n<p>创建list_person.py文件来进行反序列化，代码如上所示。通过<code>python list_person.py input.txt</code>命令来执行上述代码，输出结果如下所示。</p>\n<blockquote>\n<p>Person ID: 1001<br>Name: 1001<br>E-mail address: hello@email.com<br>Work phone #:  10010</p>\n</blockquote>\n<h3 id=\"4-RPC简介\"><a href=\"#4-RPC简介\" class=\"headerlink\" title=\"4.RPC简介\"></a>4.RPC简介</h3><p>这里引用知乎用户<strong>用心阁</strong>关于<strong>谁能用通俗的语言解释一下什么是 RPC 框架？</strong>的问题答案来解释什么是RPC。<strong>RPC(Remote Procedure Call)</strong>是指远程过程调用，也就是说两台服务器A、B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间上，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。如果需要实现RPC，那么需要解决如下几个问题。</p>\n<ul>\n<li>通讯：主要是通过在客户端和服务器之间建立TCP连接，远程过程调用的所有交换的数据都在这个连接里传输。连接可以是按需连接，调用结束后就断掉，也可以是长连接，多个远程过程调用共享同一个连接。 </li>\n<li>寻址：A服务器上的应用怎么告诉底层的RPC框架，如何连接到B服务器（如主机或IP地址）以及特定的端口，方法的名称名称是什么。</li>\n<li>序列化：当A服务器上的应用发起远程过程调用时，方法的参数需要通过底层的网络协议，如TCP传递到B服务器。由于网络协议是基于二进制的，内存中的参数值要序列化成二进制的形式，也就是序列化（Serialize）或编组（marshal），通过寻址和传输将序列化的二进制发送给B服务器。 B服务器收到请求后，需要对参数进行反序列化，恢复为内存中的表达方式，然后找到对应的方法进行本地调用，然后得到返回值。 返回值还要发送回服务器A上的应用，也要经过序列化的方式发送，服务器A接到后，再反序列化，恢复为内存中的表达方式，交给A服务器上的应用 。</li>\n</ul>\n<p><img src=\"/2018/08/14/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片01.png\" alt=\"基于google-protobuf的gRPC实现-python版图片01\"></p>\n<p>总结来说，RPC提供一种透明调用机制让使用者不必显示区分本地调用还是远程调用。如上图所示，客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理<code>RpcProxy</code> 。代理封装调用信息并将调用转交给<code>RpcInvoker</code> 去实际执行。在客户端的<code>RpcInvoker</code> 通过连接器<code>RpcConnector</code> 去维持与服务端的通道<code>RpcChannel</code>，并使用<code>RpcProtocol</code> 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。RPC 服务端接收器 <code>RpcAcceptor</code> 接收客户端的调用请求，同样使用<code>RpcProtocol</code> 执行协议解码（decode）。解码后的调用信息传递给<code>RpcProcessor</code> 去控制处理调用过程，最后再委托调用给<code>RpcInvoker</code> 去实际执行并返回调用结果。</p>\n<h3 id=\"5-基于google-protobuf的gRPC实现\"><a href=\"#5-基于google-protobuf的gRPC实现\" class=\"headerlink\" title=\"5.基于google protobuf的gRPC实现\"></a>5.基于google protobuf的gRPC实现</h3><p>我们可以利用protobuf实现序列化和反序列化，但如何实现RPC通信呢。为简单起见，我们先介绍gRPC，gRPC是google构建的RPC框架，这样我们就不再考虑如何写通信方法。</p>\n<h4 id=\"5-1gRPC安装\"><a href=\"#5-1gRPC安装\" class=\"headerlink\" title=\"5.1gRPC安装\"></a>5.1gRPC安装</h4><p>首先安装gRPC，安装命令如下所示。</p>\n<blockquote>\n<p>pip install grpcio</p>\n</blockquote>\n<p>然后安装protobuf相关的依赖库。</p>\n<blockquote>\n<p>pip install protobuf</p>\n</blockquote>\n<p>然后安装python gRPC相关的protobuf相关文件。</p>\n<blockquote>\n<p>pip install grpcio-tools</p>\n</blockquote>\n<h4 id=\"5-2gRPC实例\"><a href=\"#5-2gRPC实例\" class=\"headerlink\" title=\"5.2gRPC实例\"></a>5.2gRPC实例</h4><p>创建三个文件夹，名称为example、server、client，里面内容如下所示，具体含义在后面解释。</p>\n<blockquote>\n<ul>\n<li>example<ul>\n<li>__init__.py</li>\n<li>data.proto</li>\n<li>data_pb2.py</li>\n<li>data_pb2_grpc.py</li>\n</ul>\n</li>\n<li>server<ul>\n<li>server.py</li>\n</ul>\n</li>\n<li>client<ul>\n<li>client.py</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<h5 id=\"5-2-1-example\"><a href=\"#5-2-1-example\" class=\"headerlink\" title=\"5.2.1 example\"></a>5.2.1 example</h5><p>example主要用于编写.proto文件并生成data接口，其中__init__.py的作用是方便其他文件夹引用example文件夹中文件，data.proto文件内容如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">syntax=<span class=\"string\">\"proto3\"</span>;</span><br><span class=\"line\">package example;</span><br><span class=\"line\"></span><br><span class=\"line\">message Data&#123;</span><br><span class=\"line\">    string text=<span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">service FormatData&#123;</span><br><span class=\"line\">    rpc DoFormat(Data) returns (Data) &#123;&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后在example目录下利用下述命令生成data_pb2.py和data_pb2_grpc.py文件。data_pb2.py用于序列化信息，data_pb2_grpc.py用于通信。</p>\n<blockquote>\n<p>python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. ./data.proto</p>\n</blockquote>\n<h5 id=\"5-2-2-server\"><a href=\"#5-2-2-server\" class=\"headerlink\" title=\"5.2.2 server\"></a>5.2.2 server</h5><p>server为服务器端，server.py实现接受客户端发送的数据，并对数据进行处理后返回给客户端。FormatData的作用是将服务器端传过来的数据转换为大写，具体含义见相关代码和注释。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#! /usr/bin/env python</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> grpc</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">from</span> concurrent <span class=\"keyword\">import</span> futures <span class=\"comment\">#具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> example <span class=\"keyword\">import</span> data_pb2</span><br><span class=\"line\"><span class=\"keyword\">from</span> example <span class=\"keyword\">import</span> data_pb2_grpc</span><br><span class=\"line\"></span><br><span class=\"line\">_ONE_DAY_IN_SECONDS = <span class=\"number\">60</span>*<span class=\"number\">60</span>*<span class=\"number\">24</span></span><br><span class=\"line\">_HOST=<span class=\"string\">'localhost'</span></span><br><span class=\"line\">_PORT=<span class=\"string\">'8080'</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FormatData</span><span class=\"params\">(data_pb2_grpc.FormatDataServicer)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">DoFormat</span><span class=\"params\">(self,request,context)</span>:</span></span><br><span class=\"line\">        str=request.text</span><br><span class=\"line\">        <span class=\"keyword\">return</span> data_pb2.Data(text=str.upper())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">serve</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    grpcServer=grpc.server(futures.ThreadPoolExecutor(max_workers=<span class=\"number\">4</span>))<span class=\"comment\">#最多有多少work并行执行任务</span></span><br><span class=\"line\">    data_pb2_grpc.add_FormatDataServicer_to_server(FormatData(),grpcServer)<span class=\"comment\"># 添加函数方法和服务器，服务器端会进行反序列化。</span></span><br><span class=\"line\">    grpcServer.add_insecure_port(_HOST+<span class=\"string\">':'</span>+_PORT) <span class=\"comment\">#建立服务器和端口</span></span><br><span class=\"line\">    grpcServer.start()<span class=\"comment\"># 启动服务端</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">            time.sleep(_ONE_DAY_IN_SECONDS)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> KeyboardInterrupt:</span><br><span class=\"line\">        grpcServer.stop(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    serve()</span><br></pre></td></tr></table></figure>\n<h5 id=\"5-2-3-client\"><a href=\"#5-2-3-client\" class=\"headerlink\" title=\"5.2.3 client\"></a>5.2.3 client</h5><p>clinet为客户端，client.py实现客户端发送数据，并接受server处理后返回的数据，具体含义见相关代码和注释。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#! /usr/bin/env python</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> grpc</span><br><span class=\"line\"><span class=\"keyword\">from</span> example <span class=\"keyword\">import</span> data_pb2,data_pb2_grpc</span><br><span class=\"line\"></span><br><span class=\"line\">_HOST=<span class=\"string\">'localhost'</span></span><br><span class=\"line\">_PORT=<span class=\"string\">'8080'</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    conn=grpc.insecure_channel(_HOST+<span class=\"string\">':'</span>+_PORT)<span class=\"comment\"># 服务器信息</span></span><br><span class=\"line\">    client=data_pb2_grpc.FormatDataStub(channel=conn) <span class=\"comment\">#客户端建立连接</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,<span class=\"number\">5</span>):</span><br><span class=\"line\">        respnse = client.DoFormat(data_pb2.Data(text=<span class=\"string\">'hello,world!'</span>))  <span class=\"comment\"># 序列化数据传递过去</span></span><br><span class=\"line\">        print(<span class=\"string\">\"received: \"</span> + respnse.text)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    run()</span><br></pre></td></tr></table></figure>\n<p>接下来运行server.py来启动服务器，然后运行client.py便可以得到结果，可以看到所有数据均已大写。最后需要关闭服务器端，否则一直会处于运行状态。</p>\n<blockquote>\n<p>received: HELLO,WORLD!<br>received: HELLO,WORLD!<br>received: HELLO,WORLD!<br>received: HELLO,WORLD!<br>received: HELLO,WORLD!</p>\n</blockquote>\n<h3 id=\"6-基于google-protobuf的RPC实现\"><a href=\"#6-基于google-protobuf的RPC实现\" class=\"headerlink\" title=\"6.基于google protobuf的RPC实现\"></a>6.基于google protobuf的RPC实现</h3><p>因为RPC需要我们实现通信，所以会有一定难度，代码量很大程度上也有增加，不方便在文中展现出来。所以我把代码放到了github上面，地址在<a href=\"https://github.com/weizhixiaoyi/google-protobuf-service\" target=\"_blank\" rel=\"noopener\">https://github.com/weizhixiaoyi/google-protobuf-service</a>，有兴趣的可以看下。</p>\n<p>总的来说，protobuf RPC定义了一个抽象的RPC框架，RpcServiceStub和RpcService类是protobuf编译器根据proto定义生成的类，RpcService定义了服务端暴露给客户端的函数接口，具体实现需要用户自己继承这个类来实现。RpcServiceStub定义了服务端暴露函数的描述，并将客户端对RpcServiceStub中函数的调用统一转换到调用RpcChannel中的CallMethod方法，CallMethod通过RpcServiceStub传过来的函数描述符和函数参数对该次rpc调用进行encode，最终通过RpcConnecor发送给服务方。对方以客户端相反的过程最终调用RpcSerivice中定义的函数。</p>\n<p><img src=\"/2018/08/14/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片02.png\" alt=\"基于google-protobuf的gRPC实现-python版图片02\"></p>\n<p>事实上，protobuf rpc的框架只是RpcChannel中定义了空的CallMethod，所以具体怎样进行encode和调用RpcConnector都要自己实现。RpcConnector在protobuf中没有定义，所以这个完成由用户自己实现，它的作用就是收发rpc消息包。在服务端，RpcChannel通过调用RpcService中的CallMethod来具体调用RpcService中暴露给客户端的函数。</p>\n<p><strong>参考</strong></p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/question/25536695\" target=\"_blank\" rel=\"noopener\">用心阁-谁能用通俗的语言解释一下什么是 RPC 框架？</a></p>\n<p><a href=\"https://www.cnblogs.com/chengxuyuancc/p/5245749.html\" target=\"_blank\" rel=\"noopener\">在于思考-python通过protobuf实现rpc</a></p>\n</blockquote>\n<h3 id=\"7-推广\"><a href=\"#7-推广\" class=\"headerlink\" title=\"7.推广\"></a>7.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/08/14/基于google-protobuf的gRPC实现-python版/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Protobuf简介\"><a href=\"#1-Protobuf简介\" class=\"headerlink\" title=\"1.Protobuf简介\"></a>1.Protobuf简介</h3><p><strong>Protobuf(Google Protocol Buffers)</strong>提供一种灵活、高效、自动化的机制，用于序列化结构数据。Protobuf仅需自定义一次所需要的数据格式，然后我们就可以使用Protobuf编译器自动生成各种语言的源码，方便我们读写自定义的格式化数据。另外Protobuf的使用与平台和语言无关，可以在不破坏原数据格式的基础上，扩展新的数据。</p>\n<p>我们可以将Protobuf与XML进行对比，但Protobuf更小、更快、更加简单。总结来说具有一下特点：</p>\n<ul>\n<li>性能好、效率高。Protobuf作用与XML、json类似，但它是二进制格式，所以性能更好。但同时因为是二进制格式，所以缺点也就是可读性差。</li>\n<li>代码生成机制，易于使用。</li>\n<li>解析速度快。</li>\n<li>支持多种语言，例C++、C#、Go、Java、Python等。</li>\n<li>向前兼容，向后兼容。</li>\n</ul>\n<h3 id=\"2-Protobuf安装\"><a href=\"#2-Protobuf安装\" class=\"headerlink\" title=\"2.Protobuf安装\"></a>2.Protobuf安装</h3><p>Mac用户可以使用brew进行安装，命令如下所示。</p>\n<blockquote>\n<p>brew install protobuf</p>\n</blockquote>\n<p>如需要安装特定版本，可以先进行搜索有哪些版本，命令如下所示。搜索完成之后，采用上述brew安装方法，安装特定版本即可。</p>\n<blockquote>\n<p>brew search protobuf</p>\n</blockquote>\n<p>安装完成之后，可以通过protoc --version查看是否安装成功。</p>\n<blockquote>\n<p>protoc --version<br>libprotoc 3.6.0</p>\n</blockquote>\n<p>另外可以通过which protoc命令查看protoc安装所在的位置。</p>\n<blockquote>\n<p>which protoc<br>/usr/local/bin/protoc</p>\n</blockquote>\n<h3 id=\"3-Protobuf实例\"><a href=\"#3-Protobuf实例\" class=\"headerlink\" title=\"3.Protobuf实例\"></a>3.Protobuf实例</h3><h4 id=\"3-1编译-proto文件\"><a href=\"#3-1编译-proto文件\" class=\"headerlink\" title=\"3.1编译.proto文件\"></a>3.1编译.proto文件</h4><p>首先我们需要创建一个以<strong>.proto</strong>结尾的文件，可以在其中定义<strong>message</strong>来指定所需要序列化的数据格式。每一个message都是一个小的信息逻辑单元，包含一系列的name-value值对。以官网上的示例，我们创建一个addressbook.proto文件，内容如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">syntax = <span class=\"string\">\"proto2\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">package tutorial;</span><br><span class=\"line\"></span><br><span class=\"line\">message Person &#123;</span><br><span class=\"line\">  required string name = <span class=\"number\">1</span>;</span><br><span class=\"line\">  required int32 id = <span class=\"number\">2</span>;</span><br><span class=\"line\">  optional string email = <span class=\"number\">3</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  enum PhoneType &#123;</span><br><span class=\"line\">    MOBILE = <span class=\"number\">0</span>;</span><br><span class=\"line\">    HOME = <span class=\"number\">1</span>;</span><br><span class=\"line\">    WORK = <span class=\"number\">2</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  message PhoneNumber &#123;</span><br><span class=\"line\">    required string number = <span class=\"number\">1</span>;</span><br><span class=\"line\">    optional PhoneType type = <span class=\"number\">2</span> [default = HOME];</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  repeated PhoneNumber phones = <span class=\"number\">4</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">message AddressBook &#123;</span><br><span class=\"line\">  repeated Person people = <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>syntax=”proto2”</strong>代表版本，目前支持proto2和proto3，不写默认proto2。</li>\n<li><strong>package</strong>类似于C++中的namespace概念。</li>\n<li><strong>message</strong>是包含了各种类型字段的聚集，相当于struct，并且可以嵌套。</li>\n<li>proto3版本去掉了required和optional类型，保留了repeated(数组)。其中“＝1”，“＝2”表示每个元素的标识号，它会用在二进制编码中对域的标识，[1,15]之内的标志符在使用时占用一个字节，[16,2047]之内的标识号则占用2个字节，所以从最优化角度考虑，可以将[1,15]使用在一些较常用或repeated的元素上。同时为了考虑将来可能会增加新的标志符，我们要事先预留一些标志符。</li>\n</ul>\n<p>构建好addressbook.proto文件后，运行Protobuf编译器编译.proto文件，运行方法如下所示。其中-I表示.protoc所在的路径，--python_out表示指定生成的目标文件存在的路径，最后的参数表示要编译的.proto文件。</p>\n<blockquote>\n<p>protoc -I=$SRC_DIR --python_out=$DST_DIR $SRC_DIR/addressbook.proto </p>\n</blockquote>\n<p>其中SRC_DIR为目录，如果处于当前目录的话，可通过如下所示命令来编译.proto文件。</p>\n<blockquote>\n<p>protoc -I=. --python_out=. addressbook.proto</p>\n</blockquote>\n<p>编译完成之后会生成addressbook_pb2.py文件，里面包含序列化和反序列化等方法。</p>\n<h3 id=\"3-2序列化\"><a href=\"#3-2序列化\" class=\"headerlink\" title=\"3.2序列化\"></a>3.2序列化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> addressbook_pb2</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">PromptForAddress</span><span class=\"params\">(person)</span>:</span></span><br><span class=\"line\">  person.id = int(raw_input(<span class=\"string\">\"Enter person ID number: \"</span>))</span><br><span class=\"line\">  person.name = raw_input(<span class=\"string\">\"Enter name: \"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  email = raw_input(<span class=\"string\">\"Enter email address (blank for none): \"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">if</span> email != <span class=\"string\">\"\"</span>:</span><br><span class=\"line\">    person.email = email</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">    number = raw_input(<span class=\"string\">\"Enter a phone number (or leave blank to finish): \"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> number == <span class=\"string\">\"\"</span>:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    phone_number = person.phones.add()</span><br><span class=\"line\">    phone_number.number = number</span><br><span class=\"line\"></span><br><span class=\"line\">    type = raw_input(<span class=\"string\">\"Is this a mobile, home, or work phone? \"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> type == <span class=\"string\">\"mobile\"</span>:</span><br><span class=\"line\">        phone_number.type = addressbook_pb2.Person.MOBILE</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> type == <span class=\"string\">\"home\"</span>:</span><br><span class=\"line\">        phone_number.type = addressbook_pb2.Person.HOME</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> type == <span class=\"string\">\"work\"</span>:</span><br><span class=\"line\">        phone_number.type = addressbook_pb2.Person.WORK</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"Unknown phone type; leaving as default value.\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(sys.argv) != <span class=\"number\">2</span>:</span><br><span class=\"line\">  <span class=\"keyword\">print</span> <span class=\"string\">\"Usage:\"</span>, sys.argv[<span class=\"number\">0</span>], <span class=\"string\">\"ADDRESS_BOOK_FILE\"</span></span><br><span class=\"line\">  sys.exit(<span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">address_book = addressbook_pb2.AddressBook()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the existing address book.</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">  f = open(sys.argv[<span class=\"number\">1</span>], <span class=\"string\">\"rb\"</span>)</span><br><span class=\"line\">  address_book.ParseFromString(f.read())</span><br><span class=\"line\">  f.close()</span><br><span class=\"line\"><span class=\"keyword\">except</span> IOError:</span><br><span class=\"line\">  <span class=\"keyword\">print</span> sys.argv[<span class=\"number\">1</span>] + <span class=\"string\">\": Could not open file.  Creating a new one.\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Add an address.</span></span><br><span class=\"line\">PromptForAddress(address_book.people.add())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Write the new address book back to disk.</span></span><br><span class=\"line\">f = open(sys.argv[<span class=\"number\">1</span>], <span class=\"string\">\"wb\"</span>)</span><br><span class=\"line\">f.write(address_book.SerializeToString())</span><br><span class=\"line\">f.close()</span><br></pre></td></tr></table></figure>\n<p>创建add_person.py文件，代码如上所示，然后通过SerializeToString()方法来进行序列化addressbook.proto中所定义的信息。如果想要运行上述代码的话，我们首先需要创建一个输入文件，例如命名为input.txt，不需输入值。然后采用<code>python add_person input.txt</code>，便可进行序列化所输入的数据。如果运行<code>python add_person</code>的话，不指定输入文件，则会报错。</p>\n<blockquote>\n<p>Enter person ID number: 1001<br>Enter name: 1001<br>Enter email address (blank for none): hello@email.com<br>Enter a phone number (or leave blank to finish): 10010<br>Is this a mobile, home, or work phone? work<br>Enter a phone number (or leave blank to finish): </p>\n</blockquote>\n<h3 id=\"3-3反序列化\"><a href=\"#3-3反序列化\" class=\"headerlink\" title=\"3.3反序列化\"></a>3.3反序列化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#! /usr/bin/python</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> addressbook_pb2</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Iterates though all people in the AddressBook and prints info about them.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ListPeople</span><span class=\"params\">(address_book)</span>:</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> person <span class=\"keyword\">in</span> address_book.people:</span><br><span class=\"line\">    <span class=\"keyword\">print</span> <span class=\"string\">\"Person ID:\"</span>, person.id</span><br><span class=\"line\">    <span class=\"keyword\">print</span> <span class=\"string\">\"  Name:\"</span>, person.name</span><br><span class=\"line\">    <span class=\"keyword\">if</span> person.HasField(<span class=\"string\">'email'</span>):</span><br><span class=\"line\">      <span class=\"keyword\">print</span> <span class=\"string\">\"  E-mail address:\"</span>, person.email</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> phone_number <span class=\"keyword\">in</span> person.phones:</span><br><span class=\"line\">      <span class=\"keyword\">if</span> phone_number.type == addressbook_pb2.Person.MOBILE:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"  Mobile phone #: \"</span>,</span><br><span class=\"line\">      <span class=\"keyword\">elif</span> phone_number.type == addressbook_pb2.Person.HOME:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"  Home phone #: \"</span>,</span><br><span class=\"line\">      <span class=\"keyword\">elif</span> phone_number.type == addressbook_pb2.Person.WORK:</span><br><span class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"  Work phone #: \"</span>,</span><br><span class=\"line\">      <span class=\"keyword\">print</span> phone_number.number</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Main procedure:  Reads the entire address book from a file and prints all</span></span><br><span class=\"line\"><span class=\"comment\">#   the information inside.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(sys.argv) != <span class=\"number\">2</span>:</span><br><span class=\"line\">  <span class=\"keyword\">print</span> <span class=\"string\">\"Usage:\"</span>, sys.argv[<span class=\"number\">0</span>], <span class=\"string\">\"ADDRESS_BOOK_FILE\"</span></span><br><span class=\"line\">  sys.exit(<span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">address_book = addressbook_pb2.AddressBook()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the existing address book.</span></span><br><span class=\"line\">f = open(sys.argv[<span class=\"number\">1</span>], <span class=\"string\">\"rb\"</span>)</span><br><span class=\"line\">address_book.ParseFromString(f.read())</span><br><span class=\"line\">f.close()</span><br><span class=\"line\"></span><br><span class=\"line\">ListPeople(address_book)</span><br></pre></td></tr></table></figure>\n<p>创建list_person.py文件来进行反序列化，代码如上所示。通过<code>python list_person.py input.txt</code>命令来执行上述代码，输出结果如下所示。</p>\n<blockquote>\n<p>Person ID: 1001<br>Name: 1001<br>E-mail address: hello@email.com<br>Work phone #:  10010</p>\n</blockquote>\n<h3 id=\"4-RPC简介\"><a href=\"#4-RPC简介\" class=\"headerlink\" title=\"4.RPC简介\"></a>4.RPC简介</h3><p>这里引用知乎用户<strong>用心阁</strong>关于<strong>谁能用通俗的语言解释一下什么是 RPC 框架？</strong>的问题答案来解释什么是RPC。<strong>RPC(Remote Procedure Call)</strong>是指远程过程调用，也就是说两台服务器A、B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间上，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。如果需要实现RPC，那么需要解决如下几个问题。</p>\n<ul>\n<li>通讯：主要是通过在客户端和服务器之间建立TCP连接，远程过程调用的所有交换的数据都在这个连接里传输。连接可以是按需连接，调用结束后就断掉，也可以是长连接，多个远程过程调用共享同一个连接。 </li>\n<li>寻址：A服务器上的应用怎么告诉底层的RPC框架，如何连接到B服务器（如主机或IP地址）以及特定的端口，方法的名称名称是什么。</li>\n<li>序列化：当A服务器上的应用发起远程过程调用时，方法的参数需要通过底层的网络协议，如TCP传递到B服务器。由于网络协议是基于二进制的，内存中的参数值要序列化成二进制的形式，也就是序列化（Serialize）或编组（marshal），通过寻址和传输将序列化的二进制发送给B服务器。 B服务器收到请求后，需要对参数进行反序列化，恢复为内存中的表达方式，然后找到对应的方法进行本地调用，然后得到返回值。 返回值还要发送回服务器A上的应用，也要经过序列化的方式发送，服务器A接到后，再反序列化，恢复为内存中的表达方式，交给A服务器上的应用 。</li>\n</ul>\n<p><img src=\"/2018/08/14/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片01.png\" alt=\"基于google-protobuf的gRPC实现-python版图片01\"></p>\n<p>总结来说，RPC提供一种透明调用机制让使用者不必显示区分本地调用还是远程调用。如上图所示，客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理<code>RpcProxy</code> 。代理封装调用信息并将调用转交给<code>RpcInvoker</code> 去实际执行。在客户端的<code>RpcInvoker</code> 通过连接器<code>RpcConnector</code> 去维持与服务端的通道<code>RpcChannel</code>，并使用<code>RpcProtocol</code> 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。RPC 服务端接收器 <code>RpcAcceptor</code> 接收客户端的调用请求，同样使用<code>RpcProtocol</code> 执行协议解码（decode）。解码后的调用信息传递给<code>RpcProcessor</code> 去控制处理调用过程，最后再委托调用给<code>RpcInvoker</code> 去实际执行并返回调用结果。</p>\n<h3 id=\"5-基于google-protobuf的gRPC实现\"><a href=\"#5-基于google-protobuf的gRPC实现\" class=\"headerlink\" title=\"5.基于google protobuf的gRPC实现\"></a>5.基于google protobuf的gRPC实现</h3><p>我们可以利用protobuf实现序列化和反序列化，但如何实现RPC通信呢。为简单起见，我们先介绍gRPC，gRPC是google构建的RPC框架，这样我们就不再考虑如何写通信方法。</p>\n<h4 id=\"5-1gRPC安装\"><a href=\"#5-1gRPC安装\" class=\"headerlink\" title=\"5.1gRPC安装\"></a>5.1gRPC安装</h4><p>首先安装gRPC，安装命令如下所示。</p>\n<blockquote>\n<p>pip install grpcio</p>\n</blockquote>\n<p>然后安装protobuf相关的依赖库。</p>\n<blockquote>\n<p>pip install protobuf</p>\n</blockquote>\n<p>然后安装python gRPC相关的protobuf相关文件。</p>\n<blockquote>\n<p>pip install grpcio-tools</p>\n</blockquote>\n<h4 id=\"5-2gRPC实例\"><a href=\"#5-2gRPC实例\" class=\"headerlink\" title=\"5.2gRPC实例\"></a>5.2gRPC实例</h4><p>创建三个文件夹，名称为example、server、client，里面内容如下所示，具体含义在后面解释。</p>\n<blockquote>\n<ul>\n<li>example<ul>\n<li>__init__.py</li>\n<li>data.proto</li>\n<li>data_pb2.py</li>\n<li>data_pb2_grpc.py</li>\n</ul>\n</li>\n<li>server<ul>\n<li>server.py</li>\n</ul>\n</li>\n<li>client<ul>\n<li>client.py</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<h5 id=\"5-2-1-example\"><a href=\"#5-2-1-example\" class=\"headerlink\" title=\"5.2.1 example\"></a>5.2.1 example</h5><p>example主要用于编写.proto文件并生成data接口，其中__init__.py的作用是方便其他文件夹引用example文件夹中文件，data.proto文件内容如下所示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">syntax=<span class=\"string\">\"proto3\"</span>;</span><br><span class=\"line\">package example;</span><br><span class=\"line\"></span><br><span class=\"line\">message Data&#123;</span><br><span class=\"line\">    string text=<span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">service FormatData&#123;</span><br><span class=\"line\">    rpc DoFormat(Data) returns (Data) &#123;&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后在example目录下利用下述命令生成data_pb2.py和data_pb2_grpc.py文件。data_pb2.py用于序列化信息，data_pb2_grpc.py用于通信。</p>\n<blockquote>\n<p>python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. ./data.proto</p>\n</blockquote>\n<h5 id=\"5-2-2-server\"><a href=\"#5-2-2-server\" class=\"headerlink\" title=\"5.2.2 server\"></a>5.2.2 server</h5><p>server为服务器端，server.py实现接受客户端发送的数据，并对数据进行处理后返回给客户端。FormatData的作用是将服务器端传过来的数据转换为大写，具体含义见相关代码和注释。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#! /usr/bin/env python</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> grpc</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">from</span> concurrent <span class=\"keyword\">import</span> futures <span class=\"comment\">#具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> example <span class=\"keyword\">import</span> data_pb2</span><br><span class=\"line\"><span class=\"keyword\">from</span> example <span class=\"keyword\">import</span> data_pb2_grpc</span><br><span class=\"line\"></span><br><span class=\"line\">_ONE_DAY_IN_SECONDS = <span class=\"number\">60</span>*<span class=\"number\">60</span>*<span class=\"number\">24</span></span><br><span class=\"line\">_HOST=<span class=\"string\">'localhost'</span></span><br><span class=\"line\">_PORT=<span class=\"string\">'8080'</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FormatData</span><span class=\"params\">(data_pb2_grpc.FormatDataServicer)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">DoFormat</span><span class=\"params\">(self,request,context)</span>:</span></span><br><span class=\"line\">        str=request.text</span><br><span class=\"line\">        <span class=\"keyword\">return</span> data_pb2.Data(text=str.upper())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">serve</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    grpcServer=grpc.server(futures.ThreadPoolExecutor(max_workers=<span class=\"number\">4</span>))<span class=\"comment\">#最多有多少work并行执行任务</span></span><br><span class=\"line\">    data_pb2_grpc.add_FormatDataServicer_to_server(FormatData(),grpcServer)<span class=\"comment\"># 添加函数方法和服务器，服务器端会进行反序列化。</span></span><br><span class=\"line\">    grpcServer.add_insecure_port(_HOST+<span class=\"string\">':'</span>+_PORT) <span class=\"comment\">#建立服务器和端口</span></span><br><span class=\"line\">    grpcServer.start()<span class=\"comment\"># 启动服务端</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">            time.sleep(_ONE_DAY_IN_SECONDS)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> KeyboardInterrupt:</span><br><span class=\"line\">        grpcServer.stop(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    serve()</span><br></pre></td></tr></table></figure>\n<h5 id=\"5-2-3-client\"><a href=\"#5-2-3-client\" class=\"headerlink\" title=\"5.2.3 client\"></a>5.2.3 client</h5><p>clinet为客户端，client.py实现客户端发送数据，并接受server处理后返回的数据，具体含义见相关代码和注释。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#! /usr/bin/env python</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> grpc</span><br><span class=\"line\"><span class=\"keyword\">from</span> example <span class=\"keyword\">import</span> data_pb2,data_pb2_grpc</span><br><span class=\"line\"></span><br><span class=\"line\">_HOST=<span class=\"string\">'localhost'</span></span><br><span class=\"line\">_PORT=<span class=\"string\">'8080'</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    conn=grpc.insecure_channel(_HOST+<span class=\"string\">':'</span>+_PORT)<span class=\"comment\"># 服务器信息</span></span><br><span class=\"line\">    client=data_pb2_grpc.FormatDataStub(channel=conn) <span class=\"comment\">#客户端建立连接</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,<span class=\"number\">5</span>):</span><br><span class=\"line\">        respnse = client.DoFormat(data_pb2.Data(text=<span class=\"string\">'hello,world!'</span>))  <span class=\"comment\"># 序列化数据传递过去</span></span><br><span class=\"line\">        print(<span class=\"string\">\"received: \"</span> + respnse.text)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    run()</span><br></pre></td></tr></table></figure>\n<p>接下来运行server.py来启动服务器，然后运行client.py便可以得到结果，可以看到所有数据均已大写。最后需要关闭服务器端，否则一直会处于运行状态。</p>\n<blockquote>\n<p>received: HELLO,WORLD!<br>received: HELLO,WORLD!<br>received: HELLO,WORLD!<br>received: HELLO,WORLD!<br>received: HELLO,WORLD!</p>\n</blockquote>\n<h3 id=\"6-基于google-protobuf的RPC实现\"><a href=\"#6-基于google-protobuf的RPC实现\" class=\"headerlink\" title=\"6.基于google protobuf的RPC实现\"></a>6.基于google protobuf的RPC实现</h3><p>因为RPC需要我们实现通信，所以会有一定难度，代码量很大程度上也有增加，不方便在文中展现出来。所以我把代码放到了github上面，地址在<a href=\"https://github.com/weizhixiaoyi/google-protobuf-service\" target=\"_blank\" rel=\"noopener\">https://github.com/weizhixiaoyi/google-protobuf-service</a>，有兴趣的可以看下。</p>\n<p>总的来说，protobuf RPC定义了一个抽象的RPC框架，RpcServiceStub和RpcService类是protobuf编译器根据proto定义生成的类，RpcService定义了服务端暴露给客户端的函数接口，具体实现需要用户自己继承这个类来实现。RpcServiceStub定义了服务端暴露函数的描述，并将客户端对RpcServiceStub中函数的调用统一转换到调用RpcChannel中的CallMethod方法，CallMethod通过RpcServiceStub传过来的函数描述符和函数参数对该次rpc调用进行encode，最终通过RpcConnecor发送给服务方。对方以客户端相反的过程最终调用RpcSerivice中定义的函数。</p>\n<p><img src=\"/2018/08/14/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片02.png\" alt=\"基于google-protobuf的gRPC实现-python版图片02\"></p>\n<p>事实上，protobuf rpc的框架只是RpcChannel中定义了空的CallMethod，所以具体怎样进行encode和调用RpcConnector都要自己实现。RpcConnector在protobuf中没有定义，所以这个完成由用户自己实现，它的作用就是收发rpc消息包。在服务端，RpcChannel通过调用RpcService中的CallMethod来具体调用RpcService中暴露给客户端的函数。</p>\n<p><strong>参考</strong></p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/question/25536695\" target=\"_blank\" rel=\"noopener\">用心阁-谁能用通俗的语言解释一下什么是 RPC 框架？</a></p>\n<p><a href=\"https://www.cnblogs.com/chengxuyuancc/p/5245749.html\" target=\"_blank\" rel=\"noopener\">在于思考-python通过protobuf实现rpc</a></p>\n</blockquote>\n<h3 id=\"7-推广\"><a href=\"#7-推广\" class=\"headerlink\" title=\"7.推广\"></a>7.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/08/14/基于google-protobuf的gRPC实现-python版/推广.png\" alt=\"推广\"></p>\n"},{"title":"效率软件推荐（一）","date":"2018-06-12T12:39:52.000Z","comments":1,"_content":"\n工欲善其事，必先利其器，为此给大家推荐几款效率软件，帮助你高效学习和工作。\n+  滴答清单：解决拖延症&&健忘症，高效完成任务和规划时间。\n+  印象笔记： 收集各类信息，捕捉灵感，做你的**第二大脑**。\n+  夸克浏览器：极简风格，满足你对浏览器最本质的需求。\n\n### 1.滴答清单\n身边的人多多少都有点拖延症(包括我)，不到DeadLine的时候，都不会想到去解决问题。为此还耽误了几次事情，但被老师或同学批评几次之后，痛定思痛，决定认真改变拖延症的习惯。奈何自制力又不是太强，没监督或者提醒的话，不久之后又恢复到以前的状态。某天闲逛知乎中，发现滴答清单这款软件，真心不错，用一段时间之后，我多年的老(tuo)寒(yan)腿(zheng)也给治愈啦。另外，多逛知乎也是有用处的，硬是逛出来一篇中文核心论文《面向知乎的个性化推荐模型研究》，在知网现在也能检索到啦，大雾。\n\n解决拖延症，首要具备的便是任务提醒功能，滴答清单默认每天9点通知该天内所有事项。另外，如果为某件事情设置了具体时间，滴答将会在该时间点时进行提醒。对于未完成的任务，滴答清单图标右上角会时刻显示通知，督促自己完成。\n\n针对需要完成的每项任务，可增加标签和项目分组。通过标签和分组的结合，能够将任务具体到生活中的各个方面。对于不同的任务，可设置高、中、低、无优先级，优先完成最重要的事情，即使在有限的时间内，也能高效的完成工作。比如**文章写作**这项任务，我定义为中午12点之前完成，标签为公众号和博客，分组到写作模块。当然想要对某项任务添加更具体的描述时，滴答清单支持照片、位置、录音、附件等信息。当完成某项任务，消除待办事项时，发出滴答的声音，满满的自豪感。\n\n![软件推荐滴答清单01](效率软件推荐（一）/软件推荐滴答清单01.PNG)\n\n滴答清单支持在日历上添加任务，当接到某项任务时，随手记下来，当作备忘录，以防忘记。如需回顾过去某天的任务，可根据日期进行查看具体事项。另外滴答清单内置番茄计时，让自己更加专注工作。\n\n![软件推荐滴答清单02](效率软件推荐（一）/软件推荐滴答清单02.PNG)\n\n滴答清单风格偏于简约型，没有任何多余功能，看着就非常舒服。当然想要体验更多功能的话，可以升级到高级会员，但我也没有体验过，在此也就不再介绍，其实普通版足以满足个人需求。利用滴答清单设定任务提醒，能够有效督促自己，克服拖延症。对于生活中的日常事情，可随手添加到滴答之中，以防忘记。通过滴答清单，帮助你高效完成任务和规划时间，在滴答清单中记录和规划事情，用更少的时间达到目标。\n\n### 2.印象笔记\n学习和工作之中，必然包含着各种各样的信息，如果仅仅是通过大脑记忆的话，无法达到对信息的有效收集和整理。另外当需要寻找以往的知识或资料时，很难能够从大脑之中检索出来，当然学霸除外。因此，需要寻找**第二大脑**来帮助我们收集和整理信息，当需要利用以往知识点的时候，快速检索并加以利用。\n好，主角也该上场啦，印象笔记。通过印象笔记在手机、平板、电脑上捕捉和共享灵感，跨平台的印象笔记帮助你永久保存所有信息，将想法转换为行动，做你的第二大脑。\n先说一下我是怎么利用利用印象笔记的吧。印象笔记支持标签和笔记本两种分类模式，两种方式的结合能够覆盖到生活和学习之中的方方面面。信息对于我来说分为三个模式，分别为**信息输入**、**信息加工(学习)**、**信息输出**，所以我也就将笔记本分为三种模式，记为0、1、2。\n\n信息输入模块包含两个笔记本，第一个笔记本为杂乱信息收集(0Inbox)，白天收集到的信息全部放在该笔记本之中，然后晚上将收集到的信息细分到信息加工模块。第二个笔记本为灵感收集(0InSpiration)，将突然想到的灵感记录到该笔记本之中，然后利用空闲时间进行扩充和整理。\n\n信息加工(学习)模块用于记录生活、学习、工作之中的事情。生活(1HeartOfLife)、学习(1SelfStudy)、工作(1WorkStatus)笔记本组再分为相应笔记本，笔记本之中的每个笔记另外加上标签，当我们需要寻找过去的某条笔记或者知识点时，利用印象笔记的搜索功能能够秒搜索到所需要的信息。\n\n信息输出模块是将以往所收集到的信息进行总结、深度加工整理到该笔记本之中，另外个人重要信息也会收集到之中。比如我在信息加工(学习)模块总结得到的经验和教训、所写的文章、项目规划等都会记录到信息输出模块。从杂乱的信息之中，经过上述三步便能进行结构化处理，方便自己学习，也方便以后利用。\n\n![软件推荐印象笔记01](效率软件推荐（一）/软件推荐印象笔记01.PNG)\n\n印象笔记的笔记本只支持二级分组，如果想要覆盖生活中所有知识的话，创建很多笔记本，难免会冗余。此时我们可以利用印象笔记的分类标签，标签支持无限嵌套。写笔记时对每个笔记添加不同的标签，这样就可以覆盖各个方面的知识点。\n\n![软件推荐印象笔记02](效率软件推荐（一）/软件推荐印象笔记02.PNG)\n\n另外印象笔记支持多种收集方式，比如在网页之中，如果看到有趣的信息，便可利用印象笔记剪辑功能，一步保存网页之中的信息。另外我个人也关注了很多公众号，对于有趣的信息，通常想要收藏到印象笔记之中。我们只需要预先关注**我的印象笔记**服务号，绑定个人信息之后，找到想要收藏的公众号文章，只需一键发送到公众号，便可收集到个人印象笔记之中。同时印象笔记支持添加语音，pdf文档等形式的信息。\n\n![软件推荐印象笔记03](效率软件推荐（一）/软件推荐印象笔记03.PNG)\n\n![软件推荐印象笔记04](效率软件推荐（一）/软件推荐印象笔记04.PNG)\n\n印象笔记分为三种模式，分别为免费账户、标准账户、高级账户。免费账户功能较少，支持两台同步设备，每月上传流量为60M，支持搜索图片内的文字等功能。标准账户和高级账户功能差不多，但目前印象笔记高级账户在做活动，价格更便宜一些，年费价格大概是80左右。对于付费产品，我的看法是价格反映的是产品的品质，至于贵不贵，取决于它对人的价值。\n\n印象笔记高级账户支持多平台登陆(无限制)、同步所有设备、每月10G上传流量、标注pdf功能、笔记演示、文档搜索等功能。说实话，印象笔记中的搜索功能非常强，图片和文档之中的信息能够轻松搜索出来。即使忘记笔记放在哪个笔记本分组里，也可以秒搜索到。最后还有个很惊艳的功能，文档扫描。大一时候记了很多高数笔记，但到大三之后，又不舍的扔，又没有空闲地方去放置。所以利用印象笔记文档扫描功能将所有的高数笔记扫描到印象笔记之中，扫描的内容非常清晰，而且还可以在扫描后的文档上进行编辑。笔记移动到印象笔记之后，想要找高数中知识点，只需搜索相应内容即可，基本上是秒找到高数内容。如果是纸质笔记本的话，还需要一页页去查询，很繁琐。利用印象笔记的话，节省了很大功夫，现在纸质笔记本也就可以直接扔啦。\n\n利用印象笔记，记录生活、工作、学习中方方面面知识点，将所有信息保存到**第二大脑**之中，然后结构化管理，智能记录，有序生活。\n\n![软件推荐印象笔记05](效率软件推荐（一）/软件推荐印象笔记05.PNG)\n\n### 3.夸克浏览器\n\n电脑上浏览器我也就不推荐啦，多数用的都是Google Chrome浏览器，优点很多，大家慢慢探索就好。在这儿推荐下手机上的一个小众浏览器，**夸克**。\n\n我也曾经用过iPhone版Chrome浏览器，但每次想要搜索东西的时候，打开软件的时间就需要5s，就算进入到搜索界面之后也会有一些杂乱信息，很容易让自己分心，影响工作效率。这里更别提手机版的百度首页了，信息杂乱程度惨不忍睹。而夸克浏览器就不一样了，打开软件时间基本上是在1s左右，进入软件之后，只留有一个搜索框，没必要做任何多余的事情，果然是够快、够简约。\n\n![软件推荐夸克浏览器01](效率软件推荐（一）/软件推荐夸克浏览器01.PNG)\n\n简约并不意味着简单，夸克浏览器简约之中更是带着很多亮点功能。浏览器底部可以进入一些常用网站，比如少数派、爱范儿、简书等，在小程序出来之前，我经常用这种方式看资讯，使用体验相当不错。对于一些需要用到的网站，但是又不想去下载APP，这样倒是一个不错的解决方法。当然现在小程序出来之后，在小程序看资讯也是很好的方法。\n\n![软件推荐夸克浏览器02](效率软件推荐（一）/软件推荐夸克浏览器02.PNG)\n\n总而言之、言而总之，夸克是一款以轻、快为核心，设计风格简约的浏览器，是一款专注用户浏览体验的信息获取工具。启动时无任何多余加载项，瞬间启动无需等待。利用极简思路解决信息冗余，满足用户对于浏览器最本质的需求。同时浏览器本身从底栏自动缩放、菜单分层设计、导航栏设置等方面，给用户带来沉浸式的浏览体验。\n\n介于篇幅有限，这篇文章就介绍这么多，下篇文章将重点介绍YoMail、Kindle、Typora、Ulysse软件。\n\n### 4.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/效率软件推荐（一）.md","raw":"---\ntitle: 效率软件推荐（一）\ndate: 2018-06-12 20:39:52\ntags: [效率软件]\ncategories: 软件\ncomments: true\n---\n\n工欲善其事，必先利其器，为此给大家推荐几款效率软件，帮助你高效学习和工作。\n+  滴答清单：解决拖延症&&健忘症，高效完成任务和规划时间。\n+  印象笔记： 收集各类信息，捕捉灵感，做你的**第二大脑**。\n+  夸克浏览器：极简风格，满足你对浏览器最本质的需求。\n\n### 1.滴答清单\n身边的人多多少都有点拖延症(包括我)，不到DeadLine的时候，都不会想到去解决问题。为此还耽误了几次事情，但被老师或同学批评几次之后，痛定思痛，决定认真改变拖延症的习惯。奈何自制力又不是太强，没监督或者提醒的话，不久之后又恢复到以前的状态。某天闲逛知乎中，发现滴答清单这款软件，真心不错，用一段时间之后，我多年的老(tuo)寒(yan)腿(zheng)也给治愈啦。另外，多逛知乎也是有用处的，硬是逛出来一篇中文核心论文《面向知乎的个性化推荐模型研究》，在知网现在也能检索到啦，大雾。\n\n解决拖延症，首要具备的便是任务提醒功能，滴答清单默认每天9点通知该天内所有事项。另外，如果为某件事情设置了具体时间，滴答将会在该时间点时进行提醒。对于未完成的任务，滴答清单图标右上角会时刻显示通知，督促自己完成。\n\n针对需要完成的每项任务，可增加标签和项目分组。通过标签和分组的结合，能够将任务具体到生活中的各个方面。对于不同的任务，可设置高、中、低、无优先级，优先完成最重要的事情，即使在有限的时间内，也能高效的完成工作。比如**文章写作**这项任务，我定义为中午12点之前完成，标签为公众号和博客，分组到写作模块。当然想要对某项任务添加更具体的描述时，滴答清单支持照片、位置、录音、附件等信息。当完成某项任务，消除待办事项时，发出滴答的声音，满满的自豪感。\n\n![软件推荐滴答清单01](效率软件推荐（一）/软件推荐滴答清单01.PNG)\n\n滴答清单支持在日历上添加任务，当接到某项任务时，随手记下来，当作备忘录，以防忘记。如需回顾过去某天的任务，可根据日期进行查看具体事项。另外滴答清单内置番茄计时，让自己更加专注工作。\n\n![软件推荐滴答清单02](效率软件推荐（一）/软件推荐滴答清单02.PNG)\n\n滴答清单风格偏于简约型，没有任何多余功能，看着就非常舒服。当然想要体验更多功能的话，可以升级到高级会员，但我也没有体验过，在此也就不再介绍，其实普通版足以满足个人需求。利用滴答清单设定任务提醒，能够有效督促自己，克服拖延症。对于生活中的日常事情，可随手添加到滴答之中，以防忘记。通过滴答清单，帮助你高效完成任务和规划时间，在滴答清单中记录和规划事情，用更少的时间达到目标。\n\n### 2.印象笔记\n学习和工作之中，必然包含着各种各样的信息，如果仅仅是通过大脑记忆的话，无法达到对信息的有效收集和整理。另外当需要寻找以往的知识或资料时，很难能够从大脑之中检索出来，当然学霸除外。因此，需要寻找**第二大脑**来帮助我们收集和整理信息，当需要利用以往知识点的时候，快速检索并加以利用。\n好，主角也该上场啦，印象笔记。通过印象笔记在手机、平板、电脑上捕捉和共享灵感，跨平台的印象笔记帮助你永久保存所有信息，将想法转换为行动，做你的第二大脑。\n先说一下我是怎么利用利用印象笔记的吧。印象笔记支持标签和笔记本两种分类模式，两种方式的结合能够覆盖到生活和学习之中的方方面面。信息对于我来说分为三个模式，分别为**信息输入**、**信息加工(学习)**、**信息输出**，所以我也就将笔记本分为三种模式，记为0、1、2。\n\n信息输入模块包含两个笔记本，第一个笔记本为杂乱信息收集(0Inbox)，白天收集到的信息全部放在该笔记本之中，然后晚上将收集到的信息细分到信息加工模块。第二个笔记本为灵感收集(0InSpiration)，将突然想到的灵感记录到该笔记本之中，然后利用空闲时间进行扩充和整理。\n\n信息加工(学习)模块用于记录生活、学习、工作之中的事情。生活(1HeartOfLife)、学习(1SelfStudy)、工作(1WorkStatus)笔记本组再分为相应笔记本，笔记本之中的每个笔记另外加上标签，当我们需要寻找过去的某条笔记或者知识点时，利用印象笔记的搜索功能能够秒搜索到所需要的信息。\n\n信息输出模块是将以往所收集到的信息进行总结、深度加工整理到该笔记本之中，另外个人重要信息也会收集到之中。比如我在信息加工(学习)模块总结得到的经验和教训、所写的文章、项目规划等都会记录到信息输出模块。从杂乱的信息之中，经过上述三步便能进行结构化处理，方便自己学习，也方便以后利用。\n\n![软件推荐印象笔记01](效率软件推荐（一）/软件推荐印象笔记01.PNG)\n\n印象笔记的笔记本只支持二级分组，如果想要覆盖生活中所有知识的话，创建很多笔记本，难免会冗余。此时我们可以利用印象笔记的分类标签，标签支持无限嵌套。写笔记时对每个笔记添加不同的标签，这样就可以覆盖各个方面的知识点。\n\n![软件推荐印象笔记02](效率软件推荐（一）/软件推荐印象笔记02.PNG)\n\n另外印象笔记支持多种收集方式，比如在网页之中，如果看到有趣的信息，便可利用印象笔记剪辑功能，一步保存网页之中的信息。另外我个人也关注了很多公众号，对于有趣的信息，通常想要收藏到印象笔记之中。我们只需要预先关注**我的印象笔记**服务号，绑定个人信息之后，找到想要收藏的公众号文章，只需一键发送到公众号，便可收集到个人印象笔记之中。同时印象笔记支持添加语音，pdf文档等形式的信息。\n\n![软件推荐印象笔记03](效率软件推荐（一）/软件推荐印象笔记03.PNG)\n\n![软件推荐印象笔记04](效率软件推荐（一）/软件推荐印象笔记04.PNG)\n\n印象笔记分为三种模式，分别为免费账户、标准账户、高级账户。免费账户功能较少，支持两台同步设备，每月上传流量为60M，支持搜索图片内的文字等功能。标准账户和高级账户功能差不多，但目前印象笔记高级账户在做活动，价格更便宜一些，年费价格大概是80左右。对于付费产品，我的看法是价格反映的是产品的品质，至于贵不贵，取决于它对人的价值。\n\n印象笔记高级账户支持多平台登陆(无限制)、同步所有设备、每月10G上传流量、标注pdf功能、笔记演示、文档搜索等功能。说实话，印象笔记中的搜索功能非常强，图片和文档之中的信息能够轻松搜索出来。即使忘记笔记放在哪个笔记本分组里，也可以秒搜索到。最后还有个很惊艳的功能，文档扫描。大一时候记了很多高数笔记，但到大三之后，又不舍的扔，又没有空闲地方去放置。所以利用印象笔记文档扫描功能将所有的高数笔记扫描到印象笔记之中，扫描的内容非常清晰，而且还可以在扫描后的文档上进行编辑。笔记移动到印象笔记之后，想要找高数中知识点，只需搜索相应内容即可，基本上是秒找到高数内容。如果是纸质笔记本的话，还需要一页页去查询，很繁琐。利用印象笔记的话，节省了很大功夫，现在纸质笔记本也就可以直接扔啦。\n\n利用印象笔记，记录生活、工作、学习中方方面面知识点，将所有信息保存到**第二大脑**之中，然后结构化管理，智能记录，有序生活。\n\n![软件推荐印象笔记05](效率软件推荐（一）/软件推荐印象笔记05.PNG)\n\n### 3.夸克浏览器\n\n电脑上浏览器我也就不推荐啦，多数用的都是Google Chrome浏览器，优点很多，大家慢慢探索就好。在这儿推荐下手机上的一个小众浏览器，**夸克**。\n\n我也曾经用过iPhone版Chrome浏览器，但每次想要搜索东西的时候，打开软件的时间就需要5s，就算进入到搜索界面之后也会有一些杂乱信息，很容易让自己分心，影响工作效率。这里更别提手机版的百度首页了，信息杂乱程度惨不忍睹。而夸克浏览器就不一样了，打开软件时间基本上是在1s左右，进入软件之后，只留有一个搜索框，没必要做任何多余的事情，果然是够快、够简约。\n\n![软件推荐夸克浏览器01](效率软件推荐（一）/软件推荐夸克浏览器01.PNG)\n\n简约并不意味着简单，夸克浏览器简约之中更是带着很多亮点功能。浏览器底部可以进入一些常用网站，比如少数派、爱范儿、简书等，在小程序出来之前，我经常用这种方式看资讯，使用体验相当不错。对于一些需要用到的网站，但是又不想去下载APP，这样倒是一个不错的解决方法。当然现在小程序出来之后，在小程序看资讯也是很好的方法。\n\n![软件推荐夸克浏览器02](效率软件推荐（一）/软件推荐夸克浏览器02.PNG)\n\n总而言之、言而总之，夸克是一款以轻、快为核心，设计风格简约的浏览器，是一款专注用户浏览体验的信息获取工具。启动时无任何多余加载项，瞬间启动无需等待。利用极简思路解决信息冗余，满足用户对于浏览器最本质的需求。同时浏览器本身从底栏自动缩放、菜单分层设计、导航栏设置等方面，给用户带来沉浸式的浏览体验。\n\n介于篇幅有限，这篇文章就介绍这么多，下篇文章将重点介绍YoMail、Kindle、Typora、Ulysse软件。\n\n### 4.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"效率软件推荐（一）","published":1,"updated":"2018-06-13T08:00:00.537Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3o000ojiz52klutbv4","content":"<p>工欲善其事，必先利其器，为此给大家推荐几款效率软件，帮助你高效学习和工作。</p>\n<ul>\n<li>滴答清单：解决拖延症&amp;&amp;健忘症，高效完成任务和规划时间。</li>\n<li>印象笔记： 收集各类信息，捕捉灵感，做你的<strong>第二大脑</strong>。</li>\n<li>夸克浏览器：极简风格，满足你对浏览器最本质的需求。</li>\n</ul>\n<h3 id=\"1-滴答清单\"><a href=\"#1-滴答清单\" class=\"headerlink\" title=\"1.滴答清单\"></a>1.滴答清单</h3><p>身边的人多多少都有点拖延症(包括我)，不到DeadLine的时候，都不会想到去解决问题。为此还耽误了几次事情，但被老师或同学批评几次之后，痛定思痛，决定认真改变拖延症的习惯。奈何自制力又不是太强，没监督或者提醒的话，不久之后又恢复到以前的状态。某天闲逛知乎中，发现滴答清单这款软件，真心不错，用一段时间之后，我多年的老(tuo)寒(yan)腿(zheng)也给治愈啦。另外，多逛知乎也是有用处的，硬是逛出来一篇中文核心论文《面向知乎的个性化推荐模型研究》，在知网现在也能检索到啦，大雾。</p>\n<p>解决拖延症，首要具备的便是任务提醒功能，滴答清单默认每天9点通知该天内所有事项。另外，如果为某件事情设置了具体时间，滴答将会在该时间点时进行提醒。对于未完成的任务，滴答清单图标右上角会时刻显示通知，督促自己完成。</p>\n<p>针对需要完成的每项任务，可增加标签和项目分组。通过标签和分组的结合，能够将任务具体到生活中的各个方面。对于不同的任务，可设置高、中、低、无优先级，优先完成最重要的事情，即使在有限的时间内，也能高效的完成工作。比如<strong>文章写作</strong>这项任务，我定义为中午12点之前完成，标签为公众号和博客，分组到写作模块。当然想要对某项任务添加更具体的描述时，滴答清单支持照片、位置、录音、附件等信息。当完成某项任务，消除待办事项时，发出滴答的声音，满满的自豪感。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐滴答清单01.PNG\" alt=\"软件推荐滴答清单01\"></p>\n<p>滴答清单支持在日历上添加任务，当接到某项任务时，随手记下来，当作备忘录，以防忘记。如需回顾过去某天的任务，可根据日期进行查看具体事项。另外滴答清单内置番茄计时，让自己更加专注工作。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐滴答清单02.PNG\" alt=\"软件推荐滴答清单02\"></p>\n<p>滴答清单风格偏于简约型，没有任何多余功能，看着就非常舒服。当然想要体验更多功能的话，可以升级到高级会员，但我也没有体验过，在此也就不再介绍，其实普通版足以满足个人需求。利用滴答清单设定任务提醒，能够有效督促自己，克服拖延症。对于生活中的日常事情，可随手添加到滴答之中，以防忘记。通过滴答清单，帮助你高效完成任务和规划时间，在滴答清单中记录和规划事情，用更少的时间达到目标。</p>\n<h3 id=\"2-印象笔记\"><a href=\"#2-印象笔记\" class=\"headerlink\" title=\"2.印象笔记\"></a>2.印象笔记</h3><p>学习和工作之中，必然包含着各种各样的信息，如果仅仅是通过大脑记忆的话，无法达到对信息的有效收集和整理。另外当需要寻找以往的知识或资料时，很难能够从大脑之中检索出来，当然学霸除外。因此，需要寻找<strong>第二大脑</strong>来帮助我们收集和整理信息，当需要利用以往知识点的时候，快速检索并加以利用。<br>好，主角也该上场啦，印象笔记。通过印象笔记在手机、平板、电脑上捕捉和共享灵感，跨平台的印象笔记帮助你永久保存所有信息，将想法转换为行动，做你的第二大脑。<br>先说一下我是怎么利用利用印象笔记的吧。印象笔记支持标签和笔记本两种分类模式，两种方式的结合能够覆盖到生活和学习之中的方方面面。信息对于我来说分为三个模式，分别为<strong>信息输入</strong>、<strong>信息加工(学习)</strong>、<strong>信息输出</strong>，所以我也就将笔记本分为三种模式，记为0、1、2。</p>\n<p>信息输入模块包含两个笔记本，第一个笔记本为杂乱信息收集(0Inbox)，白天收集到的信息全部放在该笔记本之中，然后晚上将收集到的信息细分到信息加工模块。第二个笔记本为灵感收集(0InSpiration)，将突然想到的灵感记录到该笔记本之中，然后利用空闲时间进行扩充和整理。</p>\n<p>信息加工(学习)模块用于记录生活、学习、工作之中的事情。生活(1HeartOfLife)、学习(1SelfStudy)、工作(1WorkStatus)笔记本组再分为相应笔记本，笔记本之中的每个笔记另外加上标签，当我们需要寻找过去的某条笔记或者知识点时，利用印象笔记的搜索功能能够秒搜索到所需要的信息。</p>\n<p>信息输出模块是将以往所收集到的信息进行总结、深度加工整理到该笔记本之中，另外个人重要信息也会收集到之中。比如我在信息加工(学习)模块总结得到的经验和教训、所写的文章、项目规划等都会记录到信息输出模块。从杂乱的信息之中，经过上述三步便能进行结构化处理，方便自己学习，也方便以后利用。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记01.PNG\" alt=\"软件推荐印象笔记01\"></p>\n<p>印象笔记的笔记本只支持二级分组，如果想要覆盖生活中所有知识的话，创建很多笔记本，难免会冗余。此时我们可以利用印象笔记的分类标签，标签支持无限嵌套。写笔记时对每个笔记添加不同的标签，这样就可以覆盖各个方面的知识点。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记02.PNG\" alt=\"软件推荐印象笔记02\"></p>\n<p>另外印象笔记支持多种收集方式，比如在网页之中，如果看到有趣的信息，便可利用印象笔记剪辑功能，一步保存网页之中的信息。另外我个人也关注了很多公众号，对于有趣的信息，通常想要收藏到印象笔记之中。我们只需要预先关注<strong>我的印象笔记</strong>服务号，绑定个人信息之后，找到想要收藏的公众号文章，只需一键发送到公众号，便可收集到个人印象笔记之中。同时印象笔记支持添加语音，pdf文档等形式的信息。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记03.PNG\" alt=\"软件推荐印象笔记03\"></p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记04.PNG\" alt=\"软件推荐印象笔记04\"></p>\n<p>印象笔记分为三种模式，分别为免费账户、标准账户、高级账户。免费账户功能较少，支持两台同步设备，每月上传流量为60M，支持搜索图片内的文字等功能。标准账户和高级账户功能差不多，但目前印象笔记高级账户在做活动，价格更便宜一些，年费价格大概是80左右。对于付费产品，我的看法是价格反映的是产品的品质，至于贵不贵，取决于它对人的价值。</p>\n<p>印象笔记高级账户支持多平台登陆(无限制)、同步所有设备、每月10G上传流量、标注pdf功能、笔记演示、文档搜索等功能。说实话，印象笔记中的搜索功能非常强，图片和文档之中的信息能够轻松搜索出来。即使忘记笔记放在哪个笔记本分组里，也可以秒搜索到。最后还有个很惊艳的功能，文档扫描。大一时候记了很多高数笔记，但到大三之后，又不舍的扔，又没有空闲地方去放置。所以利用印象笔记文档扫描功能将所有的高数笔记扫描到印象笔记之中，扫描的内容非常清晰，而且还可以在扫描后的文档上进行编辑。笔记移动到印象笔记之后，想要找高数中知识点，只需搜索相应内容即可，基本上是秒找到高数内容。如果是纸质笔记本的话，还需要一页页去查询，很繁琐。利用印象笔记的话，节省了很大功夫，现在纸质笔记本也就可以直接扔啦。</p>\n<p>利用印象笔记，记录生活、工作、学习中方方面面知识点，将所有信息保存到<strong>第二大脑</strong>之中，然后结构化管理，智能记录，有序生活。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记05.PNG\" alt=\"软件推荐印象笔记05\"></p>\n<h3 id=\"3-夸克浏览器\"><a href=\"#3-夸克浏览器\" class=\"headerlink\" title=\"3.夸克浏览器\"></a>3.夸克浏览器</h3><p>电脑上浏览器我也就不推荐啦，多数用的都是Google Chrome浏览器，优点很多，大家慢慢探索就好。在这儿推荐下手机上的一个小众浏览器，<strong>夸克</strong>。</p>\n<p>我也曾经用过iPhone版Chrome浏览器，但每次想要搜索东西的时候，打开软件的时间就需要5s，就算进入到搜索界面之后也会有一些杂乱信息，很容易让自己分心，影响工作效率。这里更别提手机版的百度首页了，信息杂乱程度惨不忍睹。而夸克浏览器就不一样了，打开软件时间基本上是在1s左右，进入软件之后，只留有一个搜索框，没必要做任何多余的事情，果然是够快、够简约。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐夸克浏览器01.PNG\" alt=\"软件推荐夸克浏览器01\"></p>\n<p>简约并不意味着简单，夸克浏览器简约之中更是带着很多亮点功能。浏览器底部可以进入一些常用网站，比如少数派、爱范儿、简书等，在小程序出来之前，我经常用这种方式看资讯，使用体验相当不错。对于一些需要用到的网站，但是又不想去下载APP，这样倒是一个不错的解决方法。当然现在小程序出来之后，在小程序看资讯也是很好的方法。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐夸克浏览器02.PNG\" alt=\"软件推荐夸克浏览器02\"></p>\n<p>总而言之、言而总之，夸克是一款以轻、快为核心，设计风格简约的浏览器，是一款专注用户浏览体验的信息获取工具。启动时无任何多余加载项，瞬间启动无需等待。利用极简思路解决信息冗余，满足用户对于浏览器最本质的需求。同时浏览器本身从底栏自动缩放、菜单分层设计、导航栏设置等方面，给用户带来沉浸式的浏览体验。</p>\n<p>介于篇幅有限，这篇文章就介绍这么多，下篇文章将重点介绍YoMail、Kindle、Typora、Ulysse软件。</p>\n<h3 id=\"4-推广\"><a href=\"#4-推广\" class=\"headerlink\" title=\"4.推广\"></a>4.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>工欲善其事，必先利其器，为此给大家推荐几款效率软件，帮助你高效学习和工作。</p>\n<ul>\n<li>滴答清单：解决拖延症&amp;&amp;健忘症，高效完成任务和规划时间。</li>\n<li>印象笔记： 收集各类信息，捕捉灵感，做你的<strong>第二大脑</strong>。</li>\n<li>夸克浏览器：极简风格，满足你对浏览器最本质的需求。</li>\n</ul>\n<h3 id=\"1-滴答清单\"><a href=\"#1-滴答清单\" class=\"headerlink\" title=\"1.滴答清单\"></a>1.滴答清单</h3><p>身边的人多多少都有点拖延症(包括我)，不到DeadLine的时候，都不会想到去解决问题。为此还耽误了几次事情，但被老师或同学批评几次之后，痛定思痛，决定认真改变拖延症的习惯。奈何自制力又不是太强，没监督或者提醒的话，不久之后又恢复到以前的状态。某天闲逛知乎中，发现滴答清单这款软件，真心不错，用一段时间之后，我多年的老(tuo)寒(yan)腿(zheng)也给治愈啦。另外，多逛知乎也是有用处的，硬是逛出来一篇中文核心论文《面向知乎的个性化推荐模型研究》，在知网现在也能检索到啦，大雾。</p>\n<p>解决拖延症，首要具备的便是任务提醒功能，滴答清单默认每天9点通知该天内所有事项。另外，如果为某件事情设置了具体时间，滴答将会在该时间点时进行提醒。对于未完成的任务，滴答清单图标右上角会时刻显示通知，督促自己完成。</p>\n<p>针对需要完成的每项任务，可增加标签和项目分组。通过标签和分组的结合，能够将任务具体到生活中的各个方面。对于不同的任务，可设置高、中、低、无优先级，优先完成最重要的事情，即使在有限的时间内，也能高效的完成工作。比如<strong>文章写作</strong>这项任务，我定义为中午12点之前完成，标签为公众号和博客，分组到写作模块。当然想要对某项任务添加更具体的描述时，滴答清单支持照片、位置、录音、附件等信息。当完成某项任务，消除待办事项时，发出滴答的声音，满满的自豪感。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐滴答清单01.PNG\" alt=\"软件推荐滴答清单01\"></p>\n<p>滴答清单支持在日历上添加任务，当接到某项任务时，随手记下来，当作备忘录，以防忘记。如需回顾过去某天的任务，可根据日期进行查看具体事项。另外滴答清单内置番茄计时，让自己更加专注工作。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐滴答清单02.PNG\" alt=\"软件推荐滴答清单02\"></p>\n<p>滴答清单风格偏于简约型，没有任何多余功能，看着就非常舒服。当然想要体验更多功能的话，可以升级到高级会员，但我也没有体验过，在此也就不再介绍，其实普通版足以满足个人需求。利用滴答清单设定任务提醒，能够有效督促自己，克服拖延症。对于生活中的日常事情，可随手添加到滴答之中，以防忘记。通过滴答清单，帮助你高效完成任务和规划时间，在滴答清单中记录和规划事情，用更少的时间达到目标。</p>\n<h3 id=\"2-印象笔记\"><a href=\"#2-印象笔记\" class=\"headerlink\" title=\"2.印象笔记\"></a>2.印象笔记</h3><p>学习和工作之中，必然包含着各种各样的信息，如果仅仅是通过大脑记忆的话，无法达到对信息的有效收集和整理。另外当需要寻找以往的知识或资料时，很难能够从大脑之中检索出来，当然学霸除外。因此，需要寻找<strong>第二大脑</strong>来帮助我们收集和整理信息，当需要利用以往知识点的时候，快速检索并加以利用。<br>好，主角也该上场啦，印象笔记。通过印象笔记在手机、平板、电脑上捕捉和共享灵感，跨平台的印象笔记帮助你永久保存所有信息，将想法转换为行动，做你的第二大脑。<br>先说一下我是怎么利用利用印象笔记的吧。印象笔记支持标签和笔记本两种分类模式，两种方式的结合能够覆盖到生活和学习之中的方方面面。信息对于我来说分为三个模式，分别为<strong>信息输入</strong>、<strong>信息加工(学习)</strong>、<strong>信息输出</strong>，所以我也就将笔记本分为三种模式，记为0、1、2。</p>\n<p>信息输入模块包含两个笔记本，第一个笔记本为杂乱信息收集(0Inbox)，白天收集到的信息全部放在该笔记本之中，然后晚上将收集到的信息细分到信息加工模块。第二个笔记本为灵感收集(0InSpiration)，将突然想到的灵感记录到该笔记本之中，然后利用空闲时间进行扩充和整理。</p>\n<p>信息加工(学习)模块用于记录生活、学习、工作之中的事情。生活(1HeartOfLife)、学习(1SelfStudy)、工作(1WorkStatus)笔记本组再分为相应笔记本，笔记本之中的每个笔记另外加上标签，当我们需要寻找过去的某条笔记或者知识点时，利用印象笔记的搜索功能能够秒搜索到所需要的信息。</p>\n<p>信息输出模块是将以往所收集到的信息进行总结、深度加工整理到该笔记本之中，另外个人重要信息也会收集到之中。比如我在信息加工(学习)模块总结得到的经验和教训、所写的文章、项目规划等都会记录到信息输出模块。从杂乱的信息之中，经过上述三步便能进行结构化处理，方便自己学习，也方便以后利用。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记01.PNG\" alt=\"软件推荐印象笔记01\"></p>\n<p>印象笔记的笔记本只支持二级分组，如果想要覆盖生活中所有知识的话，创建很多笔记本，难免会冗余。此时我们可以利用印象笔记的分类标签，标签支持无限嵌套。写笔记时对每个笔记添加不同的标签，这样就可以覆盖各个方面的知识点。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记02.PNG\" alt=\"软件推荐印象笔记02\"></p>\n<p>另外印象笔记支持多种收集方式，比如在网页之中，如果看到有趣的信息，便可利用印象笔记剪辑功能，一步保存网页之中的信息。另外我个人也关注了很多公众号，对于有趣的信息，通常想要收藏到印象笔记之中。我们只需要预先关注<strong>我的印象笔记</strong>服务号，绑定个人信息之后，找到想要收藏的公众号文章，只需一键发送到公众号，便可收集到个人印象笔记之中。同时印象笔记支持添加语音，pdf文档等形式的信息。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记03.PNG\" alt=\"软件推荐印象笔记03\"></p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记04.PNG\" alt=\"软件推荐印象笔记04\"></p>\n<p>印象笔记分为三种模式，分别为免费账户、标准账户、高级账户。免费账户功能较少，支持两台同步设备，每月上传流量为60M，支持搜索图片内的文字等功能。标准账户和高级账户功能差不多，但目前印象笔记高级账户在做活动，价格更便宜一些，年费价格大概是80左右。对于付费产品，我的看法是价格反映的是产品的品质，至于贵不贵，取决于它对人的价值。</p>\n<p>印象笔记高级账户支持多平台登陆(无限制)、同步所有设备、每月10G上传流量、标注pdf功能、笔记演示、文档搜索等功能。说实话，印象笔记中的搜索功能非常强，图片和文档之中的信息能够轻松搜索出来。即使忘记笔记放在哪个笔记本分组里，也可以秒搜索到。最后还有个很惊艳的功能，文档扫描。大一时候记了很多高数笔记，但到大三之后，又不舍的扔，又没有空闲地方去放置。所以利用印象笔记文档扫描功能将所有的高数笔记扫描到印象笔记之中，扫描的内容非常清晰，而且还可以在扫描后的文档上进行编辑。笔记移动到印象笔记之后，想要找高数中知识点，只需搜索相应内容即可，基本上是秒找到高数内容。如果是纸质笔记本的话，还需要一页页去查询，很繁琐。利用印象笔记的话，节省了很大功夫，现在纸质笔记本也就可以直接扔啦。</p>\n<p>利用印象笔记，记录生活、工作、学习中方方面面知识点，将所有信息保存到<strong>第二大脑</strong>之中，然后结构化管理，智能记录，有序生活。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐印象笔记05.PNG\" alt=\"软件推荐印象笔记05\"></p>\n<h3 id=\"3-夸克浏览器\"><a href=\"#3-夸克浏览器\" class=\"headerlink\" title=\"3.夸克浏览器\"></a>3.夸克浏览器</h3><p>电脑上浏览器我也就不推荐啦，多数用的都是Google Chrome浏览器，优点很多，大家慢慢探索就好。在这儿推荐下手机上的一个小众浏览器，<strong>夸克</strong>。</p>\n<p>我也曾经用过iPhone版Chrome浏览器，但每次想要搜索东西的时候，打开软件的时间就需要5s，就算进入到搜索界面之后也会有一些杂乱信息，很容易让自己分心，影响工作效率。这里更别提手机版的百度首页了，信息杂乱程度惨不忍睹。而夸克浏览器就不一样了，打开软件时间基本上是在1s左右，进入软件之后，只留有一个搜索框，没必要做任何多余的事情，果然是够快、够简约。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐夸克浏览器01.PNG\" alt=\"软件推荐夸克浏览器01\"></p>\n<p>简约并不意味着简单，夸克浏览器简约之中更是带着很多亮点功能。浏览器底部可以进入一些常用网站，比如少数派、爱范儿、简书等，在小程序出来之前，我经常用这种方式看资讯，使用体验相当不错。对于一些需要用到的网站，但是又不想去下载APP，这样倒是一个不错的解决方法。当然现在小程序出来之后，在小程序看资讯也是很好的方法。</p>\n<p><img src=\"/2018/06/12/效率软件推荐（一）/软件推荐夸克浏览器02.PNG\" alt=\"软件推荐夸克浏览器02\"></p>\n<p>总而言之、言而总之，夸克是一款以轻、快为核心，设计风格简约的浏览器，是一款专注用户浏览体验的信息获取工具。启动时无任何多余加载项，瞬间启动无需等待。利用极简思路解决信息冗余，满足用户对于浏览器最本质的需求。同时浏览器本身从底栏自动缩放、菜单分层设计、导航栏设置等方面，给用户带来沉浸式的浏览体验。</p>\n<p>介于篇幅有限，这篇文章就介绍这么多，下篇文章将重点介绍YoMail、Kindle、Typora、Ulysse软件。</p>\n<h3 id=\"4-推广\"><a href=\"#4-推广\" class=\"headerlink\" title=\"4.推广\"></a>4.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"机器学习之Apriori算法","date":"2018-05-18T02:05:37.000Z","mathjax":true,"comments":1,"_content":"\n### 1.Apriori算法简介\n\n**Apriori**算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到**大部分顾客会在一次购物中同时购买面包和牛奶**，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。\n\n+ **事务数据库：**设$I=\\{ i_1,i_2,…,i_m \\}$是一个全局项的集合，事物数据库$D=\\{ t_1,t_2,..,t_n \\}$是一个事务的集合，每个事务$t_i(1\\le i \\le n)$都对应$I$上的一个子集，例如$t_1=\\{ i_1,i_3,i_7\\}$。\n+ **关联规则：**关联规则表示项之间的关系，是形如$X\\rightarrow Y​$的蕴含表达式，其中$X​$和$Y​$是不相交的项集，$X​$称为规则的前件，$Y​$称为规则的后件。例如$\\{ cereal,milk\\}\\rightarrow \\{fruit \\}​$关联规则表示购买谷类食品和牛奶的人也会购买水果。通常关联规则的强度可以用支持度和置信度来度量。\n\n+ **支持度：**支持度表示关联数据在数据集中出现的次数或所占的比重。\n\n$$\nsupport(X\\rightarrow Y)=P(X\\cup Y)=\\frac{|X\\cup Y|}{|D|}\n$$\n\n+ **置信度：**置信度表示$Y$数据出现后，$X$数据出现的可能性，也可以说是数据的条件概率。\n\n$$\nconfidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(XY)}{P(Y)}\n$$\n\n+ **提升度：**提升度体现$X$和$Y$之间的关联关系，提升度大于1表示$X$和$Y$之间具有强关联关系，提升度小于等于1表示$X$和$Y$之间无有效的强关联关系。\n\n$$\nlift(X \\Leftarrow Y)=\\frac{confidence(X \\Leftarrow Y)}{P(X)}=\\frac{P(XY)}{P(X)P(Y)}\n$$\n\n+ **强关联规则：**满足最小支持度和最小置信度的关联规则。\n\n关联规则的挖掘目标是**找出所有的频繁项集**和**根据频繁项集产生强关联规则**。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。\n\n### 2.Apriori算法原理\n\nApriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是**找到最多的K项频繁集**。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。\n\nApriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。\n\n![机器学习之Apriori算法图片01](机器学习之Apriori算法/机器学习之Apriori算法图片01.png)\n\n数据集包含4条记录{'134','235','1235','25'}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{'1','2','3','4','5'}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{'1','2','3','5'}。根据频繁1项集连接得到候选2项集{'12','13','15','23','25','35'}，其中数据{'12','15'}低于最低支持度，进行剪枝处理，得到频繁2项集为{'13','23','25','35'}。如此迭代下去，最终能够得到频繁3项集{'235'}，由于数据无法再进行连接，算法至此结束。\n\n### 3.Apriori算法流程\n\n从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。\n\n+ 扫描数据集，得到所有出现过的数据，作为候选1项集。\n+ 挖掘频繁k项集。\n  + 扫描计算候选k项集的支持度。\n  + 剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。\n  + 基于频繁k项集，连接生成候选k+1项集。\n+ 利用步骤2，迭代得到k=k+1项集结果。\n\n### 4.Apriori算法优缺点\n\n#### 4.1优点\n\n+ 适合稀疏数据集。\n+ 算法原理简单，易实现。\n+ 适合事务数据库的关联规则挖掘。\n\n#### 4.2缺点\n\n+ 可能产生庞大的候选集。\n+ 算法需多次遍历数据集，算法效率低，耗时。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n> [刘建平Pinard-Apriori算法原理总结](http://www.cnblogs.com/pinard/p/6293298.html)\n","source":"_posts/机器学习之Apriori算法.md","raw":"---\ntitle: 机器学习之Apriori算法\ndate: 2018-05-18 10:05:37\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.Apriori算法简介\n\n**Apriori**算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到**大部分顾客会在一次购物中同时购买面包和牛奶**，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。\n\n+ **事务数据库：**设$I=\\{ i_1,i_2,…,i_m \\}$是一个全局项的集合，事物数据库$D=\\{ t_1,t_2,..,t_n \\}$是一个事务的集合，每个事务$t_i(1\\le i \\le n)$都对应$I$上的一个子集，例如$t_1=\\{ i_1,i_3,i_7\\}$。\n+ **关联规则：**关联规则表示项之间的关系，是形如$X\\rightarrow Y​$的蕴含表达式，其中$X​$和$Y​$是不相交的项集，$X​$称为规则的前件，$Y​$称为规则的后件。例如$\\{ cereal,milk\\}\\rightarrow \\{fruit \\}​$关联规则表示购买谷类食品和牛奶的人也会购买水果。通常关联规则的强度可以用支持度和置信度来度量。\n\n+ **支持度：**支持度表示关联数据在数据集中出现的次数或所占的比重。\n\n$$\nsupport(X\\rightarrow Y)=P(X\\cup Y)=\\frac{|X\\cup Y|}{|D|}\n$$\n\n+ **置信度：**置信度表示$Y$数据出现后，$X$数据出现的可能性，也可以说是数据的条件概率。\n\n$$\nconfidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(XY)}{P(Y)}\n$$\n\n+ **提升度：**提升度体现$X$和$Y$之间的关联关系，提升度大于1表示$X$和$Y$之间具有强关联关系，提升度小于等于1表示$X$和$Y$之间无有效的强关联关系。\n\n$$\nlift(X \\Leftarrow Y)=\\frac{confidence(X \\Leftarrow Y)}{P(X)}=\\frac{P(XY)}{P(X)P(Y)}\n$$\n\n+ **强关联规则：**满足最小支持度和最小置信度的关联规则。\n\n关联规则的挖掘目标是**找出所有的频繁项集**和**根据频繁项集产生强关联规则**。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。\n\n### 2.Apriori算法原理\n\nApriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是**找到最多的K项频繁集**。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。\n\nApriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。\n\n![机器学习之Apriori算法图片01](机器学习之Apriori算法/机器学习之Apriori算法图片01.png)\n\n数据集包含4条记录{'134','235','1235','25'}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{'1','2','3','4','5'}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{'1','2','3','5'}。根据频繁1项集连接得到候选2项集{'12','13','15','23','25','35'}，其中数据{'12','15'}低于最低支持度，进行剪枝处理，得到频繁2项集为{'13','23','25','35'}。如此迭代下去，最终能够得到频繁3项集{'235'}，由于数据无法再进行连接，算法至此结束。\n\n### 3.Apriori算法流程\n\n从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。\n\n+ 扫描数据集，得到所有出现过的数据，作为候选1项集。\n+ 挖掘频繁k项集。\n  + 扫描计算候选k项集的支持度。\n  + 剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。\n  + 基于频繁k项集，连接生成候选k+1项集。\n+ 利用步骤2，迭代得到k=k+1项集结果。\n\n### 4.Apriori算法优缺点\n\n#### 4.1优点\n\n+ 适合稀疏数据集。\n+ 算法原理简单，易实现。\n+ 适合事务数据库的关联规则挖掘。\n\n#### 4.2缺点\n\n+ 可能产生庞大的候选集。\n+ 算法需多次遍历数据集，算法效率低，耗时。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n> [刘建平Pinard-Apriori算法原理总结](http://www.cnblogs.com/pinard/p/6293298.html)\n","slug":"机器学习之Apriori算法","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3r000sjiz5hujxudi7","content":"<h3 id=\"1-Apriori算法简介\"><a href=\"#1-Apriori算法简介\" class=\"headerlink\" title=\"1.Apriori算法简介\"></a>1.Apriori算法简介</h3><p><strong>Apriori</strong>算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到<strong>大部分顾客会在一次购物中同时购买面包和牛奶</strong>，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。</p>\n<ul>\n<li><strong>事务数据库：</strong>设$I=\\{ i_1,i_2,…,i_m \\}$是一个全局项的集合，事物数据库$D=\\{ t_1,t_2,..,t_n \\}$是一个事务的集合，每个事务$t_i(1\\le i \\le n)$都对应$I$上的一个子集，例如$t_1=\\{ i_1,i_3,i_7\\}$。</li>\n<li><p><strong>关联规则：</strong>关联规则表示项之间的关系，是形如$X\\rightarrow Y​$的蕴含表达式，其中$X​$和$Y​$是不相交的项集，$X​$称为规则的前件，$Y​$称为规则的后件。例如$\\{ cereal,milk\\}\\rightarrow \\{fruit \\}​$关联规则表示购买谷类食品和牛奶的人也会购买水果。通常关联规则的强度可以用支持度和置信度来度量。</p>\n</li>\n<li><p><strong>支持度：</strong>支持度表示关联数据在数据集中出现的次数或所占的比重。</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nsupport(X\\rightarrow Y)=P(X\\cup Y)=\\frac{|X\\cup Y|}{|D|}</script><ul>\n<li><strong>置信度：</strong>置信度表示$Y$数据出现后，$X$数据出现的可能性，也可以说是数据的条件概率。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nconfidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(XY)}{P(Y)}</script><ul>\n<li><strong>提升度：</strong>提升度体现$X$和$Y$之间的关联关系，提升度大于1表示$X$和$Y$之间具有强关联关系，提升度小于等于1表示$X$和$Y$之间无有效的强关联关系。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nlift(X \\Leftarrow Y)=\\frac{confidence(X \\Leftarrow Y)}{P(X)}=\\frac{P(XY)}{P(X)P(Y)}</script><ul>\n<li><strong>强关联规则：</strong>满足最小支持度和最小置信度的关联规则。</li>\n</ul>\n<p>关联规则的挖掘目标是<strong>找出所有的频繁项集</strong>和<strong>根据频繁项集产生强关联规则</strong>。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。</p>\n<h3 id=\"2-Apriori算法原理\"><a href=\"#2-Apriori算法原理\" class=\"headerlink\" title=\"2.Apriori算法原理\"></a>2.Apriori算法原理</h3><p>Apriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是<strong>找到最多的K项频繁集</strong>。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。</p>\n<p>Apriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。</p>\n<p><img src=\"/2018/05/18/机器学习之Apriori算法/机器学习之Apriori算法图片01.png\" alt=\"机器学习之Apriori算法图片01\"></p>\n<p>数据集包含4条记录{‘134’,’235’,’1235’,’25’}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{‘1’,’2’,’3’,’4’,’5’}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{‘1’,’2’,’3’,’5’}。根据频繁1项集连接得到候选2项集{‘12’,’13’,’15’,’23’,’25’,’35’}，其中数据{‘12’,’15’}低于最低支持度，进行剪枝处理，得到频繁2项集为{‘13’,’23’,’25’,’35’}。如此迭代下去，最终能够得到频繁3项集{‘235’}，由于数据无法再进行连接，算法至此结束。</p>\n<h3 id=\"3-Apriori算法流程\"><a href=\"#3-Apriori算法流程\" class=\"headerlink\" title=\"3.Apriori算法流程\"></a>3.Apriori算法流程</h3><p>从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。</p>\n<ul>\n<li>扫描数据集，得到所有出现过的数据，作为候选1项集。</li>\n<li>挖掘频繁k项集。<ul>\n<li>扫描计算候选k项集的支持度。</li>\n<li>剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。</li>\n<li>基于频繁k项集，连接生成候选k+1项集。</li>\n</ul>\n</li>\n<li>利用步骤2，迭代得到k=k+1项集结果。</li>\n</ul>\n<h3 id=\"4-Apriori算法优缺点\"><a href=\"#4-Apriori算法优缺点\" class=\"headerlink\" title=\"4.Apriori算法优缺点\"></a>4.Apriori算法优缺点</h3><h4 id=\"4-1优点\"><a href=\"#4-1优点\" class=\"headerlink\" title=\"4.1优点\"></a>4.1优点</h4><ul>\n<li>适合稀疏数据集。</li>\n<li>算法原理简单，易实现。</li>\n<li>适合事务数据库的关联规则挖掘。</li>\n</ul>\n<h4 id=\"4-2缺点\"><a href=\"#4-2缺点\" class=\"headerlink\" title=\"4.2缺点\"></a>4.2缺点</h4><ul>\n<li>可能产生庞大的候选集。</li>\n<li>算法需多次遍历数据集，算法效率低，耗时。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6293298.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard-Apriori算法原理总结</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Apriori算法简介\"><a href=\"#1-Apriori算法简介\" class=\"headerlink\" title=\"1.Apriori算法简介\"></a>1.Apriori算法简介</h3><p><strong>Apriori</strong>算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到<strong>大部分顾客会在一次购物中同时购买面包和牛奶</strong>，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。</p>\n<ul>\n<li><strong>事务数据库：</strong>设$I=\\{ i_1,i_2,…,i_m \\}$是一个全局项的集合，事物数据库$D=\\{ t_1,t_2,..,t_n \\}$是一个事务的集合，每个事务$t_i(1\\le i \\le n)$都对应$I$上的一个子集，例如$t_1=\\{ i_1,i_3,i_7\\}$。</li>\n<li><p><strong>关联规则：</strong>关联规则表示项之间的关系，是形如$X\\rightarrow Y​$的蕴含表达式，其中$X​$和$Y​$是不相交的项集，$X​$称为规则的前件，$Y​$称为规则的后件。例如$\\{ cereal,milk\\}\\rightarrow \\{fruit \\}​$关联规则表示购买谷类食品和牛奶的人也会购买水果。通常关联规则的强度可以用支持度和置信度来度量。</p>\n</li>\n<li><p><strong>支持度：</strong>支持度表示关联数据在数据集中出现的次数或所占的比重。</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nsupport(X\\rightarrow Y)=P(X\\cup Y)=\\frac{|X\\cup Y|}{|D|}</script><ul>\n<li><strong>置信度：</strong>置信度表示$Y$数据出现后，$X$数据出现的可能性，也可以说是数据的条件概率。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nconfidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(XY)}{P(Y)}</script><ul>\n<li><strong>提升度：</strong>提升度体现$X$和$Y$之间的关联关系，提升度大于1表示$X$和$Y$之间具有强关联关系，提升度小于等于1表示$X$和$Y$之间无有效的强关联关系。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nlift(X \\Leftarrow Y)=\\frac{confidence(X \\Leftarrow Y)}{P(X)}=\\frac{P(XY)}{P(X)P(Y)}</script><ul>\n<li><strong>强关联规则：</strong>满足最小支持度和最小置信度的关联规则。</li>\n</ul>\n<p>关联规则的挖掘目标是<strong>找出所有的频繁项集</strong>和<strong>根据频繁项集产生强关联规则</strong>。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。</p>\n<h3 id=\"2-Apriori算法原理\"><a href=\"#2-Apriori算法原理\" class=\"headerlink\" title=\"2.Apriori算法原理\"></a>2.Apriori算法原理</h3><p>Apriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是<strong>找到最多的K项频繁集</strong>。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。</p>\n<p>Apriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。</p>\n<p><img src=\"/2018/05/18/机器学习之Apriori算法/机器学习之Apriori算法图片01.png\" alt=\"机器学习之Apriori算法图片01\"></p>\n<p>数据集包含4条记录{‘134’,’235’,’1235’,’25’}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{‘1’,’2’,’3’,’4’,’5’}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{‘1’,’2’,’3’,’5’}。根据频繁1项集连接得到候选2项集{‘12’,’13’,’15’,’23’,’25’,’35’}，其中数据{‘12’,’15’}低于最低支持度，进行剪枝处理，得到频繁2项集为{‘13’,’23’,’25’,’35’}。如此迭代下去，最终能够得到频繁3项集{‘235’}，由于数据无法再进行连接，算法至此结束。</p>\n<h3 id=\"3-Apriori算法流程\"><a href=\"#3-Apriori算法流程\" class=\"headerlink\" title=\"3.Apriori算法流程\"></a>3.Apriori算法流程</h3><p>从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。</p>\n<ul>\n<li>扫描数据集，得到所有出现过的数据，作为候选1项集。</li>\n<li>挖掘频繁k项集。<ul>\n<li>扫描计算候选k项集的支持度。</li>\n<li>剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。</li>\n<li>基于频繁k项集，连接生成候选k+1项集。</li>\n</ul>\n</li>\n<li>利用步骤2，迭代得到k=k+1项集结果。</li>\n</ul>\n<h3 id=\"4-Apriori算法优缺点\"><a href=\"#4-Apriori算法优缺点\" class=\"headerlink\" title=\"4.Apriori算法优缺点\"></a>4.Apriori算法优缺点</h3><h4 id=\"4-1优点\"><a href=\"#4-1优点\" class=\"headerlink\" title=\"4.1优点\"></a>4.1优点</h4><ul>\n<li>适合稀疏数据集。</li>\n<li>算法原理简单，易实现。</li>\n<li>适合事务数据库的关联规则挖掘。</li>\n</ul>\n<h4 id=\"4-2缺点\"><a href=\"#4-2缺点\" class=\"headerlink\" title=\"4.2缺点\"></a>4.2缺点</h4><ul>\n<li>可能产生庞大的候选集。</li>\n<li>算法需多次遍历数据集，算法效率低，耗时。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6293298.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard-Apriori算法原理总结</a></p>\n</blockquote>\n"},{"title":"机器学习之K近邻(KNN)算法","date":"2018-05-13T03:34:01.000Z","mathjax":true,"comments":1,"_content":"\n### 1.KNN简介\n\n**K近邻(K-Nearest Neighbors, KNN)**算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做**分类**预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做**回归**预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。\n\n如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设**K=3**，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设**K=5**，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。\n\n![机器学习之K近邻算法图片01](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png)\n\n从上面实例，我们可以总结下KNN算法过程\n\n1. 计算测试数据与各个训练数据之间的距离。\n2. 按照距离的递增关系进行排序，选取距离最小的K个点。\n3. 确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。\n\n从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。\n\n+ **距离度量方式：**KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。\n\n$$\nD(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\n$$\n\n$$\nD(x,y)={|x_1-y_1|+|x_2-y_2|+|x_3-y_3|...+|x_n-y_n|}={\\sum_{i=1}^{n}|x_i-y_i|}\n$$\n\n+ **K值的选取：**KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。\n+ **分类决策规则：**KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。\n\nKNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。\n\n### 2.KD树原理\n\nKD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。\n\n#### 2.1KD树建立\n\n下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。\n\n1. **寻找划分特征：**KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。\n2. **确定划分点：**选择特征nk的中位数nkv所对应的样本作为划分点。\n3. **确定左子空间和右子空间：**对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。\n4. **递归构建KD树：**对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。\n\n我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下\n\n1. **寻找划分特征：**6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。\n2. **确定划分点：**根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。\n3. **确定左子空间和右子空间：**分割超平面x=7将空间分为两部分。x<=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x>7的部分为右子空间，包含节点为{(9,6)，(8,1)}。\n4. **递归构建KD树：**用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。\n\n![机器学习之K近邻算法图片02](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png)\n\n![机器学习之K近邻算法图片03](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png)\n\n#### 2.2KD树搜索最近邻\n\n当我们生成KD树后，就可以预测测试样本集里面的样本目标点。\n\n1. **二叉搜索：**对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的**叶子节点**。\n2. **回溯：**为找到最近邻，还需要进行**回溯**操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。\n3. **更新最近邻：**返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。\n\n为方便理解上述过程，我们利用**2.1建立的KD树**来寻找(2,4.5)的最近邻。\n\n1. **二叉搜索：**首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)->(5,4)->(4,7)}。\n2. **回溯：**节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。\n3. **更新最近邻：**该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)->(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。\n\n![机器学习之K近邻算法图片04](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png)\n\n#### 2.3KD树预测\n\n根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。\n\n### 3.球树原理\n\nKD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。\n\n![机器学习之K近邻算法图片05](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png)\n\n为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。\n\n#### 3.1球树建立\n\n球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程\n\n1. **构建超球体：**超球体是可以包含所有样本的最小球体。\n2. **划分子超球体：**从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。\n3. **递归：**对上述两个子超球体，递归执行步骤2，最终得到球树。\n\n![机器学习之K近邻算法图片06](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png)\n\n#### 3.2球树搜索最近邻\n\nKD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。\n\n1. 自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。\n2. 然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。\n3. 检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。\n\n#### 3.3球树预测\n\n根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。\n\n### 4.KNN算法扩展\n\n有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。\n\n### 5.Sklearn实现KNN算法\n\n下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)。\n\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n#load iris data\niris=load_iris()\nX=iris.data\ny=iris.target\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nknn=KNeighborsClassifier(algorithm='kd_tree')\nknn.fit(X_train,y_train)\nprint(knn.predict(X_test))\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 1 2 1]\nprint(y_test)\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 2 2 1]\nprint(knn.score(X_test,y_test))\n# 0.977777777778\n```\n\n### 6.KNN优缺点\n\n#### 6.1优点\n\n+ 即可处理分类也可处理回归问题。\n+ 对数据没有假设，准确度高，对异常点不敏感。\n+ 比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。\n+ 主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。\n\n#### 6.2缺点\n\n+ 计算量大，尤其是特征维数较多时候。\n+ 样本不平衡时，对稀有类别的预测准确率低。\n+ KD树、球树之类的模型建立时需要大量的内存。\n+ 使用懒惰学习方法，基本上不学习，导致预测时速度较慢。\n\n### 7.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n>[刘建平Pinard_K近邻法(KNN)原理小结](http://www.cnblogs.com/pinard/p/6061661.html)\n>\n>[Yabea_K-近邻(KNN)算法](https://www.cnblogs.com/ybjourney/p/4702562.html)\n\n","source":"_posts/机器学习之K近邻-KNN-算法.md","raw":"---\ntitle: 机器学习之K近邻(KNN)算法\ndate: 2018-05-13 11:34:01\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.KNN简介\n\n**K近邻(K-Nearest Neighbors, KNN)**算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做**分类**预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做**回归**预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。\n\n如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设**K=3**，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设**K=5**，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。\n\n![机器学习之K近邻算法图片01](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png)\n\n从上面实例，我们可以总结下KNN算法过程\n\n1. 计算测试数据与各个训练数据之间的距离。\n2. 按照距离的递增关系进行排序，选取距离最小的K个点。\n3. 确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。\n\n从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。\n\n+ **距离度量方式：**KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。\n\n$$\nD(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\n$$\n\n$$\nD(x,y)={|x_1-y_1|+|x_2-y_2|+|x_3-y_3|...+|x_n-y_n|}={\\sum_{i=1}^{n}|x_i-y_i|}\n$$\n\n+ **K值的选取：**KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。\n+ **分类决策规则：**KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。\n\nKNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。\n\n### 2.KD树原理\n\nKD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。\n\n#### 2.1KD树建立\n\n下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。\n\n1. **寻找划分特征：**KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。\n2. **确定划分点：**选择特征nk的中位数nkv所对应的样本作为划分点。\n3. **确定左子空间和右子空间：**对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。\n4. **递归构建KD树：**对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。\n\n我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下\n\n1. **寻找划分特征：**6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。\n2. **确定划分点：**根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。\n3. **确定左子空间和右子空间：**分割超平面x=7将空间分为两部分。x<=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x>7的部分为右子空间，包含节点为{(9,6)，(8,1)}。\n4. **递归构建KD树：**用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。\n\n![机器学习之K近邻算法图片02](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png)\n\n![机器学习之K近邻算法图片03](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png)\n\n#### 2.2KD树搜索最近邻\n\n当我们生成KD树后，就可以预测测试样本集里面的样本目标点。\n\n1. **二叉搜索：**对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的**叶子节点**。\n2. **回溯：**为找到最近邻，还需要进行**回溯**操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。\n3. **更新最近邻：**返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。\n\n为方便理解上述过程，我们利用**2.1建立的KD树**来寻找(2,4.5)的最近邻。\n\n1. **二叉搜索：**首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)->(5,4)->(4,7)}。\n2. **回溯：**节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。\n3. **更新最近邻：**该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)->(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。\n\n![机器学习之K近邻算法图片04](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png)\n\n#### 2.3KD树预测\n\n根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。\n\n### 3.球树原理\n\nKD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。\n\n![机器学习之K近邻算法图片05](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png)\n\n为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。\n\n#### 3.1球树建立\n\n球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程\n\n1. **构建超球体：**超球体是可以包含所有样本的最小球体。\n2. **划分子超球体：**从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。\n3. **递归：**对上述两个子超球体，递归执行步骤2，最终得到球树。\n\n![机器学习之K近邻算法图片06](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png)\n\n#### 3.2球树搜索最近邻\n\nKD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。\n\n1. 自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。\n2. 然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。\n3. 检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。\n\n#### 3.3球树预测\n\n根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。\n\n### 4.KNN算法扩展\n\n有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。\n\n### 5.Sklearn实现KNN算法\n\n下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)。\n\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n#load iris data\niris=load_iris()\nX=iris.data\ny=iris.target\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nknn=KNeighborsClassifier(algorithm='kd_tree')\nknn.fit(X_train,y_train)\nprint(knn.predict(X_test))\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 1 2 1]\nprint(y_test)\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 2 2 1]\nprint(knn.score(X_test,y_test))\n# 0.977777777778\n```\n\n### 6.KNN优缺点\n\n#### 6.1优点\n\n+ 即可处理分类也可处理回归问题。\n+ 对数据没有假设，准确度高，对异常点不敏感。\n+ 比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。\n+ 主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。\n\n#### 6.2缺点\n\n+ 计算量大，尤其是特征维数较多时候。\n+ 样本不平衡时，对稀有类别的预测准确率低。\n+ KD树、球树之类的模型建立时需要大量的内存。\n+ 使用懒惰学习方法，基本上不学习，导致预测时速度较慢。\n\n### 7.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n>[刘建平Pinard_K近邻法(KNN)原理小结](http://www.cnblogs.com/pinard/p/6061661.html)\n>\n>[Yabea_K-近邻(KNN)算法](https://www.cnblogs.com/ybjourney/p/4702562.html)\n\n","slug":"机器学习之K近邻-KNN-算法","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3t000ujiz5667327i2","content":"<h3 id=\"1-KNN简介\"><a href=\"#1-KNN简介\" class=\"headerlink\" title=\"1.KNN简介\"></a>1.KNN简介</h3><p><strong>K近邻(K-Nearest Neighbors, KNN)</strong>算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做<strong>分类</strong>预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做<strong>回归</strong>预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。</p>\n<p>如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设<strong>K=3</strong>，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设<strong>K=5</strong>，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。</p>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png\" alt=\"机器学习之K近邻算法图片01\"></p>\n<p>从上面实例，我们可以总结下KNN算法过程</p>\n<ol>\n<li>计算测试数据与各个训练数据之间的距离。</li>\n<li>按照距离的递增关系进行排序，选取距离最小的K个点。</li>\n<li>确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。</li>\n</ol>\n<p>从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。</p>\n<ul>\n<li><strong>距离度量方式：</strong>KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nD(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}</script><script type=\"math/tex; mode=display\">\nD(x,y)={|x_1-y_1|+|x_2-y_2|+|x_3-y_3|...+|x_n-y_n|}={\\sum_{i=1}^{n}|x_i-y_i|}</script><ul>\n<li><strong>K值的选取：</strong>KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。</li>\n<li><strong>分类决策规则：</strong>KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。</li>\n</ul>\n<p>KNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。</p>\n<h3 id=\"2-KD树原理\"><a href=\"#2-KD树原理\" class=\"headerlink\" title=\"2.KD树原理\"></a>2.KD树原理</h3><p>KD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。</p>\n<h4 id=\"2-1KD树建立\"><a href=\"#2-1KD树建立\" class=\"headerlink\" title=\"2.1KD树建立\"></a>2.1KD树建立</h4><p>下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。</p>\n<ol>\n<li><strong>寻找划分特征：</strong>KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。</li>\n<li><strong>确定划分点：</strong>选择特征nk的中位数nkv所对应的样本作为划分点。</li>\n<li><strong>确定左子空间和右子空间：</strong>对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。</li>\n<li><strong>递归构建KD树：</strong>对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。</li>\n</ol>\n<p>我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下</p>\n<ol>\n<li><strong>寻找划分特征：</strong>6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。</li>\n<li><strong>确定划分点：</strong>根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。</li>\n<li><strong>确定左子空间和右子空间：</strong>分割超平面x=7将空间分为两部分。x&lt;=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x&gt;7的部分为右子空间，包含节点为{(9,6)，(8,1)}。</li>\n<li><strong>递归构建KD树：</strong>用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。</li>\n</ol>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png\" alt=\"机器学习之K近邻算法图片02\"></p>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png\" alt=\"机器学习之K近邻算法图片03\"></p>\n<h4 id=\"2-2KD树搜索最近邻\"><a href=\"#2-2KD树搜索最近邻\" class=\"headerlink\" title=\"2.2KD树搜索最近邻\"></a>2.2KD树搜索最近邻</h4><p>当我们生成KD树后，就可以预测测试样本集里面的样本目标点。</p>\n<ol>\n<li><strong>二叉搜索：</strong>对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的<strong>叶子节点</strong>。</li>\n<li><strong>回溯：</strong>为找到最近邻，还需要进行<strong>回溯</strong>操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。</li>\n<li><strong>更新最近邻：</strong>返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</li>\n</ol>\n<p>为方便理解上述过程，我们利用<strong>2.1建立的KD树</strong>来寻找(2,4.5)的最近邻。</p>\n<ol>\n<li><strong>二叉搜索：</strong>首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)-&gt;(5,4)-&gt;(4,7)}。</li>\n<li><strong>回溯：</strong>节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。</li>\n<li><strong>更新最近邻：</strong>该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)-&gt;(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。</li>\n</ol>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png\" alt=\"机器学习之K近邻算法图片04\"></p>\n<h4 id=\"2-3KD树预测\"><a href=\"#2-3KD树预测\" class=\"headerlink\" title=\"2.3KD树预测\"></a>2.3KD树预测</h4><p>根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p>\n<h3 id=\"3-球树原理\"><a href=\"#3-球树原理\" class=\"headerlink\" title=\"3.球树原理\"></a>3.球树原理</h3><p>KD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。</p>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png\" alt=\"机器学习之K近邻算法图片05\"></p>\n<p>为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。</p>\n<h4 id=\"3-1球树建立\"><a href=\"#3-1球树建立\" class=\"headerlink\" title=\"3.1球树建立\"></a>3.1球树建立</h4><p>球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程</p>\n<ol>\n<li><strong>构建超球体：</strong>超球体是可以包含所有样本的最小球体。</li>\n<li><strong>划分子超球体：</strong>从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。</li>\n<li><strong>递归：</strong>对上述两个子超球体，递归执行步骤2，最终得到球树。</li>\n</ol>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png\" alt=\"机器学习之K近邻算法图片06\"></p>\n<h4 id=\"3-2球树搜索最近邻\"><a href=\"#3-2球树搜索最近邻\" class=\"headerlink\" title=\"3.2球树搜索最近邻\"></a>3.2球树搜索最近邻</h4><p>KD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。</p>\n<ol>\n<li>自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。</li>\n<li>然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。</li>\n<li>检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。</li>\n</ol>\n<h4 id=\"3-3球树预测\"><a href=\"#3-3球树预测\" class=\"headerlink\" title=\"3.3球树预测\"></a>3.3球树预测</h4><p>根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p>\n<h3 id=\"4-KNN算法扩展\"><a href=\"#4-KNN算法扩展\" class=\"headerlink\" title=\"4.KNN算法扩展\"></a>4.KNN算法扩展</h3><p>有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。</p>\n<h3 id=\"5-Sklearn实现KNN算法\"><a href=\"#5-Sklearn实现KNN算法\" class=\"headerlink\" title=\"5.Sklearn实现KNN算法\"></a>5.Sklearn实现KNN算法</h3><p>下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load iris data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">knn=KNeighborsClassifier(algorithm=<span class=\"string\">'kd_tree'</span>)</span><br><span class=\"line\">knn.fit(X_train,y_train)</span><br><span class=\"line\">print(knn.predict(X_test))</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 1 2 1]</span></span><br><span class=\"line\">print(y_test)</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 2 2 1]</span></span><br><span class=\"line\">print(knn.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.977777777778</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"6-KNN优缺点\"><a href=\"#6-KNN优缺点\" class=\"headerlink\" title=\"6.KNN优缺点\"></a>6.KNN优缺点</h3><h4 id=\"6-1优点\"><a href=\"#6-1优点\" class=\"headerlink\" title=\"6.1优点\"></a>6.1优点</h4><ul>\n<li>即可处理分类也可处理回归问题。</li>\n<li>对数据没有假设，准确度高，对异常点不敏感。</li>\n<li>比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。</li>\n<li>主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。</li>\n</ul>\n<h4 id=\"6-2缺点\"><a href=\"#6-2缺点\" class=\"headerlink\" title=\"6.2缺点\"></a>6.2缺点</h4><ul>\n<li>计算量大，尤其是特征维数较多时候。</li>\n<li>样本不平衡时，对稀有类别的预测准确率低。</li>\n<li>KD树、球树之类的模型建立时需要大量的内存。</li>\n<li>使用懒惰学习方法，基本上不学习，导致预测时速度较慢。</li>\n</ul>\n<h3 id=\"7-推广\"><a href=\"#7-推广\" class=\"headerlink\" title=\"7.推广\"></a>7.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6061661.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_K近邻法(KNN)原理小结</a></p>\n<p><a href=\"https://www.cnblogs.com/ybjourney/p/4702562.html\" target=\"_blank\" rel=\"noopener\">Yabea_K-近邻(KNN)算法</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-KNN简介\"><a href=\"#1-KNN简介\" class=\"headerlink\" title=\"1.KNN简介\"></a>1.KNN简介</h3><p><strong>K近邻(K-Nearest Neighbors, KNN)</strong>算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做<strong>分类</strong>预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做<strong>回归</strong>预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。</p>\n<p>如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设<strong>K=3</strong>，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设<strong>K=5</strong>，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。</p>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png\" alt=\"机器学习之K近邻算法图片01\"></p>\n<p>从上面实例，我们可以总结下KNN算法过程</p>\n<ol>\n<li>计算测试数据与各个训练数据之间的距离。</li>\n<li>按照距离的递增关系进行排序，选取距离最小的K个点。</li>\n<li>确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。</li>\n</ol>\n<p>从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。</p>\n<ul>\n<li><strong>距离度量方式：</strong>KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nD(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}</script><script type=\"math/tex; mode=display\">\nD(x,y)={|x_1-y_1|+|x_2-y_2|+|x_3-y_3|...+|x_n-y_n|}={\\sum_{i=1}^{n}|x_i-y_i|}</script><ul>\n<li><strong>K值的选取：</strong>KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。</li>\n<li><strong>分类决策规则：</strong>KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。</li>\n</ul>\n<p>KNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。</p>\n<h3 id=\"2-KD树原理\"><a href=\"#2-KD树原理\" class=\"headerlink\" title=\"2.KD树原理\"></a>2.KD树原理</h3><p>KD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。</p>\n<h4 id=\"2-1KD树建立\"><a href=\"#2-1KD树建立\" class=\"headerlink\" title=\"2.1KD树建立\"></a>2.1KD树建立</h4><p>下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。</p>\n<ol>\n<li><strong>寻找划分特征：</strong>KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。</li>\n<li><strong>确定划分点：</strong>选择特征nk的中位数nkv所对应的样本作为划分点。</li>\n<li><strong>确定左子空间和右子空间：</strong>对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。</li>\n<li><strong>递归构建KD树：</strong>对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。</li>\n</ol>\n<p>我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下</p>\n<ol>\n<li><strong>寻找划分特征：</strong>6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。</li>\n<li><strong>确定划分点：</strong>根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。</li>\n<li><strong>确定左子空间和右子空间：</strong>分割超平面x=7将空间分为两部分。x&lt;=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x&gt;7的部分为右子空间，包含节点为{(9,6)，(8,1)}。</li>\n<li><strong>递归构建KD树：</strong>用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。</li>\n</ol>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png\" alt=\"机器学习之K近邻算法图片02\"></p>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png\" alt=\"机器学习之K近邻算法图片03\"></p>\n<h4 id=\"2-2KD树搜索最近邻\"><a href=\"#2-2KD树搜索最近邻\" class=\"headerlink\" title=\"2.2KD树搜索最近邻\"></a>2.2KD树搜索最近邻</h4><p>当我们生成KD树后，就可以预测测试样本集里面的样本目标点。</p>\n<ol>\n<li><strong>二叉搜索：</strong>对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的<strong>叶子节点</strong>。</li>\n<li><strong>回溯：</strong>为找到最近邻，还需要进行<strong>回溯</strong>操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。</li>\n<li><strong>更新最近邻：</strong>返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</li>\n</ol>\n<p>为方便理解上述过程，我们利用<strong>2.1建立的KD树</strong>来寻找(2,4.5)的最近邻。</p>\n<ol>\n<li><strong>二叉搜索：</strong>首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)-&gt;(5,4)-&gt;(4,7)}。</li>\n<li><strong>回溯：</strong>节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。</li>\n<li><strong>更新最近邻：</strong>该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)-&gt;(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。</li>\n</ol>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png\" alt=\"机器学习之K近邻算法图片04\"></p>\n<h4 id=\"2-3KD树预测\"><a href=\"#2-3KD树预测\" class=\"headerlink\" title=\"2.3KD树预测\"></a>2.3KD树预测</h4><p>根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p>\n<h3 id=\"3-球树原理\"><a href=\"#3-球树原理\" class=\"headerlink\" title=\"3.球树原理\"></a>3.球树原理</h3><p>KD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。</p>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png\" alt=\"机器学习之K近邻算法图片05\"></p>\n<p>为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。</p>\n<h4 id=\"3-1球树建立\"><a href=\"#3-1球树建立\" class=\"headerlink\" title=\"3.1球树建立\"></a>3.1球树建立</h4><p>球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程</p>\n<ol>\n<li><strong>构建超球体：</strong>超球体是可以包含所有样本的最小球体。</li>\n<li><strong>划分子超球体：</strong>从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。</li>\n<li><strong>递归：</strong>对上述两个子超球体，递归执行步骤2，最终得到球树。</li>\n</ol>\n<p><img src=\"/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png\" alt=\"机器学习之K近邻算法图片06\"></p>\n<h4 id=\"3-2球树搜索最近邻\"><a href=\"#3-2球树搜索最近邻\" class=\"headerlink\" title=\"3.2球树搜索最近邻\"></a>3.2球树搜索最近邻</h4><p>KD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。</p>\n<ol>\n<li>自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。</li>\n<li>然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。</li>\n<li>检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。</li>\n</ol>\n<h4 id=\"3-3球树预测\"><a href=\"#3-3球树预测\" class=\"headerlink\" title=\"3.3球树预测\"></a>3.3球树预测</h4><p>根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p>\n<h3 id=\"4-KNN算法扩展\"><a href=\"#4-KNN算法扩展\" class=\"headerlink\" title=\"4.KNN算法扩展\"></a>4.KNN算法扩展</h3><p>有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。</p>\n<h3 id=\"5-Sklearn实现KNN算法\"><a href=\"#5-Sklearn实现KNN算法\" class=\"headerlink\" title=\"5.Sklearn实现KNN算法\"></a>5.Sklearn实现KNN算法</h3><p>下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load iris data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">knn=KNeighborsClassifier(algorithm=<span class=\"string\">'kd_tree'</span>)</span><br><span class=\"line\">knn.fit(X_train,y_train)</span><br><span class=\"line\">print(knn.predict(X_test))</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 1 2 1]</span></span><br><span class=\"line\">print(y_test)</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 2 2 1]</span></span><br><span class=\"line\">print(knn.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.977777777778</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"6-KNN优缺点\"><a href=\"#6-KNN优缺点\" class=\"headerlink\" title=\"6.KNN优缺点\"></a>6.KNN优缺点</h3><h4 id=\"6-1优点\"><a href=\"#6-1优点\" class=\"headerlink\" title=\"6.1优点\"></a>6.1优点</h4><ul>\n<li>即可处理分类也可处理回归问题。</li>\n<li>对数据没有假设，准确度高，对异常点不敏感。</li>\n<li>比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。</li>\n<li>主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。</li>\n</ul>\n<h4 id=\"6-2缺点\"><a href=\"#6-2缺点\" class=\"headerlink\" title=\"6.2缺点\"></a>6.2缺点</h4><ul>\n<li>计算量大，尤其是特征维数较多时候。</li>\n<li>样本不平衡时，对稀有类别的预测准确率低。</li>\n<li>KD树、球树之类的模型建立时需要大量的内存。</li>\n<li>使用懒惰学习方法，基本上不学习，导致预测时速度较慢。</li>\n</ul>\n<h3 id=\"7-推广\"><a href=\"#7-推广\" class=\"headerlink\" title=\"7.推广\"></a>7.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6061661.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_K近邻法(KNN)原理小结</a></p>\n<p><a href=\"https://www.cnblogs.com/ybjourney/p/4702562.html\" target=\"_blank\" rel=\"noopener\">Yabea_K-近邻(KNN)算法</a></p>\n</blockquote>\n"},{"title":"机器学习之Logistic回归","date":"2018-03-27T09:49:33.000Z","comments":1,"mathjax":true,"_content":"\n### 1.Logistic回归简介\n\n线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。\n\n### 2.Sigmoid函数\n\n为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于\n$$\nexp(w_{k1}x_1+…+w_{kn}x_n)。\n$$\n因为一个数据点属于各类的概率之和为1，所以可以得到\n$$\nP(y=k)=\\frac{exp(\\sum_{i=1}^{n}w_{ki}{x_i})}{\\sum_{k'}exp(\\sum_{i=1}^{n}w_{k'i}x_i)}\n$$\n现在回到两类（0,1）的情况，此时分母上只有两项\n$$\nP(y=1)=\\frac{exp(\\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\\sum_{i=1}^{n}w_{1i}x_i)+exp(\\sum_{i=1}^{n}w_{0i}x_i)}\n$$\n公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有\n$$\nP(y=1)=\\frac{1}{1+exp(-\\sum_{i=1}^{n}w_ix_i)}\n$$\n上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。\n$$\nSigmoid Function: f(x)=\\frac{1}{1+e^{-x}}\n$$\nSigmoid函数具有如下性质\n\n+ 函数连续且单调递增\n+ 函数关于（0,0.5）对称\n+ $x\\in(-\\infty,\\infty)$时$y\\in(0,1)$\n\n```python\n#plot sigmoid function \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n##sigmoid function\nx=np.arange(-5,5,0.1)\ny=1/(1+np.exp(-x))\n\n#plot\nplt.figure()\nplt.plot(x,y,color='red',linewidth='2')\nax=plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data',0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()\n```\n\n![机器学习之Logistic回归01](机器学习之Logistic回归/机器学习之Logistic回归01.png)\n\n### 3.Logistic回归推导\n\n+ 特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。\n+ $\\theta=(\\theta_0,\\theta_1,\\theta_2…\\theta_n)$\n+ $n$表示特征数量\n+ $m$表示训练数据数量\n\n线性回归函数为$z=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n=\\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。\n$$\nh(X)=\\frac{1}{1+e^{-\\theta^TX}}\n$$\n我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)<0.5$是判断当前数据属于A类，当$h(X)>0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\\theta$。\n\n条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为\n$$\nP(y=1|X)=\\pi(X)=\\frac{1}{1+e^{-\\theta^TX}}\n$$\n条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为\n$$\nP(y=0|X)=1-\\pi(X)=\\frac{1}{1+e^{\\theta^TX}}\n$$\n因此我们可以得到事件的发生比为\n$$\nodds=\\frac{P(y=1|X)}{P(y=0|X)}\n$$\n事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。\n\n各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数\n$$\nL(\\theta)=\\prod_{i=1}^{m}[\\pi(X_i)]^{y_i}[1-\\pi(X_i)]^{1-y_i}\n$$\n目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到\n$$\nlnL(\\theta)=\\sum_{i=1}^{m}\\left \\{ y_iln[\\pi(X_i)] +(1-y_i)ln[1-\\pi(X_i)] \\right \\}\n$$\n\n$$\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_iln\\frac{\\pi(X_i)}{1-\\pi(X_i)}\n$$\n\n$$\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_i\\theta^TX\n$$\n\n$$\n=\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX\n$$\n\n通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\\theta$。首先在前面乘上负的系数$-\\frac{1}{m}$，所以$J(\\theta)$最小时的$\\theta$为最佳参数。\n$$\nJ(\\theta)=-\\frac{1}{m}lnL(\\theta)\n$$\n\n$$\n=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}\n$$\n\n### 4.梯度下降算法\n\n#### 4.1梯度下降算法简述\n\n实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。\n\n![器学习之Logistic回归0](机器学习之Logistic回归/机器学习之Logistic回归02.png)\n\n从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。\n\n#### 4.2 梯度下降算法相关概念\n\n求解梯度下降算法之前，我们先了解相关概念。\n\n- **步长（Learning Rate）**：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。\n- **特征（Feature）**：即上述描述的$X$\n- **假设函数（Hypothesis Function）**：监督学习中，为了拟合输入样本，而使用假设函数。\n- **损失函数（Loss Function）**：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为\n\n$$\nJ(\\theta)=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}\n$$\n\n我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。\n\n#### 4.3梯度下降算法过程\n\n- 随机选取一组$\\theta$。\n- 不断变化$\\theta$，让$J(\\theta)$变小，$\\alpha$为学习步长。\n\n$$\n\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n$$\n\n+ 直到$J(\\theta)$得到最小值，$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)$为$J(\\theta)$对$\\theta_k$的偏导。\n\n$$\n\\frac{\\partial J(\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{1+e^{\\theta^TX}}e^{\\theta^TX}X_{ij}-\\sum_{i=1}^{m}y_iX_{ij}\n$$\n\n$$\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\frac{e^{\\theta^TX}}{1+e^{\\theta^Tx}}-y_i]\n$$\n\n$$\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]\n$$\n\n因此梯度下降算法的迭代最终表述为\n$$\n\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]\n$$\n梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到Logistic函数。\n\n### 5.Logistic回归实现\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan', 'gray')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl)\n    # highlight test samples\n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        plt.scatter(X_test[:, 0], X_test[:, 1], c='blue', alpha=1.0, linewidth=1, marker='o', s=55, label='test set')\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n#为了追求机器学习的最佳性能，我们将特征缩放\nsc = StandardScaler()\nsc.fit(X_train)#估算每个特征的平均值和标准差\nX_train_std=sc.transform(X_train)#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性\nX_test_std=sc.transform(X_test)\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\n#训练感知机模型\nlr = LogisticRegression(C=1000.0,random_state=0)#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序\nlr.fit(X_train_std, y_train)\nlr.predict_proba(X_test_std)\n\n#绘图\nplot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105,150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\n```\n\n![机器学习之Logistic回归03](机器学习之Logistic回归/机器学习之Logistic回归03.png)\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![](机器学习之Logistic回归/推广.png)\n\n+ 参考\n\n[^1]: https://www.zhihu.com/people/maigo/activities\n[^2]: https://blog.csdn.net/programmer_wei/article/details/52072939\n[^3]: https://blog.csdn.net/javaisnotgood/article/details/78873819\n\n","source":"_posts/机器学习之Logistic回归.md","raw":"---\ntitle: 机器学习之Logistic回归\ndate: 2018-03-27 17:49:33\ntags: [机器学习,算法]\ncategories: 机器学习\ncomments: true\nmathjax: true\n---\n\n### 1.Logistic回归简介\n\n线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。\n\n### 2.Sigmoid函数\n\n为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于\n$$\nexp(w_{k1}x_1+…+w_{kn}x_n)。\n$$\n因为一个数据点属于各类的概率之和为1，所以可以得到\n$$\nP(y=k)=\\frac{exp(\\sum_{i=1}^{n}w_{ki}{x_i})}{\\sum_{k'}exp(\\sum_{i=1}^{n}w_{k'i}x_i)}\n$$\n现在回到两类（0,1）的情况，此时分母上只有两项\n$$\nP(y=1)=\\frac{exp(\\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\\sum_{i=1}^{n}w_{1i}x_i)+exp(\\sum_{i=1}^{n}w_{0i}x_i)}\n$$\n公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有\n$$\nP(y=1)=\\frac{1}{1+exp(-\\sum_{i=1}^{n}w_ix_i)}\n$$\n上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。\n$$\nSigmoid Function: f(x)=\\frac{1}{1+e^{-x}}\n$$\nSigmoid函数具有如下性质\n\n+ 函数连续且单调递增\n+ 函数关于（0,0.5）对称\n+ $x\\in(-\\infty,\\infty)$时$y\\in(0,1)$\n\n```python\n#plot sigmoid function \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n##sigmoid function\nx=np.arange(-5,5,0.1)\ny=1/(1+np.exp(-x))\n\n#plot\nplt.figure()\nplt.plot(x,y,color='red',linewidth='2')\nax=plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data',0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()\n```\n\n![机器学习之Logistic回归01](机器学习之Logistic回归/机器学习之Logistic回归01.png)\n\n### 3.Logistic回归推导\n\n+ 特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。\n+ $\\theta=(\\theta_0,\\theta_1,\\theta_2…\\theta_n)$\n+ $n$表示特征数量\n+ $m$表示训练数据数量\n\n线性回归函数为$z=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n=\\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。\n$$\nh(X)=\\frac{1}{1+e^{-\\theta^TX}}\n$$\n我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)<0.5$是判断当前数据属于A类，当$h(X)>0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\\theta$。\n\n条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为\n$$\nP(y=1|X)=\\pi(X)=\\frac{1}{1+e^{-\\theta^TX}}\n$$\n条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为\n$$\nP(y=0|X)=1-\\pi(X)=\\frac{1}{1+e^{\\theta^TX}}\n$$\n因此我们可以得到事件的发生比为\n$$\nodds=\\frac{P(y=1|X)}{P(y=0|X)}\n$$\n事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。\n\n各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数\n$$\nL(\\theta)=\\prod_{i=1}^{m}[\\pi(X_i)]^{y_i}[1-\\pi(X_i)]^{1-y_i}\n$$\n目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到\n$$\nlnL(\\theta)=\\sum_{i=1}^{m}\\left \\{ y_iln[\\pi(X_i)] +(1-y_i)ln[1-\\pi(X_i)] \\right \\}\n$$\n\n$$\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_iln\\frac{\\pi(X_i)}{1-\\pi(X_i)}\n$$\n\n$$\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_i\\theta^TX\n$$\n\n$$\n=\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX\n$$\n\n通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\\theta$。首先在前面乘上负的系数$-\\frac{1}{m}$，所以$J(\\theta)$最小时的$\\theta$为最佳参数。\n$$\nJ(\\theta)=-\\frac{1}{m}lnL(\\theta)\n$$\n\n$$\n=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}\n$$\n\n### 4.梯度下降算法\n\n#### 4.1梯度下降算法简述\n\n实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。\n\n![器学习之Logistic回归0](机器学习之Logistic回归/机器学习之Logistic回归02.png)\n\n从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。\n\n#### 4.2 梯度下降算法相关概念\n\n求解梯度下降算法之前，我们先了解相关概念。\n\n- **步长（Learning Rate）**：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。\n- **特征（Feature）**：即上述描述的$X$\n- **假设函数（Hypothesis Function）**：监督学习中，为了拟合输入样本，而使用假设函数。\n- **损失函数（Loss Function）**：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为\n\n$$\nJ(\\theta)=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}\n$$\n\n我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。\n\n#### 4.3梯度下降算法过程\n\n- 随机选取一组$\\theta$。\n- 不断变化$\\theta$，让$J(\\theta)$变小，$\\alpha$为学习步长。\n\n$$\n\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n$$\n\n+ 直到$J(\\theta)$得到最小值，$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)$为$J(\\theta)$对$\\theta_k$的偏导。\n\n$$\n\\frac{\\partial J(\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{1+e^{\\theta^TX}}e^{\\theta^TX}X_{ij}-\\sum_{i=1}^{m}y_iX_{ij}\n$$\n\n$$\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\frac{e^{\\theta^TX}}{1+e^{\\theta^Tx}}-y_i]\n$$\n\n$$\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]\n$$\n\n因此梯度下降算法的迭代最终表述为\n$$\n\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]\n$$\n梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到Logistic函数。\n\n### 5.Logistic回归实现\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan', 'gray')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl)\n    # highlight test samples\n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        plt.scatter(X_test[:, 0], X_test[:, 1], c='blue', alpha=1.0, linewidth=1, marker='o', s=55, label='test set')\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n#为了追求机器学习的最佳性能，我们将特征缩放\nsc = StandardScaler()\nsc.fit(X_train)#估算每个特征的平均值和标准差\nX_train_std=sc.transform(X_train)#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性\nX_test_std=sc.transform(X_test)\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\n#训练感知机模型\nlr = LogisticRegression(C=1000.0,random_state=0)#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序\nlr.fit(X_train_std, y_train)\nlr.predict_proba(X_test_std)\n\n#绘图\nplot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105,150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\n```\n\n![机器学习之Logistic回归03](机器学习之Logistic回归/机器学习之Logistic回归03.png)\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![](机器学习之Logistic回归/推广.png)\n\n+ 参考\n\n[^1]: https://www.zhihu.com/people/maigo/activities\n[^2]: https://blog.csdn.net/programmer_wei/article/details/52072939\n[^3]: https://blog.csdn.net/javaisnotgood/article/details/78873819\n\n","slug":"机器学习之Logistic回归","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3x000xjiz561kzqk0s","content":"<h3 id=\"1-Logistic回归简介\"><a href=\"#1-Logistic回归简介\" class=\"headerlink\" title=\"1.Logistic回归简介\"></a>1.Logistic回归简介</h3><p>线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。</p>\n<h3 id=\"2-Sigmoid函数\"><a href=\"#2-Sigmoid函数\" class=\"headerlink\" title=\"2.Sigmoid函数\"></a>2.Sigmoid函数</h3><p>为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于</p>\n<script type=\"math/tex; mode=display\">\nexp(w_{k1}x_1+…+w_{kn}x_n)。</script><p>因为一个数据点属于各类的概率之和为1，所以可以得到</p>\n<script type=\"math/tex; mode=display\">\nP(y=k)=\\frac{exp(\\sum_{i=1}^{n}w_{ki}{x_i})}{\\sum_{k'}exp(\\sum_{i=1}^{n}w_{k'i}x_i)}</script><p>现在回到两类（0,1）的情况，此时分母上只有两项</p>\n<script type=\"math/tex; mode=display\">\nP(y=1)=\\frac{exp(\\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\\sum_{i=1}^{n}w_{1i}x_i)+exp(\\sum_{i=1}^{n}w_{0i}x_i)}</script><p>公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有</p>\n<script type=\"math/tex; mode=display\">\nP(y=1)=\\frac{1}{1+exp(-\\sum_{i=1}^{n}w_ix_i)}</script><p>上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。</p>\n<script type=\"math/tex; mode=display\">\nSigmoid Function: f(x)=\\frac{1}{1+e^{-x}}</script><p>Sigmoid函数具有如下性质</p>\n<ul>\n<li>函数连续且单调递增</li>\n<li>函数关于（0,0.5）对称</li>\n<li>$x\\in(-\\infty,\\infty)$时$y\\in(0,1)$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#plot sigmoid function </span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">##sigmoid function</span></span><br><span class=\"line\">x=np.arange(<span class=\"number\">-5</span>,<span class=\"number\">5</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">y=<span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(x,y,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"string\">'2'</span>)</span><br><span class=\"line\">ax=plt.gca()</span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'independent variable'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'dependent variable'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归01.png\" alt=\"机器学习之Logistic回归01\"></p>\n<h3 id=\"3-Logistic回归推导\"><a href=\"#3-Logistic回归推导\" class=\"headerlink\" title=\"3.Logistic回归推导\"></a>3.Logistic回归推导</h3><ul>\n<li>特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。</li>\n<li>$\\theta=(\\theta_0,\\theta_1,\\theta_2…\\theta_n)$</li>\n<li>$n$表示特征数量</li>\n<li>$m$表示训练数据数量</li>\n</ul>\n<p>线性回归函数为$z=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n=\\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。</p>\n<script type=\"math/tex; mode=display\">\nh(X)=\\frac{1}{1+e^{-\\theta^TX}}</script><p>我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)<0.5$是判断当前数据属于a类，当$h(x)>0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\\theta$。</0.5$是判断当前数据属于a类，当$h(x)></p>\n<p>条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为</p>\n<script type=\"math/tex; mode=display\">\nP(y=1|X)=\\pi(X)=\\frac{1}{1+e^{-\\theta^TX}}</script><p>条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为</p>\n<script type=\"math/tex; mode=display\">\nP(y=0|X)=1-\\pi(X)=\\frac{1}{1+e^{\\theta^TX}}</script><p>因此我们可以得到事件的发生比为</p>\n<script type=\"math/tex; mode=display\">\nodds=\\frac{P(y=1|X)}{P(y=0|X)}</script><p>事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。</p>\n<p>各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta)=\\prod_{i=1}^{m}[\\pi(X_i)]^{y_i}[1-\\pi(X_i)]^{1-y_i}</script><p>目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到</p>\n<script type=\"math/tex; mode=display\">\nlnL(\\theta)=\\sum_{i=1}^{m}\\left \\{ y_iln[\\pi(X_i)] +(1-y_i)ln[1-\\pi(X_i)] \\right \\}</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_iln\\frac{\\pi(X_i)}{1-\\pi(X_i)}</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_i\\theta^TX</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX</script><p>通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\\theta$。首先在前面乘上负的系数$-\\frac{1}{m}$，所以$J(\\theta)$最小时的$\\theta$为最佳参数。</p>\n<script type=\"math/tex; mode=display\">\nJ(\\theta)=-\\frac{1}{m}lnL(\\theta)</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}</script><h3 id=\"4-梯度下降算法\"><a href=\"#4-梯度下降算法\" class=\"headerlink\" title=\"4.梯度下降算法\"></a>4.梯度下降算法</h3><h4 id=\"4-1梯度下降算法简述\"><a href=\"#4-1梯度下降算法简述\" class=\"headerlink\" title=\"4.1梯度下降算法简述\"></a>4.1梯度下降算法简述</h4><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。</p>\n<p><img src=\"/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归02.png\" alt=\"器学习之Logistic回归0\"></p>\n<p>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p>\n<h4 id=\"4-2-梯度下降算法相关概念\"><a href=\"#4-2-梯度下降算法相关概念\" class=\"headerlink\" title=\"4.2 梯度下降算法相关概念\"></a>4.2 梯度下降算法相关概念</h4><p>求解梯度下降算法之前，我们先了解相关概念。</p>\n<ul>\n<li><strong>步长（Learning Rate）</strong>：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。</li>\n<li><strong>特征（Feature）</strong>：即上述描述的$X$</li>\n<li><strong>假设函数（Hypothesis Function）</strong>：监督学习中，为了拟合输入样本，而使用假设函数。</li>\n<li><strong>损失函数（Loss Function）</strong>：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ(\\theta)=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}</script><p>我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。</p>\n<h4 id=\"4-3梯度下降算法过程\"><a href=\"#4-3梯度下降算法过程\" class=\"headerlink\" title=\"4.3梯度下降算法过程\"></a>4.3梯度下降算法过程</h4><ul>\n<li>随机选取一组$\\theta$。</li>\n<li>不断变化$\\theta$，让$J(\\theta)$变小，$\\alpha$为学习步长。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)</script><ul>\n<li>直到$J(\\theta)$得到最小值，$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)$为$J(\\theta)$对$\\theta_k$的偏导。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J(\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{1+e^{\\theta^TX}}e^{\\theta^TX}X_{ij}-\\sum_{i=1}^{m}y_iX_{ij}</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\frac{e^{\\theta^TX}}{1+e^{\\theta^Tx}}-y_i]</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]</script><p>因此梯度下降算法的迭代最终表述为</p>\n<script type=\"math/tex; mode=display\">\n\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]</script><p>梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到Logistic函数。</p>\n<h3 id=\"5-Logistic回归实现\"><a href=\"#5-Logistic回归实现\" class=\"headerlink\" title=\"5.Logistic回归实现\"></a>5.Logistic回归实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib.colors <span class=\"keyword\">import</span> ListedColormap</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_regions</span><span class=\"params\">(X, y, classifier, test_idx=None, resolution=<span class=\"number\">0.02</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># setup marker generator and color map</span></span><br><span class=\"line\">    markers = (<span class=\"string\">'s'</span>, <span class=\"string\">'x'</span>, <span class=\"string\">'o'</span>, <span class=\"string\">'^'</span>, <span class=\"string\">'v'</span>)</span><br><span class=\"line\">    colors = (<span class=\"string\">'red'</span>, <span class=\"string\">'blue'</span>, <span class=\"string\">'lightgreen'</span>, <span class=\"string\">'cyan'</span>, <span class=\"string\">'gray'</span>)</span><br><span class=\"line\">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class=\"line\">    <span class=\"comment\"># plot the decision surface</span></span><br><span class=\"line\">    x1_min, x1_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    x2_min, x2_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))</span><br><span class=\"line\">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class=\"line\">    Z = Z.reshape(xx1.shape)</span><br><span class=\"line\">    plt.contourf(xx1, xx2, Z, alpha=<span class=\"number\">0.4</span>, cmap=cmap)</span><br><span class=\"line\">    plt.xlim(xx1.min(), xx1.max())</span><br><span class=\"line\">    plt.ylim(xx2.min(), xx2.max())</span><br><span class=\"line\">    <span class=\"comment\"># plot class samples</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx, cl <span class=\"keyword\">in</span> enumerate(np.unique(y)):</span><br><span class=\"line\">        plt.scatter(x=X[y == cl, <span class=\"number\">0</span>], y=X[y == cl, <span class=\"number\">1</span>],alpha=<span class=\"number\">0.8</span>, c=cmap(idx),marker=markers[idx], label=cl)</span><br><span class=\"line\">    <span class=\"comment\"># highlight test samples</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> test_idx:</span><br><span class=\"line\">        X_test, y_test = X[test_idx, :], y[test_idx]</span><br><span class=\"line\">        plt.scatter(X_test[:, <span class=\"number\">0</span>], X_test[:, <span class=\"number\">1</span>], c=<span class=\"string\">'blue'</span>, alpha=<span class=\"number\">1.0</span>, linewidth=<span class=\"number\">1</span>, marker=<span class=\"string\">'o'</span>, s=<span class=\"number\">55</span>, label=<span class=\"string\">'test set'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data[:, [<span class=\"number\">2</span>, <span class=\"number\">3</span>]]</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.3</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"comment\">#为了追求机器学习的最佳性能，我们将特征缩放</span></span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit(X_train)<span class=\"comment\">#估算每个特征的平均值和标准差</span></span><br><span class=\"line\">X_train_std=sc.transform(X_train)<span class=\"comment\">#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性</span></span><br><span class=\"line\">X_test_std=sc.transform(X_test)</span><br><span class=\"line\">X_combined_std = np.vstack((X_train_std, X_test_std))</span><br><span class=\"line\">y_combined = np.hstack((y_train, y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练感知机模型</span></span><br><span class=\"line\">lr = LogisticRegression(C=<span class=\"number\">1000.0</span>,random_state=<span class=\"number\">0</span>)<span class=\"comment\">#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序</span></span><br><span class=\"line\">lr.fit(X_train_std, y_train)</span><br><span class=\"line\">lr.predict_proba(X_test_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘图</span></span><br><span class=\"line\">plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(<span class=\"number\">105</span>,<span class=\"number\">150</span>))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'petal length [standardized]'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'petal width [standardized]'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">'upper left'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归03.png\" alt=\"机器学习之Logistic回归03\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/27/机器学习之Logistic回归/推广.png\" alt=\"\"></p>\n<ul>\n<li>参考</li>\n</ul>\n<blockquote id=\"fn_1\">\n<sup>1</sup>. <a href=\"https://www.zhihu.com/people/maigo/activities\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/people/maigo/activities</a><a href=\"#reffn_1\" title=\"Jump back to footnote [1] in the text.\"> &#8617;</a>\n</blockquote>\n<blockquote id=\"fn_2\">\n<sup>2</sup>. <a href=\"https://blog.csdn.net/programmer_wei/article/details/52072939\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/programmer_wei/article/details/52072939</a><a href=\"#reffn_2\" title=\"Jump back to footnote [2] in the text.\"> &#8617;</a>\n</blockquote>\n<blockquote id=\"fn_3\">\n<sup>3</sup>. <a href=\"https://blog.csdn.net/javaisnotgood/article/details/78873819\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/javaisnotgood/article/details/78873819</a><a href=\"#reffn_3\" title=\"Jump back to footnote [3] in the text.\"> &#8617;</a>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Logistic回归简介\"><a href=\"#1-Logistic回归简介\" class=\"headerlink\" title=\"1.Logistic回归简介\"></a>1.Logistic回归简介</h3><p>线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。</p>\n<h3 id=\"2-Sigmoid函数\"><a href=\"#2-Sigmoid函数\" class=\"headerlink\" title=\"2.Sigmoid函数\"></a>2.Sigmoid函数</h3><p>为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于</p>\n<script type=\"math/tex; mode=display\">\nexp(w_{k1}x_1+…+w_{kn}x_n)。</script><p>因为一个数据点属于各类的概率之和为1，所以可以得到</p>\n<script type=\"math/tex; mode=display\">\nP(y=k)=\\frac{exp(\\sum_{i=1}^{n}w_{ki}{x_i})}{\\sum_{k'}exp(\\sum_{i=1}^{n}w_{k'i}x_i)}</script><p>现在回到两类（0,1）的情况，此时分母上只有两项</p>\n<script type=\"math/tex; mode=display\">\nP(y=1)=\\frac{exp(\\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\\sum_{i=1}^{n}w_{1i}x_i)+exp(\\sum_{i=1}^{n}w_{0i}x_i)}</script><p>公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有</p>\n<script type=\"math/tex; mode=display\">\nP(y=1)=\\frac{1}{1+exp(-\\sum_{i=1}^{n}w_ix_i)}</script><p>上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。</p>\n<script type=\"math/tex; mode=display\">\nSigmoid Function: f(x)=\\frac{1}{1+e^{-x}}</script><p>Sigmoid函数具有如下性质</p>\n<ul>\n<li>函数连续且单调递增</li>\n<li>函数关于（0,0.5）对称</li>\n<li>$x\\in(-\\infty,\\infty)$时$y\\in(0,1)$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#plot sigmoid function </span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">##sigmoid function</span></span><br><span class=\"line\">x=np.arange(<span class=\"number\">-5</span>,<span class=\"number\">5</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">y=<span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(x,y,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"string\">'2'</span>)</span><br><span class=\"line\">ax=plt.gca()</span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'independent variable'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'dependent variable'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归01.png\" alt=\"机器学习之Logistic回归01\"></p>\n<h3 id=\"3-Logistic回归推导\"><a href=\"#3-Logistic回归推导\" class=\"headerlink\" title=\"3.Logistic回归推导\"></a>3.Logistic回归推导</h3><ul>\n<li>特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。</li>\n<li>$\\theta=(\\theta_0,\\theta_1,\\theta_2…\\theta_n)$</li>\n<li>$n$表示特征数量</li>\n<li>$m$表示训练数据数量</li>\n</ul>\n<p>线性回归函数为$z=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n=\\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。</p>\n<script type=\"math/tex; mode=display\">\nh(X)=\\frac{1}{1+e^{-\\theta^TX}}</script><p>我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)<0.5$是判断当前数据属于a类，当$h(x)>0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\\theta$。</0.5$是判断当前数据属于a类，当$h(x)></p>\n<p>条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为</p>\n<script type=\"math/tex; mode=display\">\nP(y=1|X)=\\pi(X)=\\frac{1}{1+e^{-\\theta^TX}}</script><p>条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为</p>\n<script type=\"math/tex; mode=display\">\nP(y=0|X)=1-\\pi(X)=\\frac{1}{1+e^{\\theta^TX}}</script><p>因此我们可以得到事件的发生比为</p>\n<script type=\"math/tex; mode=display\">\nodds=\\frac{P(y=1|X)}{P(y=0|X)}</script><p>事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。</p>\n<p>各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta)=\\prod_{i=1}^{m}[\\pi(X_i)]^{y_i}[1-\\pi(X_i)]^{1-y_i}</script><p>目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到</p>\n<script type=\"math/tex; mode=display\">\nlnL(\\theta)=\\sum_{i=1}^{m}\\left \\{ y_iln[\\pi(X_i)] +(1-y_i)ln[1-\\pi(X_i)] \\right \\}</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_iln\\frac{\\pi(X_i)}{1-\\pi(X_i)}</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_i\\theta^TX</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX</script><p>通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\\theta$。首先在前面乘上负的系数$-\\frac{1}{m}$，所以$J(\\theta)$最小时的$\\theta$为最佳参数。</p>\n<script type=\"math/tex; mode=display\">\nJ(\\theta)=-\\frac{1}{m}lnL(\\theta)</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}</script><h3 id=\"4-梯度下降算法\"><a href=\"#4-梯度下降算法\" class=\"headerlink\" title=\"4.梯度下降算法\"></a>4.梯度下降算法</h3><h4 id=\"4-1梯度下降算法简述\"><a href=\"#4-1梯度下降算法简述\" class=\"headerlink\" title=\"4.1梯度下降算法简述\"></a>4.1梯度下降算法简述</h4><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。</p>\n<p><img src=\"/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归02.png\" alt=\"器学习之Logistic回归0\"></p>\n<p>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p>\n<h4 id=\"4-2-梯度下降算法相关概念\"><a href=\"#4-2-梯度下降算法相关概念\" class=\"headerlink\" title=\"4.2 梯度下降算法相关概念\"></a>4.2 梯度下降算法相关概念</h4><p>求解梯度下降算法之前，我们先了解相关概念。</p>\n<ul>\n<li><strong>步长（Learning Rate）</strong>：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。</li>\n<li><strong>特征（Feature）</strong>：即上述描述的$X$</li>\n<li><strong>假设函数（Hypothesis Function）</strong>：监督学习中，为了拟合输入样本，而使用假设函数。</li>\n<li><strong>损失函数（Loss Function）</strong>：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ(\\theta)=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}</script><p>我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。</p>\n<h4 id=\"4-3梯度下降算法过程\"><a href=\"#4-3梯度下降算法过程\" class=\"headerlink\" title=\"4.3梯度下降算法过程\"></a>4.3梯度下降算法过程</h4><ul>\n<li>随机选取一组$\\theta$。</li>\n<li>不断变化$\\theta$，让$J(\\theta)$变小，$\\alpha$为学习步长。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)</script><ul>\n<li>直到$J(\\theta)$得到最小值，$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)$为$J(\\theta)$对$\\theta_k$的偏导。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J(\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{1+e^{\\theta^TX}}e^{\\theta^TX}X_{ij}-\\sum_{i=1}^{m}y_iX_{ij}</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\frac{e^{\\theta^TX}}{1+e^{\\theta^Tx}}-y_i]</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]</script><p>因此梯度下降算法的迭代最终表述为</p>\n<script type=\"math/tex; mode=display\">\n\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]</script><p>梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到Logistic函数。</p>\n<h3 id=\"5-Logistic回归实现\"><a href=\"#5-Logistic回归实现\" class=\"headerlink\" title=\"5.Logistic回归实现\"></a>5.Logistic回归实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib.colors <span class=\"keyword\">import</span> ListedColormap</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_regions</span><span class=\"params\">(X, y, classifier, test_idx=None, resolution=<span class=\"number\">0.02</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># setup marker generator and color map</span></span><br><span class=\"line\">    markers = (<span class=\"string\">'s'</span>, <span class=\"string\">'x'</span>, <span class=\"string\">'o'</span>, <span class=\"string\">'^'</span>, <span class=\"string\">'v'</span>)</span><br><span class=\"line\">    colors = (<span class=\"string\">'red'</span>, <span class=\"string\">'blue'</span>, <span class=\"string\">'lightgreen'</span>, <span class=\"string\">'cyan'</span>, <span class=\"string\">'gray'</span>)</span><br><span class=\"line\">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class=\"line\">    <span class=\"comment\"># plot the decision surface</span></span><br><span class=\"line\">    x1_min, x1_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    x2_min, x2_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))</span><br><span class=\"line\">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class=\"line\">    Z = Z.reshape(xx1.shape)</span><br><span class=\"line\">    plt.contourf(xx1, xx2, Z, alpha=<span class=\"number\">0.4</span>, cmap=cmap)</span><br><span class=\"line\">    plt.xlim(xx1.min(), xx1.max())</span><br><span class=\"line\">    plt.ylim(xx2.min(), xx2.max())</span><br><span class=\"line\">    <span class=\"comment\"># plot class samples</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx, cl <span class=\"keyword\">in</span> enumerate(np.unique(y)):</span><br><span class=\"line\">        plt.scatter(x=X[y == cl, <span class=\"number\">0</span>], y=X[y == cl, <span class=\"number\">1</span>],alpha=<span class=\"number\">0.8</span>, c=cmap(idx),marker=markers[idx], label=cl)</span><br><span class=\"line\">    <span class=\"comment\"># highlight test samples</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> test_idx:</span><br><span class=\"line\">        X_test, y_test = X[test_idx, :], y[test_idx]</span><br><span class=\"line\">        plt.scatter(X_test[:, <span class=\"number\">0</span>], X_test[:, <span class=\"number\">1</span>], c=<span class=\"string\">'blue'</span>, alpha=<span class=\"number\">1.0</span>, linewidth=<span class=\"number\">1</span>, marker=<span class=\"string\">'o'</span>, s=<span class=\"number\">55</span>, label=<span class=\"string\">'test set'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data[:, [<span class=\"number\">2</span>, <span class=\"number\">3</span>]]</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.3</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"comment\">#为了追求机器学习的最佳性能，我们将特征缩放</span></span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit(X_train)<span class=\"comment\">#估算每个特征的平均值和标准差</span></span><br><span class=\"line\">X_train_std=sc.transform(X_train)<span class=\"comment\">#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性</span></span><br><span class=\"line\">X_test_std=sc.transform(X_test)</span><br><span class=\"line\">X_combined_std = np.vstack((X_train_std, X_test_std))</span><br><span class=\"line\">y_combined = np.hstack((y_train, y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练感知机模型</span></span><br><span class=\"line\">lr = LogisticRegression(C=<span class=\"number\">1000.0</span>,random_state=<span class=\"number\">0</span>)<span class=\"comment\">#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序</span></span><br><span class=\"line\">lr.fit(X_train_std, y_train)</span><br><span class=\"line\">lr.predict_proba(X_test_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘图</span></span><br><span class=\"line\">plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(<span class=\"number\">105</span>,<span class=\"number\">150</span>))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'petal length [standardized]'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'petal width [standardized]'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">'upper left'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归03.png\" alt=\"机器学习之Logistic回归03\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/27/机器学习之Logistic回归/推广.png\" alt=\"\"></p>\n<ul>\n<li>参考</li>\n</ul>\n<blockquote id=\"fn_1\">\n<sup>1</sup>. <a href=\"https://www.zhihu.com/people/maigo/activities\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/people/maigo/activities</a><a href=\"#reffn_1\" title=\"Jump back to footnote [1] in the text.\"> &#8617;</a>\n</blockquote>\n<blockquote id=\"fn_2\">\n<sup>2</sup>. <a href=\"https://blog.csdn.net/programmer_wei/article/details/52072939\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/programmer_wei/article/details/52072939</a><a href=\"#reffn_2\" title=\"Jump back to footnote [2] in the text.\"> &#8617;</a>\n</blockquote>\n<blockquote id=\"fn_3\">\n<sup>3</sup>. <a href=\"https://blog.csdn.net/javaisnotgood/article/details/78873819\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/javaisnotgood/article/details/78873819</a><a href=\"#reffn_3\" title=\"Jump back to footnote [3] in the text.\"> &#8617;</a>\n</blockquote>\n"},{"title":"机器学习之K均值(K-Means)算法","date":"2018-05-12T05:44:19.000Z","mathjax":true,"comments":1,"_content":"\n### 1.K-Means简介\n\n**K均值(K-Means)**算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化**K-Means++**算法，距离计算优化**Elkan K-Means**算法和大样本情况下**Mini Batch K-Means**算法。\n\nK-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。\n\n假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。\n$$\nE=\\sum _{i=1}^{k} \\sum _{x \\in C_i}||x-\\mu _i||^2\n$$\n其中μi是簇Ci的均值向量，也可称作质心，表达式为\n$$\n\\mu _i=\\frac{1}{|C_i|}\\sum _{x \\in C_i}x\n$$\n如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。\n\n+ 如图(a)所示：表示初始化数据集。\n+ 如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。\n+ 如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。\n+ 如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。\n+ 如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。\n+ 如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。\n\n![机器学习值K均值算法图片01](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png)\n\n### 2.K-Means算法流程\n\n假设输入样本集$D={x_1,x_2,…,x_m}$，聚类簇数为K，最大迭代次数为N。输出的簇划分为$C={C_1,C_2,…,C_m}$。\n\n+ 从数据集D中随机选择K个样本作为初始的质心向量$ \\mu=\\{ \\mu_1,\\mu_2,\\mu_3,...,\\mu_k \\}$。\n\n+ 迭代$n=1,2,…,N$。\n\n  + 划分初始化簇$C_t=\\varnothing ;\\ t=1,2,…,k$。\n  + 对于$i=1,2,…,m$，计算样本$x_i$和各个质心向量$\\mu_j(\\ j=1,2,…,k)$的距离$d_{ij}$。将$x_i$标记为最小的$d_{ij}$所对应的类别$\\lambda_i$，此时更新$C_{\\lambda i}=C_{\\lambda i} \\cup{x_i}$。\n\n  $$\n  d_{ij}=||x_i-\\mu_j||^2\n  $$\n\n  + 对于$j=1,2,…,k$，对$C_j$中所有样本点重新计算新的质心。\n\n  $$\n  \\mu_j=\\frac{1}{|C_j|}\\sum_{x\\in C_j}x\n  $$\n\n  + 如果K个质心向量都不再发生变化，则结束迭代。\n\n+ 输出K个划分簇$C$，$C=\\{C_1,C_2,C_3,…,C_k \\}$。\n\n对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。\n\n+ **对于K值的选择:**我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。\n+ **对于K个初始化质心:**由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。\n\n### 3.初始化优化K-Means++\n\n如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。\n\n+ 从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。\n+ 对于数据集中的每个点xi，计算与他最近的聚类中心距离。\n\n$$\nD(x)=\\arg \\min_{r=1}^{k_{selected}}||x_i-\\mu_r||^2\n$$\n\n+ 选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。\n+ 重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。\n\n### 4.距离计算优化Elkan K-Means算法\n\n传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。\n\n+ 对于一个样本点$x$和两个质心$\\mu_{j1},\\mu_{j2}$，如果我们预先计算出这两个质心之间的距离$D(j_1,j_2)$，如果发现$2D(x,j_1)≤D(j_1,j_2)$，那么我们便能得到$D(x,j_1)≤D(x,j_2)$。此时我们不再计算$D(x,j_2)$，也就节省了一步距离计算。\n+ 对于一个样本点$x$和两个质心$μ_{j1},μ_{j2}$  ,我们能够得到$D(x,j_2)≥max{0,D(x,j_1)−D(j_1,j_2)}$。\n\nElkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。\n\n### 5.大样本优化Mini Batch K-Means算法\n\n传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。\n\nMini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。\n\nMini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。\n\n### 6.Sklearn实现K-Means算法\n\n我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)。\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n#load iris\niris=load_iris()\nX=iris.data[:,:2]\nprint(X.shape)\n#150,2\n\n#plot data\nplt.figure()\nplt.scatter(X[:,0],X[:,1],c='blue',\n            marker='o',label='point')\nplt.legend(loc=2)\nplt.show()\n```\n\n![机器学习值K均值算法图片02](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png)\n\n```python\n# fit data\nkmeans=KMeans(n_clusters=3)\nkmeans.fit(X)\nlabel_pred=kmeans.labels_\n\n#plot answer\nplt.figure()\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\nplt.scatter(x0[:, 0], x0[:, 1], c = \"red\",\n            marker='o', label='label0')\nplt.scatter(x1[:, 0], x1[:, 1], c = \"green\",\n            marker='*', label='label1')\nplt.scatter(x2[:, 0], x2[:, 1], c = \"blue\",\n            marker='+', label='label2')\nplt.legend(loc=2)\nplt.show()\n```\n\n![机器学习值K均值算法图片03](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png)\n\n### 7.K-Means算法优缺点\n\n#### 7.1优点\n\n+ 聚类效果较优。\n\n+ 原理简单，实现容易，收敛速度快。\n+ 需要调整的参数较少，通常只需要调整簇数K。\n\n#### 7.2缺点\n\n+ K值选取不好把握。\n+ 对噪音和异常点比较敏感。\n+ 采用迭代方法，得到的结果是局部最优。\n+ 如果各隐含类别的数据不平衡，则聚类效果不佳。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n+ [刘建平Pinard_K-Means聚类算法原理](http://www.cnblogs.com/pinard/p/6164214.html)","source":"_posts/机器学习之K均值-K-Means-算法.md","raw":"---\ntitle: 机器学习之K均值(K-Means)算法\ndate: 2018-05-12 13:44:19\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.K-Means简介\n\n**K均值(K-Means)**算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化**K-Means++**算法，距离计算优化**Elkan K-Means**算法和大样本情况下**Mini Batch K-Means**算法。\n\nK-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。\n\n假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。\n$$\nE=\\sum _{i=1}^{k} \\sum _{x \\in C_i}||x-\\mu _i||^2\n$$\n其中μi是簇Ci的均值向量，也可称作质心，表达式为\n$$\n\\mu _i=\\frac{1}{|C_i|}\\sum _{x \\in C_i}x\n$$\n如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。\n\n+ 如图(a)所示：表示初始化数据集。\n+ 如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。\n+ 如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。\n+ 如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。\n+ 如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。\n+ 如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。\n\n![机器学习值K均值算法图片01](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png)\n\n### 2.K-Means算法流程\n\n假设输入样本集$D={x_1,x_2,…,x_m}$，聚类簇数为K，最大迭代次数为N。输出的簇划分为$C={C_1,C_2,…,C_m}$。\n\n+ 从数据集D中随机选择K个样本作为初始的质心向量$ \\mu=\\{ \\mu_1,\\mu_2,\\mu_3,...,\\mu_k \\}$。\n\n+ 迭代$n=1,2,…,N$。\n\n  + 划分初始化簇$C_t=\\varnothing ;\\ t=1,2,…,k$。\n  + 对于$i=1,2,…,m$，计算样本$x_i$和各个质心向量$\\mu_j(\\ j=1,2,…,k)$的距离$d_{ij}$。将$x_i$标记为最小的$d_{ij}$所对应的类别$\\lambda_i$，此时更新$C_{\\lambda i}=C_{\\lambda i} \\cup{x_i}$。\n\n  $$\n  d_{ij}=||x_i-\\mu_j||^2\n  $$\n\n  + 对于$j=1,2,…,k$，对$C_j$中所有样本点重新计算新的质心。\n\n  $$\n  \\mu_j=\\frac{1}{|C_j|}\\sum_{x\\in C_j}x\n  $$\n\n  + 如果K个质心向量都不再发生变化，则结束迭代。\n\n+ 输出K个划分簇$C$，$C=\\{C_1,C_2,C_3,…,C_k \\}$。\n\n对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。\n\n+ **对于K值的选择:**我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。\n+ **对于K个初始化质心:**由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。\n\n### 3.初始化优化K-Means++\n\n如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。\n\n+ 从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。\n+ 对于数据集中的每个点xi，计算与他最近的聚类中心距离。\n\n$$\nD(x)=\\arg \\min_{r=1}^{k_{selected}}||x_i-\\mu_r||^2\n$$\n\n+ 选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。\n+ 重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。\n\n### 4.距离计算优化Elkan K-Means算法\n\n传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。\n\n+ 对于一个样本点$x$和两个质心$\\mu_{j1},\\mu_{j2}$，如果我们预先计算出这两个质心之间的距离$D(j_1,j_2)$，如果发现$2D(x,j_1)≤D(j_1,j_2)$，那么我们便能得到$D(x,j_1)≤D(x,j_2)$。此时我们不再计算$D(x,j_2)$，也就节省了一步距离计算。\n+ 对于一个样本点$x$和两个质心$μ_{j1},μ_{j2}$  ,我们能够得到$D(x,j_2)≥max{0,D(x,j_1)−D(j_1,j_2)}$。\n\nElkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。\n\n### 5.大样本优化Mini Batch K-Means算法\n\n传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。\n\nMini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。\n\nMini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。\n\n### 6.Sklearn实现K-Means算法\n\n我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)。\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n#load iris\niris=load_iris()\nX=iris.data[:,:2]\nprint(X.shape)\n#150,2\n\n#plot data\nplt.figure()\nplt.scatter(X[:,0],X[:,1],c='blue',\n            marker='o',label='point')\nplt.legend(loc=2)\nplt.show()\n```\n\n![机器学习值K均值算法图片02](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png)\n\n```python\n# fit data\nkmeans=KMeans(n_clusters=3)\nkmeans.fit(X)\nlabel_pred=kmeans.labels_\n\n#plot answer\nplt.figure()\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\nplt.scatter(x0[:, 0], x0[:, 1], c = \"red\",\n            marker='o', label='label0')\nplt.scatter(x1[:, 0], x1[:, 1], c = \"green\",\n            marker='*', label='label1')\nplt.scatter(x2[:, 0], x2[:, 1], c = \"blue\",\n            marker='+', label='label2')\nplt.legend(loc=2)\nplt.show()\n```\n\n![机器学习值K均值算法图片03](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png)\n\n### 7.K-Means算法优缺点\n\n#### 7.1优点\n\n+ 聚类效果较优。\n\n+ 原理简单，实现容易，收敛速度快。\n+ 需要调整的参数较少，通常只需要调整簇数K。\n\n#### 7.2缺点\n\n+ K值选取不好把握。\n+ 对噪音和异常点比较敏感。\n+ 采用迭代方法，得到的结果是局部最优。\n+ 如果各隐含类别的数据不平衡，则聚类效果不佳。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n+ [刘建平Pinard_K-Means聚类算法原理](http://www.cnblogs.com/pinard/p/6164214.html)","slug":"机器学习之K均值-K-Means-算法","published":1,"updated":"2018-06-26T04:03:04.710Z","layout":"post","photos":[],"link":"","_id":"cjktv5d3z0010jiz5r86vcdsl","content":"<h3 id=\"1-K-Means简介\"><a href=\"#1-K-Means简介\" class=\"headerlink\" title=\"1.K-Means简介\"></a>1.K-Means简介</h3><p><strong>K均值(K-Means)</strong>算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化<strong>K-Means++</strong>算法，距离计算优化<strong>Elkan K-Means</strong>算法和大样本情况下<strong>Mini Batch K-Means</strong>算法。</p>\n<p>K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。</p>\n<p>假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。</p>\n<script type=\"math/tex; mode=display\">\nE=\\sum _{i=1}^{k} \\sum _{x \\in C_i}||x-\\mu _i||^2</script><p>其中μi是簇Ci的均值向量，也可称作质心，表达式为</p>\n<script type=\"math/tex; mode=display\">\n\\mu _i=\\frac{1}{|C_i|}\\sum _{x \\in C_i}x</script><p>如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。</p>\n<ul>\n<li>如图(a)所示：表示初始化数据集。</li>\n<li>如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。</li>\n<li>如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。</li>\n<li>如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。</li>\n<li>如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。</li>\n<li>如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。</li>\n</ul>\n<p><img src=\"/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png\" alt=\"机器学习值K均值算法图片01\"></p>\n<h3 id=\"2-K-Means算法流程\"><a href=\"#2-K-Means算法流程\" class=\"headerlink\" title=\"2.K-Means算法流程\"></a>2.K-Means算法流程</h3><p>假设输入样本集$D={x_1,x_2,…,x_m}$，聚类簇数为K，最大迭代次数为N。输出的簇划分为$C={C_1,C_2,…,C_m}$。</p>\n<ul>\n<li><p>从数据集D中随机选择K个样本作为初始的质心向量$ \\mu=\\{ \\mu_1,\\mu_2,\\mu_3,…,\\mu_k \\}$。</p>\n</li>\n<li><p>迭代$n=1,2,…,N$。</p>\n<ul>\n<li>划分初始化簇$C_t=\\varnothing ;\\ t=1,2,…,k$。</li>\n<li>对于$i=1,2,…,m$，计算样本$x_i$和各个质心向量$\\mu_j(\\ j=1,2,…,k)$的距离$d_{ij}$。将$x_i$标记为最小的$d_{ij}$所对应的类别$\\lambda_i$，此时更新$C_{\\lambda i}=C_{\\lambda i} \\cup{x_i}$。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nd_{ij}=||x_i-\\mu_j||^2</script><ul>\n<li>对于$j=1,2,…,k$，对$C_j$中所有样本点重新计算新的质心。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\mu_j=\\frac{1}{|C_j|}\\sum_{x\\in C_j}x</script><ul>\n<li>如果K个质心向量都不再发生变化，则结束迭代。</li>\n</ul>\n</li>\n<li><p>输出K个划分簇$C$，$C=\\{C_1,C_2,C_3,…,C_k \\}$。</p>\n</li>\n</ul>\n<p>对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。</p>\n<ul>\n<li><strong>对于K值的选择:</strong>我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。</li>\n<li><strong>对于K个初始化质心:</strong>由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。</li>\n</ul>\n<h3 id=\"3-初始化优化K-Means\"><a href=\"#3-初始化优化K-Means\" class=\"headerlink\" title=\"3.初始化优化K-Means++\"></a>3.初始化优化K-Means++</h3><p>如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。</p>\n<ul>\n<li>从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。</li>\n<li>对于数据集中的每个点xi，计算与他最近的聚类中心距离。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nD(x)=\\arg \\min_{r=1}^{k_{selected}}||x_i-\\mu_r||^2</script><ul>\n<li>选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。</li>\n<li>重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。</li>\n</ul>\n<h3 id=\"4-距离计算优化Elkan-K-Means算法\"><a href=\"#4-距离计算优化Elkan-K-Means算法\" class=\"headerlink\" title=\"4.距离计算优化Elkan K-Means算法\"></a>4.距离计算优化Elkan K-Means算法</h3><p>传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。</p>\n<ul>\n<li>对于一个样本点$x$和两个质心$\\mu_{j1},\\mu_{j2}$，如果我们预先计算出这两个质心之间的距离$D(j_1,j_2)$，如果发现$2D(x,j_1)≤D(j_1,j_2)$，那么我们便能得到$D(x,j_1)≤D(x,j_2)$。此时我们不再计算$D(x,j_2)$，也就节省了一步距离计算。</li>\n<li>对于一个样本点$x$和两个质心$μ_{j1},μ_{j2}$  ,我们能够得到$D(x,j_2)≥max{0,D(x,j_1)−D(j_1,j_2)}$。</li>\n</ul>\n<p>Elkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。</p>\n<h3 id=\"5-大样本优化Mini-Batch-K-Means算法\"><a href=\"#5-大样本优化Mini-Batch-K-Means算法\" class=\"headerlink\" title=\"5.大样本优化Mini Batch K-Means算法\"></a>5.大样本优化Mini Batch K-Means算法</h3><p>传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。</p>\n<p>Mini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。</p>\n<p>Mini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。</p>\n<h3 id=\"6-Sklearn实现K-Means算法\"><a href=\"#6-Sklearn实现K-Means算法\" class=\"headerlink\" title=\"6.Sklearn实现K-Means算法\"></a>6.Sklearn实现K-Means算法</h3><p>我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load iris</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data[:,:<span class=\"number\">2</span>]</span><br><span class=\"line\">print(X.shape)</span><br><span class=\"line\"><span class=\"comment\">#150,2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot data</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],c=<span class=\"string\">'blue'</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'o'</span>,label=<span class=\"string\">'point'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png\" alt=\"机器学习值K均值算法图片02\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># fit data</span></span><br><span class=\"line\">kmeans=KMeans(n_clusters=<span class=\"number\">3</span>)</span><br><span class=\"line\">kmeans.fit(X)</span><br><span class=\"line\">label_pred=kmeans.labels_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot answer</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">x0 = X[label_pred == <span class=\"number\">0</span>]</span><br><span class=\"line\">x1 = X[label_pred == <span class=\"number\">1</span>]</span><br><span class=\"line\">x2 = X[label_pred == <span class=\"number\">2</span>]</span><br><span class=\"line\">plt.scatter(x0[:, <span class=\"number\">0</span>], x0[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"red\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'o'</span>, label=<span class=\"string\">'label0'</span>)</span><br><span class=\"line\">plt.scatter(x1[:, <span class=\"number\">0</span>], x1[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"green\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'*'</span>, label=<span class=\"string\">'label1'</span>)</span><br><span class=\"line\">plt.scatter(x2[:, <span class=\"number\">0</span>], x2[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"blue\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'+'</span>, label=<span class=\"string\">'label2'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png\" alt=\"机器学习值K均值算法图片03\"></p>\n<h3 id=\"7-K-Means算法优缺点\"><a href=\"#7-K-Means算法优缺点\" class=\"headerlink\" title=\"7.K-Means算法优缺点\"></a>7.K-Means算法优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li><p>聚类效果较优。</p>\n</li>\n<li><p>原理简单，实现容易，收敛速度快。</p>\n</li>\n<li>需要调整的参数较少，通常只需要调整簇数K。</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li>K值选取不好把握。</li>\n<li>对噪音和异常点比较敏感。</li>\n<li>采用迭代方法，得到的结果是局部最优。</li>\n<li>如果各隐含类别的数据不平衡，则聚类效果不佳。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<ul>\n<li><a href=\"http://www.cnblogs.com/pinard/p/6164214.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_K-Means聚类算法原理</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-K-Means简介\"><a href=\"#1-K-Means简介\" class=\"headerlink\" title=\"1.K-Means简介\"></a>1.K-Means简介</h3><p><strong>K均值(K-Means)</strong>算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化<strong>K-Means++</strong>算法，距离计算优化<strong>Elkan K-Means</strong>算法和大样本情况下<strong>Mini Batch K-Means</strong>算法。</p>\n<p>K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。</p>\n<p>假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。</p>\n<script type=\"math/tex; mode=display\">\nE=\\sum _{i=1}^{k} \\sum _{x \\in C_i}||x-\\mu _i||^2</script><p>其中μi是簇Ci的均值向量，也可称作质心，表达式为</p>\n<script type=\"math/tex; mode=display\">\n\\mu _i=\\frac{1}{|C_i|}\\sum _{x \\in C_i}x</script><p>如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。</p>\n<ul>\n<li>如图(a)所示：表示初始化数据集。</li>\n<li>如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。</li>\n<li>如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。</li>\n<li>如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。</li>\n<li>如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。</li>\n<li>如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。</li>\n</ul>\n<p><img src=\"/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png\" alt=\"机器学习值K均值算法图片01\"></p>\n<h3 id=\"2-K-Means算法流程\"><a href=\"#2-K-Means算法流程\" class=\"headerlink\" title=\"2.K-Means算法流程\"></a>2.K-Means算法流程</h3><p>假设输入样本集$D={x_1,x_2,…,x_m}$，聚类簇数为K，最大迭代次数为N。输出的簇划分为$C={C_1,C_2,…,C_m}$。</p>\n<ul>\n<li><p>从数据集D中随机选择K个样本作为初始的质心向量$ \\mu=\\{ \\mu_1,\\mu_2,\\mu_3,…,\\mu_k \\}$。</p>\n</li>\n<li><p>迭代$n=1,2,…,N$。</p>\n<ul>\n<li>划分初始化簇$C_t=\\varnothing ;\\ t=1,2,…,k$。</li>\n<li>对于$i=1,2,…,m$，计算样本$x_i$和各个质心向量$\\mu_j(\\ j=1,2,…,k)$的距离$d_{ij}$。将$x_i$标记为最小的$d_{ij}$所对应的类别$\\lambda_i$，此时更新$C_{\\lambda i}=C_{\\lambda i} \\cup{x_i}$。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nd_{ij}=||x_i-\\mu_j||^2</script><ul>\n<li>对于$j=1,2,…,k$，对$C_j$中所有样本点重新计算新的质心。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\mu_j=\\frac{1}{|C_j|}\\sum_{x\\in C_j}x</script><ul>\n<li>如果K个质心向量都不再发生变化，则结束迭代。</li>\n</ul>\n</li>\n<li><p>输出K个划分簇$C$，$C=\\{C_1,C_2,C_3,…,C_k \\}$。</p>\n</li>\n</ul>\n<p>对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。</p>\n<ul>\n<li><strong>对于K值的选择:</strong>我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。</li>\n<li><strong>对于K个初始化质心:</strong>由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。</li>\n</ul>\n<h3 id=\"3-初始化优化K-Means\"><a href=\"#3-初始化优化K-Means\" class=\"headerlink\" title=\"3.初始化优化K-Means++\"></a>3.初始化优化K-Means++</h3><p>如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。</p>\n<ul>\n<li>从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。</li>\n<li>对于数据集中的每个点xi，计算与他最近的聚类中心距离。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nD(x)=\\arg \\min_{r=1}^{k_{selected}}||x_i-\\mu_r||^2</script><ul>\n<li>选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。</li>\n<li>重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。</li>\n</ul>\n<h3 id=\"4-距离计算优化Elkan-K-Means算法\"><a href=\"#4-距离计算优化Elkan-K-Means算法\" class=\"headerlink\" title=\"4.距离计算优化Elkan K-Means算法\"></a>4.距离计算优化Elkan K-Means算法</h3><p>传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。</p>\n<ul>\n<li>对于一个样本点$x$和两个质心$\\mu_{j1},\\mu_{j2}$，如果我们预先计算出这两个质心之间的距离$D(j_1,j_2)$，如果发现$2D(x,j_1)≤D(j_1,j_2)$，那么我们便能得到$D(x,j_1)≤D(x,j_2)$。此时我们不再计算$D(x,j_2)$，也就节省了一步距离计算。</li>\n<li>对于一个样本点$x$和两个质心$μ_{j1},μ_{j2}$  ,我们能够得到$D(x,j_2)≥max{0,D(x,j_1)−D(j_1,j_2)}$。</li>\n</ul>\n<p>Elkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。</p>\n<h3 id=\"5-大样本优化Mini-Batch-K-Means算法\"><a href=\"#5-大样本优化Mini-Batch-K-Means算法\" class=\"headerlink\" title=\"5.大样本优化Mini Batch K-Means算法\"></a>5.大样本优化Mini Batch K-Means算法</h3><p>传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。</p>\n<p>Mini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。</p>\n<p>Mini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。</p>\n<h3 id=\"6-Sklearn实现K-Means算法\"><a href=\"#6-Sklearn实现K-Means算法\" class=\"headerlink\" title=\"6.Sklearn实现K-Means算法\"></a>6.Sklearn实现K-Means算法</h3><p>我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load iris</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data[:,:<span class=\"number\">2</span>]</span><br><span class=\"line\">print(X.shape)</span><br><span class=\"line\"><span class=\"comment\">#150,2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot data</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],c=<span class=\"string\">'blue'</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'o'</span>,label=<span class=\"string\">'point'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png\" alt=\"机器学习值K均值算法图片02\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># fit data</span></span><br><span class=\"line\">kmeans=KMeans(n_clusters=<span class=\"number\">3</span>)</span><br><span class=\"line\">kmeans.fit(X)</span><br><span class=\"line\">label_pred=kmeans.labels_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot answer</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">x0 = X[label_pred == <span class=\"number\">0</span>]</span><br><span class=\"line\">x1 = X[label_pred == <span class=\"number\">1</span>]</span><br><span class=\"line\">x2 = X[label_pred == <span class=\"number\">2</span>]</span><br><span class=\"line\">plt.scatter(x0[:, <span class=\"number\">0</span>], x0[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"red\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'o'</span>, label=<span class=\"string\">'label0'</span>)</span><br><span class=\"line\">plt.scatter(x1[:, <span class=\"number\">0</span>], x1[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"green\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'*'</span>, label=<span class=\"string\">'label1'</span>)</span><br><span class=\"line\">plt.scatter(x2[:, <span class=\"number\">0</span>], x2[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"blue\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'+'</span>, label=<span class=\"string\">'label2'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png\" alt=\"机器学习值K均值算法图片03\"></p>\n<h3 id=\"7-K-Means算法优缺点\"><a href=\"#7-K-Means算法优缺点\" class=\"headerlink\" title=\"7.K-Means算法优缺点\"></a>7.K-Means算法优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li><p>聚类效果较优。</p>\n</li>\n<li><p>原理简单，实现容易，收敛速度快。</p>\n</li>\n<li>需要调整的参数较少，通常只需要调整簇数K。</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li>K值选取不好把握。</li>\n<li>对噪音和异常点比较敏感。</li>\n<li>采用迭代方法，得到的结果是局部最优。</li>\n<li>如果各隐含类别的数据不平衡，则聚类效果不佳。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<ul>\n<li><a href=\"http://www.cnblogs.com/pinard/p/6164214.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_K-Means聚类算法原理</a></li>\n</ul>\n"},{"title":"机器学习之SVM支持向量机（一）","date":"2018-03-29T07:51:41.000Z","comment":true,"mathjax":true,"_content":"\n我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。\n\n![器学习之SVM支持向量机0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png)\n\n### 1.SVM损失函数\n\n针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。\n\n首先我们先复习下Logistic Regression Function\n$$\nh_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}\n$$\n如果$y=1$，我们希望$h_{\\theta}\\approx1$，那么$\\theta^Tx\\gg0$。如果$y=0$，我们希望$h_{\\theta}\\approx0$，那么$\\theta^Tx\\ll0$。我们以Logistic Regression为例\n\n$$\nLR Cost Example=-\\left( (ylogh_\\theta(x))+(1-y)log(1-h_\\theta(x))\\right)\n$$\n\n$$\n=-ylog\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})\n$$\n\n+ 当$y=1$时，此时$\\theta^Tx\\gg0$，上述公式为$-ylog\\frac{1}{1+e^{-\\theta^Tx}}$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。\n+ 当$y=0$时，此时$\\theta^Tx\\ll0$，上述公式为$-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。\n+ $cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。\n\n![幕快照 2018-04-02 下午9.57.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png)\n\nLogistic Regression的损失函数:\n$$\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}(-logh_{\\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n因此对于SVM，我们得到:\n$$\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\\lambda B$，我们通过设置$\\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\\frac{1}{\\lambda}$作用一致，因此我们便得到SVM的损失函数。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n\n### 2.最大间隔分类\n\nSVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n\n+ 当为正样本时$y=1$，我们希望$\\theta^Tx\\ge1$，而不是 $\\theta^Tx\\ge0$。\n+ 当为正样本时$y=0$，我们希望$\\theta^Tx\\le-1$，而不是$\\theta^Tx\\le0$。\n\n![幕快照 2018-04-02 下午10.00.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png)\n\n我们设**C为非常大的值**，例如1000000。\n\n+ 当$y^{(i)}=1$时，$\\theta^Tx^{(i)}\\ge1$，此时SVM损失函数中第一项为0。\n+ 当$y^{(i)}=0$时，$\\theta^Tx^{(i)}\\le-1$，此时SVM损失函数中第一项为0。\n\n那么我们便得到:\n$$\nminC*0+\\frac{1}{2}\\sum_{i=1}^{m}\\theta_j^2\n$$\n\n+ 约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。\n+ 约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。\n\nSVM是一个**最大间隔分类器**，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在**模块3**中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。\n\n![幕快照 2018-04-02 下午3.57.3](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png)\n\n我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。\n\n![幕快照 2018-04-02 下午4.02.5](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png)\n\n### 3.SVM最大间隔分类\n\n首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为\n$$\nu^Tv=||u||\\cdot||v||\\cdot cos\\theta=||u||\\cdot p=u_1v_1+u_2v_2\n$$\n![幕快照 2018-04-02 下午4.11.5](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png)\n\n现在我们来看SVM损失函数:\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n由于C设置的非常大，那么SVM损失函数为:\n$$\nmin_{\\theta} \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2\n$$\n\n- 约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。\n- 约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。\n\n下面我们举例说明SVM，为了简化，假设n=2,$\\theta_0=0$，我们得到\n$$\nmin\\frac{1}{2}\\sum_{j=1}^{2}\\theta_j^2=\\frac{1}{2}{(\\theta_1^2+\\theta_2^2)}=\\frac{1}{2}\\left (\\sqrt{\\theta_1^2+\\theta_2^2}  \\right )^2=\\frac{1}{2}||\\theta||^2\n$$\n我们来更深层次的理解$\\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为\n$$\n\\theta^Tx^{(i)}=p^{(i)}\\cdot||\\theta||=\\theta_1x_1^{(i)}+\\theta_2x_1^{(2)}\n$$\n$\\theta^Tx^{(i)}$我们可以利用$p^{(i)}\\cdot||\\theta||$表示，同时SVM随时函数目标是极小化$||\\theta^2||$。\n\n![幕快照 2018-04-02 下午4.52.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png)\n\n下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\\theta||$，并且要满足$p^{(i)}\\cdot ||\\theta||\\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\\theta||$更大，不满足最小化$||\\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\\theta||$便较小，满足最小化$||\\theta||$的需求。SVM便是通过最大化分类间隔来让$||\\theta||$更小，这便是SVM中为什么要最大化分类间隔。\n\n![幕快照 2018-04-02 下午5.14.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png)\n\n### 4.核函数\n\n上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,...$\n\n+ $f_1,f_2,f_3…$为提取出来的特征。\n+ 定义预测方程$h_{\\theta}(x)$为多项式sigmoid函数。$h_{\\theta}(x)=g(\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n)$，其中$f_n$为x的幂次项组合。\n+ 当$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n\\ge0$时$h_{\\theta}(x)=1$，否则$h_{\\theta}(x)=0$。\n\n![幕快照 2018-04-02 下午5.40.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png)\n\n那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。\n\n![幕快照 2018-04-02 下午5.53.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png)\n\nx和l越相似，f越接近于1。x和l相差越远，f越接近于0。\n\n![幕快照 2018-04-02 下午6.02.1](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png)\n\n下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。\n\n![幕快照 2018-04-02 下午6.36.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png)\n\n下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。\n\n+ 假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\\theta(\\theta_0,\\theta_1,\\theta_2,\\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\\theta$值在下面会详细介绍。\n+ 对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。\n\n![幕快照 2018-04-02 下午6.44.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png)\n\n### 5.SVM中Gaussian Kernel的使用\n\n上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。\n$$\nf_i=similarity(x,l^{(i)})=exp\\left ( -\\frac{||x-l^{(i)}||^2}{2\\sigma^2}\\right)\n$$\n我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。\n\n![幕快照 2018-04-02 下午7.16.3](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png)\n\n![幕快照 2018-04-02 下午7.17.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png)\n\n那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\\theta$，并根据该参数$\\theta$进行预测。\n\n+ $\\theta^Tf\\ge0$，预测 y=1。\n+ $\\theta^Tf\\le0$，预测y=0。\n\n如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。\n\n![幕快照 2018-04-02 下午7.26.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png)\n\n最后我们介绍下如何选取C和$\\sigma^2$。由于$C=\\frac{1}{\\lambda}$，所以\n\n+ C大，$\\lambda$小，overfit，产生low bias，high variance。\n+ C小，$\\lambda$大，underfoot，underfoot，产生high bias，low variance。\n\n对于方差$\\sigma^2$\n\n+ $\\sigma^2$大，x-f相似性图像较为扁平。\n+ $\\sigma^2小$，x-f相似性图像较为窄尖。\n\n![幕快照 2018-04-02 下午8.03.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png)\n\n通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。\n\n由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。\n\n### 5.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png)","source":"_posts/机器学习之SVM支持向量机（一）.md","raw":"---\ntitle: 机器学习之SVM支持向量机（一）\ndate: 2018-03-29 15:51:41\ntags: [机器学习,算法]\ncategories: 机器学习\ncomment: true\nmathjax: true\n---\n\n我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。\n\n![器学习之SVM支持向量机0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png)\n\n### 1.SVM损失函数\n\n针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。\n\n首先我们先复习下Logistic Regression Function\n$$\nh_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}\n$$\n如果$y=1$，我们希望$h_{\\theta}\\approx1$，那么$\\theta^Tx\\gg0$。如果$y=0$，我们希望$h_{\\theta}\\approx0$，那么$\\theta^Tx\\ll0$。我们以Logistic Regression为例\n\n$$\nLR Cost Example=-\\left( (ylogh_\\theta(x))+(1-y)log(1-h_\\theta(x))\\right)\n$$\n\n$$\n=-ylog\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})\n$$\n\n+ 当$y=1$时，此时$\\theta^Tx\\gg0$，上述公式为$-ylog\\frac{1}{1+e^{-\\theta^Tx}}$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。\n+ 当$y=0$时，此时$\\theta^Tx\\ll0$，上述公式为$-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。\n+ $cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。\n\n![幕快照 2018-04-02 下午9.57.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png)\n\nLogistic Regression的损失函数:\n$$\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}(-logh_{\\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n因此对于SVM，我们得到:\n$$\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\\lambda B$，我们通过设置$\\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\\frac{1}{\\lambda}$作用一致，因此我们便得到SVM的损失函数。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n\n### 2.最大间隔分类\n\nSVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n\n+ 当为正样本时$y=1$，我们希望$\\theta^Tx\\ge1$，而不是 $\\theta^Tx\\ge0$。\n+ 当为正样本时$y=0$，我们希望$\\theta^Tx\\le-1$，而不是$\\theta^Tx\\le0$。\n\n![幕快照 2018-04-02 下午10.00.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png)\n\n我们设**C为非常大的值**，例如1000000。\n\n+ 当$y^{(i)}=1$时，$\\theta^Tx^{(i)}\\ge1$，此时SVM损失函数中第一项为0。\n+ 当$y^{(i)}=0$时，$\\theta^Tx^{(i)}\\le-1$，此时SVM损失函数中第一项为0。\n\n那么我们便得到:\n$$\nminC*0+\\frac{1}{2}\\sum_{i=1}^{m}\\theta_j^2\n$$\n\n+ 约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。\n+ 约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。\n\nSVM是一个**最大间隔分类器**，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在**模块3**中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。\n\n![幕快照 2018-04-02 下午3.57.3](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png)\n\n我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。\n\n![幕快照 2018-04-02 下午4.02.5](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png)\n\n### 3.SVM最大间隔分类\n\n首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为\n$$\nu^Tv=||u||\\cdot||v||\\cdot cos\\theta=||u||\\cdot p=u_1v_1+u_2v_2\n$$\n![幕快照 2018-04-02 下午4.11.5](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png)\n\n现在我们来看SVM损失函数:\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n由于C设置的非常大，那么SVM损失函数为:\n$$\nmin_{\\theta} \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2\n$$\n\n- 约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。\n- 约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。\n\n下面我们举例说明SVM，为了简化，假设n=2,$\\theta_0=0$，我们得到\n$$\nmin\\frac{1}{2}\\sum_{j=1}^{2}\\theta_j^2=\\frac{1}{2}{(\\theta_1^2+\\theta_2^2)}=\\frac{1}{2}\\left (\\sqrt{\\theta_1^2+\\theta_2^2}  \\right )^2=\\frac{1}{2}||\\theta||^2\n$$\n我们来更深层次的理解$\\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为\n$$\n\\theta^Tx^{(i)}=p^{(i)}\\cdot||\\theta||=\\theta_1x_1^{(i)}+\\theta_2x_1^{(2)}\n$$\n$\\theta^Tx^{(i)}$我们可以利用$p^{(i)}\\cdot||\\theta||$表示，同时SVM随时函数目标是极小化$||\\theta^2||$。\n\n![幕快照 2018-04-02 下午4.52.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png)\n\n下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\\theta||$，并且要满足$p^{(i)}\\cdot ||\\theta||\\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\\theta||$更大，不满足最小化$||\\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\\theta||$便较小，满足最小化$||\\theta||$的需求。SVM便是通过最大化分类间隔来让$||\\theta||$更小，这便是SVM中为什么要最大化分类间隔。\n\n![幕快照 2018-04-02 下午5.14.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png)\n\n### 4.核函数\n\n上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,...$\n\n+ $f_1,f_2,f_3…$为提取出来的特征。\n+ 定义预测方程$h_{\\theta}(x)$为多项式sigmoid函数。$h_{\\theta}(x)=g(\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n)$，其中$f_n$为x的幂次项组合。\n+ 当$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n\\ge0$时$h_{\\theta}(x)=1$，否则$h_{\\theta}(x)=0$。\n\n![幕快照 2018-04-02 下午5.40.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png)\n\n那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。\n\n![幕快照 2018-04-02 下午5.53.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png)\n\nx和l越相似，f越接近于1。x和l相差越远，f越接近于0。\n\n![幕快照 2018-04-02 下午6.02.1](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png)\n\n下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。\n\n![幕快照 2018-04-02 下午6.36.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png)\n\n下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。\n\n+ 假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\\theta(\\theta_0,\\theta_1,\\theta_2,\\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\\theta$值在下面会详细介绍。\n+ 对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。\n\n![幕快照 2018-04-02 下午6.44.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png)\n\n### 5.SVM中Gaussian Kernel的使用\n\n上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。\n$$\nf_i=similarity(x,l^{(i)})=exp\\left ( -\\frac{||x-l^{(i)}||^2}{2\\sigma^2}\\right)\n$$\n我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。\n\n![幕快照 2018-04-02 下午7.16.3](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png)\n\n![幕快照 2018-04-02 下午7.17.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png)\n\n那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\\theta$，并根据该参数$\\theta$进行预测。\n\n+ $\\theta^Tf\\ge0$，预测 y=1。\n+ $\\theta^Tf\\le0$，预测y=0。\n\n如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。\n\n![幕快照 2018-04-02 下午7.26.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png)\n\n最后我们介绍下如何选取C和$\\sigma^2$。由于$C=\\frac{1}{\\lambda}$，所以\n\n+ C大，$\\lambda$小，overfit，产生low bias，high variance。\n+ C小，$\\lambda$大，underfoot，underfoot，产生high bias，low variance。\n\n对于方差$\\sigma^2$\n\n+ $\\sigma^2$大，x-f相似性图像较为扁平。\n+ $\\sigma^2小$，x-f相似性图像较为窄尖。\n\n![幕快照 2018-04-02 下午8.03.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png)\n\n通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。\n\n由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。\n\n### 5.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png)","slug":"机器学习之SVM支持向量机（一）","published":1,"updated":"2018-06-07T17:52:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjktv5d430014jiz52d2qvolh","content":"<p>我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png\" alt=\"器学习之SVM支持向量机0\"></p>\n<h3 id=\"1-SVM损失函数\"><a href=\"#1-SVM损失函数\" class=\"headerlink\" title=\"1.SVM损失函数\"></a>1.SVM损失函数</h3><p>针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。</p>\n<p>首先我们先复习下Logistic Regression Function</p>\n<script type=\"math/tex; mode=display\">\nh_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}</script><p>如果$y=1$，我们希望$h_{\\theta}\\approx1$，那么$\\theta^Tx\\gg0$。如果$y=0$，我们希望$h_{\\theta}\\approx0$，那么$\\theta^Tx\\ll0$。我们以Logistic Regression为例</p>\n<script type=\"math/tex; mode=display\">\nLR Cost Example=-\\left( (ylogh_\\theta(x))+(1-y)log(1-h_\\theta(x))\\right)</script><script type=\"math/tex; mode=display\">\n=-ylog\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})</script><ul>\n<li>当$y=1$时，此时$\\theta^Tx\\gg0$，上述公式为$-ylog\\frac{1}{1+e^{-\\theta^Tx}}$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。</li>\n<li>当$y=0$时，此时$\\theta^Tx\\ll0$，上述公式为$-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。</li>\n<li>$cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png\" alt=\"幕快照 2018-04-02 下午9.57.2\"></p>\n<p>Logistic Regression的损失函数:</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}(-logh_{\\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><p>因此对于SVM，我们得到:</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><p>因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\\lambda B$，我们通过设置$\\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\\frac{1}{\\lambda}$作用一致，因此我们便得到SVM的损失函数。</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><h3 id=\"2-最大间隔分类\"><a href=\"#2-最大间隔分类\" class=\"headerlink\" title=\"2.最大间隔分类\"></a>2.最大间隔分类</h3><p>SVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><ul>\n<li>当为正样本时$y=1$，我们希望$\\theta^Tx\\ge1$，而不是 $\\theta^Tx\\ge0$。</li>\n<li>当为正样本时$y=0$，我们希望$\\theta^Tx\\le-1$，而不是$\\theta^Tx\\le0$。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png\" alt=\"幕快照 2018-04-02 下午10.00.0\"></p>\n<p>我们设<strong>C为非常大的值</strong>，例如1000000。</p>\n<ul>\n<li>当$y^{(i)}=1$时，$\\theta^Tx^{(i)}\\ge1$，此时SVM损失函数中第一项为0。</li>\n<li>当$y^{(i)}=0$时，$\\theta^Tx^{(i)}\\le-1$，此时SVM损失函数中第一项为0。</li>\n</ul>\n<p>那么我们便得到:</p>\n<script type=\"math/tex; mode=display\">\nminC*0+\\frac{1}{2}\\sum_{i=1}^{m}\\theta_j^2</script><ul>\n<li>约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。</li>\n<li>约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。</li>\n</ul>\n<p>SVM是一个<strong>最大间隔分类器</strong>，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在<strong>模块3</strong>中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png\" alt=\"幕快照 2018-04-02 下午3.57.3\"></p>\n<p>我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png\" alt=\"幕快照 2018-04-02 下午4.02.5\"></p>\n<h3 id=\"3-SVM最大间隔分类\"><a href=\"#3-SVM最大间隔分类\" class=\"headerlink\" title=\"3.SVM最大间隔分类\"></a>3.SVM最大间隔分类</h3><p>首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为</p>\n<script type=\"math/tex; mode=display\">\nu^Tv=||u||\\cdot||v||\\cdot cos\\theta=||u||\\cdot p=u_1v_1+u_2v_2</script><p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png\" alt=\"幕快照 2018-04-02 下午4.11.5\"></p>\n<p>现在我们来看SVM损失函数:</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><p>由于C设置的非常大，那么SVM损失函数为:</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta} \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2</script><ul>\n<li>约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。</li>\n<li>约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。</li>\n</ul>\n<p>下面我们举例说明SVM，为了简化，假设n=2,$\\theta_0=0$，我们得到</p>\n<script type=\"math/tex; mode=display\">\nmin\\frac{1}{2}\\sum_{j=1}^{2}\\theta_j^2=\\frac{1}{2}{(\\theta_1^2+\\theta_2^2)}=\\frac{1}{2}\\left (\\sqrt{\\theta_1^2+\\theta_2^2}  \\right )^2=\\frac{1}{2}||\\theta||^2</script><p>我们来更深层次的理解$\\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为</p>\n<script type=\"math/tex; mode=display\">\n\\theta^Tx^{(i)}=p^{(i)}\\cdot||\\theta||=\\theta_1x_1^{(i)}+\\theta_2x_1^{(2)}</script><p>$\\theta^Tx^{(i)}$我们可以利用$p^{(i)}\\cdot||\\theta||$表示，同时SVM随时函数目标是极小化$||\\theta^2||$。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png\" alt=\"幕快照 2018-04-02 下午4.52.4\"></p>\n<p>下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\\theta||$，并且要满足$p^{(i)}\\cdot ||\\theta||\\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\\theta||$更大，不满足最小化$||\\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\\theta||$便较小，满足最小化$||\\theta||$的需求。SVM便是通过最大化分类间隔来让$||\\theta||$更小，这便是SVM中为什么要最大化分类间隔。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png\" alt=\"幕快照 2018-04-02 下午5.14.2\"></p>\n<h3 id=\"4-核函数\"><a href=\"#4-核函数\" class=\"headerlink\" title=\"4.核函数\"></a>4.核函数</h3><p>上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,…$</p>\n<ul>\n<li>$f_1,f_2,f_3…$为提取出来的特征。</li>\n<li>定义预测方程$h_{\\theta}(x)$为多项式sigmoid函数。$h_{\\theta}(x)=g(\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n)$，其中$f_n$为x的幂次项组合。</li>\n<li>当$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n\\ge0$时$h_{\\theta}(x)=1$，否则$h_{\\theta}(x)=0$。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png\" alt=\"幕快照 2018-04-02 下午5.40.4\"></p>\n<p>那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png\" alt=\"幕快照 2018-04-02 下午5.53.4\"></p>\n<p>x和l越相似，f越接近于1。x和l相差越远，f越接近于0。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png\" alt=\"幕快照 2018-04-02 下午6.02.1\"></p>\n<p>下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png\" alt=\"幕快照 2018-04-02 下午6.36.0\"></p>\n<p>下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。</p>\n<ul>\n<li>假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\\theta(\\theta_0,\\theta_1,\\theta_2,\\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\\theta$值在下面会详细介绍。</li>\n<li>对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png\" alt=\"幕快照 2018-04-02 下午6.44.2\"></p>\n<h3 id=\"5-SVM中Gaussian-Kernel的使用\"><a href=\"#5-SVM中Gaussian-Kernel的使用\" class=\"headerlink\" title=\"5.SVM中Gaussian Kernel的使用\"></a>5.SVM中Gaussian Kernel的使用</h3><p>上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。</p>\n<script type=\"math/tex; mode=display\">\nf_i=similarity(x,l^{(i)})=exp\\left ( -\\frac{||x-l^{(i)}||^2}{2\\sigma^2}\\right)</script><p>我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png\" alt=\"幕快照 2018-04-02 下午7.16.3\"></p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png\" alt=\"幕快照 2018-04-02 下午7.17.2\"></p>\n<p>那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\\theta$，并根据该参数$\\theta$进行预测。</p>\n<ul>\n<li>$\\theta^Tf\\ge0$，预测 y=1。</li>\n<li>$\\theta^Tf\\le0$，预测y=0。</li>\n</ul>\n<p>如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png\" alt=\"幕快照 2018-04-02 下午7.26.4\"></p>\n<p>最后我们介绍下如何选取C和$\\sigma^2$。由于$C=\\frac{1}{\\lambda}$，所以</p>\n<ul>\n<li>C大，$\\lambda$小，overfit，产生low bias，high variance。</li>\n<li>C小，$\\lambda$大，underfoot，underfoot，产生high bias，low variance。</li>\n</ul>\n<p>对于方差$\\sigma^2$</p>\n<ul>\n<li>$\\sigma^2$大，x-f相似性图像较为扁平。</li>\n<li>$\\sigma^2小$，x-f相似性图像较为窄尖。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png\" alt=\"幕快照 2018-04-02 下午8.03.0\"></p>\n<p>通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。</p>\n<p>由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。</p>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png\" alt=\"器学习之SVM支持向量机0\"></p>\n<h3 id=\"1-SVM损失函数\"><a href=\"#1-SVM损失函数\" class=\"headerlink\" title=\"1.SVM损失函数\"></a>1.SVM损失函数</h3><p>针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。</p>\n<p>首先我们先复习下Logistic Regression Function</p>\n<script type=\"math/tex; mode=display\">\nh_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}</script><p>如果$y=1$，我们希望$h_{\\theta}\\approx1$，那么$\\theta^Tx\\gg0$。如果$y=0$，我们希望$h_{\\theta}\\approx0$，那么$\\theta^Tx\\ll0$。我们以Logistic Regression为例</p>\n<script type=\"math/tex; mode=display\">\nLR Cost Example=-\\left( (ylogh_\\theta(x))+(1-y)log(1-h_\\theta(x))\\right)</script><script type=\"math/tex; mode=display\">\n=-ylog\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})</script><ul>\n<li>当$y=1$时，此时$\\theta^Tx\\gg0$，上述公式为$-ylog\\frac{1}{1+e^{-\\theta^Tx}}$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。</li>\n<li>当$y=0$时，此时$\\theta^Tx\\ll0$，上述公式为$-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。</li>\n<li>$cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png\" alt=\"幕快照 2018-04-02 下午9.57.2\"></p>\n<p>Logistic Regression的损失函数:</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}(-logh_{\\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><p>因此对于SVM，我们得到:</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><p>因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\\lambda B$，我们通过设置$\\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\\frac{1}{\\lambda}$作用一致，因此我们便得到SVM的损失函数。</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><h3 id=\"2-最大间隔分类\"><a href=\"#2-最大间隔分类\" class=\"headerlink\" title=\"2.最大间隔分类\"></a>2.最大间隔分类</h3><p>SVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><ul>\n<li>当为正样本时$y=1$，我们希望$\\theta^Tx\\ge1$，而不是 $\\theta^Tx\\ge0$。</li>\n<li>当为正样本时$y=0$，我们希望$\\theta^Tx\\le-1$，而不是$\\theta^Tx\\le0$。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png\" alt=\"幕快照 2018-04-02 下午10.00.0\"></p>\n<p>我们设<strong>C为非常大的值</strong>，例如1000000。</p>\n<ul>\n<li>当$y^{(i)}=1$时，$\\theta^Tx^{(i)}\\ge1$，此时SVM损失函数中第一项为0。</li>\n<li>当$y^{(i)}=0$时，$\\theta^Tx^{(i)}\\le-1$，此时SVM损失函数中第一项为0。</li>\n</ul>\n<p>那么我们便得到:</p>\n<script type=\"math/tex; mode=display\">\nminC*0+\\frac{1}{2}\\sum_{i=1}^{m}\\theta_j^2</script><ul>\n<li>约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。</li>\n<li>约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。</li>\n</ul>\n<p>SVM是一个<strong>最大间隔分类器</strong>，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在<strong>模块3</strong>中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png\" alt=\"幕快照 2018-04-02 下午3.57.3\"></p>\n<p>我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png\" alt=\"幕快照 2018-04-02 下午4.02.5\"></p>\n<h3 id=\"3-SVM最大间隔分类\"><a href=\"#3-SVM最大间隔分类\" class=\"headerlink\" title=\"3.SVM最大间隔分类\"></a>3.SVM最大间隔分类</h3><p>首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为</p>\n<script type=\"math/tex; mode=display\">\nu^Tv=||u||\\cdot||v||\\cdot cos\\theta=||u||\\cdot p=u_1v_1+u_2v_2</script><p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png\" alt=\"幕快照 2018-04-02 下午4.11.5\"></p>\n<p>现在我们来看SVM损失函数:</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><p>由于C设置的非常大，那么SVM损失函数为:</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta} \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2</script><ul>\n<li>约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。</li>\n<li>约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。</li>\n</ul>\n<p>下面我们举例说明SVM，为了简化，假设n=2,$\\theta_0=0$，我们得到</p>\n<script type=\"math/tex; mode=display\">\nmin\\frac{1}{2}\\sum_{j=1}^{2}\\theta_j^2=\\frac{1}{2}{(\\theta_1^2+\\theta_2^2)}=\\frac{1}{2}\\left (\\sqrt{\\theta_1^2+\\theta_2^2}  \\right )^2=\\frac{1}{2}||\\theta||^2</script><p>我们来更深层次的理解$\\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为</p>\n<script type=\"math/tex; mode=display\">\n\\theta^Tx^{(i)}=p^{(i)}\\cdot||\\theta||=\\theta_1x_1^{(i)}+\\theta_2x_1^{(2)}</script><p>$\\theta^Tx^{(i)}$我们可以利用$p^{(i)}\\cdot||\\theta||$表示，同时SVM随时函数目标是极小化$||\\theta^2||$。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png\" alt=\"幕快照 2018-04-02 下午4.52.4\"></p>\n<p>下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\\theta||$，并且要满足$p^{(i)}\\cdot ||\\theta||\\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\\theta||$更大，不满足最小化$||\\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\\theta||$便较小，满足最小化$||\\theta||$的需求。SVM便是通过最大化分类间隔来让$||\\theta||$更小，这便是SVM中为什么要最大化分类间隔。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png\" alt=\"幕快照 2018-04-02 下午5.14.2\"></p>\n<h3 id=\"4-核函数\"><a href=\"#4-核函数\" class=\"headerlink\" title=\"4.核函数\"></a>4.核函数</h3><p>上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,…$</p>\n<ul>\n<li>$f_1,f_2,f_3…$为提取出来的特征。</li>\n<li>定义预测方程$h_{\\theta}(x)$为多项式sigmoid函数。$h_{\\theta}(x)=g(\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n)$，其中$f_n$为x的幂次项组合。</li>\n<li>当$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n\\ge0$时$h_{\\theta}(x)=1$，否则$h_{\\theta}(x)=0$。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png\" alt=\"幕快照 2018-04-02 下午5.40.4\"></p>\n<p>那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png\" alt=\"幕快照 2018-04-02 下午5.53.4\"></p>\n<p>x和l越相似，f越接近于1。x和l相差越远，f越接近于0。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png\" alt=\"幕快照 2018-04-02 下午6.02.1\"></p>\n<p>下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png\" alt=\"幕快照 2018-04-02 下午6.36.0\"></p>\n<p>下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。</p>\n<ul>\n<li>假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\\theta(\\theta_0,\\theta_1,\\theta_2,\\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\\theta$值在下面会详细介绍。</li>\n<li>对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png\" alt=\"幕快照 2018-04-02 下午6.44.2\"></p>\n<h3 id=\"5-SVM中Gaussian-Kernel的使用\"><a href=\"#5-SVM中Gaussian-Kernel的使用\" class=\"headerlink\" title=\"5.SVM中Gaussian Kernel的使用\"></a>5.SVM中Gaussian Kernel的使用</h3><p>上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。</p>\n<script type=\"math/tex; mode=display\">\nf_i=similarity(x,l^{(i)})=exp\\left ( -\\frac{||x-l^{(i)}||^2}{2\\sigma^2}\\right)</script><p>我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png\" alt=\"幕快照 2018-04-02 下午7.16.3\"></p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png\" alt=\"幕快照 2018-04-02 下午7.17.2\"></p>\n<p>那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\\theta$，并根据该参数$\\theta$进行预测。</p>\n<ul>\n<li>$\\theta^Tf\\ge0$，预测 y=1。</li>\n<li>$\\theta^Tf\\le0$，预测y=0。</li>\n</ul>\n<p>如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png\" alt=\"幕快照 2018-04-02 下午7.26.4\"></p>\n<p>最后我们介绍下如何选取C和$\\sigma^2$。由于$C=\\frac{1}{\\lambda}$，所以</p>\n<ul>\n<li>C大，$\\lambda$小，overfit，产生low bias，high variance。</li>\n<li>C小，$\\lambda$大，underfoot，underfoot，产生high bias，low variance。</li>\n</ul>\n<p>对于方差$\\sigma^2$</p>\n<ul>\n<li>$\\sigma^2$大，x-f相似性图像较为扁平。</li>\n<li>$\\sigma^2小$，x-f相似性图像较为窄尖。</li>\n</ul>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png\" alt=\"幕快照 2018-04-02 下午8.03.0\"></p>\n<p>通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。</p>\n<p>由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。</p>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png\" alt=\"\"></p>\n"},{"title":"机器学习之SVM支持向量机（二）","date":"2018-04-04T13:06:26.000Z","mathjax":true,"comments":1,"_content":"\n### 1.知识回顾\n\n[机器学习之SVM支持向量机（一）](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483818&idx=1&sn=50c634d8b00877134558125c4a718fd7&chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd)中我们介绍了**SVM损失函数**、**最大间隔分类**、**为什么SVM能形成最大间隔分类器**、**核函数**、**SVM中Gaussian Kernel的使用**知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。\n\n### 2.函数间隔和几何间隔\n\n上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示：\n\n![机器学习之SVM支持向量机（二）图像01](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F01.png)\n\n在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。\n\n定义函数间隔$\\hat{\\gamma}$：\n$$\n\\hat{\\gamma}=y(w^Tx+b)=yf(x)\n$$\n而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔：\n$$\n\\hat{\\gamma}=min\\hat{\\gamma} \n$$\n但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。\n\n但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。\n\n![机器学习之SVM支持向量机（二）图像02](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F02.png)\n$$\nx=x_0+\\gamma\\frac{w}{||w||}\n$$\n其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到：\n$$\n\\gamma=\\frac{w^T+b}{||w||}=\\frac{f(x)}{||w||}\n$$\n为了得到$\\gamma$绝对值，将$\\gamma$乘上相应类别$y$，即可得到几何间隔：\n$$\n\\tilde{r}=yr=\\frac{\\hat{\\gamma}}{||w||}\n$$\n从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y*(w^Tx+b)=y*f(x)$，实际上就是$|f(x)|$，几何间隔$\\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。\n\n对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\\tilde{\\gamma}$。同时需满足如下条件\n$$\ny_i(w^Tx_i+b)=\\hat{\\gamma_i}\\ge\\hat{\\gamma},i=1,2,3,...,n\n$$\n此处令函数间隔$\\hat{\\gamma}$等于1（之所以令$\\hat{\\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成：\n$$\nmax\\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n\n$$\n\n### 3.原始问题到对偶问题的求解\n\n接着考虑我们之前的目标函数，由于求$\\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以目标函数转换为\n$$\nmin\\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n\n$$\n现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。\n\n那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\\alpha$，定义拉格朗日函数为：\n$$\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)\n$$\n然后令：\n$$\n\\theta(w)=\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)\n$$\n当某个条件不满足时，例如$y_i(w^Tx+b)<1$，那么有$\\theta(w)=\\infty$。而当所有约束条件都满足时，则有$\\theta(w)=\\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为：\n$$\n\\min_{w,b}\\theta(w)=\\min_{w,b}\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)=p^*\n$$\n这里用$p^*$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下：\n$$\n\\max_{\\alpha_i\\ge0}\\min_{w,b}L(w,b,\\alpha)=d^*\n$$\n交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^*$表示，而且有$d^*\\le p^*$，在**满足某些条件**的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。\n\n此处**满足某些条件**的情况下，两者等价，此处的**满足某些条件**便是满足KKT条件。KKT最优化数学模型表示成下列标准形式:\n$$\n\\min f(x)\n$$\n\n$$\ns.t. h_j(x)=0,j=1,2,3,...,p\n$$\n\n$$\ng_k(x)\\le0,k=1,2,3,...,q\n$$\n\n$$\nx\\in X\\subset R^n\n$$\n\n其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。\n\n> 凸优化概念:$X\\subset R^n$为一凸集，$f:X->R$为一凸函数。凸优化便是寻找一点$x^*\\in X$，是的每一$x\\in X$满足$f(x^*)\\le f(x)$。\n>\n> KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。\n\nKKT条件就是上面最优化数学模型的标准形式中的最小点$x^*$必须满足下面的条件:\n$$\nh_j(x^*)=0,j=1,2,3,...,p\n$$\n\n$$\ng_k(x^*)\\le 0,k=1,2,3,...,q\n$$\n\n$$\n\\nabla f(x^*)+\\sum_{j=1}^{p}\\lambda_j\\nabla h_j(x^*)+\\sum_{k=1}^{q}\\mu_k \\nabla g_k(x^*)=0\n$$\n\n$$\n\\lambda_j \\neq0,\\mu \\ge0,\\mu_k g_k(x^*)=0\n$$\n\n此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\\alpha)$关于$w,b$的最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。\n\n首先固定$\\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。\n$$\n\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w= \\sum_{i=1}^{n}\\alpha_iy_ix_i\n$$\n\n$$\n\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n}\\alpha_iy_i=0\n$$\n\n将上述结果代入到之前的L得到：\n$$\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)\n$$\n\n$$\n=\\frac{1}{2}w^Tw-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_iy_ix_i)^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}\\sum_{i=1}^{n}\\alpha_iy_ix_i^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\n$$\n\n然后求对$\\alpha$的极大，即是关于对偶问题的最优化问题。\n$$\n\\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j\n$$\n\n$$\ns.t.,\\alpha_i\\ge0,i=1,2,3,...,n\n$$\n\n$$\n\\sum_{i=1}^{n}\\alpha_iy_i=0\n$$\n\n我们已经知道$x_i,x_j$的值，便可利用**SMO算法**求解$\\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。\n$$\nb^*=-\\frac{\\max_{i:y(i)=-1}w^Tx_i+\\min_{i:y(i)=1}w^Tx_i}{2}\n$$\n至此我们便可得出分类超平面和分类决策函数。\n\n### 4.松弛变量处理outliers方法\n\n实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。\n\n![机器学习之SVM支持向量机（二）图像03](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F03.png)\n\n为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即\n$$\ny_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n\n$$\n其中$\\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\\varepsilon_i$的总和也要尽量小。\n$$\n\\min\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i\n$$\n\n$$\ns.t.,y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n\n$$\n\n$$\n\\varepsilon_i\\ge0,i=1,2,3,...,n\n$$\n\n此处和[机器学习之SVM支持向量机（一）](https://weizhixiaoyi.com/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/)中的损失函数不同的是加入$\\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\\varepsilon_i$。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示:\n$$\nL(w,b,\\varepsilon,\\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1+\\varepsilon_i)-\\sum_{i=1}^{n}r_i\\varepsilon_i\n$$\n分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。\n\n### 5.Sklearn实现SVM支持向量机\n\n我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在**机器学习之SVM支持向量机（一）**中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇**SVM核函数的应用**。\n\n#### 5.1线性\n\n```Python\nfrom sklearn import svm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nx=np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]#正态分布产生数字20行2列\ny=[0]*20+[1]*20#20个class0,20个class1\nclf=svm.SVC(kernel='linear')#使用线性核\nclf.fit(x,y)\nw=clf.coef_[0]#获取w\na=-w[0]/w[1]#斜率\n\n#画图\nxx=np.linspace(-5,5)\nyy=a*xx-(clf.intercept_[0])/w[1]\nb=clf.support_vectors_[0]\nyy_down=a*xx+(b[1]-a*b[0])\nb=clf.support_vectors_[-1]\nyy_up=a*xx+(b[1]-a*b[0])\nplt.figure(figsize=(8,4))\nplt.plot(xx,yy)\nplt.plot(xx,yy_down)\nplt.plot(xx,yy_up)\nplt.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=80)\nplt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.Paired)\nplt.axis('tight')\nplt.show()\n```\n\n![机器学习之SVM支持向量机（二）图像05](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png)\n\n#### 5.2非线性\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import  Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nX, y = make_moons( n_samples=100, noise=0.15, random_state=42 )\n\ndef plot_dataset(X, y, axes):\n    plt.plot( X[:,0][y==0], X[:,1][y==0], \"bs\" )\n    plt.plot( X[:,0][y==1], X[:,1][y==1], \"g^\" )\n    plt.axis( axes )\n    plt.grid( True, which=\"both\" )\n    plt.xlabel(r\"$x_l$\")\n    plt.ylabel(r\"$x_2$\")\n\n# contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）\ndef plot_predict(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid( x0s, x1s )\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict( X ).reshape( x0.shape )\n    y_decision = clf.decision_function( X ).reshape( x0.shape )\n    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=0.5 )\n    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=0.2 )\n\npolynomial_svm_clf = Pipeline([ (\"poly_featutres\", PolynomialFeatures(degree=3)),\n                                (\"scaler\", StandardScaler()),\n                                (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42)  )\n                            ])#多项式核函数\npolynomial_svm_clf.fit( X, y )\nplot_dataset( X, y, [-1.5, 2.5, -1, 1.5] )\nplot_predict( polynomial_svm_clf, [-1.5, 2.5, -1, 1.5] )\nplt.show()\n```\n\n![机器学习之SVM支持向量机（二）图像06](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png)\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![机器学习之SVM支持向量机（二）图片推广](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png)\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/机器学习之SVM支持向量机（二）.md","raw":"---\ntitle: 机器学习之SVM支持向量机（二）\ndate: 2018-04-04 21:06:26\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.知识回顾\n\n[机器学习之SVM支持向量机（一）](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483818&idx=1&sn=50c634d8b00877134558125c4a718fd7&chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd)中我们介绍了**SVM损失函数**、**最大间隔分类**、**为什么SVM能形成最大间隔分类器**、**核函数**、**SVM中Gaussian Kernel的使用**知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。\n\n### 2.函数间隔和几何间隔\n\n上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示：\n\n![机器学习之SVM支持向量机（二）图像01](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F01.png)\n\n在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。\n\n定义函数间隔$\\hat{\\gamma}$：\n$$\n\\hat{\\gamma}=y(w^Tx+b)=yf(x)\n$$\n而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔：\n$$\n\\hat{\\gamma}=min\\hat{\\gamma} \n$$\n但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。\n\n但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。\n\n![机器学习之SVM支持向量机（二）图像02](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F02.png)\n$$\nx=x_0+\\gamma\\frac{w}{||w||}\n$$\n其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到：\n$$\n\\gamma=\\frac{w^T+b}{||w||}=\\frac{f(x)}{||w||}\n$$\n为了得到$\\gamma$绝对值，将$\\gamma$乘上相应类别$y$，即可得到几何间隔：\n$$\n\\tilde{r}=yr=\\frac{\\hat{\\gamma}}{||w||}\n$$\n从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y*(w^Tx+b)=y*f(x)$，实际上就是$|f(x)|$，几何间隔$\\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。\n\n对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\\tilde{\\gamma}$。同时需满足如下条件\n$$\ny_i(w^Tx_i+b)=\\hat{\\gamma_i}\\ge\\hat{\\gamma},i=1,2,3,...,n\n$$\n此处令函数间隔$\\hat{\\gamma}$等于1（之所以令$\\hat{\\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成：\n$$\nmax\\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n\n$$\n\n### 3.原始问题到对偶问题的求解\n\n接着考虑我们之前的目标函数，由于求$\\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以目标函数转换为\n$$\nmin\\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n\n$$\n现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。\n\n那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\\alpha$，定义拉格朗日函数为：\n$$\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)\n$$\n然后令：\n$$\n\\theta(w)=\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)\n$$\n当某个条件不满足时，例如$y_i(w^Tx+b)<1$，那么有$\\theta(w)=\\infty$。而当所有约束条件都满足时，则有$\\theta(w)=\\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为：\n$$\n\\min_{w,b}\\theta(w)=\\min_{w,b}\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)=p^*\n$$\n这里用$p^*$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下：\n$$\n\\max_{\\alpha_i\\ge0}\\min_{w,b}L(w,b,\\alpha)=d^*\n$$\n交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^*$表示，而且有$d^*\\le p^*$，在**满足某些条件**的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。\n\n此处**满足某些条件**的情况下，两者等价，此处的**满足某些条件**便是满足KKT条件。KKT最优化数学模型表示成下列标准形式:\n$$\n\\min f(x)\n$$\n\n$$\ns.t. h_j(x)=0,j=1,2,3,...,p\n$$\n\n$$\ng_k(x)\\le0,k=1,2,3,...,q\n$$\n\n$$\nx\\in X\\subset R^n\n$$\n\n其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。\n\n> 凸优化概念:$X\\subset R^n$为一凸集，$f:X->R$为一凸函数。凸优化便是寻找一点$x^*\\in X$，是的每一$x\\in X$满足$f(x^*)\\le f(x)$。\n>\n> KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。\n\nKKT条件就是上面最优化数学模型的标准形式中的最小点$x^*$必须满足下面的条件:\n$$\nh_j(x^*)=0,j=1,2,3,...,p\n$$\n\n$$\ng_k(x^*)\\le 0,k=1,2,3,...,q\n$$\n\n$$\n\\nabla f(x^*)+\\sum_{j=1}^{p}\\lambda_j\\nabla h_j(x^*)+\\sum_{k=1}^{q}\\mu_k \\nabla g_k(x^*)=0\n$$\n\n$$\n\\lambda_j \\neq0,\\mu \\ge0,\\mu_k g_k(x^*)=0\n$$\n\n此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\\alpha)$关于$w,b$的最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。\n\n首先固定$\\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。\n$$\n\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w= \\sum_{i=1}^{n}\\alpha_iy_ix_i\n$$\n\n$$\n\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n}\\alpha_iy_i=0\n$$\n\n将上述结果代入到之前的L得到：\n$$\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)\n$$\n\n$$\n=\\frac{1}{2}w^Tw-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_iy_ix_i)^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}\\sum_{i=1}^{n}\\alpha_iy_ix_i^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\n$$\n\n然后求对$\\alpha$的极大，即是关于对偶问题的最优化问题。\n$$\n\\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j\n$$\n\n$$\ns.t.,\\alpha_i\\ge0,i=1,2,3,...,n\n$$\n\n$$\n\\sum_{i=1}^{n}\\alpha_iy_i=0\n$$\n\n我们已经知道$x_i,x_j$的值，便可利用**SMO算法**求解$\\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。\n$$\nb^*=-\\frac{\\max_{i:y(i)=-1}w^Tx_i+\\min_{i:y(i)=1}w^Tx_i}{2}\n$$\n至此我们便可得出分类超平面和分类决策函数。\n\n### 4.松弛变量处理outliers方法\n\n实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。\n\n![机器学习之SVM支持向量机（二）图像03](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F03.png)\n\n为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即\n$$\ny_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n\n$$\n其中$\\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\\varepsilon_i$的总和也要尽量小。\n$$\n\\min\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i\n$$\n\n$$\ns.t.,y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n\n$$\n\n$$\n\\varepsilon_i\\ge0,i=1,2,3,...,n\n$$\n\n此处和[机器学习之SVM支持向量机（一）](https://weizhixiaoyi.com/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/)中的损失函数不同的是加入$\\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\\varepsilon_i$。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示:\n$$\nL(w,b,\\varepsilon,\\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1+\\varepsilon_i)-\\sum_{i=1}^{n}r_i\\varepsilon_i\n$$\n分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。\n\n### 5.Sklearn实现SVM支持向量机\n\n我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在**机器学习之SVM支持向量机（一）**中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇**SVM核函数的应用**。\n\n#### 5.1线性\n\n```Python\nfrom sklearn import svm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nx=np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]#正态分布产生数字20行2列\ny=[0]*20+[1]*20#20个class0,20个class1\nclf=svm.SVC(kernel='linear')#使用线性核\nclf.fit(x,y)\nw=clf.coef_[0]#获取w\na=-w[0]/w[1]#斜率\n\n#画图\nxx=np.linspace(-5,5)\nyy=a*xx-(clf.intercept_[0])/w[1]\nb=clf.support_vectors_[0]\nyy_down=a*xx+(b[1]-a*b[0])\nb=clf.support_vectors_[-1]\nyy_up=a*xx+(b[1]-a*b[0])\nplt.figure(figsize=(8,4))\nplt.plot(xx,yy)\nplt.plot(xx,yy_down)\nplt.plot(xx,yy_up)\nplt.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=80)\nplt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.Paired)\nplt.axis('tight')\nplt.show()\n```\n\n![机器学习之SVM支持向量机（二）图像05](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png)\n\n#### 5.2非线性\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import  Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nX, y = make_moons( n_samples=100, noise=0.15, random_state=42 )\n\ndef plot_dataset(X, y, axes):\n    plt.plot( X[:,0][y==0], X[:,1][y==0], \"bs\" )\n    plt.plot( X[:,0][y==1], X[:,1][y==1], \"g^\" )\n    plt.axis( axes )\n    plt.grid( True, which=\"both\" )\n    plt.xlabel(r\"$x_l$\")\n    plt.ylabel(r\"$x_2$\")\n\n# contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）\ndef plot_predict(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid( x0s, x1s )\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict( X ).reshape( x0.shape )\n    y_decision = clf.decision_function( X ).reshape( x0.shape )\n    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=0.5 )\n    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=0.2 )\n\npolynomial_svm_clf = Pipeline([ (\"poly_featutres\", PolynomialFeatures(degree=3)),\n                                (\"scaler\", StandardScaler()),\n                                (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42)  )\n                            ])#多项式核函数\npolynomial_svm_clf.fit( X, y )\nplot_dataset( X, y, [-1.5, 2.5, -1, 1.5] )\nplot_predict( polynomial_svm_clf, [-1.5, 2.5, -1, 1.5] )\nplt.show()\n```\n\n![机器学习之SVM支持向量机（二）图像06](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png)\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![机器学习之SVM支持向量机（二）图片推广](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png)\n\n\n\n\n\n\n\n\n\n\n\n","slug":"机器学习之SVM支持向量机（二）","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d460017jiz5gdv74el2","content":"<h3 id=\"1-知识回顾\"><a href=\"#1-知识回顾\" class=\"headerlink\" title=\"1.知识回顾\"></a>1.知识回顾</h3><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483818&amp;idx=1&amp;sn=50c634d8b00877134558125c4a718fd7&amp;chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd\" target=\"_blank\" rel=\"noopener\">机器学习之SVM支持向量机（一）</a>中我们介绍了<strong>SVM损失函数</strong>、<strong>最大间隔分类</strong>、<strong>为什么SVM能形成最大间隔分类器</strong>、<strong>核函数</strong>、<strong>SVM中Gaussian Kernel的使用</strong>知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。</p>\n<h3 id=\"2-函数间隔和几何间隔\"><a href=\"#2-函数间隔和几何间隔\" class=\"headerlink\" title=\"2.函数间隔和几何间隔\"></a>2.函数间隔和几何间隔</h3><p>上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示：</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F01.png\" alt=\"机器学习之SVM支持向量机（二）图像01\"></p>\n<p>在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。</p>\n<p>定义函数间隔$\\hat{\\gamma}$：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{\\gamma}=y(w^Tx+b)=yf(x)</script><p>而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{\\gamma}=min\\hat{\\gamma}</script><p>但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。</p>\n<p>但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F02.png\" alt=\"机器学习之SVM支持向量机（二）图像02\"></p>\n<script type=\"math/tex; mode=display\">\nx=x_0+\\gamma\\frac{w}{||w||}</script><p>其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到：</p>\n<script type=\"math/tex; mode=display\">\n\\gamma=\\frac{w^T+b}{||w||}=\\frac{f(x)}{||w||}</script><p>为了得到$\\gamma$绝对值，将$\\gamma$乘上相应类别$y$，即可得到几何间隔：</p>\n<script type=\"math/tex; mode=display\">\n\\tilde{r}=yr=\\frac{\\hat{\\gamma}}{||w||}</script><p>从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y<em>(w^Tx+b)=y</em>f(x)$，实际上就是$|f(x)|$，几何间隔$\\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。</p>\n<p>对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\\tilde{\\gamma}$。同时需满足如下条件</p>\n<script type=\"math/tex; mode=display\">\ny_i(w^Tx_i+b)=\\hat{\\gamma_i}\\ge\\hat{\\gamma},i=1,2,3,...,n</script><p>此处令函数间隔$\\hat{\\gamma}$等于1（之所以令$\\hat{\\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成：</p>\n<script type=\"math/tex; mode=display\">\nmax\\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n</script><h3 id=\"3-原始问题到对偶问题的求解\"><a href=\"#3-原始问题到对偶问题的求解\" class=\"headerlink\" title=\"3.原始问题到对偶问题的求解\"></a>3.原始问题到对偶问题的求解</h3><p>接着考虑我们之前的目标函数，由于求$\\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以目标函数转换为</p>\n<script type=\"math/tex; mode=display\">\nmin\\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n</script><p>现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。</p>\n<p>那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\\alpha$，定义拉格朗日函数为：</p>\n<script type=\"math/tex; mode=display\">\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)</script><p>然后令：</p>\n<script type=\"math/tex; mode=display\">\n\\theta(w)=\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)</script><p>当某个条件不满足时，例如$y_i(w^Tx+b)&lt;1$，那么有$\\theta(w)=\\infty$。而当所有约束条件都满足时，则有$\\theta(w)=\\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为：</p>\n<script type=\"math/tex; mode=display\">\n\\min_{w,b}\\theta(w)=\\min_{w,b}\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)=p^*</script><p>这里用$p^*$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下：</p>\n<script type=\"math/tex; mode=display\">\n\\max_{\\alpha_i\\ge0}\\min_{w,b}L(w,b,\\alpha)=d^*</script><p>交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^<em>$表示，而且有$d^</em>\\le p^<em>$，在<em>*满足某些条件</em></em>的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。</p>\n<p>此处<strong>满足某些条件</strong>的情况下，两者等价，此处的<strong>满足某些条件</strong>便是满足KKT条件。KKT最优化数学模型表示成下列标准形式:</p>\n<script type=\"math/tex; mode=display\">\n\\min f(x)</script><script type=\"math/tex; mode=display\">\ns.t. h_j(x)=0,j=1,2,3,...,p</script><script type=\"math/tex; mode=display\">\ng_k(x)\\le0,k=1,2,3,...,q</script><script type=\"math/tex; mode=display\">\nx\\in X\\subset R^n</script><p>其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。</p>\n<blockquote>\n<p>凸优化概念:$X\\subset R^n$为一凸集，$f:X-&gt;R$为一凸函数。凸优化便是寻找一点$x^<em>\\in X$，是的每一$x\\in X$满足$f(x^</em>)\\le f(x)$。</p>\n<p>KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。</p>\n</blockquote>\n<p>KKT条件就是上面最优化数学模型的标准形式中的最小点$x^*$必须满足下面的条件:</p>\n<script type=\"math/tex; mode=display\">\nh_j(x^*)=0,j=1,2,3,...,p</script><script type=\"math/tex; mode=display\">\ng_k(x^*)\\le 0,k=1,2,3,...,q</script><script type=\"math/tex; mode=display\">\n\\nabla f(x^*)+\\sum_{j=1}^{p}\\lambda_j\\nabla h_j(x^*)+\\sum_{k=1}^{q}\\mu_k \\nabla g_k(x^*)=0</script><script type=\"math/tex; mode=display\">\n\\lambda_j \\neq0,\\mu \\ge0,\\mu_k g_k(x^*)=0</script><p>此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\\alpha)$关于$w,b$的最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。</p>\n<p>首先固定$\\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w= \\sum_{i=1}^{n}\\alpha_iy_ix_i</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n}\\alpha_iy_i=0</script><p>将上述结果代入到之前的L得到：</p>\n<script type=\"math/tex; mode=display\">\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{2}w^Tw-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_iy_ix_i)^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}\\sum_{i=1}^{n}\\alpha_iy_ix_i^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j</script><p>然后求对$\\alpha$的极大，即是关于对偶问题的最优化问题。</p>\n<script type=\"math/tex; mode=display\">\n\\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j</script><script type=\"math/tex; mode=display\">\ns.t.,\\alpha_i\\ge0,i=1,2,3,...,n</script><script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{n}\\alpha_iy_i=0</script><p>我们已经知道$x_i,x_j$的值，便可利用<strong>SMO算法</strong>求解$\\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。</p>\n<script type=\"math/tex; mode=display\">\nb^*=-\\frac{\\max_{i:y(i)=-1}w^Tx_i+\\min_{i:y(i)=1}w^Tx_i}{2}</script><p>至此我们便可得出分类超平面和分类决策函数。</p>\n<h3 id=\"4-松弛变量处理outliers方法\"><a href=\"#4-松弛变量处理outliers方法\" class=\"headerlink\" title=\"4.松弛变量处理outliers方法\"></a>4.松弛变量处理outliers方法</h3><p>实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F03.png\" alt=\"机器学习之SVM支持向量机（二）图像03\"></p>\n<p>为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即</p>\n<script type=\"math/tex; mode=display\">\ny_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n</script><p>其中$\\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\\varepsilon_i$的总和也要尽量小。</p>\n<script type=\"math/tex; mode=display\">\n\\min\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i</script><script type=\"math/tex; mode=display\">\ns.t.,y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n</script><script type=\"math/tex; mode=display\">\n\\varepsilon_i\\ge0,i=1,2,3,...,n</script><p>此处和<a href=\"https://weizhixiaoyi.com/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/\">机器学习之SVM支持向量机（一）</a>中的损失函数不同的是加入$\\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\\varepsilon_i$。</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><p>那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示:</p>\n<script type=\"math/tex; mode=display\">\nL(w,b,\\varepsilon,\\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1+\\varepsilon_i)-\\sum_{i=1}^{n}r_i\\varepsilon_i</script><p>分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。</p>\n<h3 id=\"5-Sklearn实现SVM支持向量机\"><a href=\"#5-Sklearn实现SVM支持向量机\" class=\"headerlink\" title=\"5.Sklearn实现SVM支持向量机\"></a>5.Sklearn实现SVM支持向量机</h3><p>我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在<strong>机器学习之SVM支持向量机（一）</strong>中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇<strong>SVM核函数的应用</strong>。</p>\n<h4 id=\"5-1线性\"><a href=\"#5-1线性\" class=\"headerlink\" title=\"5.1线性\"></a>5.1线性</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> svm</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">x=np.r_[np.random.randn(<span class=\"number\">20</span>,<span class=\"number\">2</span>)-[<span class=\"number\">2</span>,<span class=\"number\">2</span>],np.random.randn(<span class=\"number\">20</span>,<span class=\"number\">2</span>)+[<span class=\"number\">2</span>,<span class=\"number\">2</span>]]<span class=\"comment\">#正态分布产生数字20行2列</span></span><br><span class=\"line\">y=[<span class=\"number\">0</span>]*<span class=\"number\">20</span>+[<span class=\"number\">1</span>]*<span class=\"number\">20</span><span class=\"comment\">#20个class0,20个class1</span></span><br><span class=\"line\">clf=svm.SVC(kernel=<span class=\"string\">'linear'</span>)<span class=\"comment\">#使用线性核</span></span><br><span class=\"line\">clf.fit(x,y)</span><br><span class=\"line\">w=clf.coef_[<span class=\"number\">0</span>]<span class=\"comment\">#获取w</span></span><br><span class=\"line\">a=-w[<span class=\"number\">0</span>]/w[<span class=\"number\">1</span>]<span class=\"comment\">#斜率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">xx=np.linspace(<span class=\"number\">-5</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">yy=a*xx-(clf.intercept_[<span class=\"number\">0</span>])/w[<span class=\"number\">1</span>]</span><br><span class=\"line\">b=clf.support_vectors_[<span class=\"number\">0</span>]</span><br><span class=\"line\">yy_down=a*xx+(b[<span class=\"number\">1</span>]-a*b[<span class=\"number\">0</span>])</span><br><span class=\"line\">b=clf.support_vectors_[<span class=\"number\">-1</span>]</span><br><span class=\"line\">yy_up=a*xx+(b[<span class=\"number\">1</span>]-a*b[<span class=\"number\">0</span>])</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">plt.plot(xx,yy)</span><br><span class=\"line\">plt.plot(xx,yy_down)</span><br><span class=\"line\">plt.plot(xx,yy_up)</span><br><span class=\"line\">plt.scatter(clf.support_vectors_[:,<span class=\"number\">0</span>],clf.support_vectors_[:,<span class=\"number\">1</span>],s=<span class=\"number\">80</span>)</span><br><span class=\"line\">plt.scatter(x[:,<span class=\"number\">0</span>],x[:,<span class=\"number\">1</span>],c=y,cmap=plt.cm.Paired)</span><br><span class=\"line\">plt.axis(<span class=\"string\">'tight'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png\" alt=\"机器学习之SVM支持向量机（二）图像05\"></p>\n<h4 id=\"5-2非线性\"><a href=\"#5-2非线性\" class=\"headerlink\" title=\"5.2非线性\"></a>5.2非线性</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> PolynomialFeatures</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.pipeline <span class=\"keyword\">import</span>  Pipeline</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> LinearSVC</span><br><span class=\"line\">X, y = make_moons( n_samples=<span class=\"number\">100</span>, noise=<span class=\"number\">0.15</span>, random_state=<span class=\"number\">42</span> )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_dataset</span><span class=\"params\">(X, y, axes)</span>:</span></span><br><span class=\"line\">    plt.plot( X[:,<span class=\"number\">0</span>][y==<span class=\"number\">0</span>], X[:,<span class=\"number\">1</span>][y==<span class=\"number\">0</span>], <span class=\"string\">\"bs\"</span> )</span><br><span class=\"line\">    plt.plot( X[:,<span class=\"number\">0</span>][y==<span class=\"number\">1</span>], X[:,<span class=\"number\">1</span>][y==<span class=\"number\">1</span>], <span class=\"string\">\"g^\"</span> )</span><br><span class=\"line\">    plt.axis( axes )</span><br><span class=\"line\">    plt.grid( <span class=\"keyword\">True</span>, which=<span class=\"string\">\"both\"</span> )</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">r\"$x_l$\"</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">r\"$x_2$\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_predict</span><span class=\"params\">(clf, axes)</span>:</span></span><br><span class=\"line\">    x0s = np.linspace(axes[<span class=\"number\">0</span>], axes[<span class=\"number\">1</span>], <span class=\"number\">100</span>)</span><br><span class=\"line\">    x1s = np.linspace(axes[<span class=\"number\">2</span>], axes[<span class=\"number\">3</span>], <span class=\"number\">100</span>)</span><br><span class=\"line\">    x0, x1 = np.meshgrid( x0s, x1s )</span><br><span class=\"line\">    X = np.c_[x0.ravel(), x1.ravel()]</span><br><span class=\"line\">    y_pred = clf.predict( X ).reshape( x0.shape )</span><br><span class=\"line\">    y_decision = clf.decision_function( X ).reshape( x0.shape )</span><br><span class=\"line\">    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=<span class=\"number\">0.5</span> )</span><br><span class=\"line\">    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=<span class=\"number\">0.2</span> )</span><br><span class=\"line\"></span><br><span class=\"line\">polynomial_svm_clf = Pipeline([ (<span class=\"string\">\"poly_featutres\"</span>, PolynomialFeatures(degree=<span class=\"number\">3</span>)),</span><br><span class=\"line\">                                (<span class=\"string\">\"scaler\"</span>, StandardScaler()),</span><br><span class=\"line\">                                (<span class=\"string\">\"svm_clf\"</span>, LinearSVC(C=<span class=\"number\">10</span>, loss=<span class=\"string\">\"hinge\"</span>, random_state=<span class=\"number\">42</span>)  )</span><br><span class=\"line\">                            ])<span class=\"comment\">#多项式核函数</span></span><br><span class=\"line\">polynomial_svm_clf.fit( X, y )</span><br><span class=\"line\">plot_dataset( X, y, [<span class=\"number\">-1.5</span>, <span class=\"number\">2.5</span>, <span class=\"number\">-1</span>, <span class=\"number\">1.5</span>] )</span><br><span class=\"line\">plot_predict( polynomial_svm_clf, [<span class=\"number\">-1.5</span>, <span class=\"number\">2.5</span>, <span class=\"number\">-1</span>, <span class=\"number\">1.5</span>] )</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png\" alt=\"机器学习之SVM支持向量机（二）图像06\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png\" alt=\"机器学习之SVM支持向量机（二）图片推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-知识回顾\"><a href=\"#1-知识回顾\" class=\"headerlink\" title=\"1.知识回顾\"></a>1.知识回顾</h3><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483818&amp;idx=1&amp;sn=50c634d8b00877134558125c4a718fd7&amp;chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd\" target=\"_blank\" rel=\"noopener\">机器学习之SVM支持向量机（一）</a>中我们介绍了<strong>SVM损失函数</strong>、<strong>最大间隔分类</strong>、<strong>为什么SVM能形成最大间隔分类器</strong>、<strong>核函数</strong>、<strong>SVM中Gaussian Kernel的使用</strong>知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。</p>\n<h3 id=\"2-函数间隔和几何间隔\"><a href=\"#2-函数间隔和几何间隔\" class=\"headerlink\" title=\"2.函数间隔和几何间隔\"></a>2.函数间隔和几何间隔</h3><p>上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示：</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F01.png\" alt=\"机器学习之SVM支持向量机（二）图像01\"></p>\n<p>在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。</p>\n<p>定义函数间隔$\\hat{\\gamma}$：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{\\gamma}=y(w^Tx+b)=yf(x)</script><p>而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{\\gamma}=min\\hat{\\gamma}</script><p>但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。</p>\n<p>但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F02.png\" alt=\"机器学习之SVM支持向量机（二）图像02\"></p>\n<script type=\"math/tex; mode=display\">\nx=x_0+\\gamma\\frac{w}{||w||}</script><p>其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到：</p>\n<script type=\"math/tex; mode=display\">\n\\gamma=\\frac{w^T+b}{||w||}=\\frac{f(x)}{||w||}</script><p>为了得到$\\gamma$绝对值，将$\\gamma$乘上相应类别$y$，即可得到几何间隔：</p>\n<script type=\"math/tex; mode=display\">\n\\tilde{r}=yr=\\frac{\\hat{\\gamma}}{||w||}</script><p>从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y<em>(w^Tx+b)=y</em>f(x)$，实际上就是$|f(x)|$，几何间隔$\\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。</p>\n<p>对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\\tilde{\\gamma}$。同时需满足如下条件</p>\n<script type=\"math/tex; mode=display\">\ny_i(w^Tx_i+b)=\\hat{\\gamma_i}\\ge\\hat{\\gamma},i=1,2,3,...,n</script><p>此处令函数间隔$\\hat{\\gamma}$等于1（之所以令$\\hat{\\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成：</p>\n<script type=\"math/tex; mode=display\">\nmax\\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n</script><h3 id=\"3-原始问题到对偶问题的求解\"><a href=\"#3-原始问题到对偶问题的求解\" class=\"headerlink\" title=\"3.原始问题到对偶问题的求解\"></a>3.原始问题到对偶问题的求解</h3><p>接着考虑我们之前的目标函数，由于求$\\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以目标函数转换为</p>\n<script type=\"math/tex; mode=display\">\nmin\\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n</script><p>现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。</p>\n<p>那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\\alpha$，定义拉格朗日函数为：</p>\n<script type=\"math/tex; mode=display\">\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)</script><p>然后令：</p>\n<script type=\"math/tex; mode=display\">\n\\theta(w)=\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)</script><p>当某个条件不满足时，例如$y_i(w^Tx+b)&lt;1$，那么有$\\theta(w)=\\infty$。而当所有约束条件都满足时，则有$\\theta(w)=\\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为：</p>\n<script type=\"math/tex; mode=display\">\n\\min_{w,b}\\theta(w)=\\min_{w,b}\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)=p^*</script><p>这里用$p^*$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下：</p>\n<script type=\"math/tex; mode=display\">\n\\max_{\\alpha_i\\ge0}\\min_{w,b}L(w,b,\\alpha)=d^*</script><p>交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^<em>$表示，而且有$d^</em>\\le p^<em>$，在<em>*满足某些条件</em></em>的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。</p>\n<p>此处<strong>满足某些条件</strong>的情况下，两者等价，此处的<strong>满足某些条件</strong>便是满足KKT条件。KKT最优化数学模型表示成下列标准形式:</p>\n<script type=\"math/tex; mode=display\">\n\\min f(x)</script><script type=\"math/tex; mode=display\">\ns.t. h_j(x)=0,j=1,2,3,...,p</script><script type=\"math/tex; mode=display\">\ng_k(x)\\le0,k=1,2,3,...,q</script><script type=\"math/tex; mode=display\">\nx\\in X\\subset R^n</script><p>其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。</p>\n<blockquote>\n<p>凸优化概念:$X\\subset R^n$为一凸集，$f:X-&gt;R$为一凸函数。凸优化便是寻找一点$x^<em>\\in X$，是的每一$x\\in X$满足$f(x^</em>)\\le f(x)$。</p>\n<p>KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。</p>\n</blockquote>\n<p>KKT条件就是上面最优化数学模型的标准形式中的最小点$x^*$必须满足下面的条件:</p>\n<script type=\"math/tex; mode=display\">\nh_j(x^*)=0,j=1,2,3,...,p</script><script type=\"math/tex; mode=display\">\ng_k(x^*)\\le 0,k=1,2,3,...,q</script><script type=\"math/tex; mode=display\">\n\\nabla f(x^*)+\\sum_{j=1}^{p}\\lambda_j\\nabla h_j(x^*)+\\sum_{k=1}^{q}\\mu_k \\nabla g_k(x^*)=0</script><script type=\"math/tex; mode=display\">\n\\lambda_j \\neq0,\\mu \\ge0,\\mu_k g_k(x^*)=0</script><p>此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\\alpha)$关于$w,b$的最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。</p>\n<p>首先固定$\\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w= \\sum_{i=1}^{n}\\alpha_iy_ix_i</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n}\\alpha_iy_i=0</script><p>将上述结果代入到之前的L得到：</p>\n<script type=\"math/tex; mode=display\">\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{2}w^Tw-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_iy_ix_i)^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}\\sum_{i=1}^{n}\\alpha_iy_ix_i^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j</script><p>然后求对$\\alpha$的极大，即是关于对偶问题的最优化问题。</p>\n<script type=\"math/tex; mode=display\">\n\\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j</script><script type=\"math/tex; mode=display\">\ns.t.,\\alpha_i\\ge0,i=1,2,3,...,n</script><script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{n}\\alpha_iy_i=0</script><p>我们已经知道$x_i,x_j$的值，便可利用<strong>SMO算法</strong>求解$\\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。</p>\n<script type=\"math/tex; mode=display\">\nb^*=-\\frac{\\max_{i:y(i)=-1}w^Tx_i+\\min_{i:y(i)=1}w^Tx_i}{2}</script><p>至此我们便可得出分类超平面和分类决策函数。</p>\n<h3 id=\"4-松弛变量处理outliers方法\"><a href=\"#4-松弛变量处理outliers方法\" class=\"headerlink\" title=\"4.松弛变量处理outliers方法\"></a>4.松弛变量处理outliers方法</h3><p>实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F03.png\" alt=\"机器学习之SVM支持向量机（二）图像03\"></p>\n<p>为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即</p>\n<script type=\"math/tex; mode=display\">\ny_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n</script><p>其中$\\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\\varepsilon_i$的总和也要尽量小。</p>\n<script type=\"math/tex; mode=display\">\n\\min\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i</script><script type=\"math/tex; mode=display\">\ns.t.,y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n</script><script type=\"math/tex; mode=display\">\n\\varepsilon_i\\ge0,i=1,2,3,...,n</script><p>此处和<a href=\"https://weizhixiaoyi.com/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/\">机器学习之SVM支持向量机（一）</a>中的损失函数不同的是加入$\\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\\varepsilon_i$。</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}</script><p>那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示:</p>\n<script type=\"math/tex; mode=display\">\nL(w,b,\\varepsilon,\\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1+\\varepsilon_i)-\\sum_{i=1}^{n}r_i\\varepsilon_i</script><p>分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。</p>\n<h3 id=\"5-Sklearn实现SVM支持向量机\"><a href=\"#5-Sklearn实现SVM支持向量机\" class=\"headerlink\" title=\"5.Sklearn实现SVM支持向量机\"></a>5.Sklearn实现SVM支持向量机</h3><p>我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在<strong>机器学习之SVM支持向量机（一）</strong>中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇<strong>SVM核函数的应用</strong>。</p>\n<h4 id=\"5-1线性\"><a href=\"#5-1线性\" class=\"headerlink\" title=\"5.1线性\"></a>5.1线性</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> svm</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">x=np.r_[np.random.randn(<span class=\"number\">20</span>,<span class=\"number\">2</span>)-[<span class=\"number\">2</span>,<span class=\"number\">2</span>],np.random.randn(<span class=\"number\">20</span>,<span class=\"number\">2</span>)+[<span class=\"number\">2</span>,<span class=\"number\">2</span>]]<span class=\"comment\">#正态分布产生数字20行2列</span></span><br><span class=\"line\">y=[<span class=\"number\">0</span>]*<span class=\"number\">20</span>+[<span class=\"number\">1</span>]*<span class=\"number\">20</span><span class=\"comment\">#20个class0,20个class1</span></span><br><span class=\"line\">clf=svm.SVC(kernel=<span class=\"string\">'linear'</span>)<span class=\"comment\">#使用线性核</span></span><br><span class=\"line\">clf.fit(x,y)</span><br><span class=\"line\">w=clf.coef_[<span class=\"number\">0</span>]<span class=\"comment\">#获取w</span></span><br><span class=\"line\">a=-w[<span class=\"number\">0</span>]/w[<span class=\"number\">1</span>]<span class=\"comment\">#斜率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">xx=np.linspace(<span class=\"number\">-5</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">yy=a*xx-(clf.intercept_[<span class=\"number\">0</span>])/w[<span class=\"number\">1</span>]</span><br><span class=\"line\">b=clf.support_vectors_[<span class=\"number\">0</span>]</span><br><span class=\"line\">yy_down=a*xx+(b[<span class=\"number\">1</span>]-a*b[<span class=\"number\">0</span>])</span><br><span class=\"line\">b=clf.support_vectors_[<span class=\"number\">-1</span>]</span><br><span class=\"line\">yy_up=a*xx+(b[<span class=\"number\">1</span>]-a*b[<span class=\"number\">0</span>])</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">plt.plot(xx,yy)</span><br><span class=\"line\">plt.plot(xx,yy_down)</span><br><span class=\"line\">plt.plot(xx,yy_up)</span><br><span class=\"line\">plt.scatter(clf.support_vectors_[:,<span class=\"number\">0</span>],clf.support_vectors_[:,<span class=\"number\">1</span>],s=<span class=\"number\">80</span>)</span><br><span class=\"line\">plt.scatter(x[:,<span class=\"number\">0</span>],x[:,<span class=\"number\">1</span>],c=y,cmap=plt.cm.Paired)</span><br><span class=\"line\">plt.axis(<span class=\"string\">'tight'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png\" alt=\"机器学习之SVM支持向量机（二）图像05\"></p>\n<h4 id=\"5-2非线性\"><a href=\"#5-2非线性\" class=\"headerlink\" title=\"5.2非线性\"></a>5.2非线性</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> PolynomialFeatures</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.pipeline <span class=\"keyword\">import</span>  Pipeline</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> LinearSVC</span><br><span class=\"line\">X, y = make_moons( n_samples=<span class=\"number\">100</span>, noise=<span class=\"number\">0.15</span>, random_state=<span class=\"number\">42</span> )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_dataset</span><span class=\"params\">(X, y, axes)</span>:</span></span><br><span class=\"line\">    plt.plot( X[:,<span class=\"number\">0</span>][y==<span class=\"number\">0</span>], X[:,<span class=\"number\">1</span>][y==<span class=\"number\">0</span>], <span class=\"string\">\"bs\"</span> )</span><br><span class=\"line\">    plt.plot( X[:,<span class=\"number\">0</span>][y==<span class=\"number\">1</span>], X[:,<span class=\"number\">1</span>][y==<span class=\"number\">1</span>], <span class=\"string\">\"g^\"</span> )</span><br><span class=\"line\">    plt.axis( axes )</span><br><span class=\"line\">    plt.grid( <span class=\"keyword\">True</span>, which=<span class=\"string\">\"both\"</span> )</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">r\"$x_l$\"</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">r\"$x_2$\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_predict</span><span class=\"params\">(clf, axes)</span>:</span></span><br><span class=\"line\">    x0s = np.linspace(axes[<span class=\"number\">0</span>], axes[<span class=\"number\">1</span>], <span class=\"number\">100</span>)</span><br><span class=\"line\">    x1s = np.linspace(axes[<span class=\"number\">2</span>], axes[<span class=\"number\">3</span>], <span class=\"number\">100</span>)</span><br><span class=\"line\">    x0, x1 = np.meshgrid( x0s, x1s )</span><br><span class=\"line\">    X = np.c_[x0.ravel(), x1.ravel()]</span><br><span class=\"line\">    y_pred = clf.predict( X ).reshape( x0.shape )</span><br><span class=\"line\">    y_decision = clf.decision_function( X ).reshape( x0.shape )</span><br><span class=\"line\">    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=<span class=\"number\">0.5</span> )</span><br><span class=\"line\">    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=<span class=\"number\">0.2</span> )</span><br><span class=\"line\"></span><br><span class=\"line\">polynomial_svm_clf = Pipeline([ (<span class=\"string\">\"poly_featutres\"</span>, PolynomialFeatures(degree=<span class=\"number\">3</span>)),</span><br><span class=\"line\">                                (<span class=\"string\">\"scaler\"</span>, StandardScaler()),</span><br><span class=\"line\">                                (<span class=\"string\">\"svm_clf\"</span>, LinearSVC(C=<span class=\"number\">10</span>, loss=<span class=\"string\">\"hinge\"</span>, random_state=<span class=\"number\">42</span>)  )</span><br><span class=\"line\">                            ])<span class=\"comment\">#多项式核函数</span></span><br><span class=\"line\">polynomial_svm_clf.fit( X, y )</span><br><span class=\"line\">plot_dataset( X, y, [<span class=\"number\">-1.5</span>, <span class=\"number\">2.5</span>, <span class=\"number\">-1</span>, <span class=\"number\">1.5</span>] )</span><br><span class=\"line\">plot_predict( polynomial_svm_clf, [<span class=\"number\">-1.5</span>, <span class=\"number\">2.5</span>, <span class=\"number\">-1</span>, <span class=\"number\">1.5</span>] )</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png\" alt=\"机器学习之SVM支持向量机（二）图像06\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png\" alt=\"机器学习之SVM支持向量机（二）图片推广\"></p>\n"},{"title":"机器学习之决策树(C4.5算法)","date":"2018-04-19T03:03:26.000Z","mathjax":true,"comments":1,"_content":"\n### 1.决策树简介\n\n我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。\n\n| 序号 | 天气 | 温度 | 湿度 | 风速 | 高尔夫 |\n| ---- | :--: | :--: | :--: | :--: | :----: |\n| 1    |  晴  | 炎热 |  高  |  弱  |  进行  |\n| 2    |  晴  | 炎热 |  高  |  强  |  进行  |\n| 3    |  阴  | 炎热 |  高  |  弱  |  取消  |\n| 4    |  雨  | 适中 |  高  |  弱  |  取消  |\n| 5    |  雨  | 寒冷 | 正常 |  弱  |  取消  |\n| 6    |  雨  | 寒冷 | 正常 |  强  |  进行  |\n| 7    |  阴  | 寒冷 | 正常 |  强  |  进行  |\n| 8    |  晴  | 适中 |  高  |  弱  |  进行  |\n| 9    |  晴  | 寒冷 | 正常 |  弱  |  进行  |\n| 10   |  雨  | 适中 | 正常 |  弱  |  进行  |\n| 11   |  晴  | 适中 | 正常 |  强  |  进行  |\n| 12   |  阴  | 适中 |  高  |  强  |  进行  |\n| 13   |  阴  | 炎热 | 正常 |  弱  |  取消  |\n| 14   |  雨  | 适中 |  高  |  强  |  取消  |\n\n正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。\n\n![机器学习之决策树图片01](机器学习之决策树-C4-5算法/机器学习之决策树图片01.png)\n\n### 2.C4.5算法\n\n**上古之神赐予你智慧**：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。\n\nC4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设\n\n+ 类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。\n+ 类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。\n+ 属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。\n+ 属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。\n+ $p_i$表示类别$i$的概率，比如$p(进行)=\\frac{9}{14}$。\n\n#### 2.1信息增益\n\n信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。\n\n**计算类别信息熵**:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息\n$$\nInfo(D)=-\\sum_{i=1}^{m}p_ilog_2(p_i)\n$$\n\n$$\nInfo(D)=-\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14}=0.940\n$$\n\n**计算每个属性的信息熵**:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。\n$$\nInfo_A(D)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*Info(D_j)\n$$\n\n$$\nInfo(天气)=\\frac{5}{14}*[-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5}]+\\frac{4}{14}*[-\\frac{4}{4}log_2\\frac{4}{4}]+\\frac{5}{14}*[-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}]=0.694\n$$\n\n$$\nInfo(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892\n$$\n\n**计算信息增益**:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。\n$$\nGain(A)=Info(D)-Info_A(D)\n$$\n\n$$\nGain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246\n$$\n\n$$\nGain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048\n$$\n\n但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。\n\n#### 2.2信息增益率\n\n**计算属性分裂信息度量**:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用**信息增益 / 内在信息**表示，信息增益率会导致属性的重要性随着内在信息的增大而减小**（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）**，这样算是对单纯用信息增益有所补偿。信息增益率定义如下\n$$\nSplitInfo_A(D)=-\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*log_2\\frac{|D_j|}{|D|}\n$$\n\n$$\nSplitInfo(天气)=-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{4}{14}*log_2\\frac{4}{14}=1.577\n$$\n\n$$\nSplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985\n$$\n\n$$\nGainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}\n$$\n\n$$\nGainRatio(天气)=\\frac{Gain(天气)}{SplitInfo(天气)}=\\frac{0.246}{1.577}=0.155\n$$\n\n$$\nGainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048\n$$\n\n天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。\n\n### 3.树剪枝\n\n决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。\n\n#### 3.1先剪枝\n\n先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法\n\n+ 当决策树达到一定的高度就停止决策树的生长。\n+ 到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。\n+ 计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。\n\n#### 3.2后剪枝\n\n后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。\n\n把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为\n$$\n\\frac{\\sum (E_i+0.5*L)}{\\sum N_i}\n$$\n这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。\n\n假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(**e为分布的固有属性，可以统计出来**)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。\n$$\nE(subtree\\_err\\_count)=N*e\n$$\n\n$$\nvar(subtree\\_err\\_count)=\\sqrt{N*e*(1-e)}\n$$\n\n把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为\n$$\nE(leaf\\_err\\_count)=N*e\n$$\n使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝\n$$\nE(subtree\\_err\\_count)-var(subtree\\_err\\_count)>E(leaf\\_err\\_count)\n$$\n上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。\n\n### 4.Sklearn实现决策树\n\n我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\n#引入数据\niris=load_iris()\nX=iris.data\ny=iris.target\n\n#训练数据和模型,采用ID3或C4.5训练\nclf=tree.DecisionTreeClassifier(criterion='entropy')\nclf=clf.fit(X,y)\n\n\n#引入graphviz模块用来导出图,结果图如下所示\nimport graphviz\ndot_data=tree.export_graphviz(clf,out_file=None,\n                              feature_names=iris.feature_names,\n                              class_names=iris.target_names,\n                              filled=True,rounded=True,\n                              special_characters=True)\n\ngraph=graphviz.Source(dot_data)\ngraph.view()\n```\n\n![机器学习之决策树图片02](机器学习之决策树-C4-5算法/机器学习之决策树图片02.png)\n\n### 5.实际使用技巧\n\n- 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。\n- 训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (`sample_weight`) 的和归一化为相同的值。\n- 考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。\n- 通过 `export` 功能可以可视化您的决策树。使用 `max_depth=3`作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。\n- 填充树的样本数量会增加树的每个附加级别。使用 `max_depth` 来控制树的大小防止过拟合。\n- 通过使用 `min_samples_split` 和 `min_samples_leaf` 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/机器学习之决策树-C4-5算法.md","raw":"---\ntitle: 机器学习之决策树(C4.5算法)\ndate: 2018-04-19 11:03:26\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.决策树简介\n\n我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。\n\n| 序号 | 天气 | 温度 | 湿度 | 风速 | 高尔夫 |\n| ---- | :--: | :--: | :--: | :--: | :----: |\n| 1    |  晴  | 炎热 |  高  |  弱  |  进行  |\n| 2    |  晴  | 炎热 |  高  |  强  |  进行  |\n| 3    |  阴  | 炎热 |  高  |  弱  |  取消  |\n| 4    |  雨  | 适中 |  高  |  弱  |  取消  |\n| 5    |  雨  | 寒冷 | 正常 |  弱  |  取消  |\n| 6    |  雨  | 寒冷 | 正常 |  强  |  进行  |\n| 7    |  阴  | 寒冷 | 正常 |  强  |  进行  |\n| 8    |  晴  | 适中 |  高  |  弱  |  进行  |\n| 9    |  晴  | 寒冷 | 正常 |  弱  |  进行  |\n| 10   |  雨  | 适中 | 正常 |  弱  |  进行  |\n| 11   |  晴  | 适中 | 正常 |  强  |  进行  |\n| 12   |  阴  | 适中 |  高  |  强  |  进行  |\n| 13   |  阴  | 炎热 | 正常 |  弱  |  取消  |\n| 14   |  雨  | 适中 |  高  |  强  |  取消  |\n\n正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。\n\n![机器学习之决策树图片01](机器学习之决策树-C4-5算法/机器学习之决策树图片01.png)\n\n### 2.C4.5算法\n\n**上古之神赐予你智慧**：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。\n\nC4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设\n\n+ 类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。\n+ 类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。\n+ 属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。\n+ 属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。\n+ $p_i$表示类别$i$的概率，比如$p(进行)=\\frac{9}{14}$。\n\n#### 2.1信息增益\n\n信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。\n\n**计算类别信息熵**:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息\n$$\nInfo(D)=-\\sum_{i=1}^{m}p_ilog_2(p_i)\n$$\n\n$$\nInfo(D)=-\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14}=0.940\n$$\n\n**计算每个属性的信息熵**:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。\n$$\nInfo_A(D)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*Info(D_j)\n$$\n\n$$\nInfo(天气)=\\frac{5}{14}*[-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5}]+\\frac{4}{14}*[-\\frac{4}{4}log_2\\frac{4}{4}]+\\frac{5}{14}*[-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}]=0.694\n$$\n\n$$\nInfo(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892\n$$\n\n**计算信息增益**:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。\n$$\nGain(A)=Info(D)-Info_A(D)\n$$\n\n$$\nGain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246\n$$\n\n$$\nGain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048\n$$\n\n但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。\n\n#### 2.2信息增益率\n\n**计算属性分裂信息度量**:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用**信息增益 / 内在信息**表示，信息增益率会导致属性的重要性随着内在信息的增大而减小**（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）**，这样算是对单纯用信息增益有所补偿。信息增益率定义如下\n$$\nSplitInfo_A(D)=-\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*log_2\\frac{|D_j|}{|D|}\n$$\n\n$$\nSplitInfo(天气)=-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{4}{14}*log_2\\frac{4}{14}=1.577\n$$\n\n$$\nSplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985\n$$\n\n$$\nGainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}\n$$\n\n$$\nGainRatio(天气)=\\frac{Gain(天气)}{SplitInfo(天气)}=\\frac{0.246}{1.577}=0.155\n$$\n\n$$\nGainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048\n$$\n\n天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。\n\n### 3.树剪枝\n\n决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。\n\n#### 3.1先剪枝\n\n先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法\n\n+ 当决策树达到一定的高度就停止决策树的生长。\n+ 到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。\n+ 计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。\n\n#### 3.2后剪枝\n\n后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。\n\n把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为\n$$\n\\frac{\\sum (E_i+0.5*L)}{\\sum N_i}\n$$\n这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。\n\n假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(**e为分布的固有属性，可以统计出来**)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。\n$$\nE(subtree\\_err\\_count)=N*e\n$$\n\n$$\nvar(subtree\\_err\\_count)=\\sqrt{N*e*(1-e)}\n$$\n\n把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为\n$$\nE(leaf\\_err\\_count)=N*e\n$$\n使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝\n$$\nE(subtree\\_err\\_count)-var(subtree\\_err\\_count)>E(leaf\\_err\\_count)\n$$\n上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。\n\n### 4.Sklearn实现决策树\n\n我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\n#引入数据\niris=load_iris()\nX=iris.data\ny=iris.target\n\n#训练数据和模型,采用ID3或C4.5训练\nclf=tree.DecisionTreeClassifier(criterion='entropy')\nclf=clf.fit(X,y)\n\n\n#引入graphviz模块用来导出图,结果图如下所示\nimport graphviz\ndot_data=tree.export_graphviz(clf,out_file=None,\n                              feature_names=iris.feature_names,\n                              class_names=iris.target_names,\n                              filled=True,rounded=True,\n                              special_characters=True)\n\ngraph=graphviz.Source(dot_data)\ngraph.view()\n```\n\n![机器学习之决策树图片02](机器学习之决策树-C4-5算法/机器学习之决策树图片02.png)\n\n### 5.实际使用技巧\n\n- 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。\n- 训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (`sample_weight`) 的和归一化为相同的值。\n- 考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。\n- 通过 `export` 功能可以可视化您的决策树。使用 `max_depth=3`作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。\n- 填充树的样本数量会增加树的每个附加级别。使用 `max_depth` 来控制树的大小防止过拟合。\n- 通过使用 `min_samples_split` 和 `min_samples_leaf` 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"机器学习之决策树-C4-5算法","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d49001bjiz5rdq9nnwm","content":"<h3 id=\"1-决策树简介\"><a href=\"#1-决策树简介\" class=\"headerlink\" title=\"1.决策树简介\"></a>1.决策树简介</h3><p>我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>序号</th>\n<th style=\"text-align:center\">天气</th>\n<th style=\"text-align:center\">温度</th>\n<th style=\"text-align:center\">湿度</th>\n<th style=\"text-align:center\">风速</th>\n<th style=\"text-align:center\">高尔夫</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>2</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>3</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>4</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>5</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>6</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>7</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>8</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>9</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>10</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>11</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>12</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>13</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>14</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。</p>\n<p><img src=\"/2018/04/19/机器学习之决策树-C4-5算法/机器学习之决策树图片01.png\" alt=\"机器学习之决策树图片01\"></p>\n<h3 id=\"2-C4-5算法\"><a href=\"#2-C4-5算法\" class=\"headerlink\" title=\"2.C4.5算法\"></a>2.C4.5算法</h3><p><strong>上古之神赐予你智慧</strong>：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。</p>\n<p>C4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设</p>\n<ul>\n<li>类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。</li>\n<li>类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。</li>\n<li>属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。</li>\n<li>属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。</li>\n<li>$p_i$表示类别$i$的概率，比如$p(进行)=\\frac{9}{14}$。</li>\n</ul>\n<h4 id=\"2-1信息增益\"><a href=\"#2-1信息增益\" class=\"headerlink\" title=\"2.1信息增益\"></a>2.1信息增益</h4><p>信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。</p>\n<p><strong>计算类别信息熵</strong>:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息</p>\n<script type=\"math/tex; mode=display\">\nInfo(D)=-\\sum_{i=1}^{m}p_ilog_2(p_i)</script><script type=\"math/tex; mode=display\">\nInfo(D)=-\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14}=0.940</script><p><strong>计算每个属性的信息熵</strong>:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。</p>\n<script type=\"math/tex; mode=display\">\nInfo_A(D)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*Info(D_j)</script><script type=\"math/tex; mode=display\">\nInfo(天气)=\\frac{5}{14}*[-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5}]+\\frac{4}{14}*[-\\frac{4}{4}log_2\\frac{4}{4}]+\\frac{5}{14}*[-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}]=0.694</script><script type=\"math/tex; mode=display\">\nInfo(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892</script><p><strong>计算信息增益</strong>:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。</p>\n<script type=\"math/tex; mode=display\">\nGain(A)=Info(D)-Info_A(D)</script><script type=\"math/tex; mode=display\">\nGain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246</script><script type=\"math/tex; mode=display\">\nGain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048</script><p>但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。</p>\n<h4 id=\"2-2信息增益率\"><a href=\"#2-2信息增益率\" class=\"headerlink\" title=\"2.2信息增益率\"></a>2.2信息增益率</h4><p><strong>计算属性分裂信息度量</strong>:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用<strong>信息增益 / 内在信息</strong>表示，信息增益率会导致属性的重要性随着内在信息的增大而减小<strong>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</strong>，这样算是对单纯用信息增益有所补偿。信息增益率定义如下</p>\n<script type=\"math/tex; mode=display\">\nSplitInfo_A(D)=-\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*log_2\\frac{|D_j|}{|D|}</script><script type=\"math/tex; mode=display\">\nSplitInfo(天气)=-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{4}{14}*log_2\\frac{4}{14}=1.577</script><script type=\"math/tex; mode=display\">\nSplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985</script><script type=\"math/tex; mode=display\">\nGainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}</script><script type=\"math/tex; mode=display\">\nGainRatio(天气)=\\frac{Gain(天气)}{SplitInfo(天气)}=\\frac{0.246}{1.577}=0.155</script><script type=\"math/tex; mode=display\">\nGainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048</script><p>天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。</p>\n<h3 id=\"3-树剪枝\"><a href=\"#3-树剪枝\" class=\"headerlink\" title=\"3.树剪枝\"></a>3.树剪枝</h3><p>决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。</p>\n<h4 id=\"3-1先剪枝\"><a href=\"#3-1先剪枝\" class=\"headerlink\" title=\"3.1先剪枝\"></a>3.1先剪枝</h4><p>先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法</p>\n<ul>\n<li>当决策树达到一定的高度就停止决策树的生长。</li>\n<li>到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。</li>\n<li>计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。</li>\n</ul>\n<h4 id=\"3-2后剪枝\"><a href=\"#3-2后剪枝\" class=\"headerlink\" title=\"3.2后剪枝\"></a>3.2后剪枝</h4><p>后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。</p>\n<p>把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\sum (E_i+0.5*L)}{\\sum N_i}</script><p>这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。</p>\n<p>假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(<strong>e为分布的固有属性，可以统计出来</strong>)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。</p>\n<script type=\"math/tex; mode=display\">\nE(subtree\\_err\\_count)=N*e</script><script type=\"math/tex; mode=display\">\nvar(subtree\\_err\\_count)=\\sqrt{N*e*(1-e)}</script><p>把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为</p>\n<script type=\"math/tex; mode=display\">\nE(leaf\\_err\\_count)=N*e</script><p>使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝</p>\n<script type=\"math/tex; mode=display\">\nE(subtree\\_err\\_count)-var(subtree\\_err\\_count)>E(leaf\\_err\\_count)</script><p>上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。</p>\n<h3 id=\"4-Sklearn实现决策树\"><a href=\"#4-Sklearn实现决策树\" class=\"headerlink\" title=\"4.Sklearn实现决策树\"></a>4.Sklearn实现决策树</h3><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练数据和模型,采用ID3或C4.5训练</span></span><br><span class=\"line\">clf=tree.DecisionTreeClassifier(criterion=<span class=\"string\">'entropy'</span>)</span><br><span class=\"line\">clf=clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入graphviz模块用来导出图,结果图如下所示</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> graphviz</span><br><span class=\"line\">dot_data=tree.export_graphviz(clf,out_file=<span class=\"keyword\">None</span>,</span><br><span class=\"line\">                              feature_names=iris.feature_names,</span><br><span class=\"line\">                              class_names=iris.target_names,</span><br><span class=\"line\">                              filled=<span class=\"keyword\">True</span>,rounded=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                              special_characters=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph=graphviz.Source(dot_data)</span><br><span class=\"line\">graph.view()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/19/机器学习之决策树-C4-5算法/机器学习之决策树图片02.png\" alt=\"机器学习之决策树图片02\"></p>\n<h3 id=\"5-实际使用技巧\"><a href=\"#5-实际使用技巧\" class=\"headerlink\" title=\"5.实际使用技巧\"></a>5.实际使用技巧</h3><ul>\n<li>对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。</li>\n<li>训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code>sample_weight</code>) 的和归一化为相同的值。</li>\n<li>考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。</li>\n<li>通过 <code>export</code> 功能可以可视化您的决策树。使用 <code>max_depth=3</code>作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。</li>\n<li>填充树的样本数量会增加树的每个附加级别。使用 <code>max_depth</code> 来控制树的大小防止过拟合。</li>\n<li>通过使用 <code>min_samples_split</code> 和 <code>min_samples_leaf</code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。</li>\n</ul>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-决策树简介\"><a href=\"#1-决策树简介\" class=\"headerlink\" title=\"1.决策树简介\"></a>1.决策树简介</h3><p>我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>序号</th>\n<th style=\"text-align:center\">天气</th>\n<th style=\"text-align:center\">温度</th>\n<th style=\"text-align:center\">湿度</th>\n<th style=\"text-align:center\">风速</th>\n<th style=\"text-align:center\">高尔夫</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>2</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>3</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>4</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>5</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>6</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>7</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>8</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>9</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>10</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>11</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>12</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>13</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>14</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。</p>\n<p><img src=\"/2018/04/19/机器学习之决策树-C4-5算法/机器学习之决策树图片01.png\" alt=\"机器学习之决策树图片01\"></p>\n<h3 id=\"2-C4-5算法\"><a href=\"#2-C4-5算法\" class=\"headerlink\" title=\"2.C4.5算法\"></a>2.C4.5算法</h3><p><strong>上古之神赐予你智慧</strong>：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。</p>\n<p>C4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设</p>\n<ul>\n<li>类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。</li>\n<li>类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。</li>\n<li>属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。</li>\n<li>属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。</li>\n<li>$p_i$表示类别$i$的概率，比如$p(进行)=\\frac{9}{14}$。</li>\n</ul>\n<h4 id=\"2-1信息增益\"><a href=\"#2-1信息增益\" class=\"headerlink\" title=\"2.1信息增益\"></a>2.1信息增益</h4><p>信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。</p>\n<p><strong>计算类别信息熵</strong>:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息</p>\n<script type=\"math/tex; mode=display\">\nInfo(D)=-\\sum_{i=1}^{m}p_ilog_2(p_i)</script><script type=\"math/tex; mode=display\">\nInfo(D)=-\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14}=0.940</script><p><strong>计算每个属性的信息熵</strong>:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。</p>\n<script type=\"math/tex; mode=display\">\nInfo_A(D)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*Info(D_j)</script><script type=\"math/tex; mode=display\">\nInfo(天气)=\\frac{5}{14}*[-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5}]+\\frac{4}{14}*[-\\frac{4}{4}log_2\\frac{4}{4}]+\\frac{5}{14}*[-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}]=0.694</script><script type=\"math/tex; mode=display\">\nInfo(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892</script><p><strong>计算信息增益</strong>:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。</p>\n<script type=\"math/tex; mode=display\">\nGain(A)=Info(D)-Info_A(D)</script><script type=\"math/tex; mode=display\">\nGain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246</script><script type=\"math/tex; mode=display\">\nGain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048</script><p>但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。</p>\n<h4 id=\"2-2信息增益率\"><a href=\"#2-2信息增益率\" class=\"headerlink\" title=\"2.2信息增益率\"></a>2.2信息增益率</h4><p><strong>计算属性分裂信息度量</strong>:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用<strong>信息增益 / 内在信息</strong>表示，信息增益率会导致属性的重要性随着内在信息的增大而减小<strong>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</strong>，这样算是对单纯用信息增益有所补偿。信息增益率定义如下</p>\n<script type=\"math/tex; mode=display\">\nSplitInfo_A(D)=-\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*log_2\\frac{|D_j|}{|D|}</script><script type=\"math/tex; mode=display\">\nSplitInfo(天气)=-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{4}{14}*log_2\\frac{4}{14}=1.577</script><script type=\"math/tex; mode=display\">\nSplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985</script><script type=\"math/tex; mode=display\">\nGainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}</script><script type=\"math/tex; mode=display\">\nGainRatio(天气)=\\frac{Gain(天气)}{SplitInfo(天气)}=\\frac{0.246}{1.577}=0.155</script><script type=\"math/tex; mode=display\">\nGainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048</script><p>天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。</p>\n<h3 id=\"3-树剪枝\"><a href=\"#3-树剪枝\" class=\"headerlink\" title=\"3.树剪枝\"></a>3.树剪枝</h3><p>决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。</p>\n<h4 id=\"3-1先剪枝\"><a href=\"#3-1先剪枝\" class=\"headerlink\" title=\"3.1先剪枝\"></a>3.1先剪枝</h4><p>先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法</p>\n<ul>\n<li>当决策树达到一定的高度就停止决策树的生长。</li>\n<li>到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。</li>\n<li>计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。</li>\n</ul>\n<h4 id=\"3-2后剪枝\"><a href=\"#3-2后剪枝\" class=\"headerlink\" title=\"3.2后剪枝\"></a>3.2后剪枝</h4><p>后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。</p>\n<p>把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\sum (E_i+0.5*L)}{\\sum N_i}</script><p>这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。</p>\n<p>假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(<strong>e为分布的固有属性，可以统计出来</strong>)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。</p>\n<script type=\"math/tex; mode=display\">\nE(subtree\\_err\\_count)=N*e</script><script type=\"math/tex; mode=display\">\nvar(subtree\\_err\\_count)=\\sqrt{N*e*(1-e)}</script><p>把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为</p>\n<script type=\"math/tex; mode=display\">\nE(leaf\\_err\\_count)=N*e</script><p>使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝</p>\n<script type=\"math/tex; mode=display\">\nE(subtree\\_err\\_count)-var(subtree\\_err\\_count)>E(leaf\\_err\\_count)</script><p>上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。</p>\n<h3 id=\"4-Sklearn实现决策树\"><a href=\"#4-Sklearn实现决策树\" class=\"headerlink\" title=\"4.Sklearn实现决策树\"></a>4.Sklearn实现决策树</h3><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练数据和模型,采用ID3或C4.5训练</span></span><br><span class=\"line\">clf=tree.DecisionTreeClassifier(criterion=<span class=\"string\">'entropy'</span>)</span><br><span class=\"line\">clf=clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入graphviz模块用来导出图,结果图如下所示</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> graphviz</span><br><span class=\"line\">dot_data=tree.export_graphviz(clf,out_file=<span class=\"keyword\">None</span>,</span><br><span class=\"line\">                              feature_names=iris.feature_names,</span><br><span class=\"line\">                              class_names=iris.target_names,</span><br><span class=\"line\">                              filled=<span class=\"keyword\">True</span>,rounded=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                              special_characters=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph=graphviz.Source(dot_data)</span><br><span class=\"line\">graph.view()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/19/机器学习之决策树-C4-5算法/机器学习之决策树图片02.png\" alt=\"机器学习之决策树图片02\"></p>\n<h3 id=\"5-实际使用技巧\"><a href=\"#5-实际使用技巧\" class=\"headerlink\" title=\"5.实际使用技巧\"></a>5.实际使用技巧</h3><ul>\n<li>对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。</li>\n<li>训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code>sample_weight</code>) 的和归一化为相同的值。</li>\n<li>考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。</li>\n<li>通过 <code>export</code> 功能可以可视化您的决策树。使用 <code>max_depth=3</code>作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。</li>\n<li>填充树的样本数量会增加树的每个附加级别。使用 <code>max_depth</code> 来控制树的大小防止过拟合。</li>\n<li>通过使用 <code>min_samples_split</code> 和 <code>min_samples_leaf</code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。</li>\n</ul>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"机器学习之分类与回归树(CART)","date":"2018-04-22T13:58:53.000Z","mathjax":true,"comments":1,"_content":"\n### 1.分类与回归树简介\n\n分类与回归树的英文是*Classfication And Regression Tree*，缩写为CART。CART算法采用**二分递归分割的技术**将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为**True**和**False**，左分支取值为**True**，右分支取值为**False**，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。\n\n+ 如果待预测分类是离散型数据，则CART生成分类决策树。\n+ 如果待预测分类是连续性数据，则CART生成回归决策树。\n\n### 2.CART分类树\n\n#### 2.1算法详解\n\nCART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为\n\n$$\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\n$$\n根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。\n$$\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2\n$$\n如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n$$\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)\n$$\n对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。\n$$\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))\n$$\n$$\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))\n$$\n#### 2.2实例详解\n\n|  名称  | 体温 | 胎生 | 水生 | 类标记 |\n| :----: | :--: | :--: | :--: | :----: |\n|   人   | 恒温 |  是  |  否  | 哺乳类 |\n|  巨蟒  | 冷血 |  否  |  否  | 爬行类 |\n|  鲑鱼  | 冷血 |  否  |  是  |  鱼类  |\n|   鲸   | 恒温 |  是  |  是  | 哺乳类 |\n|   蛙   | 冷血 |  否  | 有时 |  鱼类  |\n|  巨蜥  | 冷血 |  否  |  否  | 爬行类 |\n|  蝙蝠  | 恒温 |  是  |  否  | 哺乳类 |\n|   猫   | 恒温 |  是  |  否  | 哺乳类 |\n| 豹纹鲨 | 冷血 |  是  |  是  |  鱼类  |\n|  海龟  | 冷血 |  否  | 有时 | 爬行类 |\n|  豪猪  | 恒温 |  是  |  否  | 哺乳类 |\n|   鳗   | 冷血 |  否  |  是  |  鱼类  |\n|  蝾螈  | 冷血 |  否  | 有时 | 两栖类 |\n\n针对上述离散型数据，按照**体温为恒温和非恒温**进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。\n\n$$\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}\n$$\n$$\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}\n$$\n然后计算得到特征**体温**下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。\n$$\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}\n$$\n### 3.CART回归树\n\n#### 3.1算法详解\n\nCART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n$$\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}\n$$\n**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。\n\n$$\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]\n$$\n**用选定的(j,s)对，划分区域并决定相应的输出值**\n$$\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}\n$$\n\n$$\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i\n$$\n\n$$\nx\\epsilon R_m,m=1,2\n$$\n\n**继续对两个子区域调用上述步骤**，**将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。**\n\n$$\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)\n$$\n当输入空间划分确定时，可以用**平方误差**来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。\n$$\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2\n$$\n\n#### 3.2实例详解\n\n| $x_i$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |\n| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |\n\n考虑如上所示的连续性变量，根据给定的数据点，考虑**1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5**切分点。对各切分点依次求出**R1,R2,c1,c2及m(s)**，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。\n\n$$\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56\n$$\n\n$$\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50\n$$\n\n$$\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72\n$$\n\n**依次改变(j,s)对，可以得到s及m(s)的计算结果**，如下表所示。\n\n|  $s$   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |\n| :----: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |\n| $m(s)$ | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |\n\n当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。**回归树T1(x)**为\n$$\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}\n$$\n\n$$\nf_1(x)=T_1(x)\n$$\n\n**然后我们利用f1(x)拟合训练数据的残差**，如下表所示\n\n| $x_i$ |   1   |   2   |   3   |  4   |  5   |  6   |   7   |   8   |  9   |  10  |\n| :---: | :---: | :---: | :---: | :--: | :--: | :--: | :---: | :---: | :--: | :--: |\n| $y_i$ | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |\n\n**用f1(x)拟合训练数据得到平方误差**\n$$\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93\n$$\n第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到\n$$\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}\n$$\n\n$$\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}\n$$\n\n用f2(x)拟合训练数据的平方误差\n$$\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79\n$$\n继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示\n$$\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47\n$$\n\n$$\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30\n$$\n\n$$\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23\n$$\n\n$$\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}\n$$\n\n$$\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}\n$$\n\n用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。\n$$\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71\n$$\n### 4.CART剪枝\n\n> 此处我们介绍代价复杂度剪枝算法\n\n我们将一颗充分生长的树称为**T0** ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下\n$$\nC_\\alpha(T)=C(T)+\\alpha|T|\n$$\n+ T为任意子树，|T|为子树T的叶子节点个数。\n+ α是参数，权衡拟合程度与树的复杂度。\n+ C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。\n\n**那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？**准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。\n\n+ 当α很小的时候，T0 是这样的最优子树.\n+ 当α很大的时候，单独一个根节点就是最优子树。\n\n尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行**交叉验证**，找到最优的那个子树作为我们的决策树。子树序列如下\n$$\nT_0>T_1>T_2>T_3>...>T_n\n$$\n**因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，**在此不再详细介绍。\n\n### 5.Sklearn实现\n\n我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\n#load data\niris=load_iris()\nX=iris.data\ny=iris.target\nclf=tree.DecisionTreeClassifier()\nclf=clf.fit(X,y)\n\n#export the decision tree\nimport graphviz\n#export_graphviz support a variety of aesthetic options\ndot_data=tree.export_graphviz(clf,out_file=None,\n                              feature_names=iris.feature_names,\n                              class_names=iris.target_names,\n                              filled=True,rounded=True,\n                              special_characters=True)\n\ngraph=graphviz.Source(dot_data)\ngraph.view()\n```\n\n![机器学习之分类与回归树CART图片01](机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png)\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/机器学习之分类与回归树-CART.md","raw":"---\ntitle: 机器学习之分类与回归树(CART)\ndate: 2018-04-22 21:58:53\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.分类与回归树简介\n\n分类与回归树的英文是*Classfication And Regression Tree*，缩写为CART。CART算法采用**二分递归分割的技术**将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为**True**和**False**，左分支取值为**True**，右分支取值为**False**，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。\n\n+ 如果待预测分类是离散型数据，则CART生成分类决策树。\n+ 如果待预测分类是连续性数据，则CART生成回归决策树。\n\n### 2.CART分类树\n\n#### 2.1算法详解\n\nCART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为\n\n$$\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\n$$\n根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。\n$$\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2\n$$\n如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n$$\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)\n$$\n对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。\n$$\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))\n$$\n$$\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))\n$$\n#### 2.2实例详解\n\n|  名称  | 体温 | 胎生 | 水生 | 类标记 |\n| :----: | :--: | :--: | :--: | :----: |\n|   人   | 恒温 |  是  |  否  | 哺乳类 |\n|  巨蟒  | 冷血 |  否  |  否  | 爬行类 |\n|  鲑鱼  | 冷血 |  否  |  是  |  鱼类  |\n|   鲸   | 恒温 |  是  |  是  | 哺乳类 |\n|   蛙   | 冷血 |  否  | 有时 |  鱼类  |\n|  巨蜥  | 冷血 |  否  |  否  | 爬行类 |\n|  蝙蝠  | 恒温 |  是  |  否  | 哺乳类 |\n|   猫   | 恒温 |  是  |  否  | 哺乳类 |\n| 豹纹鲨 | 冷血 |  是  |  是  |  鱼类  |\n|  海龟  | 冷血 |  否  | 有时 | 爬行类 |\n|  豪猪  | 恒温 |  是  |  否  | 哺乳类 |\n|   鳗   | 冷血 |  否  |  是  |  鱼类  |\n|  蝾螈  | 冷血 |  否  | 有时 | 两栖类 |\n\n针对上述离散型数据，按照**体温为恒温和非恒温**进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。\n\n$$\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}\n$$\n$$\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}\n$$\n然后计算得到特征**体温**下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。\n$$\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}\n$$\n### 3.CART回归树\n\n#### 3.1算法详解\n\nCART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n$$\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}\n$$\n**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。\n\n$$\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]\n$$\n**用选定的(j,s)对，划分区域并决定相应的输出值**\n$$\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}\n$$\n\n$$\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i\n$$\n\n$$\nx\\epsilon R_m,m=1,2\n$$\n\n**继续对两个子区域调用上述步骤**，**将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。**\n\n$$\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)\n$$\n当输入空间划分确定时，可以用**平方误差**来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。\n$$\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2\n$$\n\n#### 3.2实例详解\n\n| $x_i$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |\n| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |\n\n考虑如上所示的连续性变量，根据给定的数据点，考虑**1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5**切分点。对各切分点依次求出**R1,R2,c1,c2及m(s)**，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。\n\n$$\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56\n$$\n\n$$\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50\n$$\n\n$$\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72\n$$\n\n**依次改变(j,s)对，可以得到s及m(s)的计算结果**，如下表所示。\n\n|  $s$   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |\n| :----: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |\n| $m(s)$ | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |\n\n当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。**回归树T1(x)**为\n$$\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}\n$$\n\n$$\nf_1(x)=T_1(x)\n$$\n\n**然后我们利用f1(x)拟合训练数据的残差**，如下表所示\n\n| $x_i$ |   1   |   2   |   3   |  4   |  5   |  6   |   7   |   8   |  9   |  10  |\n| :---: | :---: | :---: | :---: | :--: | :--: | :--: | :---: | :---: | :--: | :--: |\n| $y_i$ | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |\n\n**用f1(x)拟合训练数据得到平方误差**\n$$\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93\n$$\n第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到\n$$\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}\n$$\n\n$$\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}\n$$\n\n用f2(x)拟合训练数据的平方误差\n$$\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79\n$$\n继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示\n$$\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47\n$$\n\n$$\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30\n$$\n\n$$\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23\n$$\n\n$$\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}\n$$\n\n$$\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}\n$$\n\n用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。\n$$\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71\n$$\n### 4.CART剪枝\n\n> 此处我们介绍代价复杂度剪枝算法\n\n我们将一颗充分生长的树称为**T0** ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下\n$$\nC_\\alpha(T)=C(T)+\\alpha|T|\n$$\n+ T为任意子树，|T|为子树T的叶子节点个数。\n+ α是参数，权衡拟合程度与树的复杂度。\n+ C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。\n\n**那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？**准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。\n\n+ 当α很小的时候，T0 是这样的最优子树.\n+ 当α很大的时候，单独一个根节点就是最优子树。\n\n尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行**交叉验证**，找到最优的那个子树作为我们的决策树。子树序列如下\n$$\nT_0>T_1>T_2>T_3>...>T_n\n$$\n**因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，**在此不再详细介绍。\n\n### 5.Sklearn实现\n\n我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\n#load data\niris=load_iris()\nX=iris.data\ny=iris.target\nclf=tree.DecisionTreeClassifier()\nclf=clf.fit(X,y)\n\n#export the decision tree\nimport graphviz\n#export_graphviz support a variety of aesthetic options\ndot_data=tree.export_graphviz(clf,out_file=None,\n                              feature_names=iris.feature_names,\n                              class_names=iris.target_names,\n                              filled=True,rounded=True,\n                              special_characters=True)\n\ngraph=graphviz.Source(dot_data)\ngraph.view()\n```\n\n![机器学习之分类与回归树CART图片01](机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png)\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"机器学习之分类与回归树-CART","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4c001djiz5boejbr79","content":"<h3 id=\"1-分类与回归树简介\"><a href=\"#1-分类与回归树简介\" class=\"headerlink\" title=\"1.分类与回归树简介\"></a>1.分类与回归树简介</h3><p>分类与回归树的英文是<em>Classfication And Regression Tree</em>，缩写为CART。CART算法采用<strong>二分递归分割的技术</strong>将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为<strong>True</strong>和<strong>False</strong>，左分支取值为<strong>True</strong>，右分支取值为<strong>False</strong>，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。</p>\n<ul>\n<li>如果待预测分类是离散型数据，则CART生成分类决策树。</li>\n<li>如果待预测分类是连续性数据，则CART生成回归决策树。</li>\n</ul>\n<h3 id=\"2-CART分类树\"><a href=\"#2-CART分类树\" class=\"headerlink\" title=\"2.CART分类树\"></a>2.CART分类树</h3><h4 id=\"2-1算法详解\"><a href=\"#2-1算法详解\" class=\"headerlink\" title=\"2.1算法详解\"></a>2.1算法详解</h4><p>CART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为</p>\n<script type=\"math/tex; mode=display\">\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}</script><p>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。</p>\n<script type=\"math/tex; mode=display\">\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2</script><p>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。</p>\n<script type=\"math/tex; mode=display\">\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)</script><p>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。</p>\n<script type=\"math/tex; mode=display\">\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))</script><script type=\"math/tex; mode=display\">\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))</script><h4 id=\"2-2实例详解\"><a href=\"#2-2实例详解\" class=\"headerlink\" title=\"2.2实例详解\"></a>2.2实例详解</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">名称</th>\n<th style=\"text-align:center\">体温</th>\n<th style=\"text-align:center\">胎生</th>\n<th style=\"text-align:center\">水生</th>\n<th style=\"text-align:center\">类标记</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">人</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蟒</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲑鱼</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲸</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蛙</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蜥</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝙蝠</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">猫</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豹纹鲨</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">海龟</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豪猪</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鳗</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝾螈</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">两栖类</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。</p>\n<script type=\"math/tex; mode=display\">\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}</script><script type=\"math/tex; mode=display\">\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}</script><p>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。</p>\n<script type=\"math/tex; mode=display\">\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}</script><h3 id=\"3-CART回归树\"><a href=\"#3-CART回归树\" class=\"headerlink\" title=\"3.CART回归树\"></a>3.CART回归树</h3><h4 id=\"3-1算法详解\"><a href=\"#3-1算法详解\" class=\"headerlink\" title=\"3.1算法详解\"></a>3.1算法详解</h4><p>CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>\n<script type=\"math/tex; mode=display\">\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}</script><p><strong>选择最优切分变量j与切分点s</strong>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。</p>\n<script type=\"math/tex; mode=display\">\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]</script><p><strong>用选定的(j,s)对，划分区域并决定相应的输出值</strong></p>\n<script type=\"math/tex; mode=display\">\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}</script><script type=\"math/tex; mode=display\">\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i</script><script type=\"math/tex; mode=display\">\nx\\epsilon R_m,m=1,2</script><p><strong>继续对两个子区域调用上述步骤</strong>，<strong>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</strong></p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)</script><p>当输入空间划分确定时，可以用<strong>平方误差</strong>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2</script><h4 id=\"3-2实例详解\"><a href=\"#3-2实例详解\" class=\"headerlink\" title=\"3.2实例详解\"></a>3.2实例详解</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">5.56</td>\n<td style=\"text-align:center\">5.70</td>\n<td style=\"text-align:center\">5.91</td>\n<td style=\"text-align:center\">6.40</td>\n<td style=\"text-align:center\">6.80</td>\n<td style=\"text-align:center\">7.05</td>\n<td style=\"text-align:center\">8.90</td>\n<td style=\"text-align:center\">8.70</td>\n<td style=\"text-align:center\">9.00</td>\n<td style=\"text-align:center\">9.05</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。</p>\n<script type=\"math/tex; mode=display\">\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56</script><script type=\"math/tex; mode=display\">\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50</script><script type=\"math/tex; mode=display\">\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72</script><p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$s$</th>\n<th style=\"text-align:center\">1.5</th>\n<th style=\"text-align:center\">2.5</th>\n<th style=\"text-align:center\">3.5</th>\n<th style=\"text-align:center\">4.5</th>\n<th style=\"text-align:center\">5.5</th>\n<th style=\"text-align:center\">6.5</th>\n<th style=\"text-align:center\">7.5</th>\n<th style=\"text-align:center\">8.5</th>\n<th style=\"text-align:center\">9.5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$m(s)$</td>\n<td style=\"text-align:center\">15.72</td>\n<td style=\"text-align:center\">12.07</td>\n<td style=\"text-align:center\">8.36</td>\n<td style=\"text-align:center\">5.78</td>\n<td style=\"text-align:center\">3.91</td>\n<td style=\"text-align:center\">1.93</td>\n<td style=\"text-align:center\">8.01</td>\n<td style=\"text-align:center\">11.73</td>\n<td style=\"text-align:center\">15.74</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为</p>\n<script type=\"math/tex; mode=display\">\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_1(x)=T_1(x)</script><p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>，如下表所示</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">-0.68</td>\n<td style=\"text-align:center\">-0.54</td>\n<td style=\"text-align:center\">-0.33</td>\n<td style=\"text-align:center\">0.16</td>\n<td style=\"text-align:center\">0.56</td>\n<td style=\"text-align:center\">0.81</td>\n<td style=\"text-align:center\">-0.01</td>\n<td style=\"text-align:center\">-0.21</td>\n<td style=\"text-align:center\">0.09</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>用f1(x)拟合训练数据得到平方误差</strong></p>\n<script type=\"math/tex; mode=display\">\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93</script><p>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到</p>\n<script type=\"math/tex; mode=display\">\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}</script><p>用f2(x)拟合训练数据的平方误差</p>\n<script type=\"math/tex; mode=display\">\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79</script><p>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示</p>\n<script type=\"math/tex; mode=display\">\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47</script><script type=\"math/tex; mode=display\">\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30</script><script type=\"math/tex; mode=display\">\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23</script><script type=\"math/tex; mode=display\">\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}</script><p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。</p>\n<script type=\"math/tex; mode=display\">\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71</script><h3 id=\"4-CART剪枝\"><a href=\"#4-CART剪枝\" class=\"headerlink\" title=\"4.CART剪枝\"></a>4.CART剪枝</h3><blockquote>\n<p>此处我们介绍代价复杂度剪枝算法</p>\n</blockquote>\n<p>我们将一颗充分生长的树称为<strong>T0</strong> ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下</p>\n<script type=\"math/tex; mode=display\">\nC_\\alpha(T)=C(T)+\\alpha|T|</script><ul>\n<li>T为任意子树，|T|为子树T的叶子节点个数。</li>\n<li>α是参数，权衡拟合程度与树的复杂度。</li>\n<li>C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。</li>\n</ul>\n<p><strong>那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？</strong>准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。</p>\n<ul>\n<li>当α很小的时候，T0 是这样的最优子树.</li>\n<li>当α很大的时候，单独一个根节点就是最优子树。</li>\n</ul>\n<p>尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行<strong>交叉验证</strong>，找到最优的那个子树作为我们的决策树。子树序列如下</p>\n<script type=\"math/tex; mode=display\">\nT_0>T_1>T_2>T_3>...>T_n</script><p><strong>因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，</strong>在此不再详细介绍。</p>\n<h3 id=\"5-Sklearn实现\"><a href=\"#5-Sklearn实现\" class=\"headerlink\" title=\"5.Sklearn实现\"></a>5.Sklearn实现</h3><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">clf=tree.DecisionTreeClassifier()</span><br><span class=\"line\">clf=clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#export the decision tree</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> graphviz</span><br><span class=\"line\"><span class=\"comment\">#export_graphviz support a variety of aesthetic options</span></span><br><span class=\"line\">dot_data=tree.export_graphviz(clf,out_file=<span class=\"keyword\">None</span>,</span><br><span class=\"line\">                              feature_names=iris.feature_names,</span><br><span class=\"line\">                              class_names=iris.target_names,</span><br><span class=\"line\">                              filled=<span class=\"keyword\">True</span>,rounded=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                              special_characters=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph=graphviz.Source(dot_data)</span><br><span class=\"line\">graph.view()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/22/机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png\" alt=\"机器学习之分类与回归树CART图片01\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-分类与回归树简介\"><a href=\"#1-分类与回归树简介\" class=\"headerlink\" title=\"1.分类与回归树简介\"></a>1.分类与回归树简介</h3><p>分类与回归树的英文是<em>Classfication And Regression Tree</em>，缩写为CART。CART算法采用<strong>二分递归分割的技术</strong>将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为<strong>True</strong>和<strong>False</strong>，左分支取值为<strong>True</strong>，右分支取值为<strong>False</strong>，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。</p>\n<ul>\n<li>如果待预测分类是离散型数据，则CART生成分类决策树。</li>\n<li>如果待预测分类是连续性数据，则CART生成回归决策树。</li>\n</ul>\n<h3 id=\"2-CART分类树\"><a href=\"#2-CART分类树\" class=\"headerlink\" title=\"2.CART分类树\"></a>2.CART分类树</h3><h4 id=\"2-1算法详解\"><a href=\"#2-1算法详解\" class=\"headerlink\" title=\"2.1算法详解\"></a>2.1算法详解</h4><p>CART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为</p>\n<script type=\"math/tex; mode=display\">\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}</script><p>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。</p>\n<script type=\"math/tex; mode=display\">\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2</script><p>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。</p>\n<script type=\"math/tex; mode=display\">\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)</script><p>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。</p>\n<script type=\"math/tex; mode=display\">\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))</script><script type=\"math/tex; mode=display\">\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))</script><h4 id=\"2-2实例详解\"><a href=\"#2-2实例详解\" class=\"headerlink\" title=\"2.2实例详解\"></a>2.2实例详解</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">名称</th>\n<th style=\"text-align:center\">体温</th>\n<th style=\"text-align:center\">胎生</th>\n<th style=\"text-align:center\">水生</th>\n<th style=\"text-align:center\">类标记</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">人</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蟒</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲑鱼</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲸</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蛙</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蜥</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝙蝠</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">猫</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豹纹鲨</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">海龟</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豪猪</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鳗</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝾螈</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">两栖类</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。</p>\n<script type=\"math/tex; mode=display\">\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}</script><script type=\"math/tex; mode=display\">\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}</script><p>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。</p>\n<script type=\"math/tex; mode=display\">\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}</script><h3 id=\"3-CART回归树\"><a href=\"#3-CART回归树\" class=\"headerlink\" title=\"3.CART回归树\"></a>3.CART回归树</h3><h4 id=\"3-1算法详解\"><a href=\"#3-1算法详解\" class=\"headerlink\" title=\"3.1算法详解\"></a>3.1算法详解</h4><p>CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>\n<script type=\"math/tex; mode=display\">\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}</script><p><strong>选择最优切分变量j与切分点s</strong>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。</p>\n<script type=\"math/tex; mode=display\">\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]</script><p><strong>用选定的(j,s)对，划分区域并决定相应的输出值</strong></p>\n<script type=\"math/tex; mode=display\">\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}</script><script type=\"math/tex; mode=display\">\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i</script><script type=\"math/tex; mode=display\">\nx\\epsilon R_m,m=1,2</script><p><strong>继续对两个子区域调用上述步骤</strong>，<strong>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</strong></p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)</script><p>当输入空间划分确定时，可以用<strong>平方误差</strong>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2</script><h4 id=\"3-2实例详解\"><a href=\"#3-2实例详解\" class=\"headerlink\" title=\"3.2实例详解\"></a>3.2实例详解</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">5.56</td>\n<td style=\"text-align:center\">5.70</td>\n<td style=\"text-align:center\">5.91</td>\n<td style=\"text-align:center\">6.40</td>\n<td style=\"text-align:center\">6.80</td>\n<td style=\"text-align:center\">7.05</td>\n<td style=\"text-align:center\">8.90</td>\n<td style=\"text-align:center\">8.70</td>\n<td style=\"text-align:center\">9.00</td>\n<td style=\"text-align:center\">9.05</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。</p>\n<script type=\"math/tex; mode=display\">\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56</script><script type=\"math/tex; mode=display\">\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50</script><script type=\"math/tex; mode=display\">\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72</script><p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$s$</th>\n<th style=\"text-align:center\">1.5</th>\n<th style=\"text-align:center\">2.5</th>\n<th style=\"text-align:center\">3.5</th>\n<th style=\"text-align:center\">4.5</th>\n<th style=\"text-align:center\">5.5</th>\n<th style=\"text-align:center\">6.5</th>\n<th style=\"text-align:center\">7.5</th>\n<th style=\"text-align:center\">8.5</th>\n<th style=\"text-align:center\">9.5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$m(s)$</td>\n<td style=\"text-align:center\">15.72</td>\n<td style=\"text-align:center\">12.07</td>\n<td style=\"text-align:center\">8.36</td>\n<td style=\"text-align:center\">5.78</td>\n<td style=\"text-align:center\">3.91</td>\n<td style=\"text-align:center\">1.93</td>\n<td style=\"text-align:center\">8.01</td>\n<td style=\"text-align:center\">11.73</td>\n<td style=\"text-align:center\">15.74</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为</p>\n<script type=\"math/tex; mode=display\">\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_1(x)=T_1(x)</script><p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>，如下表所示</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">-0.68</td>\n<td style=\"text-align:center\">-0.54</td>\n<td style=\"text-align:center\">-0.33</td>\n<td style=\"text-align:center\">0.16</td>\n<td style=\"text-align:center\">0.56</td>\n<td style=\"text-align:center\">0.81</td>\n<td style=\"text-align:center\">-0.01</td>\n<td style=\"text-align:center\">-0.21</td>\n<td style=\"text-align:center\">0.09</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>用f1(x)拟合训练数据得到平方误差</strong></p>\n<script type=\"math/tex; mode=display\">\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93</script><p>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到</p>\n<script type=\"math/tex; mode=display\">\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}</script><p>用f2(x)拟合训练数据的平方误差</p>\n<script type=\"math/tex; mode=display\">\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79</script><p>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示</p>\n<script type=\"math/tex; mode=display\">\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47</script><script type=\"math/tex; mode=display\">\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30</script><script type=\"math/tex; mode=display\">\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23</script><script type=\"math/tex; mode=display\">\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}</script><p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。</p>\n<script type=\"math/tex; mode=display\">\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71</script><h3 id=\"4-CART剪枝\"><a href=\"#4-CART剪枝\" class=\"headerlink\" title=\"4.CART剪枝\"></a>4.CART剪枝</h3><blockquote>\n<p>此处我们介绍代价复杂度剪枝算法</p>\n</blockquote>\n<p>我们将一颗充分生长的树称为<strong>T0</strong> ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下</p>\n<script type=\"math/tex; mode=display\">\nC_\\alpha(T)=C(T)+\\alpha|T|</script><ul>\n<li>T为任意子树，|T|为子树T的叶子节点个数。</li>\n<li>α是参数，权衡拟合程度与树的复杂度。</li>\n<li>C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。</li>\n</ul>\n<p><strong>那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？</strong>准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。</p>\n<ul>\n<li>当α很小的时候，T0 是这样的最优子树.</li>\n<li>当α很大的时候，单独一个根节点就是最优子树。</li>\n</ul>\n<p>尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行<strong>交叉验证</strong>，找到最优的那个子树作为我们的决策树。子树序列如下</p>\n<script type=\"math/tex; mode=display\">\nT_0>T_1>T_2>T_3>...>T_n</script><p><strong>因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，</strong>在此不再详细介绍。</p>\n<h3 id=\"5-Sklearn实现\"><a href=\"#5-Sklearn实现\" class=\"headerlink\" title=\"5.Sklearn实现\"></a>5.Sklearn实现</h3><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">clf=tree.DecisionTreeClassifier()</span><br><span class=\"line\">clf=clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#export the decision tree</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> graphviz</span><br><span class=\"line\"><span class=\"comment\">#export_graphviz support a variety of aesthetic options</span></span><br><span class=\"line\">dot_data=tree.export_graphviz(clf,out_file=<span class=\"keyword\">None</span>,</span><br><span class=\"line\">                              feature_names=iris.feature_names,</span><br><span class=\"line\">                              class_names=iris.target_names,</span><br><span class=\"line\">                              filled=<span class=\"keyword\">True</span>,rounded=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                              special_characters=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph=graphviz.Source(dot_data)</span><br><span class=\"line\">graph.view()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/04/22/机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png\" alt=\"机器学习之分类与回归树CART图片01\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"机器学习之最大期望(EM)算法","date":"2018-05-09T02:20:41.000Z","mathjax":true,"comments":1,"_content":"\n### 1.EM算法简介\n\n**最大期望(Expectation Maximum)算法**是一种迭代优化算法，其计算方法是每次迭代分为**期望(E)步**和**最大(M)步**。我们先看下最大期望算法能够解决什么样的问题。\n\n假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。\n$$\nN_1(\\mu_1,\\sigma_1^2)\\ ; N_2(\\mu_2,\\sigma_2^2)\n$$\n但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ \n\nEM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。\n\n### 2.EM算法实例\n\n假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。\n\n| 硬币 |    结果    |  统计   |\n| :--: | :--------: | :-----: |\n|  1   | 正正反正反 | 3正-2反 |\n|  2   | 反反正正反 | 2正-3反 |\n|  1   | 正反反反反 | 1正-4反 |\n|  2   | 正反反正正 | 3正-2反 |\n|  1   | 反正正反反 | 2正-3反 |\n\n我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。\n$$\nP1=\\frac{3+1+2}{15}=0.4\n$$\n\n$$\nP2=\\frac{2+3}{10}=0.5\n$$\n\n下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?\n\n|  硬币   |    结果    |  统计   |\n| :-----: | :--------: | :-----: |\n| Unknown | 正正反正反 | 3正-2反 |\n| Unknown | 反反正正反 | 2正-3反 |\n| Unknown | 正反反反反 | 1正-4反 |\n| Unknown | 正反反正正 | 3正-2反 |\n| Unknown | 反正正反反 | 2正-3反 |\n\n此时我们加入**隐含变量z**，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。\n\n我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。\n\n例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2\\*0.2\\*0.2\\*0.8\\*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7\\*0.7\\*0.7\\*0.3\\*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。\n\n| 轮数 | 若是硬币1 | 若是硬币2 | 最有可能硬币 |\n| :--: | :-------: | :-------: | :----------: |\n|  1   |  0.00512  |  0.03087  |    硬币2     |\n|  2   |  0.02048  |  0.01323  |    硬币1     |\n|  3   |  0.08192  |  0.00567  |    硬币1     |\n|  4   |  0.00512  |  0.03087  |    硬币2     |\n|  5   |  0.02048  |  0.01323  |    硬币1     |\n\n我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到\n$$\nP1=\\frac{2+1+2}{15}=0.33\n$$\n\n$$\nP2=\\frac{3+3}{10}=0.6\n$$\n\nP1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。\n\n| 初始化的P1 | 估计的P1 | 真实的P1 | 初始化的P2 | 估计的P2 | 真实的P2 |\n| :--------: | :------: | :------: | :--------: | :------: | :------: |\n|    0.2     |   0.33   |   0.4    |    0.7     |   0.6    |   0.5    |\n\n可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。\n\n上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?\n\n但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。\n\n| 轮数 | 若是硬币1 | 若是硬币2 |\n| :--: | :-------: | :-------: |\n|  1   |  0.00512  |  0.03087  |\n|  2   |  0.02048  |  0.01323  |\n|  3   |  0.08192  |  0.00567  |\n|  4   |  0.00512  |  0.03087  |\n|  5   |  0.02048  |  0.01323  |\n\n利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率\n$$\nz_1=\\frac{0.00512}{0.00512+0.03087}=0.14\n$$\n相应的算出其他4轮的概率。\n\n| 轮数 | z_i=硬币1 | z_i=硬币2 |\n| :--: | :-------: | :-------: |\n|  1   |   0.14    |   0.86    |\n|  2   |   0.61    |   0.39    |\n|  3   |   0.94    |   0.06    |\n|  4   |   0.14    |   0.86    |\n|  5   |   0.61    |   0.39    |\n\n上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为**E步**。\n\n按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14\\*3=0.42的概率为正，有0.14\\*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为**M步**。\n\n| 轮数 | 正面 | 反面 |\n| :--: | :--: | :--: |\n|  1   | 0.42 | 0.28 |\n|  2   | 1.22 | 1.83 |\n|  3   | 0.94 | 3.76 |\n|  4   | 0.42 | 0.28 |\n|  5   | 1.22 | 1.93 |\n| 总计 | 4.22 | 7.98 |\n\n$$\nP1=\\frac{4.22}{4.22+7.98}=0.35\n$$\n上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。\n\n### 3.EM算法推导\n\n对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数$\\theta$，极大化模型分布的对数似然函数如下所示\n$$\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)\n$$\n\n\n如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下\n$$\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)\n$$\n\n$$\n=\\arg \\max_{\\theta} \\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)\n$$\n\n上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入**Jensen不等式**。\n\n> 设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。\n>\n> **Jensen不等式定义：**如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。\n\n我们再回到上述推导过程，得到如下方程式。\n$$\n\\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta) \n$$\n\n$$\n=\\sum _{i=1}^{m}log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\\ \\ \\ \\ \\ \\ \\ (1)\n$$\n\n$$\n\\ge \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})} \\ \\ \\ \\ \\ \\ \\ (2)\n$$\n\n我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。\n\n首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$是$\\frac{P(x^{(i)},z^{(i);\\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$便相当于f(E(X))，同时$\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。\n\n如果要满足Jensen不等式的等号，那么需要满足X为常量，即为\n$$\n\\frac{P(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c,\\ c为常量\n$$\n那么稍加改变能够得到\n$$\nc Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\\theta)\\ ,c为常量\n$$\n\n$$\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta),\\ c为常量\n$$\n\n其中c是不依赖于$z^{(i)}$的常量，由于$Q_i(z^{(i)})$是一个分布，所以满足\n$$\n\\sum_{z}Q_i(z^{(i)})=1\n$$\n\n$$\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta)= c\n$$\n\n因此得到下列方程，其中方程(3)利用到条件概率公式。\n$$\nQ_i(z^{(i)})=\\frac{P(x^{(i)},z^{(i)};\\theta)}{c}=\\frac{P(x^{(i)},z^{(i)};\\theta)}{\\sum _z P(x^{(i)},z^{(i)};\\theta)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n$$\n$$\n=\\frac{P(x^{(i)},z^{(i)};\\theta)}{ P(x^{(i)};\\theta)}=P(z^{(i)}|x^{(i)};\\theta)  \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\   (3)\n$$\n\n\n\n如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)$，那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式\n$$\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\n$$\n去掉上式中常数部分，则我们需要极大化的对数似然下界为\n$$\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\\theta)-log {Q_i(z^{(i)})}]\n$$\n$$\n=\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)\n$$\n\n注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)$可以理解为$logP(x^{(i)},z^{(i)};\\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中**E步**。极大化方程式(4)也就是我们EM算法中的**M步**。\n\n### 4.EM算法流程\n\n现在我们总结下EM算法流程。\n\n输入：观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，联合分布$P(x^{(i)},z^{(i)};\\theta)$，条件分布$P(z^{(i)}|x^{(i)};\\theta)$，最大迭代次数$J$。\n\n+ 随机初始化模型参数$\\theta$的初始值$\\theta^0$。\n\n+ $for\\  j\\  from \\ 1\\  to \\ J$开始EM算法迭代。\n\n  + E步:计算联合分布的条件概率期望\n\n  $$\n  Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)\n  $$\n\n  $$\n  L(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\n  $$\n\n  + M步:极大化$L(\\theta,\\theta^j)$，得到$\\theta^{j+1}$\n\n  $$\n  \\theta^{j+1}=\\arg \\max _{x} L(\\theta,\\theta^j)\n  $$\n\n  + 如果$\\theta^{j+1}$收敛，则算法结束，否则继续迭代。\n\n+ 输出模型参数$\\theta$。\n\n### 5.EM算法的收敛性\n\n我们现在来解答下**2.EM算法实例**中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？\n\n首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即\n$$\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})\\ge \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j})\n$$\n由于\n$$\nL(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(x^{(i)},z^{(i)};\\theta)\n$$\n令\n\n\n$$\nH(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(z^{(i)}|x^{(i)};\\theta)\n$$\n上两式相减得到\n\n\n$$\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta)=L(\\theta,\\theta^j)-H(\\theta,\\theta^j)\n$$\n在上式中分别取$\\theta$为$\\theta^j$和$\\theta^{j+1}$，并相减得到\n\n\n$$\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta^{j+1})-\\sum_{i=1}^{m}log P(x^{(i)};\\theta^j)=[L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)]-[H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)]\n$$\n要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。由于$\\theta^{j+1}$使得$L(\\theta,\\theta^j)$极大，因此有\n\n\n$$\nL(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)\\ge 0\n$$\n而对于第二部分，我们有\n\n\n$$\nH(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log \\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\ \\ \\ \\ \\ \\ \\ \\ \\ (5)\n$$\n$$\n\\le \\sum _{i=1}^{m}log \\{ \\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)\\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\}\\ \\ \\ \\ \\ \\ \\ \\ \\ (6)\n$$\n\n$$\n= \\sum _{i=1}^{m}log (\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{j+1}))=0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7)\n$$\n\n其中第(6)式用到了Jensen不等式，只不过和第**3.EM算法推导**中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到\n$$\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})- \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j}) \\ge 0\n$$\n证明了EM算法的收敛性。\n\n从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。\n\n### 6.Sklearn实现EM算法\n\n高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去[这儿](https://mp.weixin.qq.com/s?src=11&timestamp=1525932817&ver=867&signature=U4UxviydVELD71Ju*bRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm*budZlpZdMQMLOcXB-FnvKKlkV2H&new=1)。下列代码来自于[Sklearn官网GMM模块](http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py)，利用高斯混合模型确定iris聚类。\n\n```python\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import StratifiedKFold\n\ncolors = ['navy', 'turquoise', 'darkorange']\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate(colors):\n        if gmm.covariance_type == 'full':\n            covariances = gmm.covariances_[n][:2, :2]\n        elif gmm.covariance_type == 'tied':\n            covariances = gmm.covariances_[:2, :2]\n        elif gmm.covariance_type == 'diag':\n            covariances = np.diag(gmm.covariances_[n][:2])\n        elif gmm.covariance_type == 'spherical':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n                                  180 + angle, color=color)\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n\niris = datasets.load_iris()\n\n# Break up the dataset into non-overlapping training (75%)\n# and testing (25%) sets.\nskf = StratifiedKFold(n_splits=4)\n# Only take the first fold.\ntrain_index, test_index = next(iter(skf.split(iris.data, iris.target)))\n\n\nX_train = iris.data[train_index]\ny_train = iris.target[train_index]\nX_test = iris.data[test_index]\ny_test = iris.target[test_index]\n\nn_classes = len(np.unique(y_train))\n\n# Try GMMs using different types of covariances.\nestimators = dict((cov_type, GaussianMixture(n_components=n_classes,\n                   covariance_type=cov_type, max_iter=20, random_state=0))\n                  for cov_type in ['spherical', 'diag', 'tied', 'full'])\n\nn_estimators = len(estimators)\n\nplt.figure(figsize=(3 * n_estimators // 2, 6))\nplt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n                    left=.01, right=.99)\n\n\nfor index, (name, estimator) in enumerate(estimators.items()):\n    # Since we have class labels for the training data, we can\n    # initialize the GMM parameters in a supervised manner.\n    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n                                    for i in range(n_classes)])\n\n    # Train the other parameters using the EM algorithm.\n    estimator.fit(X_train)\n\n    h = plt.subplot(2, n_estimators // 2, index + 1)\n    make_ellipses(estimator, h)\n\n    for n, color in enumerate(colors):\n        data = iris.data[iris.target == n]\n        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n                    label=iris.target_names[n])\n    # Plot the test data with crosses\n    for n, color in enumerate(colors):\n        data = X_test[y_test == n]\n        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n\n    y_train_pred = estimator.predict(X_train)\n    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n             transform=h.transAxes)\n\n    y_test_pred = estimator.predict(X_test)\n    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n             transform=h.transAxes)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(name)\n\nplt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\nplt.show()\n```\n\n![机器学习之最大期望算法图片01](机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png)\n\n### 7.EM算法优缺点\n\n#### 7.1优点\n\n+ 聚类。\n\n+ 算法计算结果稳定、准确。\n\n+ EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。\n\n#### 7.2缺点\n\n+ 对初始化数据敏感。\n\n+ EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。\n\n+ 当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n+ [milter_如何感性地理解EM算法](https://www.jianshu.com/p/1121509ac1dc)\n+ [刘建平Pinard_EM算法原理总结](https://www.cnblogs.com/pinard/p/6912636.html)\n+ [Gaussian mixture models](http://scikit-learn.org/stable/modules/mixture.html)\n\n","source":"_posts/机器学习之最大期望-EM-算法.md","raw":"---\ntitle: 机器学习之最大期望(EM)算法\ndate: 2018-05-09 10:20:41\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.EM算法简介\n\n**最大期望(Expectation Maximum)算法**是一种迭代优化算法，其计算方法是每次迭代分为**期望(E)步**和**最大(M)步**。我们先看下最大期望算法能够解决什么样的问题。\n\n假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。\n$$\nN_1(\\mu_1,\\sigma_1^2)\\ ; N_2(\\mu_2,\\sigma_2^2)\n$$\n但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ \n\nEM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。\n\n### 2.EM算法实例\n\n假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。\n\n| 硬币 |    结果    |  统计   |\n| :--: | :--------: | :-----: |\n|  1   | 正正反正反 | 3正-2反 |\n|  2   | 反反正正反 | 2正-3反 |\n|  1   | 正反反反反 | 1正-4反 |\n|  2   | 正反反正正 | 3正-2反 |\n|  1   | 反正正反反 | 2正-3反 |\n\n我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。\n$$\nP1=\\frac{3+1+2}{15}=0.4\n$$\n\n$$\nP2=\\frac{2+3}{10}=0.5\n$$\n\n下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?\n\n|  硬币   |    结果    |  统计   |\n| :-----: | :--------: | :-----: |\n| Unknown | 正正反正反 | 3正-2反 |\n| Unknown | 反反正正反 | 2正-3反 |\n| Unknown | 正反反反反 | 1正-4反 |\n| Unknown | 正反反正正 | 3正-2反 |\n| Unknown | 反正正反反 | 2正-3反 |\n\n此时我们加入**隐含变量z**，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。\n\n我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。\n\n例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2\\*0.2\\*0.2\\*0.8\\*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7\\*0.7\\*0.7\\*0.3\\*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。\n\n| 轮数 | 若是硬币1 | 若是硬币2 | 最有可能硬币 |\n| :--: | :-------: | :-------: | :----------: |\n|  1   |  0.00512  |  0.03087  |    硬币2     |\n|  2   |  0.02048  |  0.01323  |    硬币1     |\n|  3   |  0.08192  |  0.00567  |    硬币1     |\n|  4   |  0.00512  |  0.03087  |    硬币2     |\n|  5   |  0.02048  |  0.01323  |    硬币1     |\n\n我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到\n$$\nP1=\\frac{2+1+2}{15}=0.33\n$$\n\n$$\nP2=\\frac{3+3}{10}=0.6\n$$\n\nP1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。\n\n| 初始化的P1 | 估计的P1 | 真实的P1 | 初始化的P2 | 估计的P2 | 真实的P2 |\n| :--------: | :------: | :------: | :--------: | :------: | :------: |\n|    0.2     |   0.33   |   0.4    |    0.7     |   0.6    |   0.5    |\n\n可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。\n\n上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?\n\n但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。\n\n| 轮数 | 若是硬币1 | 若是硬币2 |\n| :--: | :-------: | :-------: |\n|  1   |  0.00512  |  0.03087  |\n|  2   |  0.02048  |  0.01323  |\n|  3   |  0.08192  |  0.00567  |\n|  4   |  0.00512  |  0.03087  |\n|  5   |  0.02048  |  0.01323  |\n\n利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率\n$$\nz_1=\\frac{0.00512}{0.00512+0.03087}=0.14\n$$\n相应的算出其他4轮的概率。\n\n| 轮数 | z_i=硬币1 | z_i=硬币2 |\n| :--: | :-------: | :-------: |\n|  1   |   0.14    |   0.86    |\n|  2   |   0.61    |   0.39    |\n|  3   |   0.94    |   0.06    |\n|  4   |   0.14    |   0.86    |\n|  5   |   0.61    |   0.39    |\n\n上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为**E步**。\n\n按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14\\*3=0.42的概率为正，有0.14\\*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为**M步**。\n\n| 轮数 | 正面 | 反面 |\n| :--: | :--: | :--: |\n|  1   | 0.42 | 0.28 |\n|  2   | 1.22 | 1.83 |\n|  3   | 0.94 | 3.76 |\n|  4   | 0.42 | 0.28 |\n|  5   | 1.22 | 1.93 |\n| 总计 | 4.22 | 7.98 |\n\n$$\nP1=\\frac{4.22}{4.22+7.98}=0.35\n$$\n上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。\n\n### 3.EM算法推导\n\n对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数$\\theta$，极大化模型分布的对数似然函数如下所示\n$$\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)\n$$\n\n\n如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下\n$$\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)\n$$\n\n$$\n=\\arg \\max_{\\theta} \\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)\n$$\n\n上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入**Jensen不等式**。\n\n> 设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。\n>\n> **Jensen不等式定义：**如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。\n\n我们再回到上述推导过程，得到如下方程式。\n$$\n\\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta) \n$$\n\n$$\n=\\sum _{i=1}^{m}log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\\ \\ \\ \\ \\ \\ \\ (1)\n$$\n\n$$\n\\ge \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})} \\ \\ \\ \\ \\ \\ \\ (2)\n$$\n\n我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。\n\n首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$是$\\frac{P(x^{(i)},z^{(i);\\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$便相当于f(E(X))，同时$\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。\n\n如果要满足Jensen不等式的等号，那么需要满足X为常量，即为\n$$\n\\frac{P(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c,\\ c为常量\n$$\n那么稍加改变能够得到\n$$\nc Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\\theta)\\ ,c为常量\n$$\n\n$$\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta),\\ c为常量\n$$\n\n其中c是不依赖于$z^{(i)}$的常量，由于$Q_i(z^{(i)})$是一个分布，所以满足\n$$\n\\sum_{z}Q_i(z^{(i)})=1\n$$\n\n$$\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta)= c\n$$\n\n因此得到下列方程，其中方程(3)利用到条件概率公式。\n$$\nQ_i(z^{(i)})=\\frac{P(x^{(i)},z^{(i)};\\theta)}{c}=\\frac{P(x^{(i)},z^{(i)};\\theta)}{\\sum _z P(x^{(i)},z^{(i)};\\theta)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n$$\n$$\n=\\frac{P(x^{(i)},z^{(i)};\\theta)}{ P(x^{(i)};\\theta)}=P(z^{(i)}|x^{(i)};\\theta)  \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\   (3)\n$$\n\n\n\n如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)$，那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式\n$$\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\n$$\n去掉上式中常数部分，则我们需要极大化的对数似然下界为\n$$\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\\theta)-log {Q_i(z^{(i)})}]\n$$\n$$\n=\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)\n$$\n\n注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)$可以理解为$logP(x^{(i)},z^{(i)};\\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中**E步**。极大化方程式(4)也就是我们EM算法中的**M步**。\n\n### 4.EM算法流程\n\n现在我们总结下EM算法流程。\n\n输入：观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，联合分布$P(x^{(i)},z^{(i)};\\theta)$，条件分布$P(z^{(i)}|x^{(i)};\\theta)$，最大迭代次数$J$。\n\n+ 随机初始化模型参数$\\theta$的初始值$\\theta^0$。\n\n+ $for\\  j\\  from \\ 1\\  to \\ J$开始EM算法迭代。\n\n  + E步:计算联合分布的条件概率期望\n\n  $$\n  Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)\n  $$\n\n  $$\n  L(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\n  $$\n\n  + M步:极大化$L(\\theta,\\theta^j)$，得到$\\theta^{j+1}$\n\n  $$\n  \\theta^{j+1}=\\arg \\max _{x} L(\\theta,\\theta^j)\n  $$\n\n  + 如果$\\theta^{j+1}$收敛，则算法结束，否则继续迭代。\n\n+ 输出模型参数$\\theta$。\n\n### 5.EM算法的收敛性\n\n我们现在来解答下**2.EM算法实例**中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？\n\n首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即\n$$\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})\\ge \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j})\n$$\n由于\n$$\nL(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(x^{(i)},z^{(i)};\\theta)\n$$\n令\n\n\n$$\nH(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(z^{(i)}|x^{(i)};\\theta)\n$$\n上两式相减得到\n\n\n$$\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta)=L(\\theta,\\theta^j)-H(\\theta,\\theta^j)\n$$\n在上式中分别取$\\theta$为$\\theta^j$和$\\theta^{j+1}$，并相减得到\n\n\n$$\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta^{j+1})-\\sum_{i=1}^{m}log P(x^{(i)};\\theta^j)=[L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)]-[H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)]\n$$\n要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。由于$\\theta^{j+1}$使得$L(\\theta,\\theta^j)$极大，因此有\n\n\n$$\nL(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)\\ge 0\n$$\n而对于第二部分，我们有\n\n\n$$\nH(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log \\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\ \\ \\ \\ \\ \\ \\ \\ \\ (5)\n$$\n$$\n\\le \\sum _{i=1}^{m}log \\{ \\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)\\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\}\\ \\ \\ \\ \\ \\ \\ \\ \\ (6)\n$$\n\n$$\n= \\sum _{i=1}^{m}log (\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{j+1}))=0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7)\n$$\n\n其中第(6)式用到了Jensen不等式，只不过和第**3.EM算法推导**中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到\n$$\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})- \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j}) \\ge 0\n$$\n证明了EM算法的收敛性。\n\n从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。\n\n### 6.Sklearn实现EM算法\n\n高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去[这儿](https://mp.weixin.qq.com/s?src=11&timestamp=1525932817&ver=867&signature=U4UxviydVELD71Ju*bRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm*budZlpZdMQMLOcXB-FnvKKlkV2H&new=1)。下列代码来自于[Sklearn官网GMM模块](http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py)，利用高斯混合模型确定iris聚类。\n\n```python\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import StratifiedKFold\n\ncolors = ['navy', 'turquoise', 'darkorange']\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate(colors):\n        if gmm.covariance_type == 'full':\n            covariances = gmm.covariances_[n][:2, :2]\n        elif gmm.covariance_type == 'tied':\n            covariances = gmm.covariances_[:2, :2]\n        elif gmm.covariance_type == 'diag':\n            covariances = np.diag(gmm.covariances_[n][:2])\n        elif gmm.covariance_type == 'spherical':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n                                  180 + angle, color=color)\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n\niris = datasets.load_iris()\n\n# Break up the dataset into non-overlapping training (75%)\n# and testing (25%) sets.\nskf = StratifiedKFold(n_splits=4)\n# Only take the first fold.\ntrain_index, test_index = next(iter(skf.split(iris.data, iris.target)))\n\n\nX_train = iris.data[train_index]\ny_train = iris.target[train_index]\nX_test = iris.data[test_index]\ny_test = iris.target[test_index]\n\nn_classes = len(np.unique(y_train))\n\n# Try GMMs using different types of covariances.\nestimators = dict((cov_type, GaussianMixture(n_components=n_classes,\n                   covariance_type=cov_type, max_iter=20, random_state=0))\n                  for cov_type in ['spherical', 'diag', 'tied', 'full'])\n\nn_estimators = len(estimators)\n\nplt.figure(figsize=(3 * n_estimators // 2, 6))\nplt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n                    left=.01, right=.99)\n\n\nfor index, (name, estimator) in enumerate(estimators.items()):\n    # Since we have class labels for the training data, we can\n    # initialize the GMM parameters in a supervised manner.\n    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n                                    for i in range(n_classes)])\n\n    # Train the other parameters using the EM algorithm.\n    estimator.fit(X_train)\n\n    h = plt.subplot(2, n_estimators // 2, index + 1)\n    make_ellipses(estimator, h)\n\n    for n, color in enumerate(colors):\n        data = iris.data[iris.target == n]\n        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n                    label=iris.target_names[n])\n    # Plot the test data with crosses\n    for n, color in enumerate(colors):\n        data = X_test[y_test == n]\n        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n\n    y_train_pred = estimator.predict(X_train)\n    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n             transform=h.transAxes)\n\n    y_test_pred = estimator.predict(X_test)\n    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n             transform=h.transAxes)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(name)\n\nplt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\nplt.show()\n```\n\n![机器学习之最大期望算法图片01](机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png)\n\n### 7.EM算法优缺点\n\n#### 7.1优点\n\n+ 聚类。\n\n+ 算法计算结果稳定、准确。\n\n+ EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。\n\n#### 7.2缺点\n\n+ 对初始化数据敏感。\n\n+ EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。\n\n+ 当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n+ [milter_如何感性地理解EM算法](https://www.jianshu.com/p/1121509ac1dc)\n+ [刘建平Pinard_EM算法原理总结](https://www.cnblogs.com/pinard/p/6912636.html)\n+ [Gaussian mixture models](http://scikit-learn.org/stable/modules/mixture.html)\n\n","slug":"机器学习之最大期望-EM-算法","published":1,"updated":"2018-06-26T04:10:49.801Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4e001fjiz5jjkjhk8p","content":"<h3 id=\"1-EM算法简介\"><a href=\"#1-EM算法简介\" class=\"headerlink\" title=\"1.EM算法简介\"></a>1.EM算法简介</h3><p><strong>最大期望(Expectation Maximum)算法</strong>是一种迭代优化算法，其计算方法是每次迭代分为<strong>期望(E)步</strong>和<strong>最大(M)步</strong>。我们先看下最大期望算法能够解决什么样的问题。</p>\n<p>假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。</p>\n<script type=\"math/tex; mode=display\">\nN_1(\\mu_1,\\sigma_1^2)\\ ; N_2(\\mu_2,\\sigma_2^2)</script><p>但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ </p>\n<p>EM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。</p>\n<h3 id=\"2-EM算法实例\"><a href=\"#2-EM算法实例\" class=\"headerlink\" title=\"2.EM算法实例\"></a>2.EM算法实例</h3><p>假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">硬币</th>\n<th style=\"text-align:center\">结果</th>\n<th style=\"text-align:center\">统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正正反正反</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">反反正正反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正反反反反</td>\n<td style=\"text-align:center\">1正-4反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">正反反正正</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">反正正反反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。</p>\n<script type=\"math/tex; mode=display\">\nP1=\\frac{3+1+2}{15}=0.4</script><script type=\"math/tex; mode=display\">\nP2=\\frac{2+3}{10}=0.5</script><p>下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">硬币</th>\n<th style=\"text-align:center\">结果</th>\n<th style=\"text-align:center\">统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正正反正反</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">反反正正反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正反反反反</td>\n<td style=\"text-align:center\">1正-4反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正反反正正</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">反正正反反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>此时我们加入<strong>隐含变量z</strong>，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。</p>\n<p>我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。</p>\n<p>例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2*0.2*0.2*0.8*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7*0.7*0.7*0.3*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">若是硬币1</th>\n<th style=\"text-align:center\">若是硬币2</th>\n<th style=\"text-align:center\">最有可能硬币</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n<td style=\"text-align:center\">硬币2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.08192</td>\n<td style=\"text-align:center\">0.00567</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n<td style=\"text-align:center\">硬币2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到</p>\n<script type=\"math/tex; mode=display\">\nP1=\\frac{2+1+2}{15}=0.33</script><script type=\"math/tex; mode=display\">\nP2=\\frac{3+3}{10}=0.6</script><p>P1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">初始化的P1</th>\n<th style=\"text-align:center\">估计的P1</th>\n<th style=\"text-align:center\">真实的P1</th>\n<th style=\"text-align:center\">初始化的P2</th>\n<th style=\"text-align:center\">估计的P2</th>\n<th style=\"text-align:center\">真实的P2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.33</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">0.5</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。</p>\n<p>上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?</p>\n<p>但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">若是硬币1</th>\n<th style=\"text-align:center\">若是硬币2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.08192</td>\n<td style=\"text-align:center\">0.00567</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率</p>\n<script type=\"math/tex; mode=display\">\nz_1=\\frac{0.00512}{0.00512+0.03087}=0.14</script><p>相应的算出其他4轮的概率。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">z_i=硬币1</th>\n<th style=\"text-align:center\">z_i=硬币2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.14</td>\n<td style=\"text-align:center\">0.86</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.61</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.94</td>\n<td style=\"text-align:center\">0.06</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.14</td>\n<td style=\"text-align:center\">0.86</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.61</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为<strong>E步</strong>。</p>\n<p>按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14*3=0.42的概率为正，有0.14*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为<strong>M步</strong>。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">正面</th>\n<th style=\"text-align:center\">反面</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.42</td>\n<td style=\"text-align:center\">0.28</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1.22</td>\n<td style=\"text-align:center\">1.83</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.94</td>\n<td style=\"text-align:center\">3.76</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.42</td>\n<td style=\"text-align:center\">0.28</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">1.22</td>\n<td style=\"text-align:center\">1.93</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">总计</td>\n<td style=\"text-align:center\">4.22</td>\n<td style=\"text-align:center\">7.98</td>\n</tr>\n</tbody>\n</table>\n</div>\n<script type=\"math/tex; mode=display\">\nP1=\\frac{4.22}{4.22+7.98}=0.35</script><p>上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。</p>\n<h3 id=\"3-EM算法推导\"><a href=\"#3-EM算法推导\" class=\"headerlink\" title=\"3.EM算法推导\"></a>3.EM算法推导</h3><p>对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数$\\theta$，极大化模型分布的对数似然函数如下所示</p>\n<script type=\"math/tex; mode=display\">\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)</script><p>如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下</p>\n<script type=\"math/tex; mode=display\">\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)</script><script type=\"math/tex; mode=display\">\n=\\arg \\max_{\\theta} \\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)</script><p>上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入<strong>Jensen不等式</strong>。</p>\n<blockquote>\n<p>设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。</p>\n<p><strong>Jensen不等式定义：</strong>如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。</p>\n</blockquote>\n<p>我们再回到上述推导过程，得到如下方程式。</p>\n<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)</script><script type=\"math/tex; mode=display\">\n=\\sum _{i=1}^{m}log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\\ \\ \\ \\ \\ \\ \\ (1)</script><script type=\"math/tex; mode=display\">\n\\ge \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})} \\ \\ \\ \\ \\ \\ \\ (2)</script><p>我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。</p>\n<p>首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$是$\\frac{P(x^{(i)},z^{(i);\\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$便相当于f(E(X))，同时$\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。</p>\n<p>如果要满足Jensen不等式的等号，那么需要满足X为常量，即为</p>\n<script type=\"math/tex; mode=display\">\n\\frac{P(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c,\\ c为常量</script><p>那么稍加改变能够得到</p>\n<script type=\"math/tex; mode=display\">\nc Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\\theta)\\ ,c为常量</script><script type=\"math/tex; mode=display\">\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta),\\ c为常量</script><p>其中c是不依赖于$z^{(i)}$的常量，由于$Q_i(z^{(i)})$是一个分布，所以满足</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{z}Q_i(z^{(i)})=1</script><script type=\"math/tex; mode=display\">\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta)= c</script><p>因此得到下列方程，其中方程(3)利用到条件概率公式。</p>\n<script type=\"math/tex; mode=display\">\nQ_i(z^{(i)})=\\frac{P(x^{(i)},z^{(i)};\\theta)}{c}=\\frac{P(x^{(i)},z^{(i)};\\theta)}{\\sum _z P(x^{(i)},z^{(i)};\\theta)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\</script><script type=\"math/tex; mode=display\">\n=\\frac{P(x^{(i)},z^{(i)};\\theta)}{ P(x^{(i)};\\theta)}=P(z^{(i)}|x^{(i)};\\theta)  \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\   (3)</script><p>如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)$，那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式</p>\n<script type=\"math/tex; mode=display\">\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}</script><p>去掉上式中常数部分，则我们需要极大化的对数似然下界为</p>\n<script type=\"math/tex; mode=display\">\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\\theta)-log {Q_i(z^{(i)})}]</script><script type=\"math/tex; mode=display\">\n=\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)</script><p>注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)$可以理解为$logP(x^{(i)},z^{(i)};\\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中<strong>E步</strong>。极大化方程式(4)也就是我们EM算法中的<strong>M步</strong>。</p>\n<h3 id=\"4-EM算法流程\"><a href=\"#4-EM算法流程\" class=\"headerlink\" title=\"4.EM算法流程\"></a>4.EM算法流程</h3><p>现在我们总结下EM算法流程。</p>\n<p>输入：观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，联合分布$P(x^{(i)},z^{(i)};\\theta)$，条件分布$P(z^{(i)}|x^{(i)};\\theta)$，最大迭代次数$J$。</p>\n<ul>\n<li><p>随机初始化模型参数$\\theta$的初始值$\\theta^0$。</p>\n</li>\n<li><p>$for\\  j\\  from \\ 1\\  to \\ J$开始EM算法迭代。</p>\n<ul>\n<li>E步:计算联合分布的条件概率期望</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nQ_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)</script><script type=\"math/tex; mode=display\">\nL(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)</script><ul>\n<li>M步:极大化$L(\\theta,\\theta^j)$，得到$\\theta^{j+1}$</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\theta^{j+1}=\\arg \\max _{x} L(\\theta,\\theta^j)</script><ul>\n<li>如果$\\theta^{j+1}$收敛，则算法结束，否则继续迭代。</li>\n</ul>\n</li>\n<li><p>输出模型参数$\\theta$。</p>\n</li>\n</ul>\n<h3 id=\"5-EM算法的收敛性\"><a href=\"#5-EM算法的收敛性\" class=\"headerlink\" title=\"5.EM算法的收敛性\"></a>5.EM算法的收敛性</h3><p>我们现在来解答下<strong>2.EM算法实例</strong>中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？</p>\n<p>首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即</p>\n<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})\\ge \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j})</script><p>由于</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(x^{(i)},z^{(i)};\\theta)</script><p>令</p>\n<script type=\"math/tex; mode=display\">\nH(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(z^{(i)}|x^{(i)};\\theta)</script><p>上两式相减得到</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta)=L(\\theta,\\theta^j)-H(\\theta,\\theta^j)</script><p>在上式中分别取$\\theta$为$\\theta^j$和$\\theta^{j+1}$，并相减得到</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta^{j+1})-\\sum_{i=1}^{m}log P(x^{(i)};\\theta^j)=[L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)]-[H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)]</script><p>要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。由于$\\theta^{j+1}$使得$L(\\theta,\\theta^j)$极大，因此有</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)\\ge 0</script><p>而对于第二部分，我们有</p>\n<script type=\"math/tex; mode=display\">\nH(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log \\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\ \\ \\ \\ \\ \\ \\ \\ \\ (5)</script><script type=\"math/tex; mode=display\">\n\\le \\sum _{i=1}^{m}log \\{ \\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)\\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\}\\ \\ \\ \\ \\ \\ \\ \\ \\ (6)</script><script type=\"math/tex; mode=display\">\n= \\sum _{i=1}^{m}log (\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{j+1}))=0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7)</script><p>其中第(6)式用到了Jensen不等式，只不过和第<strong>3.EM算法推导</strong>中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到</p>\n<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})- \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j}) \\ge 0</script><p>证明了EM算法的收敛性。</p>\n<p>从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。</p>\n<h3 id=\"6-Sklearn实现EM算法\"><a href=\"#6-Sklearn实现EM算法\" class=\"headerlink\" title=\"6.Sklearn实现EM算法\"></a>6.Sklearn实现EM算法</h3><p>高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去<a href=\"https://mp.weixin.qq.com/s?src=11&amp;timestamp=1525932817&amp;ver=867&amp;signature=U4UxviydVELD71Ju*bRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm*budZlpZdMQMLOcXB-FnvKKlkV2H&amp;new=1\" target=\"_blank\" rel=\"noopener\">这儿</a>。下列代码来自于<a href=\"http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py\" target=\"_blank\" rel=\"noopener\">Sklearn官网GMM模块</a>，利用高斯混合模型确定iris聚类。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib <span class=\"keyword\">as</span> mpl</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.mixture <span class=\"keyword\">import</span> GaussianMixture</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> StratifiedKFold</span><br><span class=\"line\"></span><br><span class=\"line\">colors = [<span class=\"string\">'navy'</span>, <span class=\"string\">'turquoise'</span>, <span class=\"string\">'darkorange'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">make_ellipses</span><span class=\"params\">(gmm, ax)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gmm.covariance_type == <span class=\"string\">'full'</span>:</span><br><span class=\"line\">            covariances = gmm.covariances_[n][:<span class=\"number\">2</span>, :<span class=\"number\">2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'tied'</span>:</span><br><span class=\"line\">            covariances = gmm.covariances_[:<span class=\"number\">2</span>, :<span class=\"number\">2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'diag'</span>:</span><br><span class=\"line\">            covariances = np.diag(gmm.covariances_[n][:<span class=\"number\">2</span>])</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'spherical'</span>:</span><br><span class=\"line\">            covariances = np.eye(gmm.means_.shape[<span class=\"number\">1</span>]) * gmm.covariances_[n]</span><br><span class=\"line\">        v, w = np.linalg.eigh(covariances)</span><br><span class=\"line\">        u = w[<span class=\"number\">0</span>] / np.linalg.norm(w[<span class=\"number\">0</span>])</span><br><span class=\"line\">        angle = np.arctan2(u[<span class=\"number\">1</span>], u[<span class=\"number\">0</span>])</span><br><span class=\"line\">        angle = <span class=\"number\">180</span> * angle / np.pi  <span class=\"comment\"># convert to degrees</span></span><br><span class=\"line\">        v = <span class=\"number\">2.</span> * np.sqrt(<span class=\"number\">2.</span>) * np.sqrt(v)</span><br><span class=\"line\">        ell = mpl.patches.Ellipse(gmm.means_[n, :<span class=\"number\">2</span>], v[<span class=\"number\">0</span>], v[<span class=\"number\">1</span>],</span><br><span class=\"line\">                                  <span class=\"number\">180</span> + angle, color=color)</span><br><span class=\"line\">        ell.set_clip_box(ax.bbox)</span><br><span class=\"line\">        ell.set_alpha(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">        ax.add_artist(ell)</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Break up the dataset into non-overlapping training (75%)</span></span><br><span class=\"line\"><span class=\"comment\"># and testing (25%) sets.</span></span><br><span class=\"line\">skf = StratifiedKFold(n_splits=<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"comment\"># Only take the first fold.</span></span><br><span class=\"line\">train_index, test_index = next(iter(skf.split(iris.data, iris.target)))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X_train = iris.data[train_index]</span><br><span class=\"line\">y_train = iris.target[train_index]</span><br><span class=\"line\">X_test = iris.data[test_index]</span><br><span class=\"line\">y_test = iris.target[test_index]</span><br><span class=\"line\"></span><br><span class=\"line\">n_classes = len(np.unique(y_train))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Try GMMs using different types of covariances.</span></span><br><span class=\"line\">estimators = dict((cov_type, GaussianMixture(n_components=n_classes,</span><br><span class=\"line\">                   covariance_type=cov_type, max_iter=<span class=\"number\">20</span>, random_state=<span class=\"number\">0</span>))</span><br><span class=\"line\">                  <span class=\"keyword\">for</span> cov_type <span class=\"keyword\">in</span> [<span class=\"string\">'spherical'</span>, <span class=\"string\">'diag'</span>, <span class=\"string\">'tied'</span>, <span class=\"string\">'full'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">n_estimators = len(estimators)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">3</span> * n_estimators // <span class=\"number\">2</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">plt.subplots_adjust(bottom=<span class=\"number\">.01</span>, top=<span class=\"number\">0.95</span>, hspace=<span class=\"number\">.15</span>, wspace=<span class=\"number\">.05</span>,</span><br><span class=\"line\">                    left=<span class=\"number\">.01</span>, right=<span class=\"number\">.99</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> index, (name, estimator) <span class=\"keyword\">in</span> enumerate(estimators.items()):</span><br><span class=\"line\">    <span class=\"comment\"># Since we have class labels for the training data, we can</span></span><br><span class=\"line\">    <span class=\"comment\"># initialize the GMM parameters in a supervised manner.</span></span><br><span class=\"line\">    estimator.means_init = np.array([X_train[y_train == i].mean(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">                                    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n_classes)])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Train the other parameters using the EM algorithm.</span></span><br><span class=\"line\">    estimator.fit(X_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    h = plt.subplot(<span class=\"number\">2</span>, n_estimators // <span class=\"number\">2</span>, index + <span class=\"number\">1</span>)</span><br><span class=\"line\">    make_ellipses(estimator, h)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        data = iris.data[iris.target == n]</span><br><span class=\"line\">        plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], s=<span class=\"number\">0.8</span>, color=color,</span><br><span class=\"line\">                    label=iris.target_names[n])</span><br><span class=\"line\">    <span class=\"comment\"># Plot the test data with crosses</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        data = X_test[y_test == n]</span><br><span class=\"line\">        plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'x'</span>, color=color)</span><br><span class=\"line\"></span><br><span class=\"line\">    y_train_pred = estimator.predict(X_train)</span><br><span class=\"line\">    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * <span class=\"number\">100</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">0.05</span>, <span class=\"number\">0.9</span>, <span class=\"string\">'Train accuracy: %.1f'</span> % train_accuracy,</span><br><span class=\"line\">             transform=h.transAxes)</span><br><span class=\"line\"></span><br><span class=\"line\">    y_test_pred = estimator.predict(X_test)</span><br><span class=\"line\">    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * <span class=\"number\">100</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">0.05</span>, <span class=\"number\">0.8</span>, <span class=\"string\">'Test accuracy: %.1f'</span> % test_accuracy,</span><br><span class=\"line\">             transform=h.transAxes)</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.xticks(())</span><br><span class=\"line\">    plt.yticks(())</span><br><span class=\"line\">    plt.title(name)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.legend(scatterpoints=<span class=\"number\">1</span>, loc=<span class=\"string\">'lower right'</span>, prop=dict(size=<span class=\"number\">12</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/09/机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png\" alt=\"机器学习之最大期望算法图片01\"></p>\n<h3 id=\"7-EM算法优缺点\"><a href=\"#7-EM算法优缺点\" class=\"headerlink\" title=\"7.EM算法优缺点\"></a>7.EM算法优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li><p>聚类。</p>\n</li>\n<li><p>算法计算结果稳定、准确。</p>\n</li>\n<li><p>EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。</p>\n</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li><p>对初始化数据敏感。</p>\n</li>\n<li><p>EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。</p>\n</li>\n<li><p>当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。</p>\n</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<ul>\n<li><a href=\"https://www.jianshu.com/p/1121509ac1dc\" target=\"_blank\" rel=\"noopener\">milter_如何感性地理解EM算法</a></li>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6912636.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_EM算法原理总结</a></li>\n<li><a href=\"http://scikit-learn.org/stable/modules/mixture.html\" target=\"_blank\" rel=\"noopener\">Gaussian mixture models</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-EM算法简介\"><a href=\"#1-EM算法简介\" class=\"headerlink\" title=\"1.EM算法简介\"></a>1.EM算法简介</h3><p><strong>最大期望(Expectation Maximum)算法</strong>是一种迭代优化算法，其计算方法是每次迭代分为<strong>期望(E)步</strong>和<strong>最大(M)步</strong>。我们先看下最大期望算法能够解决什么样的问题。</p>\n<p>假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。</p>\n<script type=\"math/tex; mode=display\">\nN_1(\\mu_1,\\sigma_1^2)\\ ; N_2(\\mu_2,\\sigma_2^2)</script><p>但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ </p>\n<p>EM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。</p>\n<h3 id=\"2-EM算法实例\"><a href=\"#2-EM算法实例\" class=\"headerlink\" title=\"2.EM算法实例\"></a>2.EM算法实例</h3><p>假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">硬币</th>\n<th style=\"text-align:center\">结果</th>\n<th style=\"text-align:center\">统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正正反正反</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">反反正正反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正反反反反</td>\n<td style=\"text-align:center\">1正-4反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">正反反正正</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">反正正反反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。</p>\n<script type=\"math/tex; mode=display\">\nP1=\\frac{3+1+2}{15}=0.4</script><script type=\"math/tex; mode=display\">\nP2=\\frac{2+3}{10}=0.5</script><p>下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">硬币</th>\n<th style=\"text-align:center\">结果</th>\n<th style=\"text-align:center\">统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正正反正反</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">反反正正反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正反反反反</td>\n<td style=\"text-align:center\">1正-4反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正反反正正</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">反正正反反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>此时我们加入<strong>隐含变量z</strong>，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。</p>\n<p>我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。</p>\n<p>例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2*0.2*0.2*0.8*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7*0.7*0.7*0.3*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">若是硬币1</th>\n<th style=\"text-align:center\">若是硬币2</th>\n<th style=\"text-align:center\">最有可能硬币</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n<td style=\"text-align:center\">硬币2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.08192</td>\n<td style=\"text-align:center\">0.00567</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n<td style=\"text-align:center\">硬币2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到</p>\n<script type=\"math/tex; mode=display\">\nP1=\\frac{2+1+2}{15}=0.33</script><script type=\"math/tex; mode=display\">\nP2=\\frac{3+3}{10}=0.6</script><p>P1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">初始化的P1</th>\n<th style=\"text-align:center\">估计的P1</th>\n<th style=\"text-align:center\">真实的P1</th>\n<th style=\"text-align:center\">初始化的P2</th>\n<th style=\"text-align:center\">估计的P2</th>\n<th style=\"text-align:center\">真实的P2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.33</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">0.5</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。</p>\n<p>上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?</p>\n<p>但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">若是硬币1</th>\n<th style=\"text-align:center\">若是硬币2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.08192</td>\n<td style=\"text-align:center\">0.00567</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率</p>\n<script type=\"math/tex; mode=display\">\nz_1=\\frac{0.00512}{0.00512+0.03087}=0.14</script><p>相应的算出其他4轮的概率。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">z_i=硬币1</th>\n<th style=\"text-align:center\">z_i=硬币2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.14</td>\n<td style=\"text-align:center\">0.86</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.61</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.94</td>\n<td style=\"text-align:center\">0.06</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.14</td>\n<td style=\"text-align:center\">0.86</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.61</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为<strong>E步</strong>。</p>\n<p>按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14*3=0.42的概率为正，有0.14*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为<strong>M步</strong>。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">正面</th>\n<th style=\"text-align:center\">反面</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.42</td>\n<td style=\"text-align:center\">0.28</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1.22</td>\n<td style=\"text-align:center\">1.83</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.94</td>\n<td style=\"text-align:center\">3.76</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.42</td>\n<td style=\"text-align:center\">0.28</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">1.22</td>\n<td style=\"text-align:center\">1.93</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">总计</td>\n<td style=\"text-align:center\">4.22</td>\n<td style=\"text-align:center\">7.98</td>\n</tr>\n</tbody>\n</table>\n</div>\n<script type=\"math/tex; mode=display\">\nP1=\\frac{4.22}{4.22+7.98}=0.35</script><p>上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。</p>\n<h3 id=\"3-EM算法推导\"><a href=\"#3-EM算法推导\" class=\"headerlink\" title=\"3.EM算法推导\"></a>3.EM算法推导</h3><p>对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数$\\theta$，极大化模型分布的对数似然函数如下所示</p>\n<script type=\"math/tex; mode=display\">\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)</script><p>如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下</p>\n<script type=\"math/tex; mode=display\">\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)</script><script type=\"math/tex; mode=display\">\n=\\arg \\max_{\\theta} \\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)</script><p>上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入<strong>Jensen不等式</strong>。</p>\n<blockquote>\n<p>设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。</p>\n<p><strong>Jensen不等式定义：</strong>如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。</p>\n</blockquote>\n<p>我们再回到上述推导过程，得到如下方程式。</p>\n<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)</script><script type=\"math/tex; mode=display\">\n=\\sum _{i=1}^{m}log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\\ \\ \\ \\ \\ \\ \\ (1)</script><script type=\"math/tex; mode=display\">\n\\ge \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})} \\ \\ \\ \\ \\ \\ \\ (2)</script><p>我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。</p>\n<p>首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$是$\\frac{P(x^{(i)},z^{(i);\\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$便相当于f(E(X))，同时$\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。</p>\n<p>如果要满足Jensen不等式的等号，那么需要满足X为常量，即为</p>\n<script type=\"math/tex; mode=display\">\n\\frac{P(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c,\\ c为常量</script><p>那么稍加改变能够得到</p>\n<script type=\"math/tex; mode=display\">\nc Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\\theta)\\ ,c为常量</script><script type=\"math/tex; mode=display\">\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta),\\ c为常量</script><p>其中c是不依赖于$z^{(i)}$的常量，由于$Q_i(z^{(i)})$是一个分布，所以满足</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{z}Q_i(z^{(i)})=1</script><script type=\"math/tex; mode=display\">\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta)= c</script><p>因此得到下列方程，其中方程(3)利用到条件概率公式。</p>\n<script type=\"math/tex; mode=display\">\nQ_i(z^{(i)})=\\frac{P(x^{(i)},z^{(i)};\\theta)}{c}=\\frac{P(x^{(i)},z^{(i)};\\theta)}{\\sum _z P(x^{(i)},z^{(i)};\\theta)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\</script><script type=\"math/tex; mode=display\">\n=\\frac{P(x^{(i)},z^{(i)};\\theta)}{ P(x^{(i)};\\theta)}=P(z^{(i)}|x^{(i)};\\theta)  \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\   (3)</script><p>如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)$，那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式</p>\n<script type=\"math/tex; mode=display\">\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}</script><p>去掉上式中常数部分，则我们需要极大化的对数似然下界为</p>\n<script type=\"math/tex; mode=display\">\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\\theta)-log {Q_i(z^{(i)})}]</script><script type=\"math/tex; mode=display\">\n=\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)</script><p>注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)$可以理解为$logP(x^{(i)},z^{(i)};\\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中<strong>E步</strong>。极大化方程式(4)也就是我们EM算法中的<strong>M步</strong>。</p>\n<h3 id=\"4-EM算法流程\"><a href=\"#4-EM算法流程\" class=\"headerlink\" title=\"4.EM算法流程\"></a>4.EM算法流程</h3><p>现在我们总结下EM算法流程。</p>\n<p>输入：观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，联合分布$P(x^{(i)},z^{(i)};\\theta)$，条件分布$P(z^{(i)}|x^{(i)};\\theta)$，最大迭代次数$J$。</p>\n<ul>\n<li><p>随机初始化模型参数$\\theta$的初始值$\\theta^0$。</p>\n</li>\n<li><p>$for\\  j\\  from \\ 1\\  to \\ J$开始EM算法迭代。</p>\n<ul>\n<li>E步:计算联合分布的条件概率期望</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nQ_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)</script><script type=\"math/tex; mode=display\">\nL(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)</script><ul>\n<li>M步:极大化$L(\\theta,\\theta^j)$，得到$\\theta^{j+1}$</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\theta^{j+1}=\\arg \\max _{x} L(\\theta,\\theta^j)</script><ul>\n<li>如果$\\theta^{j+1}$收敛，则算法结束，否则继续迭代。</li>\n</ul>\n</li>\n<li><p>输出模型参数$\\theta$。</p>\n</li>\n</ul>\n<h3 id=\"5-EM算法的收敛性\"><a href=\"#5-EM算法的收敛性\" class=\"headerlink\" title=\"5.EM算法的收敛性\"></a>5.EM算法的收敛性</h3><p>我们现在来解答下<strong>2.EM算法实例</strong>中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？</p>\n<p>首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即</p>\n<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})\\ge \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j})</script><p>由于</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(x^{(i)},z^{(i)};\\theta)</script><p>令</p>\n<script type=\"math/tex; mode=display\">\nH(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(z^{(i)}|x^{(i)};\\theta)</script><p>上两式相减得到</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta)=L(\\theta,\\theta^j)-H(\\theta,\\theta^j)</script><p>在上式中分别取$\\theta$为$\\theta^j$和$\\theta^{j+1}$，并相减得到</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta^{j+1})-\\sum_{i=1}^{m}log P(x^{(i)};\\theta^j)=[L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)]-[H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)]</script><p>要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。由于$\\theta^{j+1}$使得$L(\\theta,\\theta^j)$极大，因此有</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)\\ge 0</script><p>而对于第二部分，我们有</p>\n<script type=\"math/tex; mode=display\">\nH(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log \\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\ \\ \\ \\ \\ \\ \\ \\ \\ (5)</script><script type=\"math/tex; mode=display\">\n\\le \\sum _{i=1}^{m}log \\{ \\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)\\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\}\\ \\ \\ \\ \\ \\ \\ \\ \\ (6)</script><script type=\"math/tex; mode=display\">\n= \\sum _{i=1}^{m}log (\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{j+1}))=0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7)</script><p>其中第(6)式用到了Jensen不等式，只不过和第<strong>3.EM算法推导</strong>中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到</p>\n<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})- \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j}) \\ge 0</script><p>证明了EM算法的收敛性。</p>\n<p>从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。</p>\n<h3 id=\"6-Sklearn实现EM算法\"><a href=\"#6-Sklearn实现EM算法\" class=\"headerlink\" title=\"6.Sklearn实现EM算法\"></a>6.Sklearn实现EM算法</h3><p>高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去<a href=\"https://mp.weixin.qq.com/s?src=11&amp;timestamp=1525932817&amp;ver=867&amp;signature=U4UxviydVELD71Ju*bRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm*budZlpZdMQMLOcXB-FnvKKlkV2H&amp;new=1\" target=\"_blank\" rel=\"noopener\">这儿</a>。下列代码来自于<a href=\"http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py\" target=\"_blank\" rel=\"noopener\">Sklearn官网GMM模块</a>，利用高斯混合模型确定iris聚类。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib <span class=\"keyword\">as</span> mpl</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.mixture <span class=\"keyword\">import</span> GaussianMixture</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> StratifiedKFold</span><br><span class=\"line\"></span><br><span class=\"line\">colors = [<span class=\"string\">'navy'</span>, <span class=\"string\">'turquoise'</span>, <span class=\"string\">'darkorange'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">make_ellipses</span><span class=\"params\">(gmm, ax)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gmm.covariance_type == <span class=\"string\">'full'</span>:</span><br><span class=\"line\">            covariances = gmm.covariances_[n][:<span class=\"number\">2</span>, :<span class=\"number\">2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'tied'</span>:</span><br><span class=\"line\">            covariances = gmm.covariances_[:<span class=\"number\">2</span>, :<span class=\"number\">2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'diag'</span>:</span><br><span class=\"line\">            covariances = np.diag(gmm.covariances_[n][:<span class=\"number\">2</span>])</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'spherical'</span>:</span><br><span class=\"line\">            covariances = np.eye(gmm.means_.shape[<span class=\"number\">1</span>]) * gmm.covariances_[n]</span><br><span class=\"line\">        v, w = np.linalg.eigh(covariances)</span><br><span class=\"line\">        u = w[<span class=\"number\">0</span>] / np.linalg.norm(w[<span class=\"number\">0</span>])</span><br><span class=\"line\">        angle = np.arctan2(u[<span class=\"number\">1</span>], u[<span class=\"number\">0</span>])</span><br><span class=\"line\">        angle = <span class=\"number\">180</span> * angle / np.pi  <span class=\"comment\"># convert to degrees</span></span><br><span class=\"line\">        v = <span class=\"number\">2.</span> * np.sqrt(<span class=\"number\">2.</span>) * np.sqrt(v)</span><br><span class=\"line\">        ell = mpl.patches.Ellipse(gmm.means_[n, :<span class=\"number\">2</span>], v[<span class=\"number\">0</span>], v[<span class=\"number\">1</span>],</span><br><span class=\"line\">                                  <span class=\"number\">180</span> + angle, color=color)</span><br><span class=\"line\">        ell.set_clip_box(ax.bbox)</span><br><span class=\"line\">        ell.set_alpha(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">        ax.add_artist(ell)</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Break up the dataset into non-overlapping training (75%)</span></span><br><span class=\"line\"><span class=\"comment\"># and testing (25%) sets.</span></span><br><span class=\"line\">skf = StratifiedKFold(n_splits=<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"comment\"># Only take the first fold.</span></span><br><span class=\"line\">train_index, test_index = next(iter(skf.split(iris.data, iris.target)))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X_train = iris.data[train_index]</span><br><span class=\"line\">y_train = iris.target[train_index]</span><br><span class=\"line\">X_test = iris.data[test_index]</span><br><span class=\"line\">y_test = iris.target[test_index]</span><br><span class=\"line\"></span><br><span class=\"line\">n_classes = len(np.unique(y_train))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Try GMMs using different types of covariances.</span></span><br><span class=\"line\">estimators = dict((cov_type, GaussianMixture(n_components=n_classes,</span><br><span class=\"line\">                   covariance_type=cov_type, max_iter=<span class=\"number\">20</span>, random_state=<span class=\"number\">0</span>))</span><br><span class=\"line\">                  <span class=\"keyword\">for</span> cov_type <span class=\"keyword\">in</span> [<span class=\"string\">'spherical'</span>, <span class=\"string\">'diag'</span>, <span class=\"string\">'tied'</span>, <span class=\"string\">'full'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">n_estimators = len(estimators)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">3</span> * n_estimators // <span class=\"number\">2</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">plt.subplots_adjust(bottom=<span class=\"number\">.01</span>, top=<span class=\"number\">0.95</span>, hspace=<span class=\"number\">.15</span>, wspace=<span class=\"number\">.05</span>,</span><br><span class=\"line\">                    left=<span class=\"number\">.01</span>, right=<span class=\"number\">.99</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> index, (name, estimator) <span class=\"keyword\">in</span> enumerate(estimators.items()):</span><br><span class=\"line\">    <span class=\"comment\"># Since we have class labels for the training data, we can</span></span><br><span class=\"line\">    <span class=\"comment\"># initialize the GMM parameters in a supervised manner.</span></span><br><span class=\"line\">    estimator.means_init = np.array([X_train[y_train == i].mean(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">                                    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n_classes)])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Train the other parameters using the EM algorithm.</span></span><br><span class=\"line\">    estimator.fit(X_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    h = plt.subplot(<span class=\"number\">2</span>, n_estimators // <span class=\"number\">2</span>, index + <span class=\"number\">1</span>)</span><br><span class=\"line\">    make_ellipses(estimator, h)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        data = iris.data[iris.target == n]</span><br><span class=\"line\">        plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], s=<span class=\"number\">0.8</span>, color=color,</span><br><span class=\"line\">                    label=iris.target_names[n])</span><br><span class=\"line\">    <span class=\"comment\"># Plot the test data with crosses</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        data = X_test[y_test == n]</span><br><span class=\"line\">        plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'x'</span>, color=color)</span><br><span class=\"line\"></span><br><span class=\"line\">    y_train_pred = estimator.predict(X_train)</span><br><span class=\"line\">    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * <span class=\"number\">100</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">0.05</span>, <span class=\"number\">0.9</span>, <span class=\"string\">'Train accuracy: %.1f'</span> % train_accuracy,</span><br><span class=\"line\">             transform=h.transAxes)</span><br><span class=\"line\"></span><br><span class=\"line\">    y_test_pred = estimator.predict(X_test)</span><br><span class=\"line\">    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * <span class=\"number\">100</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">0.05</span>, <span class=\"number\">0.8</span>, <span class=\"string\">'Test accuracy: %.1f'</span> % test_accuracy,</span><br><span class=\"line\">             transform=h.transAxes)</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.xticks(())</span><br><span class=\"line\">    plt.yticks(())</span><br><span class=\"line\">    plt.title(name)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.legend(scatterpoints=<span class=\"number\">1</span>, loc=<span class=\"string\">'lower right'</span>, prop=dict(size=<span class=\"number\">12</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/09/机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png\" alt=\"机器学习之最大期望算法图片01\"></p>\n<h3 id=\"7-EM算法优缺点\"><a href=\"#7-EM算法优缺点\" class=\"headerlink\" title=\"7.EM算法优缺点\"></a>7.EM算法优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li><p>聚类。</p>\n</li>\n<li><p>算法计算结果稳定、准确。</p>\n</li>\n<li><p>EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。</p>\n</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li><p>对初始化数据敏感。</p>\n</li>\n<li><p>EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。</p>\n</li>\n<li><p>当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。</p>\n</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<ul>\n<li><a href=\"https://www.jianshu.com/p/1121509ac1dc\" target=\"_blank\" rel=\"noopener\">milter_如何感性地理解EM算法</a></li>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6912636.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_EM算法原理总结</a></li>\n<li><a href=\"http://scikit-learn.org/stable/modules/mixture.html\" target=\"_blank\" rel=\"noopener\">Gaussian mixture models</a></li>\n</ul>\n"},{"title":"机器学习之朴素贝叶斯算法","date":"2018-05-14T03:22:17.000Z","mathjax":true,"comments":1,"_content":"\n### 1.朴素贝叶斯简介\n\n**朴素贝叶斯(Naive Bayesian)**算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布$P(X,Y)$ ，然后用$P(Y|X)=\\frac{P(X,Y)}{P(X)}$得出。\n\n朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。\n\n### 2.朴素贝叶斯算法模型\n\n#### 2.1统计知识回顾\n\n深入算法原理之前，我们先来回顾下统计学的相关知识。\n\n+ **条件概率公式**\n\n$$\nP(X,Y)=P(X)P(Y)\\ X、Y相互独立\n$$\n\n+ **条件概率公式**\n\n$$\nP(Y|X)=\\frac{P(X,Y)}{P(X)}\n\\\\ P(X|Y)=\\frac{P(X,Y)}{P(Y)}\n$$\n\n+ **全概率公式**\n\n$$\nP(X)=\\sum_{j}P(X|Y=Y_j)P(Y_j)\\ 其中\\sum _{j}P(Y_j)=1\n$$\n\n经过上面统计学知识，我们能够得出贝叶斯公式。\n$$\nP(Y_k|X)=\\frac{P(X,Y_k)}{P(X)}=\\frac{P(X|Y_k)P(Y_k)}{\\sum_{k=1}^{K}P(X|Y=Y_k)P(Y_k)}\n$$\n\n#### 2.2朴素贝叶斯模型\n\n假设我们已经有部分数据，并且能从数据中得到先验概率，那么如何得到后验概率$P(Y_k|X)$呢？下面我们通过构建朴素贝叶斯分类模型来解决后验概率问题，假设分类模型数据有m个样本，每个样本有n个特征，特征输出有K个类别，定义为${C1,C2,…,Ck}$。\n$$\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}\n$$\n从样本中我们能够学习得到朴素贝叶斯的先验分布概率\n$$\nP(Y=C_k)(k=1,2,…,K)\n$$\n然后学习得到条件概率分布\n$$\nP(X|Y=C_k)=P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\n$$\n最后结合贝叶斯公式便可得到X和Y的联合分布$P(X,Y=C_k)$\n$$\nP(X,Y=C_k)=P(Y=C_k)P(X|Y=C_k)\\ \\ \\ \\ (1)\n$$\n$$\n=P(Y=C_k)P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k) \\ \\ \\ \\ (2)\n$$\n\n上式中$P(Y=C_k)$可以通过极大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)$很难求出，因此朴素贝叶斯模型做一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出\n$$\nP(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\n$$\n$$\n=P(X_1=x^{(1)}|Y=C_k)P(X_2=x^{(2)}|Y=C_k),...,P(X_n=x^{(n)}|Y=C_k)\n$$\n\n这样我们便得到X和Y的联合分布，最后的问题是给定测试集$(x_1^{(test)},x_2^{(test)},…,x_n^{(test)})$，如何判断它属于哪个类型？\n\n#### 2.3朴素贝叶斯推断\n\n假如我们预测的类别结果为$C_{result}$，其中$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大的类别，数学表达式为\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k|X=X^{(test)})\n$$\n$$\n= \\underset{C_k}{\\underbrace{\\arg\\max}} \\frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}\n$$\n\n由于对所有类别计算$P(Y=C_k|X=X^{(test)})$时，分母都是$P(X=X^{(test)})$，因此我们的预测公式可以简化为\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(X=X^{(test)}|Y=C_k)P(Y=C_k)\n$$\n然后利用朴素贝叶斯的独立性假设，就可以得到朴素贝叶斯推断公式\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n#### 2.4朴素贝叶斯参数估计\n\n对于**2.3**中朴素贝叶斯推断公式，我们只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$，就可以得到预测结果。对于$P(Y=C_k)$比较简单，通过极大似然估计能够得到$C_k$出现的概率，也就是样本类别$C_k$出现的次数$m_k$除以样本总数m。而对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$则取决于我们的先验条件。\n\n+ 如果$X_j$是**离散值**，那么可以假设$X_j$服从多项式分布，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别中$X_j^{test}$出现的频率，公式如下所示。其中$m_k$为样本类别$C_k$出现的次数，而$m_{kj}^{(test)}$为类别是$C_k$的样本中，第j维特征$X_j^{(test)}$出现的次数。\n  $$\n  P(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}}{m_k}\n  $$\n  某些时候可能一些类别在样本中从来没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，因此会影响后验概率的估计。为了解决此类情况，我们引入拉普拉斯平滑，公式如下所示。其中$\\lambda$为大于0的常数，常取为1，$O_j$为第j个特征的取值个数。\n\n$$\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}+\\lambda}{m_k+O_j \\lambda}\n$$\n\n+ 如果$X_j$是**稀疏的离散值**，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中$X_j^{(test)}$出现的频率，公式如下所示。其中$X_j^{(test)}$取值为0和1。\n\n$$\nP(X_j=X_j^{(test)}|Y=C_k)=P(X_j|Y=C_k)X_j^{(test)}+(1-P(X_j|Y=C_k))(1-X_j^{(test)})\n$$\n\n+ 如果$X_j$是**连续值**，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布公式如下所示。其中$\\mu_k$和$\\sigma_{k}^2$是正态分布的期望和方差，$\\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\\mu_k$和$\\sigma _{k}^2$可以通过极大似然估计求得。\n  $$\n  P(X_j=X_j^{(test)}|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp(-\\frac{(X_j^{(test)}- \\mu_k)^2}{2\\sigma_{k}^{2}})\n  $$\n\n\n\n\n\n### 3.朴素贝叶斯算法流程\n\n我们总结下朴素贝叶斯算法流程，假设训练集有m个样本n个维度，训练样本有K个特征输出类别，分别为$C1,C2,…,C_K$，每个特征输出类别的样本个数为$m_1,m_2,…,m_K$。第K个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$，其中$l$的取值为$1,2,…,S_j$，$S_j$为特征$j$不同的取值数。\n$$\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}\n$$\n预测结果为$X^{(test)}​$的分类，算法流程如下所示\n\n+ 如果没有Y的先验概率，则计算Y的K个先验概率$P(Y=C_k)=\\frac{m_k}{m}$。\n\n+ 分别计算第k个类别的第j为特征的第l个取值条件概率$P(X_j=x_{jl}|Y=C_k)$。\n\n  + 如果是离散值，则公式如下所示，其中$\\lambda$为大于0的常数，常常取为1，$O_j$为第j个特征的取值个数。\n\n  $$\n  P(X_j=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+O_j \\lambda}\n  $$\n\n  + 如果是稀疏离散值，则公式如下所示，此时$l$取值为0或1。\n\n  $$\n  P(X_j=x_{jl}|Y=C_k)=P(X_j|Y=C_k)x_{jl}+(1-P(X_j|Y=C_k))(1-x_{jl})\n  $$\n\n  + 如果是连续值，则不需要计算各个l个取值概率，直接求正态分布的参数，公式如下所示。$\\mu_k$为样本类别$C_k$中所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中所有$X_j$的方差。\n\n  $$\n  P(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}exp(-\\frac{(x_j- \\mu_k)^2}{2\\sigma_{k}^{2}})\n  $$\n\n+ 对于实例$X^{(test)}$，分别计算\n\n$$\nP(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n+ 确定实例$X^{(test)}$的分类$C_{result}$\n\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。\n\n### 4.Sklearn实现朴素贝叶斯\n\n利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.933333333333。更多关于sklearn.naive_bayes的使用技巧可以访问[官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)。\n\n```python\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n#load data\niris=load_iris()\nX=iris.data\ny=iris.target\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nmnb=GaussianNB()\nmnb.fit(X_train,y_train)\n\nprint(mnb.predict(X_test))\n# [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 1 2 1]\nprint(y_test)\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 2 2 1]\nprint(mnb.score(X_test,y_test))\n# 0.933333333333\n```\n\n### 5.朴素贝叶斯优缺点\n\n#### 5.1优点\n\n+ 具有稳定的分类效率。\n+ 对缺失数据不敏感，算法也比较简单。\n+ 对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。\n\n#### 5.2缺点\n\n+ 对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。\n+ 由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。\n+ 假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n> [刘建平_Pinard-朴素贝叶斯算法原理小结](https://www.cnblogs.com/pinard/p/6069267.html)","source":"_posts/机器学习之朴素贝叶斯算法.md","raw":"---\ntitle: 机器学习之朴素贝叶斯算法\ndate: 2018-05-14 11:22:17\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.朴素贝叶斯简介\n\n**朴素贝叶斯(Naive Bayesian)**算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布$P(X,Y)$ ，然后用$P(Y|X)=\\frac{P(X,Y)}{P(X)}$得出。\n\n朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。\n\n### 2.朴素贝叶斯算法模型\n\n#### 2.1统计知识回顾\n\n深入算法原理之前，我们先来回顾下统计学的相关知识。\n\n+ **条件概率公式**\n\n$$\nP(X,Y)=P(X)P(Y)\\ X、Y相互独立\n$$\n\n+ **条件概率公式**\n\n$$\nP(Y|X)=\\frac{P(X,Y)}{P(X)}\n\\\\ P(X|Y)=\\frac{P(X,Y)}{P(Y)}\n$$\n\n+ **全概率公式**\n\n$$\nP(X)=\\sum_{j}P(X|Y=Y_j)P(Y_j)\\ 其中\\sum _{j}P(Y_j)=1\n$$\n\n经过上面统计学知识，我们能够得出贝叶斯公式。\n$$\nP(Y_k|X)=\\frac{P(X,Y_k)}{P(X)}=\\frac{P(X|Y_k)P(Y_k)}{\\sum_{k=1}^{K}P(X|Y=Y_k)P(Y_k)}\n$$\n\n#### 2.2朴素贝叶斯模型\n\n假设我们已经有部分数据，并且能从数据中得到先验概率，那么如何得到后验概率$P(Y_k|X)$呢？下面我们通过构建朴素贝叶斯分类模型来解决后验概率问题，假设分类模型数据有m个样本，每个样本有n个特征，特征输出有K个类别，定义为${C1,C2,…,Ck}$。\n$$\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}\n$$\n从样本中我们能够学习得到朴素贝叶斯的先验分布概率\n$$\nP(Y=C_k)(k=1,2,…,K)\n$$\n然后学习得到条件概率分布\n$$\nP(X|Y=C_k)=P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\n$$\n最后结合贝叶斯公式便可得到X和Y的联合分布$P(X,Y=C_k)$\n$$\nP(X,Y=C_k)=P(Y=C_k)P(X|Y=C_k)\\ \\ \\ \\ (1)\n$$\n$$\n=P(Y=C_k)P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k) \\ \\ \\ \\ (2)\n$$\n\n上式中$P(Y=C_k)$可以通过极大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)$很难求出，因此朴素贝叶斯模型做一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出\n$$\nP(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\n$$\n$$\n=P(X_1=x^{(1)}|Y=C_k)P(X_2=x^{(2)}|Y=C_k),...,P(X_n=x^{(n)}|Y=C_k)\n$$\n\n这样我们便得到X和Y的联合分布，最后的问题是给定测试集$(x_1^{(test)},x_2^{(test)},…,x_n^{(test)})$，如何判断它属于哪个类型？\n\n#### 2.3朴素贝叶斯推断\n\n假如我们预测的类别结果为$C_{result}$，其中$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大的类别，数学表达式为\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k|X=X^{(test)})\n$$\n$$\n= \\underset{C_k}{\\underbrace{\\arg\\max}} \\frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}\n$$\n\n由于对所有类别计算$P(Y=C_k|X=X^{(test)})$时，分母都是$P(X=X^{(test)})$，因此我们的预测公式可以简化为\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(X=X^{(test)}|Y=C_k)P(Y=C_k)\n$$\n然后利用朴素贝叶斯的独立性假设，就可以得到朴素贝叶斯推断公式\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n#### 2.4朴素贝叶斯参数估计\n\n对于**2.3**中朴素贝叶斯推断公式，我们只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$，就可以得到预测结果。对于$P(Y=C_k)$比较简单，通过极大似然估计能够得到$C_k$出现的概率，也就是样本类别$C_k$出现的次数$m_k$除以样本总数m。而对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$则取决于我们的先验条件。\n\n+ 如果$X_j$是**离散值**，那么可以假设$X_j$服从多项式分布，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别中$X_j^{test}$出现的频率，公式如下所示。其中$m_k$为样本类别$C_k$出现的次数，而$m_{kj}^{(test)}$为类别是$C_k$的样本中，第j维特征$X_j^{(test)}$出现的次数。\n  $$\n  P(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}}{m_k}\n  $$\n  某些时候可能一些类别在样本中从来没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，因此会影响后验概率的估计。为了解决此类情况，我们引入拉普拉斯平滑，公式如下所示。其中$\\lambda$为大于0的常数，常取为1，$O_j$为第j个特征的取值个数。\n\n$$\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}+\\lambda}{m_k+O_j \\lambda}\n$$\n\n+ 如果$X_j$是**稀疏的离散值**，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中$X_j^{(test)}$出现的频率，公式如下所示。其中$X_j^{(test)}$取值为0和1。\n\n$$\nP(X_j=X_j^{(test)}|Y=C_k)=P(X_j|Y=C_k)X_j^{(test)}+(1-P(X_j|Y=C_k))(1-X_j^{(test)})\n$$\n\n+ 如果$X_j$是**连续值**，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布公式如下所示。其中$\\mu_k$和$\\sigma_{k}^2$是正态分布的期望和方差，$\\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\\mu_k$和$\\sigma _{k}^2$可以通过极大似然估计求得。\n  $$\n  P(X_j=X_j^{(test)}|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp(-\\frac{(X_j^{(test)}- \\mu_k)^2}{2\\sigma_{k}^{2}})\n  $$\n\n\n\n\n\n### 3.朴素贝叶斯算法流程\n\n我们总结下朴素贝叶斯算法流程，假设训练集有m个样本n个维度，训练样本有K个特征输出类别，分别为$C1,C2,…,C_K$，每个特征输出类别的样本个数为$m_1,m_2,…,m_K$。第K个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$，其中$l$的取值为$1,2,…,S_j$，$S_j$为特征$j$不同的取值数。\n$$\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}\n$$\n预测结果为$X^{(test)}​$的分类，算法流程如下所示\n\n+ 如果没有Y的先验概率，则计算Y的K个先验概率$P(Y=C_k)=\\frac{m_k}{m}$。\n\n+ 分别计算第k个类别的第j为特征的第l个取值条件概率$P(X_j=x_{jl}|Y=C_k)$。\n\n  + 如果是离散值，则公式如下所示，其中$\\lambda$为大于0的常数，常常取为1，$O_j$为第j个特征的取值个数。\n\n  $$\n  P(X_j=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+O_j \\lambda}\n  $$\n\n  + 如果是稀疏离散值，则公式如下所示，此时$l$取值为0或1。\n\n  $$\n  P(X_j=x_{jl}|Y=C_k)=P(X_j|Y=C_k)x_{jl}+(1-P(X_j|Y=C_k))(1-x_{jl})\n  $$\n\n  + 如果是连续值，则不需要计算各个l个取值概率，直接求正态分布的参数，公式如下所示。$\\mu_k$为样本类别$C_k$中所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中所有$X_j$的方差。\n\n  $$\n  P(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}exp(-\\frac{(x_j- \\mu_k)^2}{2\\sigma_{k}^{2}})\n  $$\n\n+ 对于实例$X^{(test)}$，分别计算\n\n$$\nP(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n+ 确定实例$X^{(test)}$的分类$C_{result}$\n\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。\n\n### 4.Sklearn实现朴素贝叶斯\n\n利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.933333333333。更多关于sklearn.naive_bayes的使用技巧可以访问[官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)。\n\n```python\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n#load data\niris=load_iris()\nX=iris.data\ny=iris.target\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nmnb=GaussianNB()\nmnb.fit(X_train,y_train)\n\nprint(mnb.predict(X_test))\n# [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 1 2 1]\nprint(y_test)\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 2 2 1]\nprint(mnb.score(X_test,y_test))\n# 0.933333333333\n```\n\n### 5.朴素贝叶斯优缺点\n\n#### 5.1优点\n\n+ 具有稳定的分类效率。\n+ 对缺失数据不敏感，算法也比较简单。\n+ 对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。\n\n#### 5.2缺点\n\n+ 对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。\n+ 由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。\n+ 假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n> [刘建平_Pinard-朴素贝叶斯算法原理小结](https://www.cnblogs.com/pinard/p/6069267.html)","slug":"机器学习之朴素贝叶斯算法","published":1,"updated":"2018-06-26T03:58:56.909Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4g001ijiz5yg8px6rg","content":"<h3 id=\"1-朴素贝叶斯简介\"><a href=\"#1-朴素贝叶斯简介\" class=\"headerlink\" title=\"1.朴素贝叶斯简介\"></a>1.朴素贝叶斯简介</h3><p><strong>朴素贝叶斯(Naive Bayesian)</strong>算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布$P(X,Y)$ ，然后用$P(Y|X)=\\frac{P(X,Y)}{P(X)}$得出。</p>\n<p>朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。</p>\n<h3 id=\"2-朴素贝叶斯算法模型\"><a href=\"#2-朴素贝叶斯算法模型\" class=\"headerlink\" title=\"2.朴素贝叶斯算法模型\"></a>2.朴素贝叶斯算法模型</h3><h4 id=\"2-1统计知识回顾\"><a href=\"#2-1统计知识回顾\" class=\"headerlink\" title=\"2.1统计知识回顾\"></a>2.1统计知识回顾</h4><p>深入算法原理之前，我们先来回顾下统计学的相关知识。</p>\n<ul>\n<li><strong>条件概率公式</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X,Y)=P(X)P(Y)\\ X、Y相互独立</script><ul>\n<li><strong>条件概率公式</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y|X)=\\frac{P(X,Y)}{P(X)}\n\\\\ P(X|Y)=\\frac{P(X,Y)}{P(Y)}</script><ul>\n<li><strong>全概率公式</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X)=\\sum_{j}P(X|Y=Y_j)P(Y_j)\\ 其中\\sum _{j}P(Y_j)=1</script><p>经过上面统计学知识，我们能够得出贝叶斯公式。</p>\n<script type=\"math/tex; mode=display\">\nP(Y_k|X)=\\frac{P(X,Y_k)}{P(X)}=\\frac{P(X|Y_k)P(Y_k)}{\\sum_{k=1}^{K}P(X|Y=Y_k)P(Y_k)}</script><h4 id=\"2-2朴素贝叶斯模型\"><a href=\"#2-2朴素贝叶斯模型\" class=\"headerlink\" title=\"2.2朴素贝叶斯模型\"></a>2.2朴素贝叶斯模型</h4><p>假设我们已经有部分数据，并且能从数据中得到先验概率，那么如何得到后验概率$P(Y_k|X)$呢？下面我们通过构建朴素贝叶斯分类模型来解决后验概率问题，假设分类模型数据有m个样本，每个样本有n个特征，特征输出有K个类别，定义为${C1,C2,…,Ck}$。</p>\n<script type=\"math/tex; mode=display\">\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}</script><p>从样本中我们能够学习得到朴素贝叶斯的先验分布概率</p>\n<script type=\"math/tex; mode=display\">\nP(Y=C_k)(k=1,2,…,K)</script><p>然后学习得到条件概率分布</p>\n<script type=\"math/tex; mode=display\">\nP(X|Y=C_k)=P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)</script><p>最后结合贝叶斯公式便可得到X和Y的联合分布$P(X,Y=C_k)$</p>\n<script type=\"math/tex; mode=display\">\nP(X,Y=C_k)=P(Y=C_k)P(X|Y=C_k)\\ \\ \\ \\ (1)</script><script type=\"math/tex; mode=display\">\n=P(Y=C_k)P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k) \\ \\ \\ \\ (2)</script><p>上式中$P(Y=C_k)$可以通过极大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)$很难求出，因此朴素贝叶斯模型做一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出</p>\n<script type=\"math/tex; mode=display\">\nP(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)</script><script type=\"math/tex; mode=display\">\n=P(X_1=x^{(1)}|Y=C_k)P(X_2=x^{(2)}|Y=C_k),...,P(X_n=x^{(n)}|Y=C_k)</script><p>这样我们便得到X和Y的联合分布，最后的问题是给定测试集$(x_1^{(test)},x_2^{(test)},…,x_n^{(test)})$，如何判断它属于哪个类型？</p>\n<h4 id=\"2-3朴素贝叶斯推断\"><a href=\"#2-3朴素贝叶斯推断\" class=\"headerlink\" title=\"2.3朴素贝叶斯推断\"></a>2.3朴素贝叶斯推断</h4><p>假如我们预测的类别结果为$C_{result}$，其中$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大的类别，数学表达式为</p>\n<script type=\"math/tex; mode=display\">\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k|X=X^{(test)})</script><script type=\"math/tex; mode=display\">\n= \\underset{C_k}{\\underbrace{\\arg\\max}} \\frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}</script><p>由于对所有类别计算$P(Y=C_k|X=X^{(test)})$时，分母都是$P(X=X^{(test)})$，因此我们的预测公式可以简化为</p>\n<script type=\"math/tex; mode=display\">\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(X=X^{(test)}|Y=C_k)P(Y=C_k)</script><p>然后利用朴素贝叶斯的独立性假设，就可以得到朴素贝叶斯推断公式</p>\n<script type=\"math/tex; mode=display\">\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)</script><h4 id=\"2-4朴素贝叶斯参数估计\"><a href=\"#2-4朴素贝叶斯参数估计\" class=\"headerlink\" title=\"2.4朴素贝叶斯参数估计\"></a>2.4朴素贝叶斯参数估计</h4><p>对于<strong>2.3</strong>中朴素贝叶斯推断公式，我们只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$，就可以得到预测结果。对于$P(Y=C_k)$比较简单，通过极大似然估计能够得到$C_k$出现的概率，也就是样本类别$C_k$出现的次数$m_k$除以样本总数m。而对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$则取决于我们的先验条件。</p>\n<ul>\n<li>如果$X_j$是<strong>离散值</strong>，那么可以假设$X_j$服从多项式分布，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别中$X_j^{test}$出现的频率，公式如下所示。其中$m_k$为样本类别$C_k$出现的次数，而$m_{kj}^{(test)}$为类别是$C_k$的样本中，第j维特征$X_j^{(test)}$出现的次数。<script type=\"math/tex; mode=display\">\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}}{m_k}</script>某些时候可能一些类别在样本中从来没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，因此会影响后验概率的估计。为了解决此类情况，我们引入拉普拉斯平滑，公式如下所示。其中$\\lambda$为大于0的常数，常取为1，$O_j$为第j个特征的取值个数。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}+\\lambda}{m_k+O_j \\lambda}</script><ul>\n<li>如果$X_j$是<strong>稀疏的离散值</strong>，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中$X_j^{(test)}$出现的频率，公式如下所示。其中$X_j^{(test)}$取值为0和1。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=X_j^{(test)}|Y=C_k)=P(X_j|Y=C_k)X_j^{(test)}+(1-P(X_j|Y=C_k))(1-X_j^{(test)})</script><ul>\n<li>如果$X_j$是<strong>连续值</strong>，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布公式如下所示。其中$\\mu_k$和$\\sigma_{k}^2$是正态分布的期望和方差，$\\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\\mu_k$和$\\sigma _{k}^2$可以通过极大似然估计求得。<script type=\"math/tex; mode=display\">\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp(-\\frac{(X_j^{(test)}- \\mu_k)^2}{2\\sigma_{k}^{2}})</script></li>\n</ul>\n<h3 id=\"3-朴素贝叶斯算法流程\"><a href=\"#3-朴素贝叶斯算法流程\" class=\"headerlink\" title=\"3.朴素贝叶斯算法流程\"></a>3.朴素贝叶斯算法流程</h3><p>我们总结下朴素贝叶斯算法流程，假设训练集有m个样本n个维度，训练样本有K个特征输出类别，分别为$C1,C2,…,C_K$，每个特征输出类别的样本个数为$m_1,m_2,…,m_K$。第K个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$，其中$l$的取值为$1,2,…,S_j$，$S_j$为特征$j$不同的取值数。</p>\n<script type=\"math/tex; mode=display\">\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}</script><p>预测结果为$X^{(test)}​$的分类，算法流程如下所示</p>\n<ul>\n<li><p>如果没有Y的先验概率，则计算Y的K个先验概率$P(Y=C_k)=\\frac{m_k}{m}$。</p>\n</li>\n<li><p>分别计算第k个类别的第j为特征的第l个取值条件概率$P(X_j=x_{jl}|Y=C_k)$。</p>\n<ul>\n<li>如果是离散值，则公式如下所示，其中$\\lambda$为大于0的常数，常常取为1，$O_j$为第j个特征的取值个数。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+O_j \\lambda}</script><ul>\n<li>如果是稀疏离散值，则公式如下所示，此时$l$取值为0或1。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=x_{jl}|Y=C_k)=P(X_j|Y=C_k)x_{jl}+(1-P(X_j|Y=C_k))(1-x_{jl})</script><ul>\n<li>如果是连续值，则不需要计算各个l个取值概率，直接求正态分布的参数，公式如下所示。$\\mu_k$为样本类别$C_k$中所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中所有$X_j$的方差。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}exp(-\\frac{(x_j- \\mu_k)^2}{2\\sigma_{k}^{2}})</script></li>\n<li><p>对于实例$X^{(test)}$，分别计算</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)</script><ul>\n<li>确定实例$X^{(test)}$的分类$C_{result}$</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)</script><p>从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。</p>\n<h3 id=\"4-Sklearn实现朴素贝叶斯\"><a href=\"#4-Sklearn实现朴素贝叶斯\" class=\"headerlink\" title=\"4.Sklearn实现朴素贝叶斯\"></a>4.Sklearn实现朴素贝叶斯</h3><p>利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.933333333333。更多关于sklearn.naive_bayes的使用技巧可以访问<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\" target=\"_blank\" rel=\"noopener\">官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">mnb=GaussianNB()</span><br><span class=\"line\">mnb.fit(X_train,y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">print(mnb.predict(X_test))</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 1 2 1]</span></span><br><span class=\"line\">print(y_test)</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 2 2 1]</span></span><br><span class=\"line\">print(mnb.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.933333333333</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"5-朴素贝叶斯优缺点\"><a href=\"#5-朴素贝叶斯优缺点\" class=\"headerlink\" title=\"5.朴素贝叶斯优缺点\"></a>5.朴素贝叶斯优缺点</h3><h4 id=\"5-1优点\"><a href=\"#5-1优点\" class=\"headerlink\" title=\"5.1优点\"></a>5.1优点</h4><ul>\n<li>具有稳定的分类效率。</li>\n<li>对缺失数据不敏感，算法也比较简单。</li>\n<li>对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。</li>\n</ul>\n<h4 id=\"5-2缺点\"><a href=\"#5-2缺点\" class=\"headerlink\" title=\"5.2缺点\"></a>5.2缺点</h4><ul>\n<li>对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。</li>\n<li>由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。</li>\n<li>假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。</li>\n</ul>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6069267.html\" target=\"_blank\" rel=\"noopener\">刘建平_Pinard-朴素贝叶斯算法原理小结</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-朴素贝叶斯简介\"><a href=\"#1-朴素贝叶斯简介\" class=\"headerlink\" title=\"1.朴素贝叶斯简介\"></a>1.朴素贝叶斯简介</h3><p><strong>朴素贝叶斯(Naive Bayesian)</strong>算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布$P(X,Y)$ ，然后用$P(Y|X)=\\frac{P(X,Y)}{P(X)}$得出。</p>\n<p>朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。</p>\n<h3 id=\"2-朴素贝叶斯算法模型\"><a href=\"#2-朴素贝叶斯算法模型\" class=\"headerlink\" title=\"2.朴素贝叶斯算法模型\"></a>2.朴素贝叶斯算法模型</h3><h4 id=\"2-1统计知识回顾\"><a href=\"#2-1统计知识回顾\" class=\"headerlink\" title=\"2.1统计知识回顾\"></a>2.1统计知识回顾</h4><p>深入算法原理之前，我们先来回顾下统计学的相关知识。</p>\n<ul>\n<li><strong>条件概率公式</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X,Y)=P(X)P(Y)\\ X、Y相互独立</script><ul>\n<li><strong>条件概率公式</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y|X)=\\frac{P(X,Y)}{P(X)}\n\\\\ P(X|Y)=\\frac{P(X,Y)}{P(Y)}</script><ul>\n<li><strong>全概率公式</strong></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X)=\\sum_{j}P(X|Y=Y_j)P(Y_j)\\ 其中\\sum _{j}P(Y_j)=1</script><p>经过上面统计学知识，我们能够得出贝叶斯公式。</p>\n<script type=\"math/tex; mode=display\">\nP(Y_k|X)=\\frac{P(X,Y_k)}{P(X)}=\\frac{P(X|Y_k)P(Y_k)}{\\sum_{k=1}^{K}P(X|Y=Y_k)P(Y_k)}</script><h4 id=\"2-2朴素贝叶斯模型\"><a href=\"#2-2朴素贝叶斯模型\" class=\"headerlink\" title=\"2.2朴素贝叶斯模型\"></a>2.2朴素贝叶斯模型</h4><p>假设我们已经有部分数据，并且能从数据中得到先验概率，那么如何得到后验概率$P(Y_k|X)$呢？下面我们通过构建朴素贝叶斯分类模型来解决后验概率问题，假设分类模型数据有m个样本，每个样本有n个特征，特征输出有K个类别，定义为${C1,C2,…,Ck}$。</p>\n<script type=\"math/tex; mode=display\">\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}</script><p>从样本中我们能够学习得到朴素贝叶斯的先验分布概率</p>\n<script type=\"math/tex; mode=display\">\nP(Y=C_k)(k=1,2,…,K)</script><p>然后学习得到条件概率分布</p>\n<script type=\"math/tex; mode=display\">\nP(X|Y=C_k)=P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)</script><p>最后结合贝叶斯公式便可得到X和Y的联合分布$P(X,Y=C_k)$</p>\n<script type=\"math/tex; mode=display\">\nP(X,Y=C_k)=P(Y=C_k)P(X|Y=C_k)\\ \\ \\ \\ (1)</script><script type=\"math/tex; mode=display\">\n=P(Y=C_k)P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k) \\ \\ \\ \\ (2)</script><p>上式中$P(Y=C_k)$可以通过极大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)$很难求出，因此朴素贝叶斯模型做一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出</p>\n<script type=\"math/tex; mode=display\">\nP(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)</script><script type=\"math/tex; mode=display\">\n=P(X_1=x^{(1)}|Y=C_k)P(X_2=x^{(2)}|Y=C_k),...,P(X_n=x^{(n)}|Y=C_k)</script><p>这样我们便得到X和Y的联合分布，最后的问题是给定测试集$(x_1^{(test)},x_2^{(test)},…,x_n^{(test)})$，如何判断它属于哪个类型？</p>\n<h4 id=\"2-3朴素贝叶斯推断\"><a href=\"#2-3朴素贝叶斯推断\" class=\"headerlink\" title=\"2.3朴素贝叶斯推断\"></a>2.3朴素贝叶斯推断</h4><p>假如我们预测的类别结果为$C_{result}$，其中$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大的类别，数学表达式为</p>\n<script type=\"math/tex; mode=display\">\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k|X=X^{(test)})</script><script type=\"math/tex; mode=display\">\n= \\underset{C_k}{\\underbrace{\\arg\\max}} \\frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}</script><p>由于对所有类别计算$P(Y=C_k|X=X^{(test)})$时，分母都是$P(X=X^{(test)})$，因此我们的预测公式可以简化为</p>\n<script type=\"math/tex; mode=display\">\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(X=X^{(test)}|Y=C_k)P(Y=C_k)</script><p>然后利用朴素贝叶斯的独立性假设，就可以得到朴素贝叶斯推断公式</p>\n<script type=\"math/tex; mode=display\">\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)</script><h4 id=\"2-4朴素贝叶斯参数估计\"><a href=\"#2-4朴素贝叶斯参数估计\" class=\"headerlink\" title=\"2.4朴素贝叶斯参数估计\"></a>2.4朴素贝叶斯参数估计</h4><p>对于<strong>2.3</strong>中朴素贝叶斯推断公式，我们只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$，就可以得到预测结果。对于$P(Y=C_k)$比较简单，通过极大似然估计能够得到$C_k$出现的概率，也就是样本类别$C_k$出现的次数$m_k$除以样本总数m。而对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$则取决于我们的先验条件。</p>\n<ul>\n<li>如果$X_j$是<strong>离散值</strong>，那么可以假设$X_j$服从多项式分布，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别中$X_j^{test}$出现的频率，公式如下所示。其中$m_k$为样本类别$C_k$出现的次数，而$m_{kj}^{(test)}$为类别是$C_k$的样本中，第j维特征$X_j^{(test)}$出现的次数。<script type=\"math/tex; mode=display\">\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}}{m_k}</script>某些时候可能一些类别在样本中从来没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，因此会影响后验概率的估计。为了解决此类情况，我们引入拉普拉斯平滑，公式如下所示。其中$\\lambda$为大于0的常数，常取为1，$O_j$为第j个特征的取值个数。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}+\\lambda}{m_k+O_j \\lambda}</script><ul>\n<li>如果$X_j$是<strong>稀疏的离散值</strong>，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中$X_j^{(test)}$出现的频率，公式如下所示。其中$X_j^{(test)}$取值为0和1。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=X_j^{(test)}|Y=C_k)=P(X_j|Y=C_k)X_j^{(test)}+(1-P(X_j|Y=C_k))(1-X_j^{(test)})</script><ul>\n<li>如果$X_j$是<strong>连续值</strong>，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布公式如下所示。其中$\\mu_k$和$\\sigma_{k}^2$是正态分布的期望和方差，$\\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\\mu_k$和$\\sigma _{k}^2$可以通过极大似然估计求得。<script type=\"math/tex; mode=display\">\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp(-\\frac{(X_j^{(test)}- \\mu_k)^2}{2\\sigma_{k}^{2}})</script></li>\n</ul>\n<h3 id=\"3-朴素贝叶斯算法流程\"><a href=\"#3-朴素贝叶斯算法流程\" class=\"headerlink\" title=\"3.朴素贝叶斯算法流程\"></a>3.朴素贝叶斯算法流程</h3><p>我们总结下朴素贝叶斯算法流程，假设训练集有m个样本n个维度，训练样本有K个特征输出类别，分别为$C1,C2,…,C_K$，每个特征输出类别的样本个数为$m_1,m_2,…,m_K$。第K个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$，其中$l$的取值为$1,2,…,S_j$，$S_j$为特征$j$不同的取值数。</p>\n<script type=\"math/tex; mode=display\">\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}</script><p>预测结果为$X^{(test)}​$的分类，算法流程如下所示</p>\n<ul>\n<li><p>如果没有Y的先验概率，则计算Y的K个先验概率$P(Y=C_k)=\\frac{m_k}{m}$。</p>\n</li>\n<li><p>分别计算第k个类别的第j为特征的第l个取值条件概率$P(X_j=x_{jl}|Y=C_k)$。</p>\n<ul>\n<li>如果是离散值，则公式如下所示，其中$\\lambda$为大于0的常数，常常取为1，$O_j$为第j个特征的取值个数。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+O_j \\lambda}</script><ul>\n<li>如果是稀疏离散值，则公式如下所示，此时$l$取值为0或1。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=x_{jl}|Y=C_k)=P(X_j|Y=C_k)x_{jl}+(1-P(X_j|Y=C_k))(1-x_{jl})</script><ul>\n<li>如果是连续值，则不需要计算各个l个取值概率，直接求正态分布的参数，公式如下所示。$\\mu_k$为样本类别$C_k$中所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中所有$X_j$的方差。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}exp(-\\frac{(x_j- \\mu_k)^2}{2\\sigma_{k}^{2}})</script></li>\n<li><p>对于实例$X^{(test)}$，分别计算</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nP(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)</script><ul>\n<li>确定实例$X^{(test)}$的分类$C_{result}$</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)</script><p>从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。</p>\n<h3 id=\"4-Sklearn实现朴素贝叶斯\"><a href=\"#4-Sklearn实现朴素贝叶斯\" class=\"headerlink\" title=\"4.Sklearn实现朴素贝叶斯\"></a>4.Sklearn实现朴素贝叶斯</h3><p>利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.933333333333。更多关于sklearn.naive_bayes的使用技巧可以访问<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\" target=\"_blank\" rel=\"noopener\">官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">mnb=GaussianNB()</span><br><span class=\"line\">mnb.fit(X_train,y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">print(mnb.predict(X_test))</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 1 2 1]</span></span><br><span class=\"line\">print(y_test)</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 2 2 1]</span></span><br><span class=\"line\">print(mnb.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.933333333333</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"5-朴素贝叶斯优缺点\"><a href=\"#5-朴素贝叶斯优缺点\" class=\"headerlink\" title=\"5.朴素贝叶斯优缺点\"></a>5.朴素贝叶斯优缺点</h3><h4 id=\"5-1优点\"><a href=\"#5-1优点\" class=\"headerlink\" title=\"5.1优点\"></a>5.1优点</h4><ul>\n<li>具有稳定的分类效率。</li>\n<li>对缺失数据不敏感，算法也比较简单。</li>\n<li>对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。</li>\n</ul>\n<h4 id=\"5-2缺点\"><a href=\"#5-2缺点\" class=\"headerlink\" title=\"5.2缺点\"></a>5.2缺点</h4><ul>\n<li>对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。</li>\n<li>由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。</li>\n<li>假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。</li>\n</ul>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6069267.html\" target=\"_blank\" rel=\"noopener\">刘建平_Pinard-朴素贝叶斯算法原理小结</a></p>\n</blockquote>\n"},{"title":"机器学习之梯度提升决策树(GBDT)","date":"2018-04-30T15:15:14.000Z","mathjax":true,"comments":1,"_content":"\n### 1.GBDT算法简介\n\n**GBDT(Gradient Boosting Decision Tree)**是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(**Gradient Boosting Decision Tree**)来展开推导过程。决策树(**Decision Tree**)我们已经不再陌生，在之前介绍到的[机器学习之决策树(C4.5算法)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483837&idx=1&sn=f73ca53c5d50f7cd090ba3bc0e17c56b&chksm=fcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3#rd)、[机器学习之分类与回归树(CART)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483841&idx=1&sn=b67c59dc4284f0b363b2de881c5da729&chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd)、[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中已经多次接触，在此不再赘述。但**Boosting**和**Gradient**方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。\n\n#### 1.1集成学习之Boosting\n\n集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。\n\n![机器学习之梯度提升决策树图片01](机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png)\n\n从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到**Boosting Decision Tree**。\n\n#### 1.2 Boosting Decision Tree\n\n**提升树(Boosting Decision Tree)**由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在[机器学习之分类与回归树(CART)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483841&idx=1&sn=b67c59dc4284f0b363b2de881c5da729&chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd)中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。\n\n我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。\n\n我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下\n\n![机器学习之梯度提升决策树图片02](机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png)\n\n我们能够直观的看到，预测值等于所有树值的累加，如**A的预测值=树1左节点(15)+树2左节点(-1)=14**。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下\n\n+ 初始化$f_0(x)=0$\n+ 对$t=1,2,3,…,T$\n  + 计算残差$r_{ti}=y_i-f_{t-1}(x_i),i=1,2,3,…,m$。\n  + 拟合残差$r_{ti}$学习得到回归树$h_t(x)$\n  + 更新$f_t(x)=f_{t-1}(x)+h_t(x)$\n+ 得到回归问题提升树$f_T(x)=f_0(x)+\\sum_{t=1}^{T}h_t(x)$\n\n我们介绍了**Boosting Decision Tree**的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到**Gradient Boosting Decision Tree的负梯度拟合**。\n\n#### 1.3GBDT负梯度拟合\n\nBoosting Decision Tree迭代过程中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$，我们本轮迭代的目标是找到一个回归树模型的弱学习器$h_t(x)$，让本轮的损失$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到的决策树，要让样本的损失函数尽量变得更小。\n\n我们利用损失函数$L(y_i,f(x_i))$的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为\n$$\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}\n$$\n利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一棵CART回归树，得到第t棵回归树，其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中$J$为叶子节点的个数。\n\n针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值$c_{tj}$。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。\n$$\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)\n$$\n这样我们便得到本轮的决策树拟合函数\n$$\nh_t(x)=\\sum _{j=1} ^{J} c_{tj},I(x \\in R_{tj})\n$$\n从而本轮最终得到的强学习器表达式如下\n$$\nf_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n$$\n通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。\n\n### 2.GBDT回归算法\n\n通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。\n\n假设训练集样本$T=\\{ (x,y_1),(x,y_2),…,(x,y_m)\\}$，最大迭代次数为$T$，损失函数$L$，输出是强学习器$f(x)$。回归算法过程如下所示\n\n+ 初始化弱学习器，c的均值可设置为样本y的均值。\n\n$$\nf_0(x)=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}L(y_i,c)\n$$\n\n+ 对迭代次数$t=1,2,3,…,T$有\n\n  + 对样本$i=1,2,3,…,m$，计算负梯度\n\n  $$\n  r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}\n  $$\n\n  + 利用$(x_i,r_{ti})i=1,2,3,…,m$，拟合一棵CART回归树，得到第t棵回归树，其对应的叶子节点区域为$R_{tj},j=1,2,3,…,J$。其中$J$为回归树$t$的叶子节点个数。\n  + 对叶子区域$j=1,2,3,…,J$，计算最佳拟合值\n\n  $$\n  c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)\n  $$\n\n  + 更新强学习器\n\n  $$\n  f_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n  $$\n\n+ 得到强学习器$f(x)$表达式\n  $$\n  f(x)=f_T(x)=f_0(x)+\\sum_{t=1}^{T}\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n  $$\n\n\n### 3.GBDT分类算法\n\nGBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。\n\n#### 3.1二元GBDT分类算法\n\n对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为\n$$\nL(y,f(x))=log(1+exp(-yf(x)))\n$$\n其中$y \\in \\{ -1,1\\}$。则此时的负梯度误差为\n$$\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}\n$$\n对于生成的决策树，我们各个叶子节点的最佳残差拟合值为\n$$\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum _{x_i\\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))\n$$\n由于上式比较难优化，我们一般使用近似值代替\n$$\nc_{tj}=\\frac{\\sum _{x_i\\in R_{tj}}r_{ti}}{\\sum _{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}\n$$\n除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。\n\n#### 3.2多元GBDT分类算法\n\n多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为\n$$\nL(y,f(x))=-\\sum_{k=1}^{K}y_k log(p_k(x))\n$$\n其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为\n$$\np_k(x)=\\frac {exp(f_k(x))}{\\sum _{l=1}^{K}exp(f_l(x))}\n$$\n集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为\n$$\nr_{til}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)\n$$\n其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为\n$$\nc_{tjl}=\\underset{cjl}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m} \\sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\\sum _{j=1}^{J}c_{jl},I(x_i\\in R_{tj})\n$$\n由于上式比较难优化，我们用近似值代替\n$$\nc_{tjl}=\\frac{K-1}{K}=\\frac{\\sum_{x_i\\in R_{tjl}}r_{til}}{\\sum _{x_i\\in R_{til}}|r_{til}|(1-|r_{til}|)}\n$$\n除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。\n\n### 4.GBDT损失函数\n\n对于**回归算法**，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。\n\n+ 均方差损失。\n\n$$\nL(y,f(x))=(y-f(x))^2\n$$\n\n+ 绝对损失和对应的负梯度误差。\n\n$$\nL(y,f(x))=|y-f(x)|\n$$\n\n$$\nsign(y_i-f(x_i))\n$$\n\n+ Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下\n\n$$\nL(y,f(x))=\\left\\{\\begin{matrix}\n\\frac{1}{2}(y-f(x))^2 &|y-f(x)|\\le \\delta \\\\ \n \\delta (|y-f(x)|-\\frac{\\delta}{2})&|y-f(x)|> \\delta \n\\end{matrix}\\right.\n$$\n\n$$\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\ny_i-f(x_i) &|y_i-f(x_i)|\\le \\delta \\\\ \n\\delta sign(y_i-f(x_i))&|y_i-f(x_i)|> \\delta \n\\end{matrix}\\right.\n$$\n\n+ 分位数损失和负梯度误差如下所示。其中其中$\\theta$为分位数，需要我们在回归前指定。\n\n$$\nL(y,f(x))=\\sum _{y\\ge f(x)} \\theta|y-f(x)|+\\sum _{y<f(x)}(1-\\theta)|y-f(x)|\n$$\n\n$$\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\n\\theta &y_i\\ge f(x_i) \\\\ \n\\theta -1 &y_i<f(x_i)\n\\end{matrix}\\right.\n$$\n\n对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。\n\n对于分类算法，常用损失函数有指数损失函数和对数损失函数。\n\n+ 对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。\n\n\n+ 指数损失函数\n  $$\n  L(y,f(x))=exp(-yf(x))\n  $$\n\n\n### 5.GBDT正则化\n\n针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。\n\n+ **子采样比例:**通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。\n\n\n+ **定义步长v:**针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n  $$\n  f_k(x)=f_{k-1}(x)+vh_k(x)\n  $$\n\n\n### 6.Sklearn实现GBDT算法\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)。\n\n```python\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.datasets import make_regression\n\nX,y=make_regression(n_samples=1000,n_features=4,\n                    n_informative=2,random_state=0)\n\nprint(X[0:10],y[0:10])\n### X Number\n# [[-0.34323505  0.73129362  0.07077408 -0.78422138]\n#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]\n#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]\n#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]\n#  [-0.97240289  1.49613964  1.34622107 -1.49026539]\n#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]\n#  [ 0.77083696  0.96234174  0.24316822  0.45730965]\n#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]\n#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]\n#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]\n\n### Y Number\n# [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376\n#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]\n\n\nclf=GradientBoostingRegressor(n_estimators=150,learning_rate=0.6,\n                              max_depth=15,random_state=0,loss='ls')\nclf.fit(X,y)\n\nprint(clf.predict([[1,-1,-1,1]]))\n# [ 25.62761791]\nprint(clf.score(X,y))\n# 0.999999999987\n```\n\n### 7.GBDT优缺点\n\n#### 7.1优点\n\n+ 相对少的调参时间情况下可以得到较高的准确率。\n\n\n+ 可灵活处理各种类型数据，包括连续值和离散值，使用范围广。\n+ 可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。\n\n#### 7.2缺点\n\n+ 弱学习器之间存在依赖关系，难以并行训练数据。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n文章参考\n\n+ [刘建平Pinard_梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html#!comments)\n+ [taotick_GBDT梯度提升决策树](https://blog.csdn.net/taoqick/article/details/72822727)","source":"_posts/机器学习之梯度提升决策树-GBDT.md","raw":"---\ntitle: 机器学习之梯度提升决策树(GBDT)\ndate: 2018-04-30 23:15:14\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.GBDT算法简介\n\n**GBDT(Gradient Boosting Decision Tree)**是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(**Gradient Boosting Decision Tree**)来展开推导过程。决策树(**Decision Tree**)我们已经不再陌生，在之前介绍到的[机器学习之决策树(C4.5算法)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483837&idx=1&sn=f73ca53c5d50f7cd090ba3bc0e17c56b&chksm=fcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3#rd)、[机器学习之分类与回归树(CART)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483841&idx=1&sn=b67c59dc4284f0b363b2de881c5da729&chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd)、[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中已经多次接触，在此不再赘述。但**Boosting**和**Gradient**方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。\n\n#### 1.1集成学习之Boosting\n\n集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。\n\n![机器学习之梯度提升决策树图片01](机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png)\n\n从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到**Boosting Decision Tree**。\n\n#### 1.2 Boosting Decision Tree\n\n**提升树(Boosting Decision Tree)**由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在[机器学习之分类与回归树(CART)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483841&idx=1&sn=b67c59dc4284f0b363b2de881c5da729&chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd)中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。\n\n我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。\n\n我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下\n\n![机器学习之梯度提升决策树图片02](机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png)\n\n我们能够直观的看到，预测值等于所有树值的累加，如**A的预测值=树1左节点(15)+树2左节点(-1)=14**。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下\n\n+ 初始化$f_0(x)=0$\n+ 对$t=1,2,3,…,T$\n  + 计算残差$r_{ti}=y_i-f_{t-1}(x_i),i=1,2,3,…,m$。\n  + 拟合残差$r_{ti}$学习得到回归树$h_t(x)$\n  + 更新$f_t(x)=f_{t-1}(x)+h_t(x)$\n+ 得到回归问题提升树$f_T(x)=f_0(x)+\\sum_{t=1}^{T}h_t(x)$\n\n我们介绍了**Boosting Decision Tree**的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到**Gradient Boosting Decision Tree的负梯度拟合**。\n\n#### 1.3GBDT负梯度拟合\n\nBoosting Decision Tree迭代过程中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$，我们本轮迭代的目标是找到一个回归树模型的弱学习器$h_t(x)$，让本轮的损失$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到的决策树，要让样本的损失函数尽量变得更小。\n\n我们利用损失函数$L(y_i,f(x_i))$的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为\n$$\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}\n$$\n利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一棵CART回归树，得到第t棵回归树，其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中$J$为叶子节点的个数。\n\n针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值$c_{tj}$。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。\n$$\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)\n$$\n这样我们便得到本轮的决策树拟合函数\n$$\nh_t(x)=\\sum _{j=1} ^{J} c_{tj},I(x \\in R_{tj})\n$$\n从而本轮最终得到的强学习器表达式如下\n$$\nf_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n$$\n通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。\n\n### 2.GBDT回归算法\n\n通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。\n\n假设训练集样本$T=\\{ (x,y_1),(x,y_2),…,(x,y_m)\\}$，最大迭代次数为$T$，损失函数$L$，输出是强学习器$f(x)$。回归算法过程如下所示\n\n+ 初始化弱学习器，c的均值可设置为样本y的均值。\n\n$$\nf_0(x)=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}L(y_i,c)\n$$\n\n+ 对迭代次数$t=1,2,3,…,T$有\n\n  + 对样本$i=1,2,3,…,m$，计算负梯度\n\n  $$\n  r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}\n  $$\n\n  + 利用$(x_i,r_{ti})i=1,2,3,…,m$，拟合一棵CART回归树，得到第t棵回归树，其对应的叶子节点区域为$R_{tj},j=1,2,3,…,J$。其中$J$为回归树$t$的叶子节点个数。\n  + 对叶子区域$j=1,2,3,…,J$，计算最佳拟合值\n\n  $$\n  c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)\n  $$\n\n  + 更新强学习器\n\n  $$\n  f_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n  $$\n\n+ 得到强学习器$f(x)$表达式\n  $$\n  f(x)=f_T(x)=f_0(x)+\\sum_{t=1}^{T}\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n  $$\n\n\n### 3.GBDT分类算法\n\nGBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。\n\n#### 3.1二元GBDT分类算法\n\n对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为\n$$\nL(y,f(x))=log(1+exp(-yf(x)))\n$$\n其中$y \\in \\{ -1,1\\}$。则此时的负梯度误差为\n$$\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}\n$$\n对于生成的决策树，我们各个叶子节点的最佳残差拟合值为\n$$\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum _{x_i\\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))\n$$\n由于上式比较难优化，我们一般使用近似值代替\n$$\nc_{tj}=\\frac{\\sum _{x_i\\in R_{tj}}r_{ti}}{\\sum _{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}\n$$\n除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。\n\n#### 3.2多元GBDT分类算法\n\n多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为\n$$\nL(y,f(x))=-\\sum_{k=1}^{K}y_k log(p_k(x))\n$$\n其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为\n$$\np_k(x)=\\frac {exp(f_k(x))}{\\sum _{l=1}^{K}exp(f_l(x))}\n$$\n集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为\n$$\nr_{til}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)\n$$\n其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为\n$$\nc_{tjl}=\\underset{cjl}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m} \\sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\\sum _{j=1}^{J}c_{jl},I(x_i\\in R_{tj})\n$$\n由于上式比较难优化，我们用近似值代替\n$$\nc_{tjl}=\\frac{K-1}{K}=\\frac{\\sum_{x_i\\in R_{tjl}}r_{til}}{\\sum _{x_i\\in R_{til}}|r_{til}|(1-|r_{til}|)}\n$$\n除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。\n\n### 4.GBDT损失函数\n\n对于**回归算法**，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。\n\n+ 均方差损失。\n\n$$\nL(y,f(x))=(y-f(x))^2\n$$\n\n+ 绝对损失和对应的负梯度误差。\n\n$$\nL(y,f(x))=|y-f(x)|\n$$\n\n$$\nsign(y_i-f(x_i))\n$$\n\n+ Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下\n\n$$\nL(y,f(x))=\\left\\{\\begin{matrix}\n\\frac{1}{2}(y-f(x))^2 &|y-f(x)|\\le \\delta \\\\ \n \\delta (|y-f(x)|-\\frac{\\delta}{2})&|y-f(x)|> \\delta \n\\end{matrix}\\right.\n$$\n\n$$\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\ny_i-f(x_i) &|y_i-f(x_i)|\\le \\delta \\\\ \n\\delta sign(y_i-f(x_i))&|y_i-f(x_i)|> \\delta \n\\end{matrix}\\right.\n$$\n\n+ 分位数损失和负梯度误差如下所示。其中其中$\\theta$为分位数，需要我们在回归前指定。\n\n$$\nL(y,f(x))=\\sum _{y\\ge f(x)} \\theta|y-f(x)|+\\sum _{y<f(x)}(1-\\theta)|y-f(x)|\n$$\n\n$$\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\n\\theta &y_i\\ge f(x_i) \\\\ \n\\theta -1 &y_i<f(x_i)\n\\end{matrix}\\right.\n$$\n\n对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。\n\n对于分类算法，常用损失函数有指数损失函数和对数损失函数。\n\n+ 对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。\n\n\n+ 指数损失函数\n  $$\n  L(y,f(x))=exp(-yf(x))\n  $$\n\n\n### 5.GBDT正则化\n\n针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。\n\n+ **子采样比例:**通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。\n\n\n+ **定义步长v:**针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n  $$\n  f_k(x)=f_{k-1}(x)+vh_k(x)\n  $$\n\n\n### 6.Sklearn实现GBDT算法\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)。\n\n```python\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.datasets import make_regression\n\nX,y=make_regression(n_samples=1000,n_features=4,\n                    n_informative=2,random_state=0)\n\nprint(X[0:10],y[0:10])\n### X Number\n# [[-0.34323505  0.73129362  0.07077408 -0.78422138]\n#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]\n#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]\n#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]\n#  [-0.97240289  1.49613964  1.34622107 -1.49026539]\n#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]\n#  [ 0.77083696  0.96234174  0.24316822  0.45730965]\n#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]\n#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]\n#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]\n\n### Y Number\n# [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376\n#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]\n\n\nclf=GradientBoostingRegressor(n_estimators=150,learning_rate=0.6,\n                              max_depth=15,random_state=0,loss='ls')\nclf.fit(X,y)\n\nprint(clf.predict([[1,-1,-1,1]]))\n# [ 25.62761791]\nprint(clf.score(X,y))\n# 0.999999999987\n```\n\n### 7.GBDT优缺点\n\n#### 7.1优点\n\n+ 相对少的调参时间情况下可以得到较高的准确率。\n\n\n+ 可灵活处理各种类型数据，包括连续值和离散值，使用范围广。\n+ 可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。\n\n#### 7.2缺点\n\n+ 弱学习器之间存在依赖关系，难以并行训练数据。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n文章参考\n\n+ [刘建平Pinard_梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html#!comments)\n+ [taotick_GBDT梯度提升决策树](https://blog.csdn.net/taoqick/article/details/72822727)","slug":"机器学习之梯度提升决策树-GBDT","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4h001kjiz5uby8906t","content":"<h3 id=\"1-GBDT算法简介\"><a href=\"#1-GBDT算法简介\" class=\"headerlink\" title=\"1.GBDT算法简介\"></a>1.GBDT算法简介</h3><p><strong>GBDT(Gradient Boosting Decision Tree)</strong>是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(<strong>Gradient Boosting Decision Tree</strong>)来展开推导过程。决策树(<strong>Decision Tree</strong>)我们已经不再陌生，在之前介绍到的<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483837&amp;idx=1&amp;sn=f73ca53c5d50f7cd090ba3bc0e17c56b&amp;chksm=fcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3#rd\" target=\"_blank\" rel=\"noopener\">机器学习之决策树(C4.5算法)</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483841&amp;idx=1&amp;sn=b67c59dc4284f0b363b2de881c5da729&amp;chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd\" target=\"_blank\" rel=\"noopener\">机器学习之分类与回归树(CART)</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中已经多次接触，在此不再赘述。但<strong>Boosting</strong>和<strong>Gradient</strong>方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。</p>\n<h4 id=\"1-1集成学习之Boosting\"><a href=\"#1-1集成学习之Boosting\" class=\"headerlink\" title=\"1.1集成学习之Boosting\"></a>1.1集成学习之Boosting</h4><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p>\n<p><img src=\"/2018/04/30/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png\" alt=\"机器学习之梯度提升决策树图片01\"></p>\n<p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到<strong>Boosting Decision Tree</strong>。</p>\n<h4 id=\"1-2-Boosting-Decision-Tree\"><a href=\"#1-2-Boosting-Decision-Tree\" class=\"headerlink\" title=\"1.2 Boosting Decision Tree\"></a>1.2 Boosting Decision Tree</h4><p><strong>提升树(Boosting Decision Tree)</strong>由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483841&amp;idx=1&amp;sn=b67c59dc4284f0b363b2de881c5da729&amp;chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd\" target=\"_blank\" rel=\"noopener\">机器学习之分类与回归树(CART)</a>中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。</p>\n<p>我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。</p>\n<p>我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下</p>\n<p><img src=\"/2018/04/30/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png\" alt=\"机器学习之梯度提升决策树图片02\"></p>\n<p>我们能够直观的看到，预测值等于所有树值的累加，如<strong>A的预测值=树1左节点(15)+树2左节点(-1)=14</strong>。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下</p>\n<ul>\n<li>初始化$f_0(x)=0$</li>\n<li>对$t=1,2,3,…,T$<ul>\n<li>计算残差$r_{ti}=y_i-f_{t-1}(x_i),i=1,2,3,…,m$。</li>\n<li>拟合残差$r_{ti}$学习得到回归树$h_t(x)$</li>\n<li>更新$f_t(x)=f_{t-1}(x)+h_t(x)$</li>\n</ul>\n</li>\n<li>得到回归问题提升树$f_T(x)=f_0(x)+\\sum_{t=1}^{T}h_t(x)$</li>\n</ul>\n<p>我们介绍了<strong>Boosting Decision Tree</strong>的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到<strong>Gradient Boosting Decision Tree的负梯度拟合</strong>。</p>\n<h4 id=\"1-3GBDT负梯度拟合\"><a href=\"#1-3GBDT负梯度拟合\" class=\"headerlink\" title=\"1.3GBDT负梯度拟合\"></a>1.3GBDT负梯度拟合</h4><p>Boosting Decision Tree迭代过程中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$，我们本轮迭代的目标是找到一个回归树模型的弱学习器$h_t(x)$，让本轮的损失$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到的决策树，要让样本的损失函数尽量变得更小。</p>\n<p>我们利用损失函数$L(y_i,f(x_i))$的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为</p>\n<script type=\"math/tex; mode=display\">\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}</script><p>利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一棵CART回归树，得到第t棵回归树，其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中$J$为叶子节点的个数。</p>\n<p>针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值$c_{tj}$。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。</p>\n<script type=\"math/tex; mode=display\">\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)</script><p>这样我们便得到本轮的决策树拟合函数</p>\n<script type=\"math/tex; mode=display\">\nh_t(x)=\\sum _{j=1} ^{J} c_{tj},I(x \\in R_{tj})</script><p>从而本轮最终得到的强学习器表达式如下</p>\n<script type=\"math/tex; mode=display\">\nf_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})</script><p>通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。</p>\n<h3 id=\"2-GBDT回归算法\"><a href=\"#2-GBDT回归算法\" class=\"headerlink\" title=\"2.GBDT回归算法\"></a>2.GBDT回归算法</h3><p>通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。</p>\n<p>假设训练集样本$T=\\{ (x,y_1),(x,y_2),…,(x,y_m)\\}$，最大迭代次数为$T$，损失函数$L$，输出是强学习器$f(x)$。回归算法过程如下所示</p>\n<ul>\n<li>初始化弱学习器，c的均值可设置为样本y的均值。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_0(x)=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}L(y_i,c)</script><ul>\n<li><p>对迭代次数$t=1,2,3,…,T$有</p>\n<ul>\n<li>对样本$i=1,2,3,…,m$，计算负梯度</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}</script><ul>\n<li>利用$(x_i,r_{ti})i=1,2,3,…,m$，拟合一棵CART回归树，得到第t棵回归树，其对应的叶子节点区域为$R_{tj},j=1,2,3,…,J$。其中$J$为回归树$t$的叶子节点个数。</li>\n<li>对叶子区域$j=1,2,3,…,J$，计算最佳拟合值</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)</script><ul>\n<li>更新强学习器</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})</script></li>\n<li><p>得到强学习器$f(x)$表达式</p>\n<script type=\"math/tex; mode=display\">\nf(x)=f_T(x)=f_0(x)+\\sum_{t=1}^{T}\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})</script></li>\n</ul>\n<h3 id=\"3-GBDT分类算法\"><a href=\"#3-GBDT分类算法\" class=\"headerlink\" title=\"3.GBDT分类算法\"></a>3.GBDT分类算法</h3><p>GBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。</p>\n<h4 id=\"3-1二元GBDT分类算法\"><a href=\"#3-1二元GBDT分类算法\" class=\"headerlink\" title=\"3.1二元GBDT分类算法\"></a>3.1二元GBDT分类算法</h4><p>对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为</p>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=log(1+exp(-yf(x)))</script><p>其中$y \\in \\{ -1,1\\}$。则此时的负梯度误差为</p>\n<script type=\"math/tex; mode=display\">\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}</script><p>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p>\n<script type=\"math/tex; mode=display\">\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum _{x_i\\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))</script><p>由于上式比较难优化，我们一般使用近似值代替</p>\n<script type=\"math/tex; mode=display\">\nc_{tj}=\\frac{\\sum _{x_i\\in R_{tj}}r_{ti}}{\\sum _{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}</script><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。</p>\n<h4 id=\"3-2多元GBDT分类算法\"><a href=\"#3-2多元GBDT分类算法\" class=\"headerlink\" title=\"3.2多元GBDT分类算法\"></a>3.2多元GBDT分类算法</h4><p>多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为</p>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=-\\sum_{k=1}^{K}y_k log(p_k(x))</script><p>其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为</p>\n<script type=\"math/tex; mode=display\">\np_k(x)=\\frac {exp(f_k(x))}{\\sum _{l=1}^{K}exp(f_l(x))}</script><p>集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为</p>\n<script type=\"math/tex; mode=display\">\nr_{til}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)</script><p>其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p>\n<script type=\"math/tex; mode=display\">\nc_{tjl}=\\underset{cjl}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m} \\sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\\sum _{j=1}^{J}c_{jl},I(x_i\\in R_{tj})</script><p>由于上式比较难优化，我们用近似值代替</p>\n<script type=\"math/tex; mode=display\">\nc_{tjl}=\\frac{K-1}{K}=\\frac{\\sum_{x_i\\in R_{tjl}}r_{til}}{\\sum _{x_i\\in R_{til}}|r_{til}|(1-|r_{til}|)}</script><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p>\n<h3 id=\"4-GBDT损失函数\"><a href=\"#4-GBDT损失函数\" class=\"headerlink\" title=\"4.GBDT损失函数\"></a>4.GBDT损失函数</h3><p>对于<strong>回归算法</strong>，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。</p>\n<ul>\n<li>均方差损失。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=(y-f(x))^2</script><ul>\n<li>绝对损失和对应的负梯度误差。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=|y-f(x)|</script><script type=\"math/tex; mode=display\">\nsign(y_i-f(x_i))</script><ul>\n<li>Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=\\left\\{\\begin{matrix}\n\\frac{1}{2}(y-f(x))^2 &|y-f(x)|\\le \\delta \\\\ \n \\delta (|y-f(x)|-\\frac{\\delta}{2})&|y-f(x)|> \\delta \n\\end{matrix}\\right.</script><script type=\"math/tex; mode=display\">\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\ny_i-f(x_i) &|y_i-f(x_i)|\\le \\delta \\\\ \n\\delta sign(y_i-f(x_i))&|y_i-f(x_i)|> \\delta \n\\end{matrix}\\right.</script><ul>\n<li>分位数损失和负梯度误差如下所示。其中其中$\\theta$为分位数，需要我们在回归前指定。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=\\sum _{y\\ge f(x)} \\theta|y-f(x)|+\\sum _{y<f(x)}(1-\\theta)|y-f(x)|</script><script type=\"math/tex; mode=display\">\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\n\\theta &y_i\\ge f(x_i) \\\\ \n\\theta -1 &y_i<f(x_i)\n\\end{matrix}\\right.</script><p>对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p>\n<p>对于分类算法，常用损失函数有指数损失函数和对数损失函数。</p>\n<ul>\n<li>对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。</li>\n</ul>\n<ul>\n<li>指数损失函数<script type=\"math/tex; mode=display\">\nL(y,f(x))=exp(-yf(x))</script></li>\n</ul>\n<h3 id=\"5-GBDT正则化\"><a href=\"#5-GBDT正则化\" class=\"headerlink\" title=\"5.GBDT正则化\"></a>5.GBDT正则化</h3><p>针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。</p>\n<ul>\n<li><strong>子采样比例:</strong>通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。</li>\n</ul>\n<ul>\n<li><strong>定义步长v:</strong>针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。<script type=\"math/tex; mode=display\">\nf_k(x)=f_{k-1}(x)+vh_k(x)</script></li>\n</ul>\n<h3 id=\"6-Sklearn实现GBDT算法\"><a href=\"#6-Sklearn实现GBDT算法\" class=\"headerlink\" title=\"6.Sklearn实现GBDT算法\"></a>6.Sklearn实现GBDT算法</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> GradientBoostingRegressor</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_regression</span><br><span class=\"line\"></span><br><span class=\"line\">X,y=make_regression(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                    n_informative=<span class=\"number\">2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(X[<span class=\"number\">0</span>:<span class=\"number\">10</span>],y[<span class=\"number\">0</span>:<span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"comment\">### X Number</span></span><br><span class=\"line\"><span class=\"comment\"># [[-0.34323505  0.73129362  0.07077408 -0.78422138]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.97240289  1.49613964  1.34622107 -1.49026539]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.77083696  0.96234174  0.24316822  0.45730965]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">### Y Number</span></span><br><span class=\"line\"><span class=\"comment\"># [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376</span></span><br><span class=\"line\"><span class=\"comment\">#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">clf=GradientBoostingRegressor(n_estimators=<span class=\"number\">150</span>,learning_rate=<span class=\"number\">0.6</span>,</span><br><span class=\"line\">                              max_depth=<span class=\"number\">15</span>,random_state=<span class=\"number\">0</span>,loss=<span class=\"string\">'ls'</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\">print(clf.predict([[<span class=\"number\">1</span>,<span class=\"number\">-1</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>]]))</span><br><span class=\"line\"><span class=\"comment\"># [ 25.62761791]</span></span><br><span class=\"line\">print(clf.score(X,y))</span><br><span class=\"line\"><span class=\"comment\"># 0.999999999987</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"7-GBDT优缺点\"><a href=\"#7-GBDT优缺点\" class=\"headerlink\" title=\"7.GBDT优缺点\"></a>7.GBDT优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li>相对少的调参时间情况下可以得到较高的准确率。</li>\n</ul>\n<ul>\n<li>可灵活处理各种类型数据，包括连续值和离散值，使用范围广。</li>\n<li>可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li>弱学习器之间存在依赖关系，难以并行训练数据。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>文章参考</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6140514.html#!comments\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_梯度提升树(GBDT)原理小结</a></li>\n<li><a href=\"https://blog.csdn.net/taoqick/article/details/72822727\" target=\"_blank\" rel=\"noopener\">taotick_GBDT梯度提升决策树</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-GBDT算法简介\"><a href=\"#1-GBDT算法简介\" class=\"headerlink\" title=\"1.GBDT算法简介\"></a>1.GBDT算法简介</h3><p><strong>GBDT(Gradient Boosting Decision Tree)</strong>是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(<strong>Gradient Boosting Decision Tree</strong>)来展开推导过程。决策树(<strong>Decision Tree</strong>)我们已经不再陌生，在之前介绍到的<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483837&amp;idx=1&amp;sn=f73ca53c5d50f7cd090ba3bc0e17c56b&amp;chksm=fcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3#rd\" target=\"_blank\" rel=\"noopener\">机器学习之决策树(C4.5算法)</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483841&amp;idx=1&amp;sn=b67c59dc4284f0b363b2de881c5da729&amp;chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd\" target=\"_blank\" rel=\"noopener\">机器学习之分类与回归树(CART)</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中已经多次接触，在此不再赘述。但<strong>Boosting</strong>和<strong>Gradient</strong>方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。</p>\n<h4 id=\"1-1集成学习之Boosting\"><a href=\"#1-1集成学习之Boosting\" class=\"headerlink\" title=\"1.1集成学习之Boosting\"></a>1.1集成学习之Boosting</h4><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p>\n<p><img src=\"/2018/04/30/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png\" alt=\"机器学习之梯度提升决策树图片01\"></p>\n<p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到<strong>Boosting Decision Tree</strong>。</p>\n<h4 id=\"1-2-Boosting-Decision-Tree\"><a href=\"#1-2-Boosting-Decision-Tree\" class=\"headerlink\" title=\"1.2 Boosting Decision Tree\"></a>1.2 Boosting Decision Tree</h4><p><strong>提升树(Boosting Decision Tree)</strong>由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483841&amp;idx=1&amp;sn=b67c59dc4284f0b363b2de881c5da729&amp;chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd\" target=\"_blank\" rel=\"noopener\">机器学习之分类与回归树(CART)</a>中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。</p>\n<p>我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。</p>\n<p>我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下</p>\n<p><img src=\"/2018/04/30/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png\" alt=\"机器学习之梯度提升决策树图片02\"></p>\n<p>我们能够直观的看到，预测值等于所有树值的累加，如<strong>A的预测值=树1左节点(15)+树2左节点(-1)=14</strong>。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下</p>\n<ul>\n<li>初始化$f_0(x)=0$</li>\n<li>对$t=1,2,3,…,T$<ul>\n<li>计算残差$r_{ti}=y_i-f_{t-1}(x_i),i=1,2,3,…,m$。</li>\n<li>拟合残差$r_{ti}$学习得到回归树$h_t(x)$</li>\n<li>更新$f_t(x)=f_{t-1}(x)+h_t(x)$</li>\n</ul>\n</li>\n<li>得到回归问题提升树$f_T(x)=f_0(x)+\\sum_{t=1}^{T}h_t(x)$</li>\n</ul>\n<p>我们介绍了<strong>Boosting Decision Tree</strong>的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到<strong>Gradient Boosting Decision Tree的负梯度拟合</strong>。</p>\n<h4 id=\"1-3GBDT负梯度拟合\"><a href=\"#1-3GBDT负梯度拟合\" class=\"headerlink\" title=\"1.3GBDT负梯度拟合\"></a>1.3GBDT负梯度拟合</h4><p>Boosting Decision Tree迭代过程中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$，我们本轮迭代的目标是找到一个回归树模型的弱学习器$h_t(x)$，让本轮的损失$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到的决策树，要让样本的损失函数尽量变得更小。</p>\n<p>我们利用损失函数$L(y_i,f(x_i))$的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为</p>\n<script type=\"math/tex; mode=display\">\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}</script><p>利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一棵CART回归树，得到第t棵回归树，其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中$J$为叶子节点的个数。</p>\n<p>针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值$c_{tj}$。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。</p>\n<script type=\"math/tex; mode=display\">\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)</script><p>这样我们便得到本轮的决策树拟合函数</p>\n<script type=\"math/tex; mode=display\">\nh_t(x)=\\sum _{j=1} ^{J} c_{tj},I(x \\in R_{tj})</script><p>从而本轮最终得到的强学习器表达式如下</p>\n<script type=\"math/tex; mode=display\">\nf_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})</script><p>通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。</p>\n<h3 id=\"2-GBDT回归算法\"><a href=\"#2-GBDT回归算法\" class=\"headerlink\" title=\"2.GBDT回归算法\"></a>2.GBDT回归算法</h3><p>通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。</p>\n<p>假设训练集样本$T=\\{ (x,y_1),(x,y_2),…,(x,y_m)\\}$，最大迭代次数为$T$，损失函数$L$，输出是强学习器$f(x)$。回归算法过程如下所示</p>\n<ul>\n<li>初始化弱学习器，c的均值可设置为样本y的均值。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_0(x)=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}L(y_i,c)</script><ul>\n<li><p>对迭代次数$t=1,2,3,…,T$有</p>\n<ul>\n<li>对样本$i=1,2,3,…,m$，计算负梯度</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}</script><ul>\n<li>利用$(x_i,r_{ti})i=1,2,3,…,m$，拟合一棵CART回归树，得到第t棵回归树，其对应的叶子节点区域为$R_{tj},j=1,2,3,…,J$。其中$J$为回归树$t$的叶子节点个数。</li>\n<li>对叶子区域$j=1,2,3,…,J$，计算最佳拟合值</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)</script><ul>\n<li>更新强学习器</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})</script></li>\n<li><p>得到强学习器$f(x)$表达式</p>\n<script type=\"math/tex; mode=display\">\nf(x)=f_T(x)=f_0(x)+\\sum_{t=1}^{T}\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})</script></li>\n</ul>\n<h3 id=\"3-GBDT分类算法\"><a href=\"#3-GBDT分类算法\" class=\"headerlink\" title=\"3.GBDT分类算法\"></a>3.GBDT分类算法</h3><p>GBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。</p>\n<h4 id=\"3-1二元GBDT分类算法\"><a href=\"#3-1二元GBDT分类算法\" class=\"headerlink\" title=\"3.1二元GBDT分类算法\"></a>3.1二元GBDT分类算法</h4><p>对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为</p>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=log(1+exp(-yf(x)))</script><p>其中$y \\in \\{ -1,1\\}$。则此时的负梯度误差为</p>\n<script type=\"math/tex; mode=display\">\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}</script><p>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p>\n<script type=\"math/tex; mode=display\">\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum _{x_i\\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))</script><p>由于上式比较难优化，我们一般使用近似值代替</p>\n<script type=\"math/tex; mode=display\">\nc_{tj}=\\frac{\\sum _{x_i\\in R_{tj}}r_{ti}}{\\sum _{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}</script><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。</p>\n<h4 id=\"3-2多元GBDT分类算法\"><a href=\"#3-2多元GBDT分类算法\" class=\"headerlink\" title=\"3.2多元GBDT分类算法\"></a>3.2多元GBDT分类算法</h4><p>多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为</p>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=-\\sum_{k=1}^{K}y_k log(p_k(x))</script><p>其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为</p>\n<script type=\"math/tex; mode=display\">\np_k(x)=\\frac {exp(f_k(x))}{\\sum _{l=1}^{K}exp(f_l(x))}</script><p>集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为</p>\n<script type=\"math/tex; mode=display\">\nr_{til}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)</script><p>其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p>\n<script type=\"math/tex; mode=display\">\nc_{tjl}=\\underset{cjl}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m} \\sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\\sum _{j=1}^{J}c_{jl},I(x_i\\in R_{tj})</script><p>由于上式比较难优化，我们用近似值代替</p>\n<script type=\"math/tex; mode=display\">\nc_{tjl}=\\frac{K-1}{K}=\\frac{\\sum_{x_i\\in R_{tjl}}r_{til}}{\\sum _{x_i\\in R_{til}}|r_{til}|(1-|r_{til}|)}</script><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p>\n<h3 id=\"4-GBDT损失函数\"><a href=\"#4-GBDT损失函数\" class=\"headerlink\" title=\"4.GBDT损失函数\"></a>4.GBDT损失函数</h3><p>对于<strong>回归算法</strong>，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。</p>\n<ul>\n<li>均方差损失。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=(y-f(x))^2</script><ul>\n<li>绝对损失和对应的负梯度误差。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=|y-f(x)|</script><script type=\"math/tex; mode=display\">\nsign(y_i-f(x_i))</script><ul>\n<li>Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=\\left\\{\\begin{matrix}\n\\frac{1}{2}(y-f(x))^2 &|y-f(x)|\\le \\delta \\\\ \n \\delta (|y-f(x)|-\\frac{\\delta}{2})&|y-f(x)|> \\delta \n\\end{matrix}\\right.</script><script type=\"math/tex; mode=display\">\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\ny_i-f(x_i) &|y_i-f(x_i)|\\le \\delta \\\\ \n\\delta sign(y_i-f(x_i))&|y_i-f(x_i)|> \\delta \n\\end{matrix}\\right.</script><ul>\n<li>分位数损失和负梯度误差如下所示。其中其中$\\theta$为分位数，需要我们在回归前指定。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL(y,f(x))=\\sum _{y\\ge f(x)} \\theta|y-f(x)|+\\sum _{y<f(x)}(1-\\theta)|y-f(x)|</script><script type=\"math/tex; mode=display\">\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\n\\theta &y_i\\ge f(x_i) \\\\ \n\\theta -1 &y_i<f(x_i)\n\\end{matrix}\\right.</script><p>对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p>\n<p>对于分类算法，常用损失函数有指数损失函数和对数损失函数。</p>\n<ul>\n<li>对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。</li>\n</ul>\n<ul>\n<li>指数损失函数<script type=\"math/tex; mode=display\">\nL(y,f(x))=exp(-yf(x))</script></li>\n</ul>\n<h3 id=\"5-GBDT正则化\"><a href=\"#5-GBDT正则化\" class=\"headerlink\" title=\"5.GBDT正则化\"></a>5.GBDT正则化</h3><p>针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。</p>\n<ul>\n<li><strong>子采样比例:</strong>通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。</li>\n</ul>\n<ul>\n<li><strong>定义步长v:</strong>针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。<script type=\"math/tex; mode=display\">\nf_k(x)=f_{k-1}(x)+vh_k(x)</script></li>\n</ul>\n<h3 id=\"6-Sklearn实现GBDT算法\"><a href=\"#6-Sklearn实现GBDT算法\" class=\"headerlink\" title=\"6.Sklearn实现GBDT算法\"></a>6.Sklearn实现GBDT算法</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> GradientBoostingRegressor</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_regression</span><br><span class=\"line\"></span><br><span class=\"line\">X,y=make_regression(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                    n_informative=<span class=\"number\">2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(X[<span class=\"number\">0</span>:<span class=\"number\">10</span>],y[<span class=\"number\">0</span>:<span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"comment\">### X Number</span></span><br><span class=\"line\"><span class=\"comment\"># [[-0.34323505  0.73129362  0.07077408 -0.78422138]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.97240289  1.49613964  1.34622107 -1.49026539]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.77083696  0.96234174  0.24316822  0.45730965]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">### Y Number</span></span><br><span class=\"line\"><span class=\"comment\"># [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376</span></span><br><span class=\"line\"><span class=\"comment\">#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">clf=GradientBoostingRegressor(n_estimators=<span class=\"number\">150</span>,learning_rate=<span class=\"number\">0.6</span>,</span><br><span class=\"line\">                              max_depth=<span class=\"number\">15</span>,random_state=<span class=\"number\">0</span>,loss=<span class=\"string\">'ls'</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\">print(clf.predict([[<span class=\"number\">1</span>,<span class=\"number\">-1</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>]]))</span><br><span class=\"line\"><span class=\"comment\"># [ 25.62761791]</span></span><br><span class=\"line\">print(clf.score(X,y))</span><br><span class=\"line\"><span class=\"comment\"># 0.999999999987</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"7-GBDT优缺点\"><a href=\"#7-GBDT优缺点\" class=\"headerlink\" title=\"7.GBDT优缺点\"></a>7.GBDT优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li>相对少的调参时间情况下可以得到较高的准确率。</li>\n</ul>\n<ul>\n<li>可灵活处理各种类型数据，包括连续值和离散值，使用范围广。</li>\n<li>可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li>弱学习器之间存在依赖关系，难以并行训练数据。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>文章参考</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6140514.html#!comments\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_梯度提升树(GBDT)原理小结</a></li>\n<li><a href=\"https://blog.csdn.net/taoqick/article/details/72822727\" target=\"_blank\" rel=\"noopener\">taotick_GBDT梯度提升决策树</a></li>\n</ul>\n"},{"title":"机器学习之线性回归","date":"2018-03-24T15:27:53.000Z","comments":1,"mathjax":true,"_content":"### 1.线性回归分析（ Linear Regression Analysis）\n**线性回归分析（Regression Analysis）**：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。\n通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于**监督学习**。\n![图片01](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8701.png)\n上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。\n\n### 2.模型表达\n\n建立数学模型之前，我们先定义如下变量。\n\n+ $x_i$表示输入数据（Feature）\n+ $y_i$表示输出数据（Target）\n+ $(x_i,y_i)$表示一组训练数据（Training example）\n+ m表示训练数据的个数\n+ n表示特征数量\n\n监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)->y$。针对线性回归而言，函数$h(x)$表达式为\n$$\nh(x)=\\theta_0+\\theta_1*x_i+\\theta_2*x_2+...+\\theta_n*x_n\n$$\n为方便我们使用矩阵来表达，$h(x)=\\theta^T*x$，其中$\\theta^T$为$\\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。\n\n### 3.梯度下降算法\n#### 3.1梯度下降算法简述\n实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。\n\n![图片02](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8702.png)\n从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。\n#### 3.2梯度下降算法相关概念\n\n求解梯度下降算法之前，我们先了解相关概念。\n\n+ **步长（Learning Rate）**：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。\n+ **特征（Feature）**：即上述描述的$x_i,y_i$\n+ **假设函数（Hypothesis Function）**：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$\n+ **损失函数（Loss Function）**：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数越小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为\n\n$$\nJ(\\theta)=\\frac{1}{2m}*\\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})\n$$\n\n我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。\n\n#### 3.3梯度下降算法过程\n\n- 随机选取一组$\\theta$。\n- 不断变化$\\theta$，让$J(\\theta)$变小。\n    $$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$\n$j=0,1,2...n$，$\\theta_j$是n+1个值同时变化。$\\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\\theta$前为负号。$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导。\n- 直到$J(\\theta)​$得到最小值。\n\n$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导求解过程如下：\n$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}(h_\\theta(x)-y)^2$$\n$$=2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$\n$$=(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$\n$$=(h_\\theta(x)-y)\\cdot x_j$$\n因此梯度下降算法的最终表述为\n\nRepeat Until Convergence{\n$$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{n}((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j) $$ \n\nfor every  $j$\n\n}\n\n梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到线性回归函数。\n\n### 4.线性回归算法实现\n\n为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址在[这儿](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv) ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\n\n#read_csv\nreaddata=pd.read_csv('data/Advertising.csv')\ndata=np.array(readdata.values)\n\n#训练数据\nX_train=data[0:150,1:3]\nY_train=data[0:150,3]\n\n#测试数据\nX_test=data[150:200,1:3]\nY_test=data[150:200,3]\n\n#回归分析\nregr = linear_model.LinearRegression()\n#进行training set和test set的fit，即是训练的过程\nregr.fit(X_train, Y_train)\n\n# 打印出相关系数和截距等信息\nprint('Coefficients: \\n', regr.coef_)\nprint('Intercept: ', regr.intercept_)\n# The mean square error\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - Y_test) ** 2))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(X_test, Y_test))\n\n#得出回归函数 并自定义数据\nX_line=np.linspace(0,300)\nY_line=np.linspace(0,50)\nZ_line=0.04699836*X_line+0.17913965*Y_line+3.00431061176\n\n#画图\nfig=plt.figure()\nax = plt.subplot(111, projection='3d')  # 创建一个三维的绘图工程\nax.scatter(data[:,1],data[:,2],data[:,3],c='red',)  # 绘制数据点\nax.plot(X_line,Y_line,Z_line,c='blue')#绘制回归曲线\nplt.show()\n```\n\n![图片](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8703.png)\n其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。\n\n------\n\n### 5.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8E%A8%E5%B9%BF.png)\n\n\n\n","source":"_posts/机器学习之线性回归.md","raw":"---\ntitle: 机器学习之线性回归\ndate: 2018-03-24 23:27:53\ntags: [机器学习,算法]\ncategories: 机器学习\ncomments: true\nmathjax: true\n---\n### 1.线性回归分析（ Linear Regression Analysis）\n**线性回归分析（Regression Analysis）**：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。\n通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于**监督学习**。\n![图片01](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8701.png)\n上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。\n\n### 2.模型表达\n\n建立数学模型之前，我们先定义如下变量。\n\n+ $x_i$表示输入数据（Feature）\n+ $y_i$表示输出数据（Target）\n+ $(x_i,y_i)$表示一组训练数据（Training example）\n+ m表示训练数据的个数\n+ n表示特征数量\n\n监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)->y$。针对线性回归而言，函数$h(x)$表达式为\n$$\nh(x)=\\theta_0+\\theta_1*x_i+\\theta_2*x_2+...+\\theta_n*x_n\n$$\n为方便我们使用矩阵来表达，$h(x)=\\theta^T*x$，其中$\\theta^T$为$\\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。\n\n### 3.梯度下降算法\n#### 3.1梯度下降算法简述\n实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。\n\n![图片02](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8702.png)\n从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。\n#### 3.2梯度下降算法相关概念\n\n求解梯度下降算法之前，我们先了解相关概念。\n\n+ **步长（Learning Rate）**：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。\n+ **特征（Feature）**：即上述描述的$x_i,y_i$\n+ **假设函数（Hypothesis Function）**：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$\n+ **损失函数（Loss Function）**：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数越小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为\n\n$$\nJ(\\theta)=\\frac{1}{2m}*\\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})\n$$\n\n我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。\n\n#### 3.3梯度下降算法过程\n\n- 随机选取一组$\\theta$。\n- 不断变化$\\theta$，让$J(\\theta)$变小。\n    $$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$\n$j=0,1,2...n$，$\\theta_j$是n+1个值同时变化。$\\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\\theta$前为负号。$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导。\n- 直到$J(\\theta)​$得到最小值。\n\n$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导求解过程如下：\n$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}(h_\\theta(x)-y)^2$$\n$$=2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$\n$$=(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$\n$$=(h_\\theta(x)-y)\\cdot x_j$$\n因此梯度下降算法的最终表述为\n\nRepeat Until Convergence{\n$$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{n}((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j) $$ \n\nfor every  $j$\n\n}\n\n梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到线性回归函数。\n\n### 4.线性回归算法实现\n\n为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址在[这儿](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv) ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\n\n#read_csv\nreaddata=pd.read_csv('data/Advertising.csv')\ndata=np.array(readdata.values)\n\n#训练数据\nX_train=data[0:150,1:3]\nY_train=data[0:150,3]\n\n#测试数据\nX_test=data[150:200,1:3]\nY_test=data[150:200,3]\n\n#回归分析\nregr = linear_model.LinearRegression()\n#进行training set和test set的fit，即是训练的过程\nregr.fit(X_train, Y_train)\n\n# 打印出相关系数和截距等信息\nprint('Coefficients: \\n', regr.coef_)\nprint('Intercept: ', regr.intercept_)\n# The mean square error\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - Y_test) ** 2))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(X_test, Y_test))\n\n#得出回归函数 并自定义数据\nX_line=np.linspace(0,300)\nY_line=np.linspace(0,50)\nZ_line=0.04699836*X_line+0.17913965*Y_line+3.00431061176\n\n#画图\nfig=plt.figure()\nax = plt.subplot(111, projection='3d')  # 创建一个三维的绘图工程\nax.scatter(data[:,1],data[:,2],data[:,3],c='red',)  # 绘制数据点\nax.plot(X_line,Y_line,Z_line,c='blue')#绘制回归曲线\nplt.show()\n```\n\n![图片](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8703.png)\n其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。\n\n------\n\n### 5.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8E%A8%E5%B9%BF.png)\n\n\n\n","slug":"机器学习之线性回归","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4k001pjiz5tiyre64o","content":"<h3 id=\"1-线性回归分析（-Linear-Regression-Analysis）\"><a href=\"#1-线性回归分析（-Linear-Regression-Analysis）\" class=\"headerlink\" title=\"1.线性回归分析（ Linear Regression Analysis）\"></a>1.线性回归分析（ Linear Regression Analysis）</h3><p><strong>线性回归分析（Regression Analysis）</strong>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。<br>通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于<strong>监督学习</strong>。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8701.png\" alt=\"图片01\"><br>上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。</p>\n<h3 id=\"2-模型表达\"><a href=\"#2-模型表达\" class=\"headerlink\" title=\"2.模型表达\"></a>2.模型表达</h3><p>建立数学模型之前，我们先定义如下变量。</p>\n<ul>\n<li>$x_i$表示输入数据（Feature）</li>\n<li>$y_i$表示输出数据（Target）</li>\n<li>$(x_i,y_i)$表示一组训练数据（Training example）</li>\n<li>m表示训练数据的个数</li>\n<li>n表示特征数量</li>\n</ul>\n<p>监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)-&gt;y$。针对线性回归而言，函数$h(x)$表达式为</p>\n<script type=\"math/tex; mode=display\">\nh(x)=\\theta_0+\\theta_1*x_i+\\theta_2*x_2+...+\\theta_n*x_n</script><p>为方便我们使用矩阵来表达，$h(x)=\\theta^T*x$，其中$\\theta^T$为$\\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。</p>\n<h3 id=\"3-梯度下降算法\"><a href=\"#3-梯度下降算法\" class=\"headerlink\" title=\"3.梯度下降算法\"></a>3.梯度下降算法</h3><h4 id=\"3-1梯度下降算法简述\"><a href=\"#3-1梯度下降算法简述\" class=\"headerlink\" title=\"3.1梯度下降算法简述\"></a>3.1梯度下降算法简述</h4><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8702.png\" alt=\"图片02\"><br>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p>\n<h4 id=\"3-2梯度下降算法相关概念\"><a href=\"#3-2梯度下降算法相关概念\" class=\"headerlink\" title=\"3.2梯度下降算法相关概念\"></a>3.2梯度下降算法相关概念</h4><p>求解梯度下降算法之前，我们先了解相关概念。</p>\n<ul>\n<li><strong>步长（Learning Rate）</strong>：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。</li>\n<li><strong>特征（Feature）</strong>：即上述描述的$x_i,y_i$</li>\n<li><strong>假设函数（Hypothesis Function）</strong>：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$</li>\n<li><strong>损失函数（Loss Function）</strong>：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数越小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ(\\theta)=\\frac{1}{2m}*\\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})</script><p>我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。</p>\n<h4 id=\"3-3梯度下降算法过程\"><a href=\"#3-3梯度下降算法过程\" class=\"headerlink\" title=\"3.3梯度下降算法过程\"></a>3.3梯度下降算法过程</h4><ul>\n<li>随机选取一组$\\theta$。</li>\n<li>不断变化$\\theta$，让$J(\\theta)$变小。<script type=\"math/tex; mode=display\">\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)</script>$j=0,1,2…n$，$\\theta_j$是n+1个值同时变化。$\\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\\theta$前为负号。$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导。</li>\n<li>直到$J(\\theta)​$得到最小值。</li>\n</ul>\n<p>$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导求解过程如下：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}(h_\\theta(x)-y)^2</script><script type=\"math/tex; mode=display\">=2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)</script><script type=\"math/tex; mode=display\">=(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)</script><script type=\"math/tex; mode=display\">=(h_\\theta(x)-y)\\cdot x_j</script><p>因此梯度下降算法的最终表述为</p>\n<p>Repeat Until Convergence{</p>\n<script type=\"math/tex; mode=display\">\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{n}((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j)</script><p>for every  $j$</p>\n<p>}</p>\n<p>梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到线性回归函数。</p>\n<h3 id=\"4-线性回归算法实现\"><a href=\"#4-线性回归算法实现\" class=\"headerlink\" title=\"4.线性回归算法实现\"></a>4.线性回归算法实现</h3><p>为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址在<a href=\"http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\" target=\"_blank\" rel=\"noopener\">这儿</a> ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> linear_model</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d <span class=\"keyword\">import</span> axes3d</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#read_csv</span></span><br><span class=\"line\">readdata=pd.read_csv(<span class=\"string\">'data/Advertising.csv'</span>)</span><br><span class=\"line\">data=np.array(readdata.values)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练数据</span></span><br><span class=\"line\">X_train=data[<span class=\"number\">0</span>:<span class=\"number\">150</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\">Y_train=data[<span class=\"number\">0</span>:<span class=\"number\">150</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#测试数据</span></span><br><span class=\"line\">X_test=data[<span class=\"number\">150</span>:<span class=\"number\">200</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\">Y_test=data[<span class=\"number\">150</span>:<span class=\"number\">200</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#回归分析</span></span><br><span class=\"line\">regr = linear_model.LinearRegression()</span><br><span class=\"line\"><span class=\"comment\">#进行training set和test set的fit，即是训练的过程</span></span><br><span class=\"line\">regr.fit(X_train, Y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印出相关系数和截距等信息</span></span><br><span class=\"line\">print(<span class=\"string\">'Coefficients: \\n'</span>, regr.coef_)</span><br><span class=\"line\">print(<span class=\"string\">'Intercept: '</span>, regr.intercept_)</span><br><span class=\"line\"><span class=\"comment\"># The mean square error</span></span><br><span class=\"line\">print(<span class=\"string\">\"Residual sum of squares: %.2f\"</span></span><br><span class=\"line\">      % np.mean((regr.predict(X_test) - Y_test) ** <span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"comment\"># Explained variance score: 1 is perfect prediction</span></span><br><span class=\"line\">print(<span class=\"string\">'Variance score: %.2f'</span> % regr.score(X_test, Y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#得出回归函数 并自定义数据</span></span><br><span class=\"line\">X_line=np.linspace(<span class=\"number\">0</span>,<span class=\"number\">300</span>)</span><br><span class=\"line\">Y_line=np.linspace(<span class=\"number\">0</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">Z_line=<span class=\"number\">0.04699836</span>*X_line+<span class=\"number\">0.17913965</span>*Y_line+<span class=\"number\">3.00431061176</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">fig=plt.figure()</span><br><span class=\"line\">ax = plt.subplot(<span class=\"number\">111</span>, projection=<span class=\"string\">'3d'</span>)  <span class=\"comment\"># 创建一个三维的绘图工程</span></span><br><span class=\"line\">ax.scatter(data[:,<span class=\"number\">1</span>],data[:,<span class=\"number\">2</span>],data[:,<span class=\"number\">3</span>],c=<span class=\"string\">'red'</span>,)  <span class=\"comment\"># 绘制数据点</span></span><br><span class=\"line\">ax.plot(X_line,Y_line,Z_line,c=<span class=\"string\">'blue'</span>)<span class=\"comment\">#绘制回归曲线</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8703.png\" alt=\"图片\"><br>其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。</p>\n<hr>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-线性回归分析（-Linear-Regression-Analysis）\"><a href=\"#1-线性回归分析（-Linear-Regression-Analysis）\" class=\"headerlink\" title=\"1.线性回归分析（ Linear Regression Analysis）\"></a>1.线性回归分析（ Linear Regression Analysis）</h3><p><strong>线性回归分析（Regression Analysis）</strong>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。<br>通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于<strong>监督学习</strong>。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8701.png\" alt=\"图片01\"><br>上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。</p>\n<h3 id=\"2-模型表达\"><a href=\"#2-模型表达\" class=\"headerlink\" title=\"2.模型表达\"></a>2.模型表达</h3><p>建立数学模型之前，我们先定义如下变量。</p>\n<ul>\n<li>$x_i$表示输入数据（Feature）</li>\n<li>$y_i$表示输出数据（Target）</li>\n<li>$(x_i,y_i)$表示一组训练数据（Training example）</li>\n<li>m表示训练数据的个数</li>\n<li>n表示特征数量</li>\n</ul>\n<p>监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)-&gt;y$。针对线性回归而言，函数$h(x)$表达式为</p>\n<script type=\"math/tex; mode=display\">\nh(x)=\\theta_0+\\theta_1*x_i+\\theta_2*x_2+...+\\theta_n*x_n</script><p>为方便我们使用矩阵来表达，$h(x)=\\theta^T*x$，其中$\\theta^T$为$\\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。</p>\n<h3 id=\"3-梯度下降算法\"><a href=\"#3-梯度下降算法\" class=\"headerlink\" title=\"3.梯度下降算法\"></a>3.梯度下降算法</h3><h4 id=\"3-1梯度下降算法简述\"><a href=\"#3-1梯度下降算法简述\" class=\"headerlink\" title=\"3.1梯度下降算法简述\"></a>3.1梯度下降算法简述</h4><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8702.png\" alt=\"图片02\"><br>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p>\n<h4 id=\"3-2梯度下降算法相关概念\"><a href=\"#3-2梯度下降算法相关概念\" class=\"headerlink\" title=\"3.2梯度下降算法相关概念\"></a>3.2梯度下降算法相关概念</h4><p>求解梯度下降算法之前，我们先了解相关概念。</p>\n<ul>\n<li><strong>步长（Learning Rate）</strong>：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。</li>\n<li><strong>特征（Feature）</strong>：即上述描述的$x_i,y_i$</li>\n<li><strong>假设函数（Hypothesis Function）</strong>：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$</li>\n<li><strong>损失函数（Loss Function）</strong>：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数越小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nJ(\\theta)=\\frac{1}{2m}*\\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})</script><p>我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。</p>\n<h4 id=\"3-3梯度下降算法过程\"><a href=\"#3-3梯度下降算法过程\" class=\"headerlink\" title=\"3.3梯度下降算法过程\"></a>3.3梯度下降算法过程</h4><ul>\n<li>随机选取一组$\\theta$。</li>\n<li>不断变化$\\theta$，让$J(\\theta)$变小。<script type=\"math/tex; mode=display\">\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)</script>$j=0,1,2…n$，$\\theta_j$是n+1个值同时变化。$\\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\\theta$前为负号。$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导。</li>\n<li>直到$J(\\theta)​$得到最小值。</li>\n</ul>\n<p>$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导求解过程如下：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}(h_\\theta(x)-y)^2</script><script type=\"math/tex; mode=display\">=2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)</script><script type=\"math/tex; mode=display\">=(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)</script><script type=\"math/tex; mode=display\">=(h_\\theta(x)-y)\\cdot x_j</script><p>因此梯度下降算法的最终表述为</p>\n<p>Repeat Until Convergence{</p>\n<script type=\"math/tex; mode=display\">\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{n}((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j)</script><p>for every  $j$</p>\n<p>}</p>\n<p>梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到线性回归函数。</p>\n<h3 id=\"4-线性回归算法实现\"><a href=\"#4-线性回归算法实现\" class=\"headerlink\" title=\"4.线性回归算法实现\"></a>4.线性回归算法实现</h3><p>为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址在<a href=\"http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\" target=\"_blank\" rel=\"noopener\">这儿</a> ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> linear_model</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d <span class=\"keyword\">import</span> axes3d</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#read_csv</span></span><br><span class=\"line\">readdata=pd.read_csv(<span class=\"string\">'data/Advertising.csv'</span>)</span><br><span class=\"line\">data=np.array(readdata.values)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练数据</span></span><br><span class=\"line\">X_train=data[<span class=\"number\">0</span>:<span class=\"number\">150</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\">Y_train=data[<span class=\"number\">0</span>:<span class=\"number\">150</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#测试数据</span></span><br><span class=\"line\">X_test=data[<span class=\"number\">150</span>:<span class=\"number\">200</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\">Y_test=data[<span class=\"number\">150</span>:<span class=\"number\">200</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#回归分析</span></span><br><span class=\"line\">regr = linear_model.LinearRegression()</span><br><span class=\"line\"><span class=\"comment\">#进行training set和test set的fit，即是训练的过程</span></span><br><span class=\"line\">regr.fit(X_train, Y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印出相关系数和截距等信息</span></span><br><span class=\"line\">print(<span class=\"string\">'Coefficients: \\n'</span>, regr.coef_)</span><br><span class=\"line\">print(<span class=\"string\">'Intercept: '</span>, regr.intercept_)</span><br><span class=\"line\"><span class=\"comment\"># The mean square error</span></span><br><span class=\"line\">print(<span class=\"string\">\"Residual sum of squares: %.2f\"</span></span><br><span class=\"line\">      % np.mean((regr.predict(X_test) - Y_test) ** <span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"comment\"># Explained variance score: 1 is perfect prediction</span></span><br><span class=\"line\">print(<span class=\"string\">'Variance score: %.2f'</span> % regr.score(X_test, Y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#得出回归函数 并自定义数据</span></span><br><span class=\"line\">X_line=np.linspace(<span class=\"number\">0</span>,<span class=\"number\">300</span>)</span><br><span class=\"line\">Y_line=np.linspace(<span class=\"number\">0</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">Z_line=<span class=\"number\">0.04699836</span>*X_line+<span class=\"number\">0.17913965</span>*Y_line+<span class=\"number\">3.00431061176</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">fig=plt.figure()</span><br><span class=\"line\">ax = plt.subplot(<span class=\"number\">111</span>, projection=<span class=\"string\">'3d'</span>)  <span class=\"comment\"># 创建一个三维的绘图工程</span></span><br><span class=\"line\">ax.scatter(data[:,<span class=\"number\">1</span>],data[:,<span class=\"number\">2</span>],data[:,<span class=\"number\">3</span>],c=<span class=\"string\">'red'</span>,)  <span class=\"comment\"># 绘制数据点</span></span><br><span class=\"line\">ax.plot(X_line,Y_line,Z_line,c=<span class=\"string\">'blue'</span>)<span class=\"comment\">#绘制回归曲线</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8703.png\" alt=\"图片\"><br>其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。</p>\n<hr>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"机器学习之自适应增强(Adaboost)","date":"2018-05-03T12:38:57.000Z","mathjax":true,"comments":1,"_content":"\n### 1.Adaboost简介\n\n**Adaptive boosting(自适应增强)**是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下**Boost(增强)**和**Adaptive(自适应)**的概念。\n\n#### 1.1集成学习之Boosting\n\n集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。\n\n![机器学习之Adaboost自适应增强图片01](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png)\n\n从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。\n\n#### 1.2Adaptive自适应\n\n**Adaptive(自适应)**体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。\n\n#### 1.3Adaboost流程\n\n结合**Adaptive(自适应)**和**Boost(增强)**概念，我们来具体介绍下**Adaboost**迭代算法流程。\n\n+ 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。\n+ 训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。\n+ 多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。\n\n那么我们便要思考，**如何计算学习误差率e？**,**如何得到弱学习器权重系数α?** ,**如何更新样本权重D？**,**使用哪种结合策略？**我们将在Adaboost分类和回归算法中给出详细解答。\n\n### 2.Adaboost分类算法\n\n假设我们的训练样本为\n$$\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}\n$$\n训练集在第k个弱学习器的输出权重为\n$$\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m\n$$\nAdaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的**学习误差率**为\n$$\ne_k=P(G_k(x_i)\\neq y_i)=\\sum_{i=1}^{m}w_{ki},I(G_k(x_i)\\neq y_i)\n$$\n对于二元分类问题，第k个**弱分类器Gk(x)的权重系数**为\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\n$$\n从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。\n\n假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则**更新后的第k+1个弱分类器的样本集权重系数**如下所示，此处Zk是规范化因子。\n$$\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}exp(-\\alpha_k y_i G_k(x_i))\n$$\n\n$$\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))\n$$\n\n从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)<0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述**权重系数**和**样本权重更新**公式，我们在下面讲**Adaboost损失函数**时会详细介绍。\n\n最后Adaboost分类问题采用**加权平均法结合策略**，最终的强分类器为\n$$\nf(x)=sign(\\sum_{k=1}^{K}\\alpha_k G_k(x))\n$$\n对于**Adaboost多元分类算法**，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}+log(R-1)\n$$\n\n### 3.Adaboost回归算法\n\n假设我们的训练样本为\n$$\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}\n$$\n训练集在第k个弱学习器的输出权重为\n$$\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m\n$$\n我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差\n$$\nE_k=max|y_i-G_k(x_i)|\\ i=1,2,3,...,m\n$$\n然后计算每个样本的相对误差\n$$\ne_{ki}=\\frac{|y_i-G_k(x_i)|}{E_k}\n$$\n上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。\n$$\ne_{ki}=\\frac{(y_i-G_k(x_i))^2}{E_k^2}\n$$\n\n$$\ne_{ki}=1-exp(\\frac{-y_i+G_k(x_i)}{E_k})\n$$\n\n最终得到第k个弱学习器的**学习误差率**\n$$\ne_k=\\sum_{i=1}^{m}w_{ki}e_{ki}\n$$\n那么**弱学习器的权重系数**为\n$$\n\\alpha_k=\\frac{e_k}{1-e_k}\n$$\n然后**更新样本权重D**，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。\n\n$$\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}\\alpha _k^{1-e_ { ki } }\n$$\n\n$$\nZ_k=\\sum _{i=1}^{m}w_{ki}\\alpha _k^{1-e_{ki}}\n$$\n\n\n最后Adaboost回归问题采用**加权平均法结合策略**，最终的强回归器为\n\n\n$$\nf(x)=\\sum_{k=1}^{K}(ln\\frac{1}{\\alpha_k})G_k(x)\n$$\n\n### 4.Adaboost损失函数\n\n上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是**加法模型**，学习算法为**前向分布学习算法**，损失函数为指数函数。\n\n+ **加法模型：**最终强分类器是若干个弱分类器加权平均得到。\n+ **前向分布算法：**算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。\n\n假设第k-1轮和第k轮强学习器为\n$$\nf_{k-1}(x)=\\sum_{i=1}^{k-1}\\alpha_i G_i(x)\n$$\n\n$$\nf_k(x)=\\sum_{i=1}^{k}\\alpha_i G_i(x)\n$$\n\n因此我们可以得到\n$$\nf_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)\n$$\n可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为\n$$\n\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp(-y_if_k(x))\n$$\n利用前向分布学习算法的关系可以得到损失函数为\n$$\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp[(-y_i)(f_{k-1}(x)+\\alpha_k G_k(x))]\n$$\n令${w}'_{ki}=exp(-y_if_{k-1}(x))$，它的值不依赖于$\\alpha,G$而改变，仅仅依赖于$f_{k-1}(x)$，因此${w}'_{ki}$与最小化无关。将此式代入损失函数，损失函数转化为\n\n$$\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki}exp[-y_i\\alpha_k G_k(x))]\n$$\n首先我们求Gk(x)可以得到\n$$\nG_k(x)=\\underset{G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))\n$$\n将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\\\n$$\n其中ek为我们前面介绍的**分类误差率**\n\n\n$$\ne_k=\\frac{\\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))}{\\sum_{i=1}^{m}{w}'_{ki}} \\ \\ \\ \\ \\\n$$\n$$\n=\\sum_{i=1}^{m}w_{ki},I(y_i\\neq G(x_i))\n$$\n\n\n\n最后利用$f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)$、${w}'_{k,i}=exp(-y_if_{k-1}(x))$、${w}'_{k+1,i}=exp(-y_if_{k}(x))$，我们即可得到**样本权重的更新**。\n\n\n$$\n{w}'_{k+1,i}={w}'_{k,i}exp[-y_i\\alpha_kG_k(x_i)]\n$$\n\n$$\n{w}'_{k+1,i}=\\frac{w_{k,i}}{Z_k}exp[-y_i\\alpha_kG_k(x_i)]\n$$\n\n$$\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))\n$$\n\n### 5.Adaboost算法正则化\n\n为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代\n$$\nf_k(x)=f_{k-1}(x)+\\alpha_kG_k(x)\n$$\n如果我们加上正则化项，则有\n$$\nf_k(x)=f_{k-1}(x)+v\\alpha_kG_k(x)\n$$\nv的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n\n### 6.Sklearn实现Adaboost算法\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)。\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_gaussian_quantiles\n\n#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2\nX1,y1=make_gaussian_quantiles(cov=2.0,n_samples=500,\n                              n_features=2,n_classes=2,\n                              random_state=1)\n#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5\nX2,y2=make_gaussian_quantiles(mean=(3,3),cov=1.5,\n                              n_samples=400,n_features=2,\n                              n_classes=2,random_state=1)\n#将两组数据合为一组\nX=np.concatenate((X1,X2))\ny=np.concatenate((y1,-y2+1))\n\n#绘画生成的数据点\nplt.figure()\nplt.scatter(X[:,0],X[:,1],marker='o',c=y)\nplt.show()\n```\n\n![机器学习之Adaboost自适应增强图片02](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png)\n\n```python\n# 训练数据\nclf=AdaBoostClassifier(DecisionTreeClassifier(\n                       max_depth=2,min_samples_split=20,\n                       min_samples_leaf=5),algorithm=\"SAMME\",\n                       n_estimators=200,learning_rate=0.8)\nclf.fit(X,y)\n\n#将训练结果绘画出来\nplt.figure()\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y)\nplt.show()\n\n#训练模型的得分\nprint(clf.score(X,y))\n#0.913333333333\n```\n\n![机器学习之Adaboost自适应增强图片03](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png)\n\n### 7.Adaboost算法优缺点\n\n#### 7.1Adaboost优点\n\n+ 不容易发生过拟合。\n+ Adaboost是一种有很高精度的分类器。\n+ 当使用简单分类器时，计算出的结果是可理解的。\n+ 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。\n\n\n#### 7.2Adaboost缺点\n\n+ 训练时间过长。\n+ 执行效果依赖于弱分类器的选择。\n+ 对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n文章参考\n\n- [集成学习之Adaboost算法原理小结](https://www.cnblogs.com/pinard/p/6133937.html)","source":"_posts/机器学习之自适应增强-Adaboost.md","raw":"---\ntitle: 机器学习之自适应增强(Adaboost)\ndate: 2018-05-03 20:38:57\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.Adaboost简介\n\n**Adaptive boosting(自适应增强)**是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下**Boost(增强)**和**Adaptive(自适应)**的概念。\n\n#### 1.1集成学习之Boosting\n\n集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。\n\n![机器学习之Adaboost自适应增强图片01](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png)\n\n从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。\n\n#### 1.2Adaptive自适应\n\n**Adaptive(自适应)**体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。\n\n#### 1.3Adaboost流程\n\n结合**Adaptive(自适应)**和**Boost(增强)**概念，我们来具体介绍下**Adaboost**迭代算法流程。\n\n+ 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。\n+ 训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。\n+ 多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。\n\n那么我们便要思考，**如何计算学习误差率e？**,**如何得到弱学习器权重系数α?** ,**如何更新样本权重D？**,**使用哪种结合策略？**我们将在Adaboost分类和回归算法中给出详细解答。\n\n### 2.Adaboost分类算法\n\n假设我们的训练样本为\n$$\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}\n$$\n训练集在第k个弱学习器的输出权重为\n$$\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m\n$$\nAdaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的**学习误差率**为\n$$\ne_k=P(G_k(x_i)\\neq y_i)=\\sum_{i=1}^{m}w_{ki},I(G_k(x_i)\\neq y_i)\n$$\n对于二元分类问题，第k个**弱分类器Gk(x)的权重系数**为\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\n$$\n从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。\n\n假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则**更新后的第k+1个弱分类器的样本集权重系数**如下所示，此处Zk是规范化因子。\n$$\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}exp(-\\alpha_k y_i G_k(x_i))\n$$\n\n$$\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))\n$$\n\n从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)<0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述**权重系数**和**样本权重更新**公式，我们在下面讲**Adaboost损失函数**时会详细介绍。\n\n最后Adaboost分类问题采用**加权平均法结合策略**，最终的强分类器为\n$$\nf(x)=sign(\\sum_{k=1}^{K}\\alpha_k G_k(x))\n$$\n对于**Adaboost多元分类算法**，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}+log(R-1)\n$$\n\n### 3.Adaboost回归算法\n\n假设我们的训练样本为\n$$\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}\n$$\n训练集在第k个弱学习器的输出权重为\n$$\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m\n$$\n我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差\n$$\nE_k=max|y_i-G_k(x_i)|\\ i=1,2,3,...,m\n$$\n然后计算每个样本的相对误差\n$$\ne_{ki}=\\frac{|y_i-G_k(x_i)|}{E_k}\n$$\n上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。\n$$\ne_{ki}=\\frac{(y_i-G_k(x_i))^2}{E_k^2}\n$$\n\n$$\ne_{ki}=1-exp(\\frac{-y_i+G_k(x_i)}{E_k})\n$$\n\n最终得到第k个弱学习器的**学习误差率**\n$$\ne_k=\\sum_{i=1}^{m}w_{ki}e_{ki}\n$$\n那么**弱学习器的权重系数**为\n$$\n\\alpha_k=\\frac{e_k}{1-e_k}\n$$\n然后**更新样本权重D**，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。\n\n$$\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}\\alpha _k^{1-e_ { ki } }\n$$\n\n$$\nZ_k=\\sum _{i=1}^{m}w_{ki}\\alpha _k^{1-e_{ki}}\n$$\n\n\n最后Adaboost回归问题采用**加权平均法结合策略**，最终的强回归器为\n\n\n$$\nf(x)=\\sum_{k=1}^{K}(ln\\frac{1}{\\alpha_k})G_k(x)\n$$\n\n### 4.Adaboost损失函数\n\n上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是**加法模型**，学习算法为**前向分布学习算法**，损失函数为指数函数。\n\n+ **加法模型：**最终强分类器是若干个弱分类器加权平均得到。\n+ **前向分布算法：**算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。\n\n假设第k-1轮和第k轮强学习器为\n$$\nf_{k-1}(x)=\\sum_{i=1}^{k-1}\\alpha_i G_i(x)\n$$\n\n$$\nf_k(x)=\\sum_{i=1}^{k}\\alpha_i G_i(x)\n$$\n\n因此我们可以得到\n$$\nf_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)\n$$\n可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为\n$$\n\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp(-y_if_k(x))\n$$\n利用前向分布学习算法的关系可以得到损失函数为\n$$\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp[(-y_i)(f_{k-1}(x)+\\alpha_k G_k(x))]\n$$\n令${w}'_{ki}=exp(-y_if_{k-1}(x))$，它的值不依赖于$\\alpha,G$而改变，仅仅依赖于$f_{k-1}(x)$，因此${w}'_{ki}$与最小化无关。将此式代入损失函数，损失函数转化为\n\n$$\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki}exp[-y_i\\alpha_k G_k(x))]\n$$\n首先我们求Gk(x)可以得到\n$$\nG_k(x)=\\underset{G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))\n$$\n将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\\\n$$\n其中ek为我们前面介绍的**分类误差率**\n\n\n$$\ne_k=\\frac{\\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))}{\\sum_{i=1}^{m}{w}'_{ki}} \\ \\ \\ \\ \\\n$$\n$$\n=\\sum_{i=1}^{m}w_{ki},I(y_i\\neq G(x_i))\n$$\n\n\n\n最后利用$f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)$、${w}'_{k,i}=exp(-y_if_{k-1}(x))$、${w}'_{k+1,i}=exp(-y_if_{k}(x))$，我们即可得到**样本权重的更新**。\n\n\n$$\n{w}'_{k+1,i}={w}'_{k,i}exp[-y_i\\alpha_kG_k(x_i)]\n$$\n\n$$\n{w}'_{k+1,i}=\\frac{w_{k,i}}{Z_k}exp[-y_i\\alpha_kG_k(x_i)]\n$$\n\n$$\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))\n$$\n\n### 5.Adaboost算法正则化\n\n为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代\n$$\nf_k(x)=f_{k-1}(x)+\\alpha_kG_k(x)\n$$\n如果我们加上正则化项，则有\n$$\nf_k(x)=f_{k-1}(x)+v\\alpha_kG_k(x)\n$$\nv的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n\n### 6.Sklearn实现Adaboost算法\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)。\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_gaussian_quantiles\n\n#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2\nX1,y1=make_gaussian_quantiles(cov=2.0,n_samples=500,\n                              n_features=2,n_classes=2,\n                              random_state=1)\n#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5\nX2,y2=make_gaussian_quantiles(mean=(3,3),cov=1.5,\n                              n_samples=400,n_features=2,\n                              n_classes=2,random_state=1)\n#将两组数据合为一组\nX=np.concatenate((X1,X2))\ny=np.concatenate((y1,-y2+1))\n\n#绘画生成的数据点\nplt.figure()\nplt.scatter(X[:,0],X[:,1],marker='o',c=y)\nplt.show()\n```\n\n![机器学习之Adaboost自适应增强图片02](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png)\n\n```python\n# 训练数据\nclf=AdaBoostClassifier(DecisionTreeClassifier(\n                       max_depth=2,min_samples_split=20,\n                       min_samples_leaf=5),algorithm=\"SAMME\",\n                       n_estimators=200,learning_rate=0.8)\nclf.fit(X,y)\n\n#将训练结果绘画出来\nplt.figure()\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y)\nplt.show()\n\n#训练模型的得分\nprint(clf.score(X,y))\n#0.913333333333\n```\n\n![机器学习之Adaboost自适应增强图片03](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png)\n\n### 7.Adaboost算法优缺点\n\n#### 7.1Adaboost优点\n\n+ 不容易发生过拟合。\n+ Adaboost是一种有很高精度的分类器。\n+ 当使用简单分类器时，计算出的结果是可理解的。\n+ 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。\n\n\n#### 7.2Adaboost缺点\n\n+ 训练时间过长。\n+ 执行效果依赖于弱分类器的选择。\n+ 对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n文章参考\n\n- [集成学习之Adaboost算法原理小结](https://www.cnblogs.com/pinard/p/6133937.html)","slug":"机器学习之自适应增强-Adaboost","published":1,"updated":"2018-06-26T06:35:41.919Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4m001rjiz5j3eg73t7","content":"<h3 id=\"1-Adaboost简介\"><a href=\"#1-Adaboost简介\" class=\"headerlink\" title=\"1.Adaboost简介\"></a>1.Adaboost简介</h3><p><strong>Adaptive boosting(自适应增强)</strong>是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下<strong>Boost(增强)</strong>和<strong>Adaptive(自适应)</strong>的概念。</p>\n<h4 id=\"1-1集成学习之Boosting\"><a href=\"#1-1集成学习之Boosting\" class=\"headerlink\" title=\"1.1集成学习之Boosting\"></a>1.1集成学习之Boosting</h4><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p>\n<p><img src=\"/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png\" alt=\"机器学习之Adaboost自适应增强图片01\"></p>\n<p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>\n<h4 id=\"1-2Adaptive自适应\"><a href=\"#1-2Adaptive自适应\" class=\"headerlink\" title=\"1.2Adaptive自适应\"></a>1.2Adaptive自适应</h4><p><strong>Adaptive(自适应)</strong>体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。</p>\n<h4 id=\"1-3Adaboost流程\"><a href=\"#1-3Adaboost流程\" class=\"headerlink\" title=\"1.3Adaboost流程\"></a>1.3Adaboost流程</h4><p>结合<strong>Adaptive(自适应)</strong>和<strong>Boost(增强)</strong>概念，我们来具体介绍下<strong>Adaboost</strong>迭代算法流程。</p>\n<ul>\n<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。</li>\n<li>训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。</li>\n<li>多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li>\n</ul>\n<p>那么我们便要思考，<strong>如何计算学习误差率e？</strong>,<strong>如何得到弱学习器权重系数α?</strong> ,<strong>如何更新样本权重D？</strong>,<strong>使用哪种结合策略？</strong>我们将在Adaboost分类和回归算法中给出详细解答。</p>\n<h3 id=\"2-Adaboost分类算法\"><a href=\"#2-Adaboost分类算法\" class=\"headerlink\" title=\"2.Adaboost分类算法\"></a>2.Adaboost分类算法</h3><p>假设我们的训练样本为</p>\n<script type=\"math/tex; mode=display\">\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}</script><p>训练集在第k个弱学习器的输出权重为</p>\n<script type=\"math/tex; mode=display\">\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m</script><p>Adaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的<strong>学习误差率</strong>为</p>\n<script type=\"math/tex; mode=display\">\ne_k=P(G_k(x_i)\\neq y_i)=\\sum_{i=1}^{m}w_{ki},I(G_k(x_i)\\neq y_i)</script><p>对于二元分类问题，第k个<strong>弱分类器Gk(x)的权重系数</strong>为</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}</script><p>从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。</p>\n<p>假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则<strong>更新后的第k+1个弱分类器的样本集权重系数</strong>如下所示，此处Zk是规范化因子。</p>\n<script type=\"math/tex; mode=display\">\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}exp(-\\alpha_k y_i G_k(x_i))</script><script type=\"math/tex; mode=display\">\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))</script><p>从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)&lt;0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述<strong>权重系数</strong>和<strong>样本权重更新</strong>公式，我们在下面讲<strong>Adaboost损失函数</strong>时会详细介绍。</p>\n<p>最后Adaboost分类问题采用<strong>加权平均法结合策略</strong>，最终的强分类器为</p>\n<script type=\"math/tex; mode=display\">\nf(x)=sign(\\sum_{k=1}^{K}\\alpha_k G_k(x))</script><p>对于<strong>Adaboost多元分类算法</strong>，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}+log(R-1)</script><h3 id=\"3-Adaboost回归算法\"><a href=\"#3-Adaboost回归算法\" class=\"headerlink\" title=\"3.Adaboost回归算法\"></a>3.Adaboost回归算法</h3><p>假设我们的训练样本为</p>\n<script type=\"math/tex; mode=display\">\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}</script><p>训练集在第k个弱学习器的输出权重为</p>\n<script type=\"math/tex; mode=display\">\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m</script><p>我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差</p>\n<script type=\"math/tex; mode=display\">\nE_k=max|y_i-G_k(x_i)|\\ i=1,2,3,...,m</script><p>然后计算每个样本的相对误差</p>\n<script type=\"math/tex; mode=display\">\ne_{ki}=\\frac{|y_i-G_k(x_i)|}{E_k}</script><p>上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。</p>\n<script type=\"math/tex; mode=display\">\ne_{ki}=\\frac{(y_i-G_k(x_i))^2}{E_k^2}</script><script type=\"math/tex; mode=display\">\ne_{ki}=1-exp(\\frac{-y_i+G_k(x_i)}{E_k})</script><p>最终得到第k个弱学习器的<strong>学习误差率</strong></p>\n<script type=\"math/tex; mode=display\">\ne_k=\\sum_{i=1}^{m}w_{ki}e_{ki}</script><p>那么<strong>弱学习器的权重系数</strong>为</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_k=\\frac{e_k}{1-e_k}</script><p>然后<strong>更新样本权重D</strong>，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。</p>\n<script type=\"math/tex; mode=display\">\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}\\alpha _k^{1-e_ { ki } }</script><script type=\"math/tex; mode=display\">\nZ_k=\\sum _{i=1}^{m}w_{ki}\\alpha _k^{1-e_{ki}}</script><p>最后Adaboost回归问题采用<strong>加权平均法结合策略</strong>，最终的强回归器为</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{k=1}^{K}(ln\\frac{1}{\\alpha_k})G_k(x)</script><h3 id=\"4-Adaboost损失函数\"><a href=\"#4-Adaboost损失函数\" class=\"headerlink\" title=\"4.Adaboost损失函数\"></a>4.Adaboost损失函数</h3><p>上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是<strong>加法模型</strong>，学习算法为<strong>前向分布学习算法</strong>，损失函数为指数函数。</p>\n<ul>\n<li><strong>加法模型：</strong>最终强分类器是若干个弱分类器加权平均得到。</li>\n<li><strong>前向分布算法：</strong>算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。</li>\n</ul>\n<p>假设第k-1轮和第k轮强学习器为</p>\n<script type=\"math/tex; mode=display\">\nf_{k-1}(x)=\\sum_{i=1}^{k-1}\\alpha_i G_i(x)</script><script type=\"math/tex; mode=display\">\nf_k(x)=\\sum_{i=1}^{k}\\alpha_i G_i(x)</script><p>因此我们可以得到</p>\n<script type=\"math/tex; mode=display\">\nf_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)</script><p>可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为</p>\n<script type=\"math/tex; mode=display\">\n\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp(-y_if_k(x))</script><p>利用前向分布学习算法的关系可以得到损失函数为</p>\n<script type=\"math/tex; mode=display\">\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp[(-y_i)(f_{k-1}(x)+\\alpha_k G_k(x))]</script><p>令${w}’_{ki}=exp(-y_if_{k-1}(x))$，它的值不依赖于$\\alpha,G$而改变，仅仅依赖于$f_{k-1}(x)$，因此${w}’_{ki}$与最小化无关。将此式代入损失函数，损失函数转化为</p>\n<script type=\"math/tex; mode=display\">\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki}exp[-y_i\\alpha_k G_k(x))]</script><p>首先我们求Gk(x)可以得到</p>\n<script type=\"math/tex; mode=display\">\nG_k(x)=\\underset{G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))</script><p>将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\\</script><p>其中ek为我们前面介绍的<strong>分类误差率</strong></p>\n<script type=\"math/tex; mode=display\">\ne_k=\\frac{\\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))}{\\sum_{i=1}^{m}{w}'_{ki}} \\ \\ \\ \\ \\</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{m}w_{ki},I(y_i\\neq G(x_i))</script><p>最后利用$f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)$、${w}’_{k,i}=exp(-y_if_{k-1}(x))$、${w}’_{k+1,i}=exp(-y_if_{k}(x))$，我们即可得到<strong>样本权重的更新</strong>。</p>\n<script type=\"math/tex; mode=display\">\n{w}'_{k+1,i}={w}'_{k,i}exp[-y_i\\alpha_kG_k(x_i)]</script><script type=\"math/tex; mode=display\">\n{w}'_{k+1,i}=\\frac{w_{k,i}}{Z_k}exp[-y_i\\alpha_kG_k(x_i)]</script><script type=\"math/tex; mode=display\">\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))</script><h3 id=\"5-Adaboost算法正则化\"><a href=\"#5-Adaboost算法正则化\" class=\"headerlink\" title=\"5.Adaboost算法正则化\"></a>5.Adaboost算法正则化</h3><p>为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代</p>\n<script type=\"math/tex; mode=display\">\nf_k(x)=f_{k-1}(x)+\\alpha_kG_k(x)</script><p>如果我们加上正则化项，则有</p>\n<script type=\"math/tex; mode=display\">\nf_k(x)=f_{k-1}(x)+v\\alpha_kG_k(x)</script><p>v的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>\n<h3 id=\"6-Sklearn实现Adaboost算法\"><a href=\"#6-Sklearn实现Adaboost算法\" class=\"headerlink\" title=\"6.Sklearn实现Adaboost算法\"></a>6.Sklearn实现Adaboost算法</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> AdaBoostClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_gaussian_quantiles</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2</span></span><br><span class=\"line\">X1,y1=make_gaussian_quantiles(cov=<span class=\"number\">2.0</span>,n_samples=<span class=\"number\">500</span>,</span><br><span class=\"line\">                              n_features=<span class=\"number\">2</span>,n_classes=<span class=\"number\">2</span>,</span><br><span class=\"line\">                              random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5</span></span><br><span class=\"line\">X2,y2=make_gaussian_quantiles(mean=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),cov=<span class=\"number\">1.5</span>,</span><br><span class=\"line\">                              n_samples=<span class=\"number\">400</span>,n_features=<span class=\"number\">2</span>,</span><br><span class=\"line\">                              n_classes=<span class=\"number\">2</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#将两组数据合为一组</span></span><br><span class=\"line\">X=np.concatenate((X1,X2))</span><br><span class=\"line\">y=np.concatenate((y1,-y2+<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘画生成的数据点</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],marker=<span class=\"string\">'o'</span>,c=y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png\" alt=\"机器学习之Adaboost自适应增强图片02\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 训练数据</span></span><br><span class=\"line\">clf=AdaBoostClassifier(DecisionTreeClassifier(</span><br><span class=\"line\">                       max_depth=<span class=\"number\">2</span>,min_samples_split=<span class=\"number\">20</span>,</span><br><span class=\"line\">                       min_samples_leaf=<span class=\"number\">5</span>),algorithm=<span class=\"string\">\"SAMME\"</span>,</span><br><span class=\"line\">                       n_estimators=<span class=\"number\">200</span>,learning_rate=<span class=\"number\">0.8</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将训练结果绘画出来</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">x_min, x_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">y_min, y_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class=\"number\">0.02</span>),</span><br><span class=\"line\">                     np.arange(y_min, y_max, <span class=\"number\">0.02</span>))</span><br><span class=\"line\">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">Z = Z.reshape(xx.shape)</span><br><span class=\"line\">cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=y)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练模型的得分</span></span><br><span class=\"line\">print(clf.score(X,y))</span><br><span class=\"line\"><span class=\"comment\">#0.913333333333</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png\" alt=\"机器学习之Adaboost自适应增强图片03\"></p>\n<h3 id=\"7-Adaboost算法优缺点\"><a href=\"#7-Adaboost算法优缺点\" class=\"headerlink\" title=\"7.Adaboost算法优缺点\"></a>7.Adaboost算法优缺点</h3><h4 id=\"7-1Adaboost优点\"><a href=\"#7-1Adaboost优点\" class=\"headerlink\" title=\"7.1Adaboost优点\"></a>7.1Adaboost优点</h4><ul>\n<li>不容易发生过拟合。</li>\n<li>Adaboost是一种有很高精度的分类器。</li>\n<li>当使用简单分类器时，计算出的结果是可理解的。</li>\n<li>可以使用各种方法构建子分类器，Adaboost算法提供的是框架。</li>\n</ul>\n<h4 id=\"7-2Adaboost缺点\"><a href=\"#7-2Adaboost缺点\" class=\"headerlink\" title=\"7.2Adaboost缺点\"></a>7.2Adaboost缺点</h4><ul>\n<li>训练时间过长。</li>\n<li>执行效果依赖于弱分类器的选择。</li>\n<li>对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>文章参考</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6133937.html\" target=\"_blank\" rel=\"noopener\">集成学习之Adaboost算法原理小结</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Adaboost简介\"><a href=\"#1-Adaboost简介\" class=\"headerlink\" title=\"1.Adaboost简介\"></a>1.Adaboost简介</h3><p><strong>Adaptive boosting(自适应增强)</strong>是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下<strong>Boost(增强)</strong>和<strong>Adaptive(自适应)</strong>的概念。</p>\n<h4 id=\"1-1集成学习之Boosting\"><a href=\"#1-1集成学习之Boosting\" class=\"headerlink\" title=\"1.1集成学习之Boosting\"></a>1.1集成学习之Boosting</h4><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p>\n<p><img src=\"/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png\" alt=\"机器学习之Adaboost自适应增强图片01\"></p>\n<p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>\n<h4 id=\"1-2Adaptive自适应\"><a href=\"#1-2Adaptive自适应\" class=\"headerlink\" title=\"1.2Adaptive自适应\"></a>1.2Adaptive自适应</h4><p><strong>Adaptive(自适应)</strong>体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。</p>\n<h4 id=\"1-3Adaboost流程\"><a href=\"#1-3Adaboost流程\" class=\"headerlink\" title=\"1.3Adaboost流程\"></a>1.3Adaboost流程</h4><p>结合<strong>Adaptive(自适应)</strong>和<strong>Boost(增强)</strong>概念，我们来具体介绍下<strong>Adaboost</strong>迭代算法流程。</p>\n<ul>\n<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。</li>\n<li>训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。</li>\n<li>多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li>\n</ul>\n<p>那么我们便要思考，<strong>如何计算学习误差率e？</strong>,<strong>如何得到弱学习器权重系数α?</strong> ,<strong>如何更新样本权重D？</strong>,<strong>使用哪种结合策略？</strong>我们将在Adaboost分类和回归算法中给出详细解答。</p>\n<h3 id=\"2-Adaboost分类算法\"><a href=\"#2-Adaboost分类算法\" class=\"headerlink\" title=\"2.Adaboost分类算法\"></a>2.Adaboost分类算法</h3><p>假设我们的训练样本为</p>\n<script type=\"math/tex; mode=display\">\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}</script><p>训练集在第k个弱学习器的输出权重为</p>\n<script type=\"math/tex; mode=display\">\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m</script><p>Adaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的<strong>学习误差率</strong>为</p>\n<script type=\"math/tex; mode=display\">\ne_k=P(G_k(x_i)\\neq y_i)=\\sum_{i=1}^{m}w_{ki},I(G_k(x_i)\\neq y_i)</script><p>对于二元分类问题，第k个<strong>弱分类器Gk(x)的权重系数</strong>为</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}</script><p>从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。</p>\n<p>假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则<strong>更新后的第k+1个弱分类器的样本集权重系数</strong>如下所示，此处Zk是规范化因子。</p>\n<script type=\"math/tex; mode=display\">\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}exp(-\\alpha_k y_i G_k(x_i))</script><script type=\"math/tex; mode=display\">\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))</script><p>从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)&lt;0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述<strong>权重系数</strong>和<strong>样本权重更新</strong>公式，我们在下面讲<strong>Adaboost损失函数</strong>时会详细介绍。</p>\n<p>最后Adaboost分类问题采用<strong>加权平均法结合策略</strong>，最终的强分类器为</p>\n<script type=\"math/tex; mode=display\">\nf(x)=sign(\\sum_{k=1}^{K}\\alpha_k G_k(x))</script><p>对于<strong>Adaboost多元分类算法</strong>，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}+log(R-1)</script><h3 id=\"3-Adaboost回归算法\"><a href=\"#3-Adaboost回归算法\" class=\"headerlink\" title=\"3.Adaboost回归算法\"></a>3.Adaboost回归算法</h3><p>假设我们的训练样本为</p>\n<script type=\"math/tex; mode=display\">\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}</script><p>训练集在第k个弱学习器的输出权重为</p>\n<script type=\"math/tex; mode=display\">\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m</script><p>我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差</p>\n<script type=\"math/tex; mode=display\">\nE_k=max|y_i-G_k(x_i)|\\ i=1,2,3,...,m</script><p>然后计算每个样本的相对误差</p>\n<script type=\"math/tex; mode=display\">\ne_{ki}=\\frac{|y_i-G_k(x_i)|}{E_k}</script><p>上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。</p>\n<script type=\"math/tex; mode=display\">\ne_{ki}=\\frac{(y_i-G_k(x_i))^2}{E_k^2}</script><script type=\"math/tex; mode=display\">\ne_{ki}=1-exp(\\frac{-y_i+G_k(x_i)}{E_k})</script><p>最终得到第k个弱学习器的<strong>学习误差率</strong></p>\n<script type=\"math/tex; mode=display\">\ne_k=\\sum_{i=1}^{m}w_{ki}e_{ki}</script><p>那么<strong>弱学习器的权重系数</strong>为</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_k=\\frac{e_k}{1-e_k}</script><p>然后<strong>更新样本权重D</strong>，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。</p>\n<script type=\"math/tex; mode=display\">\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}\\alpha _k^{1-e_ { ki } }</script><script type=\"math/tex; mode=display\">\nZ_k=\\sum _{i=1}^{m}w_{ki}\\alpha _k^{1-e_{ki}}</script><p>最后Adaboost回归问题采用<strong>加权平均法结合策略</strong>，最终的强回归器为</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{k=1}^{K}(ln\\frac{1}{\\alpha_k})G_k(x)</script><h3 id=\"4-Adaboost损失函数\"><a href=\"#4-Adaboost损失函数\" class=\"headerlink\" title=\"4.Adaboost损失函数\"></a>4.Adaboost损失函数</h3><p>上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是<strong>加法模型</strong>，学习算法为<strong>前向分布学习算法</strong>，损失函数为指数函数。</p>\n<ul>\n<li><strong>加法模型：</strong>最终强分类器是若干个弱分类器加权平均得到。</li>\n<li><strong>前向分布算法：</strong>算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。</li>\n</ul>\n<p>假设第k-1轮和第k轮强学习器为</p>\n<script type=\"math/tex; mode=display\">\nf_{k-1}(x)=\\sum_{i=1}^{k-1}\\alpha_i G_i(x)</script><script type=\"math/tex; mode=display\">\nf_k(x)=\\sum_{i=1}^{k}\\alpha_i G_i(x)</script><p>因此我们可以得到</p>\n<script type=\"math/tex; mode=display\">\nf_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)</script><p>可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为</p>\n<script type=\"math/tex; mode=display\">\n\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp(-y_if_k(x))</script><p>利用前向分布学习算法的关系可以得到损失函数为</p>\n<script type=\"math/tex; mode=display\">\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp[(-y_i)(f_{k-1}(x)+\\alpha_k G_k(x))]</script><p>令${w}’_{ki}=exp(-y_if_{k-1}(x))$，它的值不依赖于$\\alpha,G$而改变，仅仅依赖于$f_{k-1}(x)$，因此${w}’_{ki}$与最小化无关。将此式代入损失函数，损失函数转化为</p>\n<script type=\"math/tex; mode=display\">\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki}exp[-y_i\\alpha_k G_k(x))]</script><p>首先我们求Gk(x)可以得到</p>\n<script type=\"math/tex; mode=display\">\nG_k(x)=\\underset{G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))</script><p>将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\\</script><p>其中ek为我们前面介绍的<strong>分类误差率</strong></p>\n<script type=\"math/tex; mode=display\">\ne_k=\\frac{\\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))}{\\sum_{i=1}^{m}{w}'_{ki}} \\ \\ \\ \\ \\</script><script type=\"math/tex; mode=display\">\n=\\sum_{i=1}^{m}w_{ki},I(y_i\\neq G(x_i))</script><p>最后利用$f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)$、${w}’_{k,i}=exp(-y_if_{k-1}(x))$、${w}’_{k+1,i}=exp(-y_if_{k}(x))$，我们即可得到<strong>样本权重的更新</strong>。</p>\n<script type=\"math/tex; mode=display\">\n{w}'_{k+1,i}={w}'_{k,i}exp[-y_i\\alpha_kG_k(x_i)]</script><script type=\"math/tex; mode=display\">\n{w}'_{k+1,i}=\\frac{w_{k,i}}{Z_k}exp[-y_i\\alpha_kG_k(x_i)]</script><script type=\"math/tex; mode=display\">\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))</script><h3 id=\"5-Adaboost算法正则化\"><a href=\"#5-Adaboost算法正则化\" class=\"headerlink\" title=\"5.Adaboost算法正则化\"></a>5.Adaboost算法正则化</h3><p>为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代</p>\n<script type=\"math/tex; mode=display\">\nf_k(x)=f_{k-1}(x)+\\alpha_kG_k(x)</script><p>如果我们加上正则化项，则有</p>\n<script type=\"math/tex; mode=display\">\nf_k(x)=f_{k-1}(x)+v\\alpha_kG_k(x)</script><p>v的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>\n<h3 id=\"6-Sklearn实现Adaboost算法\"><a href=\"#6-Sklearn实现Adaboost算法\" class=\"headerlink\" title=\"6.Sklearn实现Adaboost算法\"></a>6.Sklearn实现Adaboost算法</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> AdaBoostClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_gaussian_quantiles</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2</span></span><br><span class=\"line\">X1,y1=make_gaussian_quantiles(cov=<span class=\"number\">2.0</span>,n_samples=<span class=\"number\">500</span>,</span><br><span class=\"line\">                              n_features=<span class=\"number\">2</span>,n_classes=<span class=\"number\">2</span>,</span><br><span class=\"line\">                              random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5</span></span><br><span class=\"line\">X2,y2=make_gaussian_quantiles(mean=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),cov=<span class=\"number\">1.5</span>,</span><br><span class=\"line\">                              n_samples=<span class=\"number\">400</span>,n_features=<span class=\"number\">2</span>,</span><br><span class=\"line\">                              n_classes=<span class=\"number\">2</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#将两组数据合为一组</span></span><br><span class=\"line\">X=np.concatenate((X1,X2))</span><br><span class=\"line\">y=np.concatenate((y1,-y2+<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘画生成的数据点</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],marker=<span class=\"string\">'o'</span>,c=y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png\" alt=\"机器学习之Adaboost自适应增强图片02\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 训练数据</span></span><br><span class=\"line\">clf=AdaBoostClassifier(DecisionTreeClassifier(</span><br><span class=\"line\">                       max_depth=<span class=\"number\">2</span>,min_samples_split=<span class=\"number\">20</span>,</span><br><span class=\"line\">                       min_samples_leaf=<span class=\"number\">5</span>),algorithm=<span class=\"string\">\"SAMME\"</span>,</span><br><span class=\"line\">                       n_estimators=<span class=\"number\">200</span>,learning_rate=<span class=\"number\">0.8</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将训练结果绘画出来</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">x_min, x_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">y_min, y_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class=\"number\">0.02</span>),</span><br><span class=\"line\">                     np.arange(y_min, y_max, <span class=\"number\">0.02</span>))</span><br><span class=\"line\">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">Z = Z.reshape(xx.shape)</span><br><span class=\"line\">cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=y)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练模型的得分</span></span><br><span class=\"line\">print(clf.score(X,y))</span><br><span class=\"line\"><span class=\"comment\">#0.913333333333</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png\" alt=\"机器学习之Adaboost自适应增强图片03\"></p>\n<h3 id=\"7-Adaboost算法优缺点\"><a href=\"#7-Adaboost算法优缺点\" class=\"headerlink\" title=\"7.Adaboost算法优缺点\"></a>7.Adaboost算法优缺点</h3><h4 id=\"7-1Adaboost优点\"><a href=\"#7-1Adaboost优点\" class=\"headerlink\" title=\"7.1Adaboost优点\"></a>7.1Adaboost优点</h4><ul>\n<li>不容易发生过拟合。</li>\n<li>Adaboost是一种有很高精度的分类器。</li>\n<li>当使用简单分类器时，计算出的结果是可理解的。</li>\n<li>可以使用各种方法构建子分类器，Adaboost算法提供的是框架。</li>\n</ul>\n<h4 id=\"7-2Adaboost缺点\"><a href=\"#7-2Adaboost缺点\" class=\"headerlink\" title=\"7.2Adaboost缺点\"></a>7.2Adaboost缺点</h4><ul>\n<li>训练时间过长。</li>\n<li>执行效果依赖于弱分类器的选择。</li>\n<li>对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>文章参考</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6133937.html\" target=\"_blank\" rel=\"noopener\">集成学习之Adaboost算法原理小结</a></li>\n</ul>\n"},{"title":"机器学习之随机森林","date":"2018-04-30T05:38:54.000Z","mathjax":true,"comments":1,"_content":"\n### 1.随机森林简介\n\n随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于**分类**和**回归**问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中**森林**和**随机**的概念。\n\n#### 1.1集成学习\n\n**集成学习**是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。\n\n单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了**森林**。\n\n#### 1.2随机决策树\n\n我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。\n\n那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n<N)，然后从所有特征中选出k个特征生成单个随机决策树，这便是随机森林中**随机**的概念。\n\n#### 1.3随机森林算法\n\n由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？\n\n好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示\n\n+ 从样本集N中有放回随机采样选出n个样本。\n+ 从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。\n+ 重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。\n+ 对于新数据，经过每棵决策树投票分类。\n\n![机器学习之随机森林图片01](机器学习之随机森林/机器学习之随机森林图片01.png)\n\n### 2.CART算法\n\n随机森林包含众多决策树，能够用于**分类**和**回归**问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。\n\n#### 2.1CART分类树算法推导\n\nCART分类树预测离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为\n$$\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\n$$\n根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。\n$$\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2\n$$\n如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n$$\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)\n$$\n对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。\n$$\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))\n$$\n\n$$\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))\n$$\n\n#### 2.2CART分类树实例详解\n\n|  名称  | 体温 | 胎生 | 水生 | 类标记 |\n| :----: | :--: | :--: | :--: | :----: |\n|   人   | 恒温 |  是  |  否  | 哺乳类 |\n|  巨蟒  | 冷血 |  否  |  否  | 爬行类 |\n|  鲑鱼  | 冷血 |  否  |  是  |  鱼类  |\n|   鲸   | 恒温 |  是  |  是  | 哺乳类 |\n|   蛙   | 冷血 |  否  | 有时 |  鱼类  |\n|  巨蜥  | 冷血 |  否  |  否  | 爬行类 |\n|  蝙蝠  | 恒温 |  是  |  否  | 哺乳类 |\n|   猫   | 恒温 |  是  |  否  | 哺乳类 |\n| 豹纹鲨 | 冷血 |  是  |  是  |  鱼类  |\n|  海龟  | 冷血 |  否  | 有时 | 爬行类 |\n|  豪猪  | 恒温 |  是  |  否  | 哺乳类 |\n|   鳗   | 冷血 |  否  |  是  |  鱼类  |\n|  蝾螈  | 冷血 |  否  | 有时 | 两栖类 |\n\n针对上述离散型数据，按照**体温为恒温和非恒温**进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。\n$$\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}\n$$\n\n$$\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}\n$$\n\n然后计算得到特征**体温**下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。\n$$\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}\n$$\n\n#### 2.3CART回归树算法详解\n\nCART回归树预测连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n$$\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}\n$$\n**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。\n$$\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_2)^2]\n$$\n**用选定的(j,s)对，划分区域并决定相应的输出值**\n$$\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}\n$$\n\n$$\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i\n$$\n\n$$\nx\\epsilon R_m,m=1,2\n$$\n\n**继续对两个子区域调用上述步骤**，**将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。**\n$$\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)\n$$\n当输入空间划分确定时，可以用**平方误差**来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。\n$$\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2\n$$\n\n#### 2.4CART回归树实例详解\n\n| $x_i$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |\n| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |\n\n考虑如上所示的连续性变量，根据给定的数据点，考虑**1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5**切分点。对各切分点依次求出**R1,R2,c1,c2及m(s)**，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。\n$$\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56\n$$\n\n$$\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50\n$$\n\n$$\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_2)^2]=0+15.72=15.72\n$$\n\n**依次改变(j,s)对，可以得到s及m(s)的计算结果**，如下表所示。\n\n|  $s$   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |\n| :----: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |\n| $m(s)$ | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |\n\n当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。**回归树T1(x)**为\n$$\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}\n$$\n\n$$\nf_1(x)=T_1(x)\n$$\n\n**然后我们利用f1(x)拟合训练数据的残差**，如下表所示\n\n| $x_i$ |   1   |   2   |   3   |  4   |  5   |  6   |   7   |   8   |  9   |  10  |\n| :---: | :---: | :---: | :---: | :--: | :--: | :--: | :---: | :---: | :--: | :--: |\n| $y_i$ | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |\n\n**用f1(x)拟合训练数据得到平方误差**\n$$\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93\n$$\n第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到\n$$\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}\n$$\n\n$$\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}\n$$\n\n用f2(x)拟合训练数据的平方误差\n$$\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79\n$$\n继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示\n$$\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47\n$$\n\n$$\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30\n$$\n\n$$\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23\n$$\n\n$$\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}\n$$\n\n$$\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}\n$$\n\n用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。\n$$\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71\n$$\n\n### 3.Sklearn实现随机森林\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)。\n\n```python \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nX,y=make_classification(n_samples=1000,n_features=4,\n                        n_informative=2,n_redundant=0,\n                        random_state=0,shuffle=0)\nprint(X[:10],y[:10])\n#  X\n# [[-1.66853167 -1.29901346  0.2746472  -0.60362044]\n#  [-2.9728827  -1.08878294  0.70885958  0.42281857]\n#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]\n#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]\n#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]\n#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]\n#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]\n#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]\n#  [-1.13449871 -1.27403448  0.74355352  0.21035937]\n#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]\n\n#  y\n# [0 0 0 0 0 0 0 0 0 0]\n\nclf=RandomForestClassifier(max_depth=2,random_state=0)\nclf.fit(X,y)\nprint(clf.feature_importances_)\n# [ 0.17287856  0.80608704  0.01884792  0.00218648]\nprint(clf.predict([[0,0,0,0]]))\n# [1]\n```\n\n### 4.随机森林优缺点\n\n#### 4.1优点\n\n- 决策树选择部分样本及部分特征，一定程度上避免过拟合。\n- 决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。\n- 能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。\n- 对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。\n- 训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。\n- 随机森林有oob，不需要单独划分交叉验证集。\n\n#### 4.2缺点\n\n+ 可能有很多相似决策树，掩盖真实结果。\n+ 对小数据或低维数据可能不能产生很好分类。\n+ 产生众多决策树，算法较慢。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/机器学习之随机森林.md","raw":"---\ntitle: 机器学习之随机森林\ndate: 2018-04-30 13:38:54\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.随机森林简介\n\n随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于**分类**和**回归**问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中**森林**和**随机**的概念。\n\n#### 1.1集成学习\n\n**集成学习**是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。\n\n单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了**森林**。\n\n#### 1.2随机决策树\n\n我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。\n\n那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n<N)，然后从所有特征中选出k个特征生成单个随机决策树，这便是随机森林中**随机**的概念。\n\n#### 1.3随机森林算法\n\n由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？\n\n好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示\n\n+ 从样本集N中有放回随机采样选出n个样本。\n+ 从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。\n+ 重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。\n+ 对于新数据，经过每棵决策树投票分类。\n\n![机器学习之随机森林图片01](机器学习之随机森林/机器学习之随机森林图片01.png)\n\n### 2.CART算法\n\n随机森林包含众多决策树，能够用于**分类**和**回归**问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。\n\n#### 2.1CART分类树算法推导\n\nCART分类树预测离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为\n$$\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\n$$\n根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。\n$$\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2\n$$\n如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n$$\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)\n$$\n对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。\n$$\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))\n$$\n\n$$\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))\n$$\n\n#### 2.2CART分类树实例详解\n\n|  名称  | 体温 | 胎生 | 水生 | 类标记 |\n| :----: | :--: | :--: | :--: | :----: |\n|   人   | 恒温 |  是  |  否  | 哺乳类 |\n|  巨蟒  | 冷血 |  否  |  否  | 爬行类 |\n|  鲑鱼  | 冷血 |  否  |  是  |  鱼类  |\n|   鲸   | 恒温 |  是  |  是  | 哺乳类 |\n|   蛙   | 冷血 |  否  | 有时 |  鱼类  |\n|  巨蜥  | 冷血 |  否  |  否  | 爬行类 |\n|  蝙蝠  | 恒温 |  是  |  否  | 哺乳类 |\n|   猫   | 恒温 |  是  |  否  | 哺乳类 |\n| 豹纹鲨 | 冷血 |  是  |  是  |  鱼类  |\n|  海龟  | 冷血 |  否  | 有时 | 爬行类 |\n|  豪猪  | 恒温 |  是  |  否  | 哺乳类 |\n|   鳗   | 冷血 |  否  |  是  |  鱼类  |\n|  蝾螈  | 冷血 |  否  | 有时 | 两栖类 |\n\n针对上述离散型数据，按照**体温为恒温和非恒温**进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。\n$$\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}\n$$\n\n$$\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}\n$$\n\n然后计算得到特征**体温**下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。\n$$\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}\n$$\n\n#### 2.3CART回归树算法详解\n\nCART回归树预测连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n$$\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}\n$$\n**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。\n$$\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_2)^2]\n$$\n**用选定的(j,s)对，划分区域并决定相应的输出值**\n$$\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}\n$$\n\n$$\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i\n$$\n\n$$\nx\\epsilon R_m,m=1,2\n$$\n\n**继续对两个子区域调用上述步骤**，**将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。**\n$$\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)\n$$\n当输入空间划分确定时，可以用**平方误差**来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。\n$$\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2\n$$\n\n#### 2.4CART回归树实例详解\n\n| $x_i$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |\n| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |\n\n考虑如上所示的连续性变量，根据给定的数据点，考虑**1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5**切分点。对各切分点依次求出**R1,R2,c1,c2及m(s)**，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。\n$$\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56\n$$\n\n$$\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50\n$$\n\n$$\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_2)^2]=0+15.72=15.72\n$$\n\n**依次改变(j,s)对，可以得到s及m(s)的计算结果**，如下表所示。\n\n|  $s$   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |\n| :----: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |\n| $m(s)$ | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |\n\n当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。**回归树T1(x)**为\n$$\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}\n$$\n\n$$\nf_1(x)=T_1(x)\n$$\n\n**然后我们利用f1(x)拟合训练数据的残差**，如下表所示\n\n| $x_i$ |   1   |   2   |   3   |  4   |  5   |  6   |   7   |   8   |  9   |  10  |\n| :---: | :---: | :---: | :---: | :--: | :--: | :--: | :---: | :---: | :--: | :--: |\n| $y_i$ | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |\n\n**用f1(x)拟合训练数据得到平方误差**\n$$\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93\n$$\n第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到\n$$\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}\n$$\n\n$$\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}\n$$\n\n用f2(x)拟合训练数据的平方误差\n$$\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79\n$$\n继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示\n$$\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47\n$$\n\n$$\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30\n$$\n\n$$\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23\n$$\n\n$$\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}\n$$\n\n$$\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}\n$$\n\n用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。\n$$\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71\n$$\n\n### 3.Sklearn实现随机森林\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)。\n\n```python \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nX,y=make_classification(n_samples=1000,n_features=4,\n                        n_informative=2,n_redundant=0,\n                        random_state=0,shuffle=0)\nprint(X[:10],y[:10])\n#  X\n# [[-1.66853167 -1.29901346  0.2746472  -0.60362044]\n#  [-2.9728827  -1.08878294  0.70885958  0.42281857]\n#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]\n#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]\n#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]\n#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]\n#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]\n#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]\n#  [-1.13449871 -1.27403448  0.74355352  0.21035937]\n#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]\n\n#  y\n# [0 0 0 0 0 0 0 0 0 0]\n\nclf=RandomForestClassifier(max_depth=2,random_state=0)\nclf.fit(X,y)\nprint(clf.feature_importances_)\n# [ 0.17287856  0.80608704  0.01884792  0.00218648]\nprint(clf.predict([[0,0,0,0]]))\n# [1]\n```\n\n### 4.随机森林优缺点\n\n#### 4.1优点\n\n- 决策树选择部分样本及部分特征，一定程度上避免过拟合。\n- 决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。\n- 能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。\n- 对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。\n- 训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。\n- 随机森林有oob，不需要单独划分交叉验证集。\n\n#### 4.2缺点\n\n+ 可能有很多相似决策树，掩盖真实结果。\n+ 对小数据或低维数据可能不能产生很好分类。\n+ 产生众多决策树，算法较慢。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"机器学习之随机森林","published":1,"updated":"2018-06-27T03:53:40.656Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4o001wjiz5eshooije","content":"<h3 id=\"1-随机森林简介\"><a href=\"#1-随机森林简介\" class=\"headerlink\" title=\"1.随机森林简介\"></a>1.随机森林简介</h3><p>随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于<strong>分类</strong>和<strong>回归</strong>问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中<strong>森林</strong>和<strong>随机</strong>的概念。</p>\n<h4 id=\"1-1集成学习\"><a href=\"#1-1集成学习\" class=\"headerlink\" title=\"1.1集成学习\"></a>1.1集成学习</h4><p><strong>集成学习</strong>是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。</p>\n<p>单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了<strong>森林</strong>。</p>\n<h4 id=\"1-2随机决策树\"><a href=\"#1-2随机决策树\" class=\"headerlink\" title=\"1.2随机决策树\"></a>1.2随机决策树</h4><p>我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。</p>\n<p>那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n&lt;N)，然后从所有特征中选出k个特征生成单个随机决策树，这便是随机森林中<strong>随机</strong>的概念。</p>\n<h4 id=\"1-3随机森林算法\"><a href=\"#1-3随机森林算法\" class=\"headerlink\" title=\"1.3随机森林算法\"></a>1.3随机森林算法</h4><p>由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？</p>\n<p>好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示</p>\n<ul>\n<li>从样本集N中有放回随机采样选出n个样本。</li>\n<li>从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。</li>\n<li>重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。</li>\n<li>对于新数据，经过每棵决策树投票分类。</li>\n</ul>\n<p><img src=\"/2018/04/30/机器学习之随机森林/机器学习之随机森林图片01.png\" alt=\"机器学习之随机森林图片01\"></p>\n<h3 id=\"2-CART算法\"><a href=\"#2-CART算法\" class=\"headerlink\" title=\"2.CART算法\"></a>2.CART算法</h3><p>随机森林包含众多决策树，能够用于<strong>分类</strong>和<strong>回归</strong>问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。</p>\n<h4 id=\"2-1CART分类树算法推导\"><a href=\"#2-1CART分类树算法推导\" class=\"headerlink\" title=\"2.1CART分类树算法推导\"></a>2.1CART分类树算法推导</h4><p>CART分类树预测离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为</p>\n<script type=\"math/tex; mode=display\">\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}</script><p>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。</p>\n<script type=\"math/tex; mode=display\">\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2</script><p>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。</p>\n<script type=\"math/tex; mode=display\">\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)</script><p>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。</p>\n<script type=\"math/tex; mode=display\">\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))</script><script type=\"math/tex; mode=display\">\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))</script><h4 id=\"2-2CART分类树实例详解\"><a href=\"#2-2CART分类树实例详解\" class=\"headerlink\" title=\"2.2CART分类树实例详解\"></a>2.2CART分类树实例详解</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">名称</th>\n<th style=\"text-align:center\">体温</th>\n<th style=\"text-align:center\">胎生</th>\n<th style=\"text-align:center\">水生</th>\n<th style=\"text-align:center\">类标记</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">人</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蟒</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲑鱼</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲸</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蛙</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蜥</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝙蝠</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">猫</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豹纹鲨</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">海龟</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豪猪</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鳗</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝾螈</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">两栖类</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。</p>\n<script type=\"math/tex; mode=display\">\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}</script><script type=\"math/tex; mode=display\">\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}</script><p>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。</p>\n<script type=\"math/tex; mode=display\">\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}</script><h4 id=\"2-3CART回归树算法详解\"><a href=\"#2-3CART回归树算法详解\" class=\"headerlink\" title=\"2.3CART回归树算法详解\"></a>2.3CART回归树算法详解</h4><p>CART回归树预测连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>\n<script type=\"math/tex; mode=display\">\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}</script><p><strong>选择最优切分变量j与切分点s</strong>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。</p>\n<script type=\"math/tex; mode=display\">\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_2)^2]</script><p><strong>用选定的(j,s)对，划分区域并决定相应的输出值</strong></p>\n<script type=\"math/tex; mode=display\">\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}</script><script type=\"math/tex; mode=display\">\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i</script><script type=\"math/tex; mode=display\">\nx\\epsilon R_m,m=1,2</script><p><strong>继续对两个子区域调用上述步骤</strong>，<strong>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</strong></p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)</script><p>当输入空间划分确定时，可以用<strong>平方误差</strong>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2</script><h4 id=\"2-4CART回归树实例详解\"><a href=\"#2-4CART回归树实例详解\" class=\"headerlink\" title=\"2.4CART回归树实例详解\"></a>2.4CART回归树实例详解</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">5.56</td>\n<td style=\"text-align:center\">5.70</td>\n<td style=\"text-align:center\">5.91</td>\n<td style=\"text-align:center\">6.40</td>\n<td style=\"text-align:center\">6.80</td>\n<td style=\"text-align:center\">7.05</td>\n<td style=\"text-align:center\">8.90</td>\n<td style=\"text-align:center\">8.70</td>\n<td style=\"text-align:center\">9.00</td>\n<td style=\"text-align:center\">9.05</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。</p>\n<script type=\"math/tex; mode=display\">\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56</script><script type=\"math/tex; mode=display\">\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50</script><script type=\"math/tex; mode=display\">\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_2)^2]=0+15.72=15.72</script><p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$s$</th>\n<th style=\"text-align:center\">1.5</th>\n<th style=\"text-align:center\">2.5</th>\n<th style=\"text-align:center\">3.5</th>\n<th style=\"text-align:center\">4.5</th>\n<th style=\"text-align:center\">5.5</th>\n<th style=\"text-align:center\">6.5</th>\n<th style=\"text-align:center\">7.5</th>\n<th style=\"text-align:center\">8.5</th>\n<th style=\"text-align:center\">9.5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$m(s)$</td>\n<td style=\"text-align:center\">15.72</td>\n<td style=\"text-align:center\">12.07</td>\n<td style=\"text-align:center\">8.36</td>\n<td style=\"text-align:center\">5.78</td>\n<td style=\"text-align:center\">3.91</td>\n<td style=\"text-align:center\">1.93</td>\n<td style=\"text-align:center\">8.01</td>\n<td style=\"text-align:center\">11.73</td>\n<td style=\"text-align:center\">15.74</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为</p>\n<script type=\"math/tex; mode=display\">\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_1(x)=T_1(x)</script><p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>，如下表所示</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">-0.68</td>\n<td style=\"text-align:center\">-0.54</td>\n<td style=\"text-align:center\">-0.33</td>\n<td style=\"text-align:center\">0.16</td>\n<td style=\"text-align:center\">0.56</td>\n<td style=\"text-align:center\">0.81</td>\n<td style=\"text-align:center\">-0.01</td>\n<td style=\"text-align:center\">-0.21</td>\n<td style=\"text-align:center\">0.09</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>用f1(x)拟合训练数据得到平方误差</strong></p>\n<script type=\"math/tex; mode=display\">\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93</script><p>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到</p>\n<script type=\"math/tex; mode=display\">\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}</script><p>用f2(x)拟合训练数据的平方误差</p>\n<script type=\"math/tex; mode=display\">\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79</script><p>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示</p>\n<script type=\"math/tex; mode=display\">\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47</script><script type=\"math/tex; mode=display\">\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30</script><script type=\"math/tex; mode=display\">\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23</script><script type=\"math/tex; mode=display\">\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}</script><p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。</p>\n<script type=\"math/tex; mode=display\">\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71</script><h3 id=\"3-Sklearn实现随机森林\"><a href=\"#3-Sklearn实现随机森林\" class=\"headerlink\" title=\"3.Sklearn实现随机森林\"></a>3.Sklearn实现随机森林</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"></span><br><span class=\"line\">X,y=make_classification(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                        n_informative=<span class=\"number\">2</span>,n_redundant=<span class=\"number\">0</span>,</span><br><span class=\"line\">                        random_state=<span class=\"number\">0</span>,shuffle=<span class=\"number\">0</span>)</span><br><span class=\"line\">print(X[:<span class=\"number\">10</span>],y[:<span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"comment\">#  X</span></span><br><span class=\"line\"><span class=\"comment\"># [[-1.66853167 -1.29901346  0.2746472  -0.60362044]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-2.9728827  -1.08878294  0.70885958  0.42281857]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.13449871 -1.27403448  0.74355352  0.21035937]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#  y</span></span><br><span class=\"line\"><span class=\"comment\"># [0 0 0 0 0 0 0 0 0 0]</span></span><br><span class=\"line\"></span><br><span class=\"line\">clf=RandomForestClassifier(max_depth=<span class=\"number\">2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\">print(clf.feature_importances_)</span><br><span class=\"line\"><span class=\"comment\"># [ 0.17287856  0.80608704  0.01884792  0.00218648]</span></span><br><span class=\"line\">print(clf.predict([[<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]]))</span><br><span class=\"line\"><span class=\"comment\"># [1]</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-随机森林优缺点\"><a href=\"#4-随机森林优缺点\" class=\"headerlink\" title=\"4.随机森林优缺点\"></a>4.随机森林优缺点</h3><h4 id=\"4-1优点\"><a href=\"#4-1优点\" class=\"headerlink\" title=\"4.1优点\"></a>4.1优点</h4><ul>\n<li>决策树选择部分样本及部分特征，一定程度上避免过拟合。</li>\n<li>决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。</li>\n<li>能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。</li>\n<li>对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。</li>\n<li>训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。</li>\n<li>随机森林有oob，不需要单独划分交叉验证集。</li>\n</ul>\n<h4 id=\"4-2缺点\"><a href=\"#4-2缺点\" class=\"headerlink\" title=\"4.2缺点\"></a>4.2缺点</h4><ul>\n<li>可能有很多相似决策树，掩盖真实结果。</li>\n<li>对小数据或低维数据可能不能产生很好分类。</li>\n<li>产生众多决策树，算法较慢。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-随机森林简介\"><a href=\"#1-随机森林简介\" class=\"headerlink\" title=\"1.随机森林简介\"></a>1.随机森林简介</h3><p>随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于<strong>分类</strong>和<strong>回归</strong>问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中<strong>森林</strong>和<strong>随机</strong>的概念。</p>\n<h4 id=\"1-1集成学习\"><a href=\"#1-1集成学习\" class=\"headerlink\" title=\"1.1集成学习\"></a>1.1集成学习</h4><p><strong>集成学习</strong>是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。</p>\n<p>单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了<strong>森林</strong>。</p>\n<h4 id=\"1-2随机决策树\"><a href=\"#1-2随机决策树\" class=\"headerlink\" title=\"1.2随机决策树\"></a>1.2随机决策树</h4><p>我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。</p>\n<p>那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n&lt;N)，然后从所有特征中选出k个特征生成单个随机决策树，这便是随机森林中<strong>随机</strong>的概念。</p>\n<h4 id=\"1-3随机森林算法\"><a href=\"#1-3随机森林算法\" class=\"headerlink\" title=\"1.3随机森林算法\"></a>1.3随机森林算法</h4><p>由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？</p>\n<p>好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示</p>\n<ul>\n<li>从样本集N中有放回随机采样选出n个样本。</li>\n<li>从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。</li>\n<li>重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。</li>\n<li>对于新数据，经过每棵决策树投票分类。</li>\n</ul>\n<p><img src=\"/2018/04/30/机器学习之随机森林/机器学习之随机森林图片01.png\" alt=\"机器学习之随机森林图片01\"></p>\n<h3 id=\"2-CART算法\"><a href=\"#2-CART算法\" class=\"headerlink\" title=\"2.CART算法\"></a>2.CART算法</h3><p>随机森林包含众多决策树，能够用于<strong>分类</strong>和<strong>回归</strong>问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。</p>\n<h4 id=\"2-1CART分类树算法推导\"><a href=\"#2-1CART分类树算法推导\" class=\"headerlink\" title=\"2.1CART分类树算法推导\"></a>2.1CART分类树算法推导</h4><p>CART分类树预测离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为</p>\n<script type=\"math/tex; mode=display\">\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}</script><p>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。</p>\n<script type=\"math/tex; mode=display\">\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2</script><p>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。</p>\n<script type=\"math/tex; mode=display\">\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)</script><p>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。</p>\n<script type=\"math/tex; mode=display\">\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))</script><script type=\"math/tex; mode=display\">\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))</script><h4 id=\"2-2CART分类树实例详解\"><a href=\"#2-2CART分类树实例详解\" class=\"headerlink\" title=\"2.2CART分类树实例详解\"></a>2.2CART分类树实例详解</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">名称</th>\n<th style=\"text-align:center\">体温</th>\n<th style=\"text-align:center\">胎生</th>\n<th style=\"text-align:center\">水生</th>\n<th style=\"text-align:center\">类标记</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">人</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蟒</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲑鱼</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲸</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蛙</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蜥</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝙蝠</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">猫</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豹纹鲨</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">海龟</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豪猪</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鳗</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝾螈</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">两栖类</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。</p>\n<script type=\"math/tex; mode=display\">\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}</script><script type=\"math/tex; mode=display\">\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}</script><p>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。</p>\n<script type=\"math/tex; mode=display\">\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}</script><h4 id=\"2-3CART回归树算法详解\"><a href=\"#2-3CART回归树算法详解\" class=\"headerlink\" title=\"2.3CART回归树算法详解\"></a>2.3CART回归树算法详解</h4><p>CART回归树预测连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>\n<script type=\"math/tex; mode=display\">\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}</script><p><strong>选择最优切分变量j与切分点s</strong>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。</p>\n<script type=\"math/tex; mode=display\">\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_2)^2]</script><p><strong>用选定的(j,s)对，划分区域并决定相应的输出值</strong></p>\n<script type=\"math/tex; mode=display\">\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}</script><script type=\"math/tex; mode=display\">\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i</script><script type=\"math/tex; mode=display\">\nx\\epsilon R_m,m=1,2</script><p><strong>继续对两个子区域调用上述步骤</strong>，<strong>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</strong></p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)</script><p>当输入空间划分确定时，可以用<strong>平方误差</strong>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2</script><h4 id=\"2-4CART回归树实例详解\"><a href=\"#2-4CART回归树实例详解\" class=\"headerlink\" title=\"2.4CART回归树实例详解\"></a>2.4CART回归树实例详解</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">5.56</td>\n<td style=\"text-align:center\">5.70</td>\n<td style=\"text-align:center\">5.91</td>\n<td style=\"text-align:center\">6.40</td>\n<td style=\"text-align:center\">6.80</td>\n<td style=\"text-align:center\">7.05</td>\n<td style=\"text-align:center\">8.90</td>\n<td style=\"text-align:center\">8.70</td>\n<td style=\"text-align:center\">9.00</td>\n<td style=\"text-align:center\">9.05</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。</p>\n<script type=\"math/tex; mode=display\">\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56</script><script type=\"math/tex; mode=display\">\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50</script><script type=\"math/tex; mode=display\">\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_2)^2]=0+15.72=15.72</script><p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$s$</th>\n<th style=\"text-align:center\">1.5</th>\n<th style=\"text-align:center\">2.5</th>\n<th style=\"text-align:center\">3.5</th>\n<th style=\"text-align:center\">4.5</th>\n<th style=\"text-align:center\">5.5</th>\n<th style=\"text-align:center\">6.5</th>\n<th style=\"text-align:center\">7.5</th>\n<th style=\"text-align:center\">8.5</th>\n<th style=\"text-align:center\">9.5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$m(s)$</td>\n<td style=\"text-align:center\">15.72</td>\n<td style=\"text-align:center\">12.07</td>\n<td style=\"text-align:center\">8.36</td>\n<td style=\"text-align:center\">5.78</td>\n<td style=\"text-align:center\">3.91</td>\n<td style=\"text-align:center\">1.93</td>\n<td style=\"text-align:center\">8.01</td>\n<td style=\"text-align:center\">11.73</td>\n<td style=\"text-align:center\">15.74</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为</p>\n<script type=\"math/tex; mode=display\">\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_1(x)=T_1(x)</script><p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>，如下表所示</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">-0.68</td>\n<td style=\"text-align:center\">-0.54</td>\n<td style=\"text-align:center\">-0.33</td>\n<td style=\"text-align:center\">0.16</td>\n<td style=\"text-align:center\">0.56</td>\n<td style=\"text-align:center\">0.81</td>\n<td style=\"text-align:center\">-0.01</td>\n<td style=\"text-align:center\">-0.21</td>\n<td style=\"text-align:center\">0.09</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>用f1(x)拟合训练数据得到平方误差</strong></p>\n<script type=\"math/tex; mode=display\">\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93</script><p>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到</p>\n<script type=\"math/tex; mode=display\">\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}</script><p>用f2(x)拟合训练数据的平方误差</p>\n<script type=\"math/tex; mode=display\">\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79</script><p>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示</p>\n<script type=\"math/tex; mode=display\">\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47</script><script type=\"math/tex; mode=display\">\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30</script><script type=\"math/tex; mode=display\">\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23</script><script type=\"math/tex; mode=display\">\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}</script><p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。</p>\n<script type=\"math/tex; mode=display\">\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71</script><h3 id=\"3-Sklearn实现随机森林\"><a href=\"#3-Sklearn实现随机森林\" class=\"headerlink\" title=\"3.Sklearn实现随机森林\"></a>3.Sklearn实现随机森林</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"></span><br><span class=\"line\">X,y=make_classification(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                        n_informative=<span class=\"number\">2</span>,n_redundant=<span class=\"number\">0</span>,</span><br><span class=\"line\">                        random_state=<span class=\"number\">0</span>,shuffle=<span class=\"number\">0</span>)</span><br><span class=\"line\">print(X[:<span class=\"number\">10</span>],y[:<span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"comment\">#  X</span></span><br><span class=\"line\"><span class=\"comment\"># [[-1.66853167 -1.29901346  0.2746472  -0.60362044]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-2.9728827  -1.08878294  0.70885958  0.42281857]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.13449871 -1.27403448  0.74355352  0.21035937]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#  y</span></span><br><span class=\"line\"><span class=\"comment\"># [0 0 0 0 0 0 0 0 0 0]</span></span><br><span class=\"line\"></span><br><span class=\"line\">clf=RandomForestClassifier(max_depth=<span class=\"number\">2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\">print(clf.feature_importances_)</span><br><span class=\"line\"><span class=\"comment\"># [ 0.17287856  0.80608704  0.01884792  0.00218648]</span></span><br><span class=\"line\">print(clf.predict([[<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]]))</span><br><span class=\"line\"><span class=\"comment\"># [1]</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-随机森林优缺点\"><a href=\"#4-随机森林优缺点\" class=\"headerlink\" title=\"4.随机森林优缺点\"></a>4.随机森林优缺点</h3><h4 id=\"4-1优点\"><a href=\"#4-1优点\" class=\"headerlink\" title=\"4.1优点\"></a>4.1优点</h4><ul>\n<li>决策树选择部分样本及部分特征，一定程度上避免过拟合。</li>\n<li>决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。</li>\n<li>能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。</li>\n<li>对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。</li>\n<li>训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。</li>\n<li>随机森林有oob，不需要单独划分交叉验证集。</li>\n</ul>\n<h4 id=\"4-2缺点\"><a href=\"#4-2缺点\" class=\"headerlink\" title=\"4.2缺点\"></a>4.2缺点</h4><ul>\n<li>可能有很多相似决策树，掩盖真实结果。</li>\n<li>对小数据或低维数据可能不能产生很好分类。</li>\n<li>产生众多决策树，算法较慢。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"深度神经网络之前向传播算法","date":"2018-06-27T01:13:06.000Z","mathjax":true,"_content":"\n### 1.深度神经网络简介\n\n**深度神经网络(Deep Neural Networks,DNN)**从字面上理解，也就是深层次的神经网络，从网络结构上看来就是有多个隐含层的神经网络。深度神经网络不仅能够用于分类和回归，在降维、聚类、语音识别、图像识别方面也有许多应用。由于神经网络内容较多，将分多次写作，本次主要讲解深度神经网络中的前向传播算法，后续还有反向传播算法、损失函数和激活函数、正则化。\n\n### 2.从感知机到神经网络\n\n在[机器学习之Logistic回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483814&idx=1&sn=16a56382d24e304a95ab2a2a028993c6&chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd)之中，我们利用过感知机的模型。如下图所示，也就是有若干个输入和一个输出的感知机模型。\n\n![深度神经网络01](深度神经网络之前向传播算法/深度神经网络01.png)\n\n感知机通过输入和输出学习得到一个线性模型，得到中间输出结果z。然后利用激活函数，从而得到我们希望的结果，例如1或-1。\n$$\nz=\\sum _{i=1}^{m}w_i x_i +b\n$$\n$$\nsign(z)=\\left\\{\\begin{matrix}\n-1 & z<0 \\\\ \n 1 & z \\ge0 \n\\end{matrix}\\right.\n$$\n上述模型只能用于二元分类，且无法学习比较复杂的非线形模型。而神经网络则是在感知机的模型上做扩展，主要增加以下三点。\n\n+ **增加隐含层：**如下图所示，隐含层可以有多层，增加模型的表达能力。当然隐含层增加，模型的复杂度也就会增加。\n\n![深度神经网络02](深度神经网络之前向传播算法/深度神经网络02.png)\n\n+ **输出层的神经元可以有多个输出：**这样模型便能够灵活的应用于分类和回归，以及其他的机器学习领域，比如降维和聚类。如下图所示，输出层有4个神经元。\n\n  ![深度神经网络03](深度神经网络之前向传播算法/深度神经网络03.png)\n\n+ **扩展激活函数：**感知机的激活函数sign(z)处理能力有限，因此神经网络一般使用其他激活函数，比如我们在[逻辑回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483814&idx=1&sn=16a56382d24e304a95ab2a2a028993c6&chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd)里面使用的Sigmoid函数。当然还有tanx,softmax,ReLU等激活函数，通过使用不同的激活函数，神经网络的表达能力也就不同，对于各种常用的激活函数，我们在后面会进行专门介绍。\n\n$$\nf(z)=\\frac{1}{1+e^{-z}}\n$$\n\n### 3.DNN基本结构\n\n从DNN按照不同层的位置来划分，DNN内部的神经网络层可以分为三类，分别是输入层、隐含层、输出层。如下图所示，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐含层。\n\n![深度神经网络04](深度神经网络之前向传播算法/深度神经网络04.png)\n\nDNN的层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来复杂，但是从局部模型来说，还是和感知机相同，即线性关系z加上激活函数σ(z)。由于DNN层数较多，那么线性关系系数w和偏移量b也就很多。但具体的参数在DNN之中如何定义呢？\n\n![深度神经网络05](深度神经网络之前向传播算法/深度神经网络05.png)\n\n首先我们来看看线性关系系数$w$的定义。以上述的三层DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为$w^{3}_{24}$。上标3代表线性系数$w$所在的层数，而下标对应的是输出的第三层的第2个神经元和输入的第二层的第4个神经元。\n\n但是，为什么我们不用更方便的$w^{3}_{42}$表示，即输入的第二层的第4个神经元和输出的第三层的第2个神经元，而是用$w^{3}_{24}$表示呢？这样做的目的主要是为了方便矩阵运算，如果是$w^{3}_{42}$的话，那么每次运行的都是$w^Tx+b$，需要进行矩阵转置。将输出的索引放在前面的话，线性运算则不需要转置，直接运算$wx+b$即可。总结下也就是，第$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的线形系数为$w^{l}_{jk}$。\n\n![深度神经网络06](深度神经网络之前向传播算法/深度神经网络06.png)\n\n再来看偏倚量b的定义。以上述的三层DNN为例，第二层的第三个神经元对应的偏移量定义为$b^2_3$。其中上标2代表所在的层数，下标3代表所在神经元的索引。总结下也就是第$l$层的第$j$个神经元的偏倚量为$b^l_j$。\n\n### 4.DNN前向传播算法的数学原理\n\n我们已经了解DNN中线性关系系数w和偏倚量b的定义。现在假设选择的激活函数是σ(z)，隐含层和输出层的输出值为a。则对于下述的三层DNN，我们利用和感知机一样的思路，将上一层的输出当作下一层的输入，然后计算下一层的输出，重复下去，也就是DNN的前向传播算法。\n\n![深度神经网络07](深度神经网络之前向传播算法/深度神经网络07.png)\n\n对于第二层的输出$a^2_1,a^2_2,a^2_3$，我们能够得到\n$$\na^2_1=\\sigma(z^2_1)=\\sigma(w^2_{11}x_1+w^2_{12}x_2+w^2_{13}x_3+b^2_1)\n$$\n\n$$\na^2_2=\\sigma(z^2_2)=\\sigma(w^2_{21}x_1+w^2_{22}x_2+w^2_{23}x_3+b^2_2)\n$$\n\n$$\na^2_3=\\sigma(z^2_3)=\\sigma(w^2_{31}x_1+w^2_{32}x_2+w^2_{33}x_3+b^2_3)\n$$\n\n对与第三层的输出$a^3_1$，我们有\n$$\na^3_1=\\sigma(z^3_1)=\\sigma(w^3_{11}a^2_1+w^3_{12}a^2_2+w^3_{13}a^2_3+b^3_1)\n$$\n将上面的例子一般化，假设第$l-1$层共有$m$个神经元，则对于第$l$层的第$j$个神经元的输出$a^l_j$如下所示。另外，如果l=2，则对于$a^1_k$即为输入层的$x_k$。\n$$\na^l_j=\\sigma(z^l_j)=\\sigma(\\sum _{k=1}^{m} w^l_{jk}a^{l-1}_k+b^l_j)\n$$\n从上面可以看出，使用代数法表示运算比较复杂，因此我们使用比较简洁的矩阵表示。假设第$l-1$层共有m个神经元，而第$l$层共有$n$个神经元，则第$l$层的线性系数$w$组成一个$n\\ast m$的矩阵$W^l$，第$l$层的偏倚$b$组成一个$n*1$的向量$b^l$。\n\n第$l-1$层的输出$a$组成一个$m\\ast 1$的向量$a^{l-1}$，第$l$层未激活前线性输出$z$组成一个$n\\ast 1$的向量$z^l$，第$l$层的输出$a$组成了一个$n*1$的向量$a^l$。用矩阵法进行表示，第$l$层输出为\n$$\na^l=\\sigma(z^l)=\\sigma(W^l a{^{l-1}}+b^l)\n$$\n\n### 5.DNN前向传播算法\n\nDNN前向传播算法也就是利用若干个权重系数矩阵W，偏倚向量$b$，输入值向量x进行一系列线形运算和激活运算。从输入层开始，一层层的向后进行运算，直到运算到输出层，得到输出结果为止。\n\n输入：隐含层和输出层对应的矩阵$W$，偏倚向量$b$，输入值向量$x$，层数$L$。\n\n输出：输出层结果$a^L$。\n\n+ 初始化$a^1=x$\n+ $for \\ l = 2\\  to \\ L$\n\n$$\na^l=\\sigma(z^l)=\\sigma(W^l a{^{l-1}}+b^l)\n$$\n\n单独看DNN前向传播算法，通过运算之后，得到的结果并没有什么意义，误差似乎特别大。而且怎么得到初始的矩阵W,偏倚向量b，最优的矩阵W,偏倚向量b呢？下篇文章将通过深度神经网络之反向传播算法来解决这些问题。\n\n参考\n\n> [刘建平Pinard_深度神经网络（DNN）模型与前向传播算法](http://www.cnblogs.com/pinard/p/6418668.html)\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](深度神经网络之前向传播算法/推广.png)","source":"_posts/深度神经网络之前向传播算法.md","raw":"---\ntitle: 深度神经网络之前向传播算法\ndate: 2018-06-27 09:13:06\ntags: [深度学习,算法]\ncategories: 深度学习\nmathjax: true\n---\n\n### 1.深度神经网络简介\n\n**深度神经网络(Deep Neural Networks,DNN)**从字面上理解，也就是深层次的神经网络，从网络结构上看来就是有多个隐含层的神经网络。深度神经网络不仅能够用于分类和回归，在降维、聚类、语音识别、图像识别方面也有许多应用。由于神经网络内容较多，将分多次写作，本次主要讲解深度神经网络中的前向传播算法，后续还有反向传播算法、损失函数和激活函数、正则化。\n\n### 2.从感知机到神经网络\n\n在[机器学习之Logistic回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483814&idx=1&sn=16a56382d24e304a95ab2a2a028993c6&chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd)之中，我们利用过感知机的模型。如下图所示，也就是有若干个输入和一个输出的感知机模型。\n\n![深度神经网络01](深度神经网络之前向传播算法/深度神经网络01.png)\n\n感知机通过输入和输出学习得到一个线性模型，得到中间输出结果z。然后利用激活函数，从而得到我们希望的结果，例如1或-1。\n$$\nz=\\sum _{i=1}^{m}w_i x_i +b\n$$\n$$\nsign(z)=\\left\\{\\begin{matrix}\n-1 & z<0 \\\\ \n 1 & z \\ge0 \n\\end{matrix}\\right.\n$$\n上述模型只能用于二元分类，且无法学习比较复杂的非线形模型。而神经网络则是在感知机的模型上做扩展，主要增加以下三点。\n\n+ **增加隐含层：**如下图所示，隐含层可以有多层，增加模型的表达能力。当然隐含层增加，模型的复杂度也就会增加。\n\n![深度神经网络02](深度神经网络之前向传播算法/深度神经网络02.png)\n\n+ **输出层的神经元可以有多个输出：**这样模型便能够灵活的应用于分类和回归，以及其他的机器学习领域，比如降维和聚类。如下图所示，输出层有4个神经元。\n\n  ![深度神经网络03](深度神经网络之前向传播算法/深度神经网络03.png)\n\n+ **扩展激活函数：**感知机的激活函数sign(z)处理能力有限，因此神经网络一般使用其他激活函数，比如我们在[逻辑回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483814&idx=1&sn=16a56382d24e304a95ab2a2a028993c6&chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd)里面使用的Sigmoid函数。当然还有tanx,softmax,ReLU等激活函数，通过使用不同的激活函数，神经网络的表达能力也就不同，对于各种常用的激活函数，我们在后面会进行专门介绍。\n\n$$\nf(z)=\\frac{1}{1+e^{-z}}\n$$\n\n### 3.DNN基本结构\n\n从DNN按照不同层的位置来划分，DNN内部的神经网络层可以分为三类，分别是输入层、隐含层、输出层。如下图所示，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐含层。\n\n![深度神经网络04](深度神经网络之前向传播算法/深度神经网络04.png)\n\nDNN的层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来复杂，但是从局部模型来说，还是和感知机相同，即线性关系z加上激活函数σ(z)。由于DNN层数较多，那么线性关系系数w和偏移量b也就很多。但具体的参数在DNN之中如何定义呢？\n\n![深度神经网络05](深度神经网络之前向传播算法/深度神经网络05.png)\n\n首先我们来看看线性关系系数$w$的定义。以上述的三层DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为$w^{3}_{24}$。上标3代表线性系数$w$所在的层数，而下标对应的是输出的第三层的第2个神经元和输入的第二层的第4个神经元。\n\n但是，为什么我们不用更方便的$w^{3}_{42}$表示，即输入的第二层的第4个神经元和输出的第三层的第2个神经元，而是用$w^{3}_{24}$表示呢？这样做的目的主要是为了方便矩阵运算，如果是$w^{3}_{42}$的话，那么每次运行的都是$w^Tx+b$，需要进行矩阵转置。将输出的索引放在前面的话，线性运算则不需要转置，直接运算$wx+b$即可。总结下也就是，第$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的线形系数为$w^{l}_{jk}$。\n\n![深度神经网络06](深度神经网络之前向传播算法/深度神经网络06.png)\n\n再来看偏倚量b的定义。以上述的三层DNN为例，第二层的第三个神经元对应的偏移量定义为$b^2_3$。其中上标2代表所在的层数，下标3代表所在神经元的索引。总结下也就是第$l$层的第$j$个神经元的偏倚量为$b^l_j$。\n\n### 4.DNN前向传播算法的数学原理\n\n我们已经了解DNN中线性关系系数w和偏倚量b的定义。现在假设选择的激活函数是σ(z)，隐含层和输出层的输出值为a。则对于下述的三层DNN，我们利用和感知机一样的思路，将上一层的输出当作下一层的输入，然后计算下一层的输出，重复下去，也就是DNN的前向传播算法。\n\n![深度神经网络07](深度神经网络之前向传播算法/深度神经网络07.png)\n\n对于第二层的输出$a^2_1,a^2_2,a^2_3$，我们能够得到\n$$\na^2_1=\\sigma(z^2_1)=\\sigma(w^2_{11}x_1+w^2_{12}x_2+w^2_{13}x_3+b^2_1)\n$$\n\n$$\na^2_2=\\sigma(z^2_2)=\\sigma(w^2_{21}x_1+w^2_{22}x_2+w^2_{23}x_3+b^2_2)\n$$\n\n$$\na^2_3=\\sigma(z^2_3)=\\sigma(w^2_{31}x_1+w^2_{32}x_2+w^2_{33}x_3+b^2_3)\n$$\n\n对与第三层的输出$a^3_1$，我们有\n$$\na^3_1=\\sigma(z^3_1)=\\sigma(w^3_{11}a^2_1+w^3_{12}a^2_2+w^3_{13}a^2_3+b^3_1)\n$$\n将上面的例子一般化，假设第$l-1$层共有$m$个神经元，则对于第$l$层的第$j$个神经元的输出$a^l_j$如下所示。另外，如果l=2，则对于$a^1_k$即为输入层的$x_k$。\n$$\na^l_j=\\sigma(z^l_j)=\\sigma(\\sum _{k=1}^{m} w^l_{jk}a^{l-1}_k+b^l_j)\n$$\n从上面可以看出，使用代数法表示运算比较复杂，因此我们使用比较简洁的矩阵表示。假设第$l-1$层共有m个神经元，而第$l$层共有$n$个神经元，则第$l$层的线性系数$w$组成一个$n\\ast m$的矩阵$W^l$，第$l$层的偏倚$b$组成一个$n*1$的向量$b^l$。\n\n第$l-1$层的输出$a$组成一个$m\\ast 1$的向量$a^{l-1}$，第$l$层未激活前线性输出$z$组成一个$n\\ast 1$的向量$z^l$，第$l$层的输出$a$组成了一个$n*1$的向量$a^l$。用矩阵法进行表示，第$l$层输出为\n$$\na^l=\\sigma(z^l)=\\sigma(W^l a{^{l-1}}+b^l)\n$$\n\n### 5.DNN前向传播算法\n\nDNN前向传播算法也就是利用若干个权重系数矩阵W，偏倚向量$b$，输入值向量x进行一系列线形运算和激活运算。从输入层开始，一层层的向后进行运算，直到运算到输出层，得到输出结果为止。\n\n输入：隐含层和输出层对应的矩阵$W$，偏倚向量$b$，输入值向量$x$，层数$L$。\n\n输出：输出层结果$a^L$。\n\n+ 初始化$a^1=x$\n+ $for \\ l = 2\\  to \\ L$\n\n$$\na^l=\\sigma(z^l)=\\sigma(W^l a{^{l-1}}+b^l)\n$$\n\n单独看DNN前向传播算法，通过运算之后，得到的结果并没有什么意义，误差似乎特别大。而且怎么得到初始的矩阵W,偏倚向量b，最优的矩阵W,偏倚向量b呢？下篇文章将通过深度神经网络之反向传播算法来解决这些问题。\n\n参考\n\n> [刘建平Pinard_深度神经网络（DNN）模型与前向传播算法](http://www.cnblogs.com/pinard/p/6418668.html)\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](深度神经网络之前向传播算法/推广.png)","slug":"深度神经网络之前向传播算法","published":1,"updated":"2018-06-27T08:17:09.591Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjktv5d4r001yjiz5xk6kmmyt","content":"<h3 id=\"1-深度神经网络简介\"><a href=\"#1-深度神经网络简介\" class=\"headerlink\" title=\"1.深度神经网络简介\"></a>1.深度神经网络简介</h3><p><strong>深度神经网络(Deep Neural Networks,DNN)</strong>从字面上理解，也就是深层次的神经网络，从网络结构上看来就是有多个隐含层的神经网络。深度神经网络不仅能够用于分类和回归，在降维、聚类、语音识别、图像识别方面也有许多应用。由于神经网络内容较多，将分多次写作，本次主要讲解深度神经网络中的前向传播算法，后续还有反向传播算法、损失函数和激活函数、正则化。</p>\n<h3 id=\"2-从感知机到神经网络\"><a href=\"#2-从感知机到神经网络\" class=\"headerlink\" title=\"2.从感知机到神经网络\"></a>2.从感知机到神经网络</h3><p>在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483814&amp;idx=1&amp;sn=16a56382d24e304a95ab2a2a028993c6&amp;chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd\" target=\"_blank\" rel=\"noopener\">机器学习之Logistic回归</a>之中，我们利用过感知机的模型。如下图所示，也就是有若干个输入和一个输出的感知机模型。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络01.png\" alt=\"深度神经网络01\"></p>\n<p>感知机通过输入和输出学习得到一个线性模型，得到中间输出结果z。然后利用激活函数，从而得到我们希望的结果，例如1或-1。</p>\n<script type=\"math/tex; mode=display\">\nz=\\sum _{i=1}^{m}w_i x_i +b</script><script type=\"math/tex; mode=display\">\nsign(z)=\\left\\{\\begin{matrix}\n-1 & z<0 \\\\ \n 1 & z \\ge0 \n\\end{matrix}\\right.</script><p>上述模型只能用于二元分类，且无法学习比较复杂的非线形模型。而神经网络则是在感知机的模型上做扩展，主要增加以下三点。</p>\n<ul>\n<li><strong>增加隐含层：</strong>如下图所示，隐含层可以有多层，增加模型的表达能力。当然隐含层增加，模型的复杂度也就会增加。</li>\n</ul>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络02.png\" alt=\"深度神经网络02\"></p>\n<ul>\n<li><p><strong>输出层的神经元可以有多个输出：</strong>这样模型便能够灵活的应用于分类和回归，以及其他的机器学习领域，比如降维和聚类。如下图所示，输出层有4个神经元。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络03.png\" alt=\"深度神经网络03\"></p>\n</li>\n<li><p><strong>扩展激活函数：</strong>感知机的激活函数sign(z)处理能力有限，因此神经网络一般使用其他激活函数，比如我们在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483814&amp;idx=1&amp;sn=16a56382d24e304a95ab2a2a028993c6&amp;chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd\" target=\"_blank\" rel=\"noopener\">逻辑回归</a>里面使用的Sigmoid函数。当然还有tanx,softmax,ReLU等激活函数，通过使用不同的激活函数，神经网络的表达能力也就不同，对于各种常用的激活函数，我们在后面会进行专门介绍。</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(z)=\\frac{1}{1+e^{-z}}</script><h3 id=\"3-DNN基本结构\"><a href=\"#3-DNN基本结构\" class=\"headerlink\" title=\"3.DNN基本结构\"></a>3.DNN基本结构</h3><p>从DNN按照不同层的位置来划分，DNN内部的神经网络层可以分为三类，分别是输入层、隐含层、输出层。如下图所示，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐含层。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络04.png\" alt=\"深度神经网络04\"></p>\n<p>DNN的层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来复杂，但是从局部模型来说，还是和感知机相同，即线性关系z加上激活函数σ(z)。由于DNN层数较多，那么线性关系系数w和偏移量b也就很多。但具体的参数在DNN之中如何定义呢？</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络05.png\" alt=\"深度神经网络05\"></p>\n<p>首先我们来看看线性关系系数$w$的定义。以上述的三层DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为$w^{3}_{24}$。上标3代表线性系数$w$所在的层数，而下标对应的是输出的第三层的第2个神经元和输入的第二层的第4个神经元。</p>\n<p>但是，为什么我们不用更方便的$w^{3}_{42}$表示，即输入的第二层的第4个神经元和输出的第三层的第2个神经元，而是用$w^{3}_{24}$表示呢？这样做的目的主要是为了方便矩阵运算，如果是$w^{3}_{42}$的话，那么每次运行的都是$w^Tx+b$，需要进行矩阵转置。将输出的索引放在前面的话，线性运算则不需要转置，直接运算$wx+b$即可。总结下也就是，第$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的线形系数为$w^{l}_{jk}$。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络06.png\" alt=\"深度神经网络06\"></p>\n<p>再来看偏倚量b的定义。以上述的三层DNN为例，第二层的第三个神经元对应的偏移量定义为$b^2_3$。其中上标2代表所在的层数，下标3代表所在神经元的索引。总结下也就是第$l$层的第$j$个神经元的偏倚量为$b^l_j$。</p>\n<h3 id=\"4-DNN前向传播算法的数学原理\"><a href=\"#4-DNN前向传播算法的数学原理\" class=\"headerlink\" title=\"4.DNN前向传播算法的数学原理\"></a>4.DNN前向传播算法的数学原理</h3><p>我们已经了解DNN中线性关系系数w和偏倚量b的定义。现在假设选择的激活函数是σ(z)，隐含层和输出层的输出值为a。则对于下述的三层DNN，我们利用和感知机一样的思路，将上一层的输出当作下一层的输入，然后计算下一层的输出，重复下去，也就是DNN的前向传播算法。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络07.png\" alt=\"深度神经网络07\"></p>\n<p>对于第二层的输出$a^2_1,a^2_2,a^2_3$，我们能够得到</p>\n<script type=\"math/tex; mode=display\">\na^2_1=\\sigma(z^2_1)=\\sigma(w^2_{11}x_1+w^2_{12}x_2+w^2_{13}x_3+b^2_1)</script><script type=\"math/tex; mode=display\">\na^2_2=\\sigma(z^2_2)=\\sigma(w^2_{21}x_1+w^2_{22}x_2+w^2_{23}x_3+b^2_2)</script><script type=\"math/tex; mode=display\">\na^2_3=\\sigma(z^2_3)=\\sigma(w^2_{31}x_1+w^2_{32}x_2+w^2_{33}x_3+b^2_3)</script><p>对与第三层的输出$a^3_1$，我们有</p>\n<script type=\"math/tex; mode=display\">\na^3_1=\\sigma(z^3_1)=\\sigma(w^3_{11}a^2_1+w^3_{12}a^2_2+w^3_{13}a^2_3+b^3_1)</script><p>将上面的例子一般化，假设第$l-1$层共有$m$个神经元，则对于第$l$层的第$j$个神经元的输出$a^l_j$如下所示。另外，如果l=2，则对于$a^1_k$即为输入层的$x_k$。</p>\n<script type=\"math/tex; mode=display\">\na^l_j=\\sigma(z^l_j)=\\sigma(\\sum _{k=1}^{m} w^l_{jk}a^{l-1}_k+b^l_j)</script><p>从上面可以看出，使用代数法表示运算比较复杂，因此我们使用比较简洁的矩阵表示。假设第$l-1$层共有m个神经元，而第$l$层共有$n$个神经元，则第$l$层的线性系数$w$组成一个$n\\ast m$的矩阵$W^l$，第$l$层的偏倚$b$组成一个$n*1$的向量$b^l$。</p>\n<p>第$l-1$层的输出$a$组成一个$m\\ast 1$的向量$a^{l-1}$，第$l$层未激活前线性输出$z$组成一个$n\\ast 1$的向量$z^l$，第$l$层的输出$a$组成了一个$n*1$的向量$a^l$。用矩阵法进行表示，第$l$层输出为</p>\n<script type=\"math/tex; mode=display\">\na^l=\\sigma(z^l)=\\sigma(W^l a{^{l-1}}+b^l)</script><h3 id=\"5-DNN前向传播算法\"><a href=\"#5-DNN前向传播算法\" class=\"headerlink\" title=\"5.DNN前向传播算法\"></a>5.DNN前向传播算法</h3><p>DNN前向传播算法也就是利用若干个权重系数矩阵W，偏倚向量$b$，输入值向量x进行一系列线形运算和激活运算。从输入层开始，一层层的向后进行运算，直到运算到输出层，得到输出结果为止。</p>\n<p>输入：隐含层和输出层对应的矩阵$W$，偏倚向量$b$，输入值向量$x$，层数$L$。</p>\n<p>输出：输出层结果$a^L$。</p>\n<ul>\n<li>初始化$a^1=x$</li>\n<li>$for \\ l = 2\\  to \\ L$</li>\n</ul>\n<script type=\"math/tex; mode=display\">\na^l=\\sigma(z^l)=\\sigma(W^l a{^{l-1}}+b^l)</script><p>单独看DNN前向传播算法，通过运算之后，得到的结果并没有什么意义，误差似乎特别大。而且怎么得到初始的矩阵W,偏倚向量b，最优的矩阵W,偏倚向量b呢？下篇文章将通过深度神经网络之反向传播算法来解决这些问题。</p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6418668.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_深度神经网络（DNN）模型与前向传播算法</a></p>\n</blockquote>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-深度神经网络简介\"><a href=\"#1-深度神经网络简介\" class=\"headerlink\" title=\"1.深度神经网络简介\"></a>1.深度神经网络简介</h3><p><strong>深度神经网络(Deep Neural Networks,DNN)</strong>从字面上理解，也就是深层次的神经网络，从网络结构上看来就是有多个隐含层的神经网络。深度神经网络不仅能够用于分类和回归，在降维、聚类、语音识别、图像识别方面也有许多应用。由于神经网络内容较多，将分多次写作，本次主要讲解深度神经网络中的前向传播算法，后续还有反向传播算法、损失函数和激活函数、正则化。</p>\n<h3 id=\"2-从感知机到神经网络\"><a href=\"#2-从感知机到神经网络\" class=\"headerlink\" title=\"2.从感知机到神经网络\"></a>2.从感知机到神经网络</h3><p>在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483814&amp;idx=1&amp;sn=16a56382d24e304a95ab2a2a028993c6&amp;chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd\" target=\"_blank\" rel=\"noopener\">机器学习之Logistic回归</a>之中，我们利用过感知机的模型。如下图所示，也就是有若干个输入和一个输出的感知机模型。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络01.png\" alt=\"深度神经网络01\"></p>\n<p>感知机通过输入和输出学习得到一个线性模型，得到中间输出结果z。然后利用激活函数，从而得到我们希望的结果，例如1或-1。</p>\n<script type=\"math/tex; mode=display\">\nz=\\sum _{i=1}^{m}w_i x_i +b</script><script type=\"math/tex; mode=display\">\nsign(z)=\\left\\{\\begin{matrix}\n-1 & z<0 \\\\ \n 1 & z \\ge0 \n\\end{matrix}\\right.</script><p>上述模型只能用于二元分类，且无法学习比较复杂的非线形模型。而神经网络则是在感知机的模型上做扩展，主要增加以下三点。</p>\n<ul>\n<li><strong>增加隐含层：</strong>如下图所示，隐含层可以有多层，增加模型的表达能力。当然隐含层增加，模型的复杂度也就会增加。</li>\n</ul>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络02.png\" alt=\"深度神经网络02\"></p>\n<ul>\n<li><p><strong>输出层的神经元可以有多个输出：</strong>这样模型便能够灵活的应用于分类和回归，以及其他的机器学习领域，比如降维和聚类。如下图所示，输出层有4个神经元。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络03.png\" alt=\"深度神经网络03\"></p>\n</li>\n<li><p><strong>扩展激活函数：</strong>感知机的激活函数sign(z)处理能力有限，因此神经网络一般使用其他激活函数，比如我们在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483814&amp;idx=1&amp;sn=16a56382d24e304a95ab2a2a028993c6&amp;chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd\" target=\"_blank\" rel=\"noopener\">逻辑回归</a>里面使用的Sigmoid函数。当然还有tanx,softmax,ReLU等激活函数，通过使用不同的激活函数，神经网络的表达能力也就不同，对于各种常用的激活函数，我们在后面会进行专门介绍。</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nf(z)=\\frac{1}{1+e^{-z}}</script><h3 id=\"3-DNN基本结构\"><a href=\"#3-DNN基本结构\" class=\"headerlink\" title=\"3.DNN基本结构\"></a>3.DNN基本结构</h3><p>从DNN按照不同层的位置来划分，DNN内部的神经网络层可以分为三类，分别是输入层、隐含层、输出层。如下图所示，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐含层。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络04.png\" alt=\"深度神经网络04\"></p>\n<p>DNN的层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来复杂，但是从局部模型来说，还是和感知机相同，即线性关系z加上激活函数σ(z)。由于DNN层数较多，那么线性关系系数w和偏移量b也就很多。但具体的参数在DNN之中如何定义呢？</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络05.png\" alt=\"深度神经网络05\"></p>\n<p>首先我们来看看线性关系系数$w$的定义。以上述的三层DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为$w^{3}_{24}$。上标3代表线性系数$w$所在的层数，而下标对应的是输出的第三层的第2个神经元和输入的第二层的第4个神经元。</p>\n<p>但是，为什么我们不用更方便的$w^{3}_{42}$表示，即输入的第二层的第4个神经元和输出的第三层的第2个神经元，而是用$w^{3}_{24}$表示呢？这样做的目的主要是为了方便矩阵运算，如果是$w^{3}_{42}$的话，那么每次运行的都是$w^Tx+b$，需要进行矩阵转置。将输出的索引放在前面的话，线性运算则不需要转置，直接运算$wx+b$即可。总结下也就是，第$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的线形系数为$w^{l}_{jk}$。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络06.png\" alt=\"深度神经网络06\"></p>\n<p>再来看偏倚量b的定义。以上述的三层DNN为例，第二层的第三个神经元对应的偏移量定义为$b^2_3$。其中上标2代表所在的层数，下标3代表所在神经元的索引。总结下也就是第$l$层的第$j$个神经元的偏倚量为$b^l_j$。</p>\n<h3 id=\"4-DNN前向传播算法的数学原理\"><a href=\"#4-DNN前向传播算法的数学原理\" class=\"headerlink\" title=\"4.DNN前向传播算法的数学原理\"></a>4.DNN前向传播算法的数学原理</h3><p>我们已经了解DNN中线性关系系数w和偏倚量b的定义。现在假设选择的激活函数是σ(z)，隐含层和输出层的输出值为a。则对于下述的三层DNN，我们利用和感知机一样的思路，将上一层的输出当作下一层的输入，然后计算下一层的输出，重复下去，也就是DNN的前向传播算法。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/深度神经网络07.png\" alt=\"深度神经网络07\"></p>\n<p>对于第二层的输出$a^2_1,a^2_2,a^2_3$，我们能够得到</p>\n<script type=\"math/tex; mode=display\">\na^2_1=\\sigma(z^2_1)=\\sigma(w^2_{11}x_1+w^2_{12}x_2+w^2_{13}x_3+b^2_1)</script><script type=\"math/tex; mode=display\">\na^2_2=\\sigma(z^2_2)=\\sigma(w^2_{21}x_1+w^2_{22}x_2+w^2_{23}x_3+b^2_2)</script><script type=\"math/tex; mode=display\">\na^2_3=\\sigma(z^2_3)=\\sigma(w^2_{31}x_1+w^2_{32}x_2+w^2_{33}x_3+b^2_3)</script><p>对与第三层的输出$a^3_1$，我们有</p>\n<script type=\"math/tex; mode=display\">\na^3_1=\\sigma(z^3_1)=\\sigma(w^3_{11}a^2_1+w^3_{12}a^2_2+w^3_{13}a^2_3+b^3_1)</script><p>将上面的例子一般化，假设第$l-1$层共有$m$个神经元，则对于第$l$层的第$j$个神经元的输出$a^l_j$如下所示。另外，如果l=2，则对于$a^1_k$即为输入层的$x_k$。</p>\n<script type=\"math/tex; mode=display\">\na^l_j=\\sigma(z^l_j)=\\sigma(\\sum _{k=1}^{m} w^l_{jk}a^{l-1}_k+b^l_j)</script><p>从上面可以看出，使用代数法表示运算比较复杂，因此我们使用比较简洁的矩阵表示。假设第$l-1$层共有m个神经元，而第$l$层共有$n$个神经元，则第$l$层的线性系数$w$组成一个$n\\ast m$的矩阵$W^l$，第$l$层的偏倚$b$组成一个$n*1$的向量$b^l$。</p>\n<p>第$l-1$层的输出$a$组成一个$m\\ast 1$的向量$a^{l-1}$，第$l$层未激活前线性输出$z$组成一个$n\\ast 1$的向量$z^l$，第$l$层的输出$a$组成了一个$n*1$的向量$a^l$。用矩阵法进行表示，第$l$层输出为</p>\n<script type=\"math/tex; mode=display\">\na^l=\\sigma(z^l)=\\sigma(W^l a{^{l-1}}+b^l)</script><h3 id=\"5-DNN前向传播算法\"><a href=\"#5-DNN前向传播算法\" class=\"headerlink\" title=\"5.DNN前向传播算法\"></a>5.DNN前向传播算法</h3><p>DNN前向传播算法也就是利用若干个权重系数矩阵W，偏倚向量$b$，输入值向量x进行一系列线形运算和激活运算。从输入层开始，一层层的向后进行运算，直到运算到输出层，得到输出结果为止。</p>\n<p>输入：隐含层和输出层对应的矩阵$W$，偏倚向量$b$，输入值向量$x$，层数$L$。</p>\n<p>输出：输出层结果$a^L$。</p>\n<ul>\n<li>初始化$a^1=x$</li>\n<li>$for \\ l = 2\\  to \\ L$</li>\n</ul>\n<script type=\"math/tex; mode=display\">\na^l=\\sigma(z^l)=\\sigma(W^l a{^{l-1}}+b^l)</script><p>单独看DNN前向传播算法，通过运算之后，得到的结果并没有什么意义，误差似乎特别大。而且怎么得到初始的矩阵W,偏倚向量b，最优的矩阵W,偏倚向量b呢？下篇文章将通过深度神经网络之反向传播算法来解决这些问题。</p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6418668.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_深度神经网络（DNN）模型与前向传播算法</a></p>\n</blockquote>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/06/27/深度神经网络之前向传播算法/推广.png\" alt=\"推广\"></p>\n"},{"title":"深度神经网络之反向传播算法","date":"2018-06-27T10:18:42.000Z","mathjax":true,"_content":"\n### 1.DNN反向传播算法简介\n\n回顾我们前面学到的监督问题，通常会遇到这种情况，假如有$m$个训练样本，分别为$\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_m,y_m) \\}$，其中$x$为输入变量，特征维度为n_in，y为输出向量，特征维度为n_out。现在我们利用这m个训练样本来训练模型，当有测试样本$(x_{test},?)$时，需要我们能够预测出$y_{test}$向量的输出。\n\n现在对应到我们的DNN模型之中，即输入层有n_in个神经元，输出层有n_out个神经元，再加上一些含有若干个神经元的隐含层。此时我们需要找到所有隐含层和输出层所对应的线性系数矩阵W、偏倚向量b，希望通过DNN对所有的训练样本计算后，计算结果能够等于或很接近样本输出，当有新的测试样本数据时，能够有效预测样本输出。但怎样找到合适的线形系数矩阵W和偏倚变量b呢?\n\n回顾我们前面学习的[机器学习之Logistic回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483814&idx=1&sn=16a56382d24e304a95ab2a2a028993c6&chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd)、[机器学习之SVM支持向量机](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483818&idx=1&sn=50c634d8b00877134558125c4a718fd7&chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd)等机器学习算法，很容易联想到，我们可以用一个合适的损失函数来度量训练样本的输出损失。然后对损失函数优化，求损失函数最小化的极值，此时对应的线性系数矩阵W，偏倚变量b便是我们希望得到的结果。深度神经网络中，损失函数优化极值求解的过程，通常是利用梯度下降法迭代完成的。当然也可以利用其他的迭代方法，比如牛顿法或拟牛顿法。梯度下降算法以前在[机器学习之线形回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483804&idx=1&sn=bfc7d6f51e4db6cf0028b33b60690ff6&chksm=fcd7d26acba05b7cdea7c083736b7348d1e3d3f3b5d21a34c12f35b4e57f1f7650499b73d911#rd)中有过详细介绍，有兴趣可以回顾一下。\n\n对DNN损失函数用梯度下降法进行迭代优化求极小值的过程，便是我们的**反向传播算法(Back Propagation,BP)**。\n\n### 2.DNN反向传播算法数学推导\n\n进行DNN反向传播算法之前，我们需要选择一个损失函数，来度量计算样本的输出和真实样本之间的损失。但训练时的计算样本输出怎么得到呢？\n\n初始时，我们会随机选择一系列W,b，然后利用[神经网络之前向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483903&idx=1&sn=4e3f92578399013eba9f203d35afe972&chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f#rd)中介绍到的$a^l=\\sigma(z^l)=\\sigma(W^la^{l-1}+b^l)$，计算输出层所对应的$a^L$，此时的$a^L$便是DNN计算样本的输出。为专注DNN反向传播算法的推导，我们选择较为简单的损失函数，为此我们使用最常见的均方差来度量损失。\n\n即对于每个样本，我们期望能够最小化下式，其中$a^L$和$y$为特征维度的n_out的向量，$||S||_2$为S的L2范数。\n$$\nJ(W,b,x,y)=\\frac{1}{2}||a^L-y||_{2}^{2}\n$$\n通过损失函数，我们能够用梯度下降法来迭代求解每一层的W，b。首先计算的是输出层，其中输出层的W，b满足下式\n$$\na^L=\\sigma(z^L)=\\sigma(W^La^{L-1}+b^L)\n$$\n\n$$\nJ(W,b,x,y)=\\frac{1}{2}||a^L-y||_{2}^{2}=\\frac{1}{2}||\\sigma(W^La^{L-1}+b^L)-y||_2^2\n$$\n\n然后对$W^L,b^L$分别求偏导，其中符号$\\odot$表示Hadamard积，对于两个维度的向量$A(a_1,a_2,a_3,...,a_n)^T$和$B(b_1,b_2,b_3,...,b_n)^T$，那么$A\\odot B=(a_1b_1,a_2b_2,a_3b_3,...,a_nb_n)^T$。之所以使用Hadamard积，是因为我们不了解激活函数的形式，所以用Hadamard积来乘激活函数的导数。另外补充矩阵求导的知识点，其中$\\frac{\\partial AB}{\\partial B}=A^T$。\n$$\n\\frac{\\partial J(W,b,x,y)}{\\partial W^L}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}=(a^L-y)\\odot {\\sigma}' (z^L)(a^{L-1})^T\n$$\n\n$$\n\\frac{\\partial J(W,b,x,y)}{\\partial b^L}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L)}{\\partial b^L}=(a^L-y)\\odot {\\sigma}' (z^L)\n$$\n\n注意到在求解输出层W，b的时候，有公共部分$\\frac{\\partial J(W,b,x,y)}{\\partial z^L}$，因此我们可以把公共部分先算出来，记为\n$$\n\\delta^L=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}=(a^L-y)\\odot {\\sigma}' (z^L)\n$$\n现在我们已经把输出层的梯度算出来了，那么如何求解L-1、L-2…层的梯度呢？这里我们需要进一步递推，对于第$l$层的$\\delta^l$可以表示为\n$$\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}} \\frac{\\partial z^{L-1}}{\\partial z^{L-2}}...\\frac{\\partial z^{l+1}}{\\partial z^{l}}\n$$\n如果我们能够计算出第$l$层的$\\delta^l$，那么对于该层的$W^l,b^l$也会很容易计算。为什么呢?注意到前向传播算法，我们有\n$$\nz^l=W^l a^{l-1}+b^l\n$$\n所以根据上式我们可以很方便的计算第$l$层的$W^l,b^l$\n$$\n\\frac{\\partial J(W,b,x,y)}{\\partial W^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}\\frac{\\partial z^l}{\\partial W^l}=\\delta^l (a^{l-1})^T\n$$\n\n$$\n\\frac{\\partial J(W,b,x,y)}{\\partial b^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}\\frac{\\partial z^l)}{\\partial b^l}=\\delta ^l\n$$\n\n现在问题关键便是如何求解$\\delta^l$。假设我们已经得到第$l+1$层的$\\delta^{l+1}$，那么如何得到第$l$层的$\\delta^l$呢？我们注意到\n$$\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^{l+1}}\\frac{\\partial z^{l+1}}{\\partial z^{l}} =\\delta^{l+1} \\frac{\\partial z^{l+1}}{\\partial z^l}=\n$$\n\n$$\n\\frac{\\partial (\\delta^{l+1})^T z^{l+1}}{\\partial z^l}=\\frac{\\partial (\\delta^{l+1})^T (W^{l+1}\\sigma(z^l)+ b^{l+1})}{\\partial z^l}=\\frac{\\partial (\\delta^{l+1})^T W^{l+1}\\sigma(z^l)}{\\partial z^l}=\n$$\n\n$$\n((\\delta^{l+1})^TW^{l+1})^T\\odot {\\sigma}' (z^l)=(W^{l+1})^T\\delta ^{l+1}\\odot {\\sigma}' (z^l)\n$$\n\n现在我们已经得到$\\delta^l$的递推式，只要我们求出当前隐含层的$\\delta^l$，便能够得到$W^l,b^l$。\n\n### 3.DNN反向传播算法过程\n\n梯度下降算法有批量(Batch)，小批量(Mini-Batch)，随机三种方式，采用哪种方式取决于我们的问题而定。为简化描述，这里采用最基本的批量梯度下降法来描述反向传播算法。\n\n输入：总层数L、各隐含层与输出层的神经元个数、激活函数、损失函数、迭代步长α、最大迭代次数Max、停止迭代阈值ϵ、输入的m个训练样本${(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$。\n\n输出：各隐含层与输出层的线性关系系数W和偏倚变量b。\n\n+ 初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b为随机值。\n+ $for \\ iter = 1 \\ to\\  Max$\n  + $for\\ i\\ = 1 \\ to \\ m$\n    - 将$a^1$输入值设置为$x_i$\n    - $for\\ l=2 \\ to \\ L$，进行前向传播算法，计算$a^{i,l}=\\sigma(z^{i,l})=\\sigma(W^l a^{i,l-1}+b^l)$\n    - 通过损失函数计算输出层$\\delta^{i,L}$\n    - $for\\ l = L\\ to\\ 2$，进行反向传播算法，计算$\\delta^{i,l}=(W^{l+1})^T\\delta ^{i,l+1}\\odot {\\sigma}' (z^{i,l})$\n  + $for \\ l =2 \\ to\\ L$，更新第$l$层的$W^l,b^l$\n    + $W^l=W^l-\\alpha\\sum_{i=1}^{m}\\delta^{i,l}(a^{i,l-1})^T$\n    + $b^l=b^l-\\alpha\\sum_{i=1}^{m}\\delta^{i,l}$\n  + 如果所有的W,b的变化值都小于停止迭代阈值ϵ，跳出循环。\n+ 输出各隐含层和输出层的线形关系系数矩阵W和偏倚向量b。\n\n通过深度神经网络之中的前向传播算法和反向传播算法的结合，我们能够利用DNN模型去解决各种分类或回归问题，但对于不同问题，效果如何呢？是否会过拟合呢？我们将在下次文章中详细介绍损失函数和激活函数的选择、正则化方面的知识点，来让深度神经网络能更精确的解决我们的问题。\n\n参考\n\n> [刘建平Pinard_深度神经网络(DNN)反向传播算法(BP)](https://www.cnblogs.com/pinard/p/6422831.html#!comments)\n\n### 4.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](深度神经网络之反向传播算法/推广.png)","source":"_posts/深度神经网络之反向传播算法.md","raw":"---\ntitle: 深度神经网络之反向传播算法\ndate: 2018-06-27 18:18:42\ntags: [深度学习,算法]\ncategories: 深度学习\nmathjax: true\n---\n\n### 1.DNN反向传播算法简介\n\n回顾我们前面学到的监督问题，通常会遇到这种情况，假如有$m$个训练样本，分别为$\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_m,y_m) \\}$，其中$x$为输入变量，特征维度为n_in，y为输出向量，特征维度为n_out。现在我们利用这m个训练样本来训练模型，当有测试样本$(x_{test},?)$时，需要我们能够预测出$y_{test}$向量的输出。\n\n现在对应到我们的DNN模型之中，即输入层有n_in个神经元，输出层有n_out个神经元，再加上一些含有若干个神经元的隐含层。此时我们需要找到所有隐含层和输出层所对应的线性系数矩阵W、偏倚向量b，希望通过DNN对所有的训练样本计算后，计算结果能够等于或很接近样本输出，当有新的测试样本数据时，能够有效预测样本输出。但怎样找到合适的线形系数矩阵W和偏倚变量b呢?\n\n回顾我们前面学习的[机器学习之Logistic回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483814&idx=1&sn=16a56382d24e304a95ab2a2a028993c6&chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd)、[机器学习之SVM支持向量机](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483818&idx=1&sn=50c634d8b00877134558125c4a718fd7&chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd)等机器学习算法，很容易联想到，我们可以用一个合适的损失函数来度量训练样本的输出损失。然后对损失函数优化，求损失函数最小化的极值，此时对应的线性系数矩阵W，偏倚变量b便是我们希望得到的结果。深度神经网络中，损失函数优化极值求解的过程，通常是利用梯度下降法迭代完成的。当然也可以利用其他的迭代方法，比如牛顿法或拟牛顿法。梯度下降算法以前在[机器学习之线形回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483804&idx=1&sn=bfc7d6f51e4db6cf0028b33b60690ff6&chksm=fcd7d26acba05b7cdea7c083736b7348d1e3d3f3b5d21a34c12f35b4e57f1f7650499b73d911#rd)中有过详细介绍，有兴趣可以回顾一下。\n\n对DNN损失函数用梯度下降法进行迭代优化求极小值的过程，便是我们的**反向传播算法(Back Propagation,BP)**。\n\n### 2.DNN反向传播算法数学推导\n\n进行DNN反向传播算法之前，我们需要选择一个损失函数，来度量计算样本的输出和真实样本之间的损失。但训练时的计算样本输出怎么得到呢？\n\n初始时，我们会随机选择一系列W,b，然后利用[神经网络之前向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483903&idx=1&sn=4e3f92578399013eba9f203d35afe972&chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f#rd)中介绍到的$a^l=\\sigma(z^l)=\\sigma(W^la^{l-1}+b^l)$，计算输出层所对应的$a^L$，此时的$a^L$便是DNN计算样本的输出。为专注DNN反向传播算法的推导，我们选择较为简单的损失函数，为此我们使用最常见的均方差来度量损失。\n\n即对于每个样本，我们期望能够最小化下式，其中$a^L$和$y$为特征维度的n_out的向量，$||S||_2$为S的L2范数。\n$$\nJ(W,b,x,y)=\\frac{1}{2}||a^L-y||_{2}^{2}\n$$\n通过损失函数，我们能够用梯度下降法来迭代求解每一层的W，b。首先计算的是输出层，其中输出层的W，b满足下式\n$$\na^L=\\sigma(z^L)=\\sigma(W^La^{L-1}+b^L)\n$$\n\n$$\nJ(W,b,x,y)=\\frac{1}{2}||a^L-y||_{2}^{2}=\\frac{1}{2}||\\sigma(W^La^{L-1}+b^L)-y||_2^2\n$$\n\n然后对$W^L,b^L$分别求偏导，其中符号$\\odot$表示Hadamard积，对于两个维度的向量$A(a_1,a_2,a_3,...,a_n)^T$和$B(b_1,b_2,b_3,...,b_n)^T$，那么$A\\odot B=(a_1b_1,a_2b_2,a_3b_3,...,a_nb_n)^T$。之所以使用Hadamard积，是因为我们不了解激活函数的形式，所以用Hadamard积来乘激活函数的导数。另外补充矩阵求导的知识点，其中$\\frac{\\partial AB}{\\partial B}=A^T$。\n$$\n\\frac{\\partial J(W,b,x,y)}{\\partial W^L}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}=(a^L-y)\\odot {\\sigma}' (z^L)(a^{L-1})^T\n$$\n\n$$\n\\frac{\\partial J(W,b,x,y)}{\\partial b^L}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L)}{\\partial b^L}=(a^L-y)\\odot {\\sigma}' (z^L)\n$$\n\n注意到在求解输出层W，b的时候，有公共部分$\\frac{\\partial J(W,b,x,y)}{\\partial z^L}$，因此我们可以把公共部分先算出来，记为\n$$\n\\delta^L=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}=(a^L-y)\\odot {\\sigma}' (z^L)\n$$\n现在我们已经把输出层的梯度算出来了，那么如何求解L-1、L-2…层的梯度呢？这里我们需要进一步递推，对于第$l$层的$\\delta^l$可以表示为\n$$\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}} \\frac{\\partial z^{L-1}}{\\partial z^{L-2}}...\\frac{\\partial z^{l+1}}{\\partial z^{l}}\n$$\n如果我们能够计算出第$l$层的$\\delta^l$，那么对于该层的$W^l,b^l$也会很容易计算。为什么呢?注意到前向传播算法，我们有\n$$\nz^l=W^l a^{l-1}+b^l\n$$\n所以根据上式我们可以很方便的计算第$l$层的$W^l,b^l$\n$$\n\\frac{\\partial J(W,b,x,y)}{\\partial W^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}\\frac{\\partial z^l}{\\partial W^l}=\\delta^l (a^{l-1})^T\n$$\n\n$$\n\\frac{\\partial J(W,b,x,y)}{\\partial b^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}\\frac{\\partial z^l)}{\\partial b^l}=\\delta ^l\n$$\n\n现在问题关键便是如何求解$\\delta^l$。假设我们已经得到第$l+1$层的$\\delta^{l+1}$，那么如何得到第$l$层的$\\delta^l$呢？我们注意到\n$$\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^{l+1}}\\frac{\\partial z^{l+1}}{\\partial z^{l}} =\\delta^{l+1} \\frac{\\partial z^{l+1}}{\\partial z^l}=\n$$\n\n$$\n\\frac{\\partial (\\delta^{l+1})^T z^{l+1}}{\\partial z^l}=\\frac{\\partial (\\delta^{l+1})^T (W^{l+1}\\sigma(z^l)+ b^{l+1})}{\\partial z^l}=\\frac{\\partial (\\delta^{l+1})^T W^{l+1}\\sigma(z^l)}{\\partial z^l}=\n$$\n\n$$\n((\\delta^{l+1})^TW^{l+1})^T\\odot {\\sigma}' (z^l)=(W^{l+1})^T\\delta ^{l+1}\\odot {\\sigma}' (z^l)\n$$\n\n现在我们已经得到$\\delta^l$的递推式，只要我们求出当前隐含层的$\\delta^l$，便能够得到$W^l,b^l$。\n\n### 3.DNN反向传播算法过程\n\n梯度下降算法有批量(Batch)，小批量(Mini-Batch)，随机三种方式，采用哪种方式取决于我们的问题而定。为简化描述，这里采用最基本的批量梯度下降法来描述反向传播算法。\n\n输入：总层数L、各隐含层与输出层的神经元个数、激活函数、损失函数、迭代步长α、最大迭代次数Max、停止迭代阈值ϵ、输入的m个训练样本${(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$。\n\n输出：各隐含层与输出层的线性关系系数W和偏倚变量b。\n\n+ 初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b为随机值。\n+ $for \\ iter = 1 \\ to\\  Max$\n  + $for\\ i\\ = 1 \\ to \\ m$\n    - 将$a^1$输入值设置为$x_i$\n    - $for\\ l=2 \\ to \\ L$，进行前向传播算法，计算$a^{i,l}=\\sigma(z^{i,l})=\\sigma(W^l a^{i,l-1}+b^l)$\n    - 通过损失函数计算输出层$\\delta^{i,L}$\n    - $for\\ l = L\\ to\\ 2$，进行反向传播算法，计算$\\delta^{i,l}=(W^{l+1})^T\\delta ^{i,l+1}\\odot {\\sigma}' (z^{i,l})$\n  + $for \\ l =2 \\ to\\ L$，更新第$l$层的$W^l,b^l$\n    + $W^l=W^l-\\alpha\\sum_{i=1}^{m}\\delta^{i,l}(a^{i,l-1})^T$\n    + $b^l=b^l-\\alpha\\sum_{i=1}^{m}\\delta^{i,l}$\n  + 如果所有的W,b的变化值都小于停止迭代阈值ϵ，跳出循环。\n+ 输出各隐含层和输出层的线形关系系数矩阵W和偏倚向量b。\n\n通过深度神经网络之中的前向传播算法和反向传播算法的结合，我们能够利用DNN模型去解决各种分类或回归问题，但对于不同问题，效果如何呢？是否会过拟合呢？我们将在下次文章中详细介绍损失函数和激活函数的选择、正则化方面的知识点，来让深度神经网络能更精确的解决我们的问题。\n\n参考\n\n> [刘建平Pinard_深度神经网络(DNN)反向传播算法(BP)](https://www.cnblogs.com/pinard/p/6422831.html#!comments)\n\n### 4.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](深度神经网络之反向传播算法/推广.png)","slug":"深度神经网络之反向传播算法","published":1,"updated":"2018-06-28T08:06:36.732Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjktv5d4u0021jiz5k5sp9ij6","content":"<h3 id=\"1-DNN反向传播算法简介\"><a href=\"#1-DNN反向传播算法简介\" class=\"headerlink\" title=\"1.DNN反向传播算法简介\"></a>1.DNN反向传播算法简介</h3><p>回顾我们前面学到的监督问题，通常会遇到这种情况，假如有$m$个训练样本，分别为$\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_m,y_m) \\}$，其中$x$为输入变量，特征维度为n_in，y为输出向量，特征维度为n_out。现在我们利用这m个训练样本来训练模型，当有测试样本$(x_{test},?)$时，需要我们能够预测出$y_{test}$向量的输出。</p>\n<p>现在对应到我们的DNN模型之中，即输入层有n_in个神经元，输出层有n_out个神经元，再加上一些含有若干个神经元的隐含层。此时我们需要找到所有隐含层和输出层所对应的线性系数矩阵W、偏倚向量b，希望通过DNN对所有的训练样本计算后，计算结果能够等于或很接近样本输出，当有新的测试样本数据时，能够有效预测样本输出。但怎样找到合适的线形系数矩阵W和偏倚变量b呢?</p>\n<p>回顾我们前面学习的<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483814&amp;idx=1&amp;sn=16a56382d24e304a95ab2a2a028993c6&amp;chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd\" target=\"_blank\" rel=\"noopener\">机器学习之Logistic回归</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483818&amp;idx=1&amp;sn=50c634d8b00877134558125c4a718fd7&amp;chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd\" target=\"_blank\" rel=\"noopener\">机器学习之SVM支持向量机</a>等机器学习算法，很容易联想到，我们可以用一个合适的损失函数来度量训练样本的输出损失。然后对损失函数优化，求损失函数最小化的极值，此时对应的线性系数矩阵W，偏倚变量b便是我们希望得到的结果。深度神经网络中，损失函数优化极值求解的过程，通常是利用梯度下降法迭代完成的。当然也可以利用其他的迭代方法，比如牛顿法或拟牛顿法。梯度下降算法以前在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483804&amp;idx=1&amp;sn=bfc7d6f51e4db6cf0028b33b60690ff6&amp;chksm=fcd7d26acba05b7cdea7c083736b7348d1e3d3f3b5d21a34c12f35b4e57f1f7650499b73d911#rd\" target=\"_blank\" rel=\"noopener\">机器学习之线形回归</a>中有过详细介绍，有兴趣可以回顾一下。</p>\n<p>对DNN损失函数用梯度下降法进行迭代优化求极小值的过程，便是我们的<strong>反向传播算法(Back Propagation,BP)</strong>。</p>\n<h3 id=\"2-DNN反向传播算法数学推导\"><a href=\"#2-DNN反向传播算法数学推导\" class=\"headerlink\" title=\"2.DNN反向传播算法数学推导\"></a>2.DNN反向传播算法数学推导</h3><p>进行DNN反向传播算法之前，我们需要选择一个损失函数，来度量计算样本的输出和真实样本之间的损失。但训练时的计算样本输出怎么得到呢？</p>\n<p>初始时，我们会随机选择一系列W,b，然后利用<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483903&amp;idx=1&amp;sn=4e3f92578399013eba9f203d35afe972&amp;chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f#rd\" target=\"_blank\" rel=\"noopener\">神经网络之前向传播算法</a>中介绍到的$a^l=\\sigma(z^l)=\\sigma(W^la^{l-1}+b^l)$，计算输出层所对应的$a^L$，此时的$a^L$便是DNN计算样本的输出。为专注DNN反向传播算法的推导，我们选择较为简单的损失函数，为此我们使用最常见的均方差来度量损失。</p>\n<p>即对于每个样本，我们期望能够最小化下式，其中$a^L$和$y$为特征维度的n_out的向量，$||S||_2$为S的L2范数。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b,x,y)=\\frac{1}{2}||a^L-y||_{2}^{2}</script><p>通过损失函数，我们能够用梯度下降法来迭代求解每一层的W，b。首先计算的是输出层，其中输出层的W，b满足下式</p>\n<script type=\"math/tex; mode=display\">\na^L=\\sigma(z^L)=\\sigma(W^La^{L-1}+b^L)</script><script type=\"math/tex; mode=display\">\nJ(W,b,x,y)=\\frac{1}{2}||a^L-y||_{2}^{2}=\\frac{1}{2}||\\sigma(W^La^{L-1}+b^L)-y||_2^2</script><p>然后对$W^L,b^L$分别求偏导，其中符号$\\odot$表示Hadamard积，对于两个维度的向量$A(a_1,a_2,a_3,…,a_n)^T$和$B(b_1,b_2,b_3,…,b_n)^T$，那么$A\\odot B=(a_1b_1,a_2b_2,a_3b_3,…,a_nb_n)^T$。之所以使用Hadamard积，是因为我们不了解激活函数的形式，所以用Hadamard积来乘激活函数的导数。另外补充矩阵求导的知识点，其中$\\frac{\\partial AB}{\\partial B}=A^T$。</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,x,y)}{\\partial W^L}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}=(a^L-y)\\odot {\\sigma}' (z^L)(a^{L-1})^T</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,x,y)}{\\partial b^L}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L)}{\\partial b^L}=(a^L-y)\\odot {\\sigma}' (z^L)</script><p>注意到在求解输出层W，b的时候，有公共部分$\\frac{\\partial J(W,b,x,y)}{\\partial z^L}$，因此我们可以把公共部分先算出来，记为</p>\n<script type=\"math/tex; mode=display\">\n\\delta^L=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}=(a^L-y)\\odot {\\sigma}' (z^L)</script><p>现在我们已经把输出层的梯度算出来了，那么如何求解L-1、L-2…层的梯度呢？这里我们需要进一步递推，对于第$l$层的$\\delta^l$可以表示为</p>\n<script type=\"math/tex; mode=display\">\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}} \\frac{\\partial z^{L-1}}{\\partial z^{L-2}}...\\frac{\\partial z^{l+1}}{\\partial z^{l}}</script><p>如果我们能够计算出第$l$层的$\\delta^l$，那么对于该层的$W^l,b^l$也会很容易计算。为什么呢?注意到前向传播算法，我们有</p>\n<script type=\"math/tex; mode=display\">\nz^l=W^l a^{l-1}+b^l</script><p>所以根据上式我们可以很方便的计算第$l$层的$W^l,b^l$</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,x,y)}{\\partial W^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}\\frac{\\partial z^l}{\\partial W^l}=\\delta^l (a^{l-1})^T</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,x,y)}{\\partial b^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}\\frac{\\partial z^l)}{\\partial b^l}=\\delta ^l</script><p>现在问题关键便是如何求解$\\delta^l$。假设我们已经得到第$l+1$层的$\\delta^{l+1}$，那么如何得到第$l$层的$\\delta^l$呢？我们注意到</p>\n<script type=\"math/tex; mode=display\">\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^{l+1}}\\frac{\\partial z^{l+1}}{\\partial z^{l}} =\\delta^{l+1} \\frac{\\partial z^{l+1}}{\\partial z^l}=</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial (\\delta^{l+1})^T z^{l+1}}{\\partial z^l}=\\frac{\\partial (\\delta^{l+1})^T (W^{l+1}\\sigma(z^l)+ b^{l+1})}{\\partial z^l}=\\frac{\\partial (\\delta^{l+1})^T W^{l+1}\\sigma(z^l)}{\\partial z^l}=</script><script type=\"math/tex; mode=display\">\n((\\delta^{l+1})^TW^{l+1})^T\\odot {\\sigma}' (z^l)=(W^{l+1})^T\\delta ^{l+1}\\odot {\\sigma}' (z^l)</script><p>现在我们已经得到$\\delta^l$的递推式，只要我们求出当前隐含层的$\\delta^l$，便能够得到$W^l,b^l$。</p>\n<h3 id=\"3-DNN反向传播算法过程\"><a href=\"#3-DNN反向传播算法过程\" class=\"headerlink\" title=\"3.DNN反向传播算法过程\"></a>3.DNN反向传播算法过程</h3><p>梯度下降算法有批量(Batch)，小批量(Mini-Batch)，随机三种方式，采用哪种方式取决于我们的问题而定。为简化描述，这里采用最基本的批量梯度下降法来描述反向传播算法。</p>\n<p>输入：总层数L、各隐含层与输出层的神经元个数、激活函数、损失函数、迭代步长α、最大迭代次数Max、停止迭代阈值ϵ、输入的m个训练样本${(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$。</p>\n<p>输出：各隐含层与输出层的线性关系系数W和偏倚变量b。</p>\n<ul>\n<li>初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b为随机值。</li>\n<li>$for \\ iter = 1 \\ to\\  Max$<ul>\n<li>$for\\ i\\ = 1 \\ to \\ m$<ul>\n<li>将$a^1$输入值设置为$x_i$</li>\n<li>$for\\ l=2 \\ to \\ L$，进行前向传播算法，计算$a^{i,l}=\\sigma(z^{i,l})=\\sigma(W^l a^{i,l-1}+b^l)$</li>\n<li>通过损失函数计算输出层$\\delta^{i,L}$</li>\n<li>$for\\ l = L\\ to\\ 2$，进行反向传播算法，计算$\\delta^{i,l}=(W^{l+1})^T\\delta ^{i,l+1}\\odot {\\sigma}’ (z^{i,l})$</li>\n</ul>\n</li>\n<li>$for \\ l =2 \\ to\\ L$，更新第$l$层的$W^l,b^l$<ul>\n<li>$W^l=W^l-\\alpha\\sum_{i=1}^{m}\\delta^{i,l}(a^{i,l-1})^T$</li>\n<li>$b^l=b^l-\\alpha\\sum_{i=1}^{m}\\delta^{i,l}$</li>\n</ul>\n</li>\n<li>如果所有的W,b的变化值都小于停止迭代阈值ϵ，跳出循环。</li>\n</ul>\n</li>\n<li>输出各隐含层和输出层的线形关系系数矩阵W和偏倚向量b。</li>\n</ul>\n<p>通过深度神经网络之中的前向传播算法和反向传播算法的结合，我们能够利用DNN模型去解决各种分类或回归问题，但对于不同问题，效果如何呢？是否会过拟合呢？我们将在下次文章中详细介绍损失函数和激活函数的选择、正则化方面的知识点，来让深度神经网络能更精确的解决我们的问题。</p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6422831.html#!comments\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_深度神经网络(DNN)反向传播算法(BP)</a></p>\n</blockquote>\n<h3 id=\"4-推广\"><a href=\"#4-推广\" class=\"headerlink\" title=\"4.推广\"></a>4.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/06/27/深度神经网络之反向传播算法/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-DNN反向传播算法简介\"><a href=\"#1-DNN反向传播算法简介\" class=\"headerlink\" title=\"1.DNN反向传播算法简介\"></a>1.DNN反向传播算法简介</h3><p>回顾我们前面学到的监督问题，通常会遇到这种情况，假如有$m$个训练样本，分别为$\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_m,y_m) \\}$，其中$x$为输入变量，特征维度为n_in，y为输出向量，特征维度为n_out。现在我们利用这m个训练样本来训练模型，当有测试样本$(x_{test},?)$时，需要我们能够预测出$y_{test}$向量的输出。</p>\n<p>现在对应到我们的DNN模型之中，即输入层有n_in个神经元，输出层有n_out个神经元，再加上一些含有若干个神经元的隐含层。此时我们需要找到所有隐含层和输出层所对应的线性系数矩阵W、偏倚向量b，希望通过DNN对所有的训练样本计算后，计算结果能够等于或很接近样本输出，当有新的测试样本数据时，能够有效预测样本输出。但怎样找到合适的线形系数矩阵W和偏倚变量b呢?</p>\n<p>回顾我们前面学习的<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483814&amp;idx=1&amp;sn=16a56382d24e304a95ab2a2a028993c6&amp;chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd\" target=\"_blank\" rel=\"noopener\">机器学习之Logistic回归</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483818&amp;idx=1&amp;sn=50c634d8b00877134558125c4a718fd7&amp;chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd\" target=\"_blank\" rel=\"noopener\">机器学习之SVM支持向量机</a>等机器学习算法，很容易联想到，我们可以用一个合适的损失函数来度量训练样本的输出损失。然后对损失函数优化，求损失函数最小化的极值，此时对应的线性系数矩阵W，偏倚变量b便是我们希望得到的结果。深度神经网络中，损失函数优化极值求解的过程，通常是利用梯度下降法迭代完成的。当然也可以利用其他的迭代方法，比如牛顿法或拟牛顿法。梯度下降算法以前在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483804&amp;idx=1&amp;sn=bfc7d6f51e4db6cf0028b33b60690ff6&amp;chksm=fcd7d26acba05b7cdea7c083736b7348d1e3d3f3b5d21a34c12f35b4e57f1f7650499b73d911#rd\" target=\"_blank\" rel=\"noopener\">机器学习之线形回归</a>中有过详细介绍，有兴趣可以回顾一下。</p>\n<p>对DNN损失函数用梯度下降法进行迭代优化求极小值的过程，便是我们的<strong>反向传播算法(Back Propagation,BP)</strong>。</p>\n<h3 id=\"2-DNN反向传播算法数学推导\"><a href=\"#2-DNN反向传播算法数学推导\" class=\"headerlink\" title=\"2.DNN反向传播算法数学推导\"></a>2.DNN反向传播算法数学推导</h3><p>进行DNN反向传播算法之前，我们需要选择一个损失函数，来度量计算样本的输出和真实样本之间的损失。但训练时的计算样本输出怎么得到呢？</p>\n<p>初始时，我们会随机选择一系列W,b，然后利用<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483903&amp;idx=1&amp;sn=4e3f92578399013eba9f203d35afe972&amp;chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f#rd\" target=\"_blank\" rel=\"noopener\">神经网络之前向传播算法</a>中介绍到的$a^l=\\sigma(z^l)=\\sigma(W^la^{l-1}+b^l)$，计算输出层所对应的$a^L$，此时的$a^L$便是DNN计算样本的输出。为专注DNN反向传播算法的推导，我们选择较为简单的损失函数，为此我们使用最常见的均方差来度量损失。</p>\n<p>即对于每个样本，我们期望能够最小化下式，其中$a^L$和$y$为特征维度的n_out的向量，$||S||_2$为S的L2范数。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b,x,y)=\\frac{1}{2}||a^L-y||_{2}^{2}</script><p>通过损失函数，我们能够用梯度下降法来迭代求解每一层的W，b。首先计算的是输出层，其中输出层的W，b满足下式</p>\n<script type=\"math/tex; mode=display\">\na^L=\\sigma(z^L)=\\sigma(W^La^{L-1}+b^L)</script><script type=\"math/tex; mode=display\">\nJ(W,b,x,y)=\\frac{1}{2}||a^L-y||_{2}^{2}=\\frac{1}{2}||\\sigma(W^La^{L-1}+b^L)-y||_2^2</script><p>然后对$W^L,b^L$分别求偏导，其中符号$\\odot$表示Hadamard积，对于两个维度的向量$A(a_1,a_2,a_3,…,a_n)^T$和$B(b_1,b_2,b_3,…,b_n)^T$，那么$A\\odot B=(a_1b_1,a_2b_2,a_3b_3,…,a_nb_n)^T$。之所以使用Hadamard积，是因为我们不了解激活函数的形式，所以用Hadamard积来乘激活函数的导数。另外补充矩阵求导的知识点，其中$\\frac{\\partial AB}{\\partial B}=A^T$。</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,x,y)}{\\partial W^L}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}=(a^L-y)\\odot {\\sigma}' (z^L)(a^{L-1})^T</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,x,y)}{\\partial b^L}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L)}{\\partial b^L}=(a^L-y)\\odot {\\sigma}' (z^L)</script><p>注意到在求解输出层W，b的时候，有公共部分$\\frac{\\partial J(W,b,x,y)}{\\partial z^L}$，因此我们可以把公共部分先算出来，记为</p>\n<script type=\"math/tex; mode=display\">\n\\delta^L=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}=(a^L-y)\\odot {\\sigma}' (z^L)</script><p>现在我们已经把输出层的梯度算出来了，那么如何求解L-1、L-2…层的梯度呢？这里我们需要进一步递推，对于第$l$层的$\\delta^l$可以表示为</p>\n<script type=\"math/tex; mode=display\">\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}} \\frac{\\partial z^{L-1}}{\\partial z^{L-2}}...\\frac{\\partial z^{l+1}}{\\partial z^{l}}</script><p>如果我们能够计算出第$l$层的$\\delta^l$，那么对于该层的$W^l,b^l$也会很容易计算。为什么呢?注意到前向传播算法，我们有</p>\n<script type=\"math/tex; mode=display\">\nz^l=W^l a^{l-1}+b^l</script><p>所以根据上式我们可以很方便的计算第$l$层的$W^l,b^l$</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,x,y)}{\\partial W^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}\\frac{\\partial z^l}{\\partial W^l}=\\delta^l (a^{l-1})^T</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,x,y)}{\\partial b^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}\\frac{\\partial z^l)}{\\partial b^l}=\\delta ^l</script><p>现在问题关键便是如何求解$\\delta^l$。假设我们已经得到第$l+1$层的$\\delta^{l+1}$，那么如何得到第$l$层的$\\delta^l$呢？我们注意到</p>\n<script type=\"math/tex; mode=display\">\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^{l+1}}\\frac{\\partial z^{l+1}}{\\partial z^{l}} =\\delta^{l+1} \\frac{\\partial z^{l+1}}{\\partial z^l}=</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial (\\delta^{l+1})^T z^{l+1}}{\\partial z^l}=\\frac{\\partial (\\delta^{l+1})^T (W^{l+1}\\sigma(z^l)+ b^{l+1})}{\\partial z^l}=\\frac{\\partial (\\delta^{l+1})^T W^{l+1}\\sigma(z^l)}{\\partial z^l}=</script><script type=\"math/tex; mode=display\">\n((\\delta^{l+1})^TW^{l+1})^T\\odot {\\sigma}' (z^l)=(W^{l+1})^T\\delta ^{l+1}\\odot {\\sigma}' (z^l)</script><p>现在我们已经得到$\\delta^l$的递推式，只要我们求出当前隐含层的$\\delta^l$，便能够得到$W^l,b^l$。</p>\n<h3 id=\"3-DNN反向传播算法过程\"><a href=\"#3-DNN反向传播算法过程\" class=\"headerlink\" title=\"3.DNN反向传播算法过程\"></a>3.DNN反向传播算法过程</h3><p>梯度下降算法有批量(Batch)，小批量(Mini-Batch)，随机三种方式，采用哪种方式取决于我们的问题而定。为简化描述，这里采用最基本的批量梯度下降法来描述反向传播算法。</p>\n<p>输入：总层数L、各隐含层与输出层的神经元个数、激活函数、损失函数、迭代步长α、最大迭代次数Max、停止迭代阈值ϵ、输入的m个训练样本${(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$。</p>\n<p>输出：各隐含层与输出层的线性关系系数W和偏倚变量b。</p>\n<ul>\n<li>初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b为随机值。</li>\n<li>$for \\ iter = 1 \\ to\\  Max$<ul>\n<li>$for\\ i\\ = 1 \\ to \\ m$<ul>\n<li>将$a^1$输入值设置为$x_i$</li>\n<li>$for\\ l=2 \\ to \\ L$，进行前向传播算法，计算$a^{i,l}=\\sigma(z^{i,l})=\\sigma(W^l a^{i,l-1}+b^l)$</li>\n<li>通过损失函数计算输出层$\\delta^{i,L}$</li>\n<li>$for\\ l = L\\ to\\ 2$，进行反向传播算法，计算$\\delta^{i,l}=(W^{l+1})^T\\delta ^{i,l+1}\\odot {\\sigma}’ (z^{i,l})$</li>\n</ul>\n</li>\n<li>$for \\ l =2 \\ to\\ L$，更新第$l$层的$W^l,b^l$<ul>\n<li>$W^l=W^l-\\alpha\\sum_{i=1}^{m}\\delta^{i,l}(a^{i,l-1})^T$</li>\n<li>$b^l=b^l-\\alpha\\sum_{i=1}^{m}\\delta^{i,l}$</li>\n</ul>\n</li>\n<li>如果所有的W,b的变化值都小于停止迭代阈值ϵ，跳出循环。</li>\n</ul>\n</li>\n<li>输出各隐含层和输出层的线形关系系数矩阵W和偏倚向量b。</li>\n</ul>\n<p>通过深度神经网络之中的前向传播算法和反向传播算法的结合，我们能够利用DNN模型去解决各种分类或回归问题，但对于不同问题，效果如何呢？是否会过拟合呢？我们将在下次文章中详细介绍损失函数和激活函数的选择、正则化方面的知识点，来让深度神经网络能更精确的解决我们的问题。</p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6422831.html#!comments\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_深度神经网络(DNN)反向传播算法(BP)</a></p>\n</blockquote>\n<h3 id=\"4-推广\"><a href=\"#4-推广\" class=\"headerlink\" title=\"4.推广\"></a>4.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/06/27/深度神经网络之反向传播算法/推广.png\" alt=\"推广\"></p>\n"},{"title":"机器学习知识体系","date":"2018-03-24T04:30:14.000Z","comments":1,"toc":true,"_content":"\n### 1.什么是机器学习\n\n>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n\n上述为**百度百科**定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。\n\n+ 给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。\n\n![图片01](机器学习知识体系/图片01.png)\n\n\n\n+ 以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。\n\n![图片02](机器学习知识体系/图片02.png)\n\n### 2.机器学习体系概括\n\n机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。\n\n![图片03](机器学习知识体系/图片03.png)\n\n下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。\n\n![图片04](机器学习知识体系/图片04.png)\n\n机器学习算法中常用到的便是**监督学习**和**无监督学习**，监督学习包含**回归**和**分类**两方面，无监督学习为**聚类**。\n\n**监督学习（Supervised Learning）**\n\n当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为**回归分析（Regression Analysis）**和**分类（Classification）**两类。\n\n+ **回归分析（Regression Analysis）**：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。\n+ **分类（Classfication）**：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。\n\n**无监督学习（Unsupervised Learning）**\n\n我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。\n\n所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。\n\n### 3.如何开始学习\n\n开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。\n\n很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。\n\n---\n\n### 4.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](机器学习知识体系/推广.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/机器学习知识体系.md","raw":"---\ntitle: 机器学习知识体系\ndate: 2018-03-24 12:30:14\ntags: 机器学习\ncategories: 机器学习\ncomments: true\ntoc: true\n---\n\n### 1.什么是机器学习\n\n>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n\n上述为**百度百科**定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。\n\n+ 给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。\n\n![图片01](机器学习知识体系/图片01.png)\n\n\n\n+ 以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。\n\n![图片02](机器学习知识体系/图片02.png)\n\n### 2.机器学习体系概括\n\n机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。\n\n![图片03](机器学习知识体系/图片03.png)\n\n下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。\n\n![图片04](机器学习知识体系/图片04.png)\n\n机器学习算法中常用到的便是**监督学习**和**无监督学习**，监督学习包含**回归**和**分类**两方面，无监督学习为**聚类**。\n\n**监督学习（Supervised Learning）**\n\n当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为**回归分析（Regression Analysis）**和**分类（Classification）**两类。\n\n+ **回归分析（Regression Analysis）**：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。\n+ **分类（Classfication）**：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。\n\n**无监督学习（Unsupervised Learning）**\n\n我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。\n\n所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。\n\n### 3.如何开始学习\n\n开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。\n\n很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。\n\n---\n\n### 4.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](机器学习知识体系/推广.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"机器学习知识体系","published":1,"updated":"2018-06-07T17:52:51.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4x0025jiz502px4h2s","content":"<h3 id=\"1-什么是机器学习\"><a href=\"#1-什么是机器学习\" class=\"headerlink\" title=\"1.什么是机器学习\"></a>1.什么是机器学习</h3><blockquote>\n<p>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。</p>\n</blockquote>\n<p>上述为<strong>百度百科</strong>定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。</p>\n<ul>\n<li>给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。</li>\n</ul>\n<p><img src=\"/2018/03/24/机器学习知识体系/图片01.png\" alt=\"图片01\"></p>\n<ul>\n<li>以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。</li>\n</ul>\n<p><img src=\"/2018/03/24/机器学习知识体系/图片02.png\" alt=\"图片02\"></p>\n<h3 id=\"2-机器学习体系概括\"><a href=\"#2-机器学习体系概括\" class=\"headerlink\" title=\"2.机器学习体系概括\"></a>2.机器学习体系概括</h3><p>机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。</p>\n<p><img src=\"/2018/03/24/机器学习知识体系/图片03.png\" alt=\"图片03\"></p>\n<p>下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。</p>\n<p><img src=\"/2018/03/24/机器学习知识体系/图片04.png\" alt=\"图片04\"></p>\n<p>机器学习算法中常用到的便是<strong>监督学习</strong>和<strong>无监督学习</strong>，监督学习包含<strong>回归</strong>和<strong>分类</strong>两方面，无监督学习为<strong>聚类</strong>。</p>\n<p><strong>监督学习（Supervised Learning）</strong></p>\n<p>当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为<strong>回归分析（Regression Analysis）</strong>和<strong>分类（Classification）</strong>两类。</p>\n<ul>\n<li><strong>回归分析（Regression Analysis）</strong>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。</li>\n<li><strong>分类（Classfication）</strong>：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。</li>\n</ul>\n<p><strong>无监督学习（Unsupervised Learning）</strong></p>\n<p>我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。</p>\n<p>所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。</p>\n<h3 id=\"3-如何开始学习\"><a href=\"#3-如何开始学习\" class=\"headerlink\" title=\"3.如何开始学习\"></a>3.如何开始学习</h3><p>开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。</p>\n<p>很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。</p>\n<hr>\n<h3 id=\"4-推广\"><a href=\"#4-推广\" class=\"headerlink\" title=\"4.推广\"></a>4.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/24/机器学习知识体系/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-什么是机器学习\"><a href=\"#1-什么是机器学习\" class=\"headerlink\" title=\"1.什么是机器学习\"></a>1.什么是机器学习</h3><blockquote>\n<p>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。</p>\n</blockquote>\n<p>上述为<strong>百度百科</strong>定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。</p>\n<ul>\n<li>给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。</li>\n</ul>\n<p><img src=\"/2018/03/24/机器学习知识体系/图片01.png\" alt=\"图片01\"></p>\n<ul>\n<li>以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。</li>\n</ul>\n<p><img src=\"/2018/03/24/机器学习知识体系/图片02.png\" alt=\"图片02\"></p>\n<h3 id=\"2-机器学习体系概括\"><a href=\"#2-机器学习体系概括\" class=\"headerlink\" title=\"2.机器学习体系概括\"></a>2.机器学习体系概括</h3><p>机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。</p>\n<p><img src=\"/2018/03/24/机器学习知识体系/图片03.png\" alt=\"图片03\"></p>\n<p>下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。</p>\n<p><img src=\"/2018/03/24/机器学习知识体系/图片04.png\" alt=\"图片04\"></p>\n<p>机器学习算法中常用到的便是<strong>监督学习</strong>和<strong>无监督学习</strong>，监督学习包含<strong>回归</strong>和<strong>分类</strong>两方面，无监督学习为<strong>聚类</strong>。</p>\n<p><strong>监督学习（Supervised Learning）</strong></p>\n<p>当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为<strong>回归分析（Regression Analysis）</strong>和<strong>分类（Classification）</strong>两类。</p>\n<ul>\n<li><strong>回归分析（Regression Analysis）</strong>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。</li>\n<li><strong>分类（Classfication）</strong>：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。</li>\n</ul>\n<p><strong>无监督学习（Unsupervised Learning）</strong></p>\n<p>我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。</p>\n<p>所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。</p>\n<h3 id=\"3-如何开始学习\"><a href=\"#3-如何开始学习\" class=\"headerlink\" title=\"3.如何开始学习\"></a>3.如何开始学习</h3><p>开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。</p>\n<p>很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。</p>\n<hr>\n<h3 id=\"4-推广\"><a href=\"#4-推广\" class=\"headerlink\" title=\"4.推广\"></a>4.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/03/24/机器学习知识体系/推广.png\" alt=\"推广\"></p>\n"},{"title":"面向知乎的个性化推荐模型研究","date":"2018-03-12T05:18:48.000Z","toc":true,"comments":1,"_content":"### 面向知乎的个性化推荐模型研究\n《[面向知乎的个性化推荐模型研究](https://github.com/XiaoYiii/Paper/blob/master/Paper/%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6.pdf)》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。\n\n![001](面向知乎的个性化推荐模型研究论文/001.png)\n![002](面向知乎的个性化推荐模型研究论文/002.png)\n![003](面向知乎的个性化推荐模型研究论文/003.png)\n![004](面向知乎的个性化推荐模型研究论文/004.png)\n![005](面向知乎的个性化推荐模型研究论文/005.png)\n\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n","source":"_posts/面向知乎的个性化推荐模型研究论文.md","raw":"---\ntitle: 面向知乎的个性化推荐模型研究\ndate: 2018-03-12 13:18:48\ntags: 推荐系统\ntoc: true\ncategories: 推荐系统\ncomments: true\n---\n### 面向知乎的个性化推荐模型研究\n《[面向知乎的个性化推荐模型研究](https://github.com/XiaoYiii/Paper/blob/master/Paper/%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6.pdf)》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。\n\n![001](面向知乎的个性化推荐模型研究论文/001.png)\n![002](面向知乎的个性化推荐模型研究论文/002.png)\n![003](面向知乎的个性化推荐模型研究论文/003.png)\n![004](面向知乎的个性化推荐模型研究论文/004.png)\n![005](面向知乎的个性化推荐模型研究论文/005.png)\n\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n","slug":"面向知乎的个性化推荐模型研究论文","published":1,"updated":"2018-06-26T03:49:25.409Z","layout":"post","photos":[],"link":"","_id":"cjktv5d4z0028jiz5cr4m2yg9","content":"<h3 id=\"面向知乎的个性化推荐模型研究\"><a href=\"#面向知乎的个性化推荐模型研究\" class=\"headerlink\" title=\"面向知乎的个性化推荐模型研究\"></a>面向知乎的个性化推荐模型研究</h3><p>《<a href=\"https://github.com/XiaoYiii/Paper/blob/master/Paper/%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6.pdf\" target=\"_blank\" rel=\"noopener\">面向知乎的个性化推荐模型研究</a>》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。</p>\n<p><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/001.png\" alt=\"001\"><br><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/002.png\" alt=\"002\"><br><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/003.png\" alt=\"003\"><br><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/004.png\" alt=\"004\"><br><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/005.png\" alt=\"005\"></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"面向知乎的个性化推荐模型研究\"><a href=\"#面向知乎的个性化推荐模型研究\" class=\"headerlink\" title=\"面向知乎的个性化推荐模型研究\"></a>面向知乎的个性化推荐模型研究</h3><p>《<a href=\"https://github.com/XiaoYiii/Paper/blob/master/Paper/%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6.pdf\" target=\"_blank\" rel=\"noopener\">面向知乎的个性化推荐模型研究</a>》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。</p>\n<p><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/001.png\" alt=\"001\"><br><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/002.png\" alt=\"002\"><br><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/003.png\" alt=\"003\"><br><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/004.png\" alt=\"004\"><br><img src=\"/2018/03/12/面向知乎的个性化推荐模型研究论文/005.png\" alt=\"005\"></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"深度神经网络之损失函数和激活函数","date":"2018-06-30T04:51:43.000Z","mathjax":true,"_content":"\n### 1.损失函数和激活函数简介\n\n通过前面[深度神经网络之前向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483903&idx=1&sn=4e3f92578399013eba9f203d35afe972&chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f&scene=38#wechat_redirect)和[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)的学习，我们能够了解到损失函数是用来评估模型的预测值与真实值之间的差异程度。另外损失函数也是神经网络中优化的目标函数，神经网络训练或者优化的过程就是最小化损失函数的过程，损失函数越小，说明模型的预测值就越接近真实值，模型的准确性也就越好。前面我们已经学习过**平方损失函数**，**对数损失函数**、**交叉熵损失函数**等不同形式的损失函数，这里也就不做太多介绍。\n\n那么在深度神经网络之中，激活函数的作用又是什么呢？首先我们来看单层感知机模型，如下图所示，感知机可以利用分割线将平面分割开来。![深度神经网络之损失函数和激活函数图片01](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片01.png)\n\n现在我们利用多个感知机进行组合，获得更强的分类能力，模型分类效果如下图所示。\n\n![神经网络之损失函数和激活函数图片02](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片02.png)\n\n但无论怎样组合，模型输出的时候都只是线性模型，如何解决非线性分类呢？好吧，上面是我们没有增加激活函数的情况。那么现在我们在每一层迭代完之后，增加一个激活函数，如下图的y=σ(a)所示，这样模型的输出便能解决非线性情况。将多个有激活函数的神经元组合起来，我们就可以得到一个相当复杂的函数。\n\n![深度神经网络之损失函数和激活函数图片03](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片03.png)\n\n引入非线性激活函数之中，模型的表达能力增强，能够有效解决非线性情况。通过不同形式的激活函数，模型也就能够学习到不同形式的分类方式，比如**平滑分类平面**，方面我们解决各种问题。\n\n![深度神经网络之损失函数和激活函数图片04](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片04.png)\n\n通过上面的介绍，我们能够了解到神经网络之中损失函数和激活函数的作用，但实际上DNN可以使用的损失函数和激活函数有不少，这时我们应该如何去做选择呢？下面我们介绍一些DNN之中常见的损失函数和激活函数。\n\n### 2.交叉熵损失函数和Sigmoid激活函数\n\n在[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)之中，我们用的是均方差损失函数和Sigmoid激活函数，首先我们看看**均方差损失函数和Sigmoid激活函数**有什么问题。如下所示，是我们已经非常熟悉的Sigmoid激活函数表达式\n$$\n\\sigma(z)=\\frac{1}{1+e^{-z}}\n$$\n其中σ(z)的图像如下图所示，从图中可以看出，当z越来越大时，函数曲线也就变得越平缓，意味着此时导数σ′(z)也越小。同样，当z越来越小时，也会出现σ′(z)也越小。仅仅当z取值为0的附近时，导数σ′(z)取值较大。\n\n![深度神经网络之损失函数和激活函数图片05](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片05.png)\n\n在[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)之中，我们了解到每次反向迭代递推时，都要乘以σ′(z)得到梯度变化值。而Sigmoid的曲线意味着在大多数时候，DNN的梯度变化值较小，则会导致W,b更新到极值的速度很慢。那么有什么办法可以改变这种情况呢？\n\n常见的方法是选用交叉熵损失函数来代替均方差损失函数，首先来看看交叉熵损失函数的形式。其中 **∙** 为向量内积，我们在[机器学习之Logistic回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483814&idx=1&sn=16a56382d24e304a95ab2a2a028993c6&chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd)中便用到类似的交叉熵损失函数形式。\n$$\nJ(W,b,a,y)=-y \\cdot lna-(1-y)\\cdot ln(1-a)\n$$\n然后应用交叉熵损失函数之后，输出层 $\\delta ^L$的梯度变化情况如下所示。\n$$\n\\delta^L=\\frac{J(W,b,a^L,y)}{\\partial z^L}=-y\\frac{1}{a^L}(1-a^L)+(1-y)\\frac{1}{1-a^L}a^L(1-a^L)\n$$\n\n$$\n=-y(1-a^L)+(1-y)a^L=a^L-y\n$$\n\n使用交叉熵损失函数与[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)中使用均方差损失函数的$\\delta^L$有什么区别呢？我们发现使用均方差损失函数时$\\delta^L=(a^L-y)\\odot {\\sigma}'(z)$，对比发现使用交叉熵损失函数没有${\\sigma}'(z)$。这样求得的$w^l,b^l$便不包含${\\sigma}'(z)$，因此避免了反向传播收敛速度慢的问题。通常情况下，使用Sigmoid激活函数时，交叉熵损失函数比均方差损失函数好用。\n\n### 3.对数似然损失函数和softmax激活函数\n\n前面我们假设模型的输出都是连续可导的值，但如果是分类问题，输出的是不同类别，那么怎么用DNN解决呢？比如我们有三个类别的分类问题，这样DNN输出层对应的便是三个神经元，每个神经元分别代表类别1、类别2、类别3，这样我们的期望输出应该是(1,0,0)、(0,1,0)、(0,0,1)，即样本真实类别对应的神经元输出应该无限接近或等于1。或者说，我们希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即DNN模型对于输入值进行各类别的输出预测，同时这若干个概率之和为1。\n\n很明显，现在普通DNN无法满足目前要求，我们需要作出相应改变，来让DNN分类模型输出层的输出值在0到1之间，同时所有输出值之和为1。为此，我们定义输出层第i个神经元的激活函数如下所示\n$$\na_i^L=\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}}\n$$\n其中$n_L$为输出层的神经元个数，或者说分类问题的类别数。很容易看出，所有的$a_i^L$都是在(0,1)之间的数字，而$\\sum_{j=1}^{n_L}e^{z_j^L}$保证所有的$a_i^L$之和为1。\n\n下面我们通过例子来描述softmax激活函数在前向传播算法中的应用，假设输出层为三个神经元，未激活的输出为(3,1,-3)，求出各自的指数表达式为(20,2.7,0.05)，归一化后为22.75，求出三个类别的概率为(0.88,0.12,0)。\n\n![深度神经网络之损失函数和激活函数图片06](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片06.png)\n\n对于用作分类的softmax激活函数，对应的损失函数一般都是用对数似然函数，函数表达式如下所示。\n$$\nJ(W,b,a^L,y)=-\\sum_ky_klna_k^l\n$$\n其中$y_k$的取值为0或1，如果某一训练样本的输出为第i类，则$y_i=1$，其余的$j\\neq i$都有$y_j=0$。由于每个样本只属于一个类别，所以对数似然函数简化为下式，其中i即为训练样本的类别序号。\n$$\nJ(W,b,a^L,y)=-lna_i^L\n$$\n可见损失函数只是和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元梯度导数为0。对于真实类别第i类，所对应的梯度计算为\n$$\n\\frac{\\partial J(W,b,a^L,y)}{\\partial W_i^L}=\\frac{\\partial J(W,b,a^L,y)}{\\partial a_i^L}\\frac{\\partial a_i^L}{\\partial z_i^L}\\frac{\\partial z_i^L}{\\partial W_i^L}\n$$\n\n$$\n=-\\frac{1}{a_i^L}\\frac{(e^{z_i^L})(\\sum_{j=1}^{n_L}e^{z_j^L})-e^{z_i^L}e^{z_i^L}}{(\\sum_{j=1}^{n_L}e^{z_j^L})^2}a_i^{L-1}=-\\frac{1}{a_i^L}\\{ \\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}} -\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}}\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}} \\}a_i^{L-1}\n$$\n\n$$\n=-\\frac{1}{a_i^L}a_i^L(1-a_i^L) a_i^{L-1}=(a_i^L-1)a_i^L\n$$\n\n$$\n\\frac{\\partial J(W,b,a^L,y)}{\\partial b_i^L}=a_i^L-1\n$$\n\n可见，梯度计算相对较简单，也不会出现前面训练速度慢的问题。同样对于上面的例子，经过softmax函数激活后的概率输出为(0.88,0.12,0)，对第二类训练样本反向传播时，反向传播梯度的偏倚向量为(0.88,0.12-1,0)。\n\n### 4.DNN其他激活函数\n\n#### 4.1 ReLU激活函数\n\nReLU(Rectified Linear Unit)表达式如下所示，也就是说，大于等于0则激活后不变，小于0则激活后为0。这个函数有什么意义呢？ReLU激活函数在梯度爆炸和梯度消失方面有重要应用。\n$$\n\\sigma(z)=max(0,z)\n$$\n那什么是梯度爆炸和梯度消失呢？可以简单理解为，反向传播算法过程中，由于我们使用的是矩阵求导的链式法则，会有一系列连乘运算。如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，最后导致梯度爆炸。同理，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，最后导致梯度消失。\n\n例如在[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)中介绍到的$\\delta^l$的计算，表达式如下所示。如果每层$\\frac{\\partial z^{l+1}}{\\partial z^{l}}$都小于1，则随着反向传播算法的进行，$\\delta ^l$会越来越小，甚至接近于0，导致梯度消失。进而导致隐含层中的W,b参数随着迭代的进行，几乎没什么改变，更谈不上收敛。对于梯度爆炸和梯度消失问题，有很多解决方法，这里不做详细介绍，后续再专门写一篇文章进行讲解。\n$$\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}} \\frac{\\partial z^{L-1}}{\\partial z^{L-2}}...\\frac{\\partial z^{l+1}}{\\partial z^{l}}\n$$\n\n#### 4.2 Tanh激活函数\n\nTanh激活函数是Sigmoid函数的变种，Tanh表达式如下所示。Tanh和Sigmoid函数的不同点是Tanh函数的输出值落在[-1,1]之间，因此Tanh输出可以进行标准化。同时Tanh自变量变化较大时，曲线变得平坦的幅度没有Sigmoid那么大，这样求梯度变化值有一些优势。当然，是使用Tanh函数还是使用Sigmoid函数需要根据具体问题而定。\n$$\ntanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\n$$\n\n$$\ntanh(z)=2Sigmoid(2z)-1\n$$\n\n#### 4.3 Softplus激活函数\n\nSoftplus激活函数是Sigmoid函数的原函数，表达式如下所示，Softplus函数和ReLU函数图像类似。\n$$\nsoftplus(z)=log(1+e^z)\n$$\n![深度神经网络之损失函数和激活函数图片07](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片07.png)\n\n#### 4.4 PReLU激活函数\n\nPReLU激活函数是ReLU的变种，特点是如果激活值小于0，激活值不是简单的变为0，而是逐渐的变化。\n\n![深度神经网络之损失函数和激活函数图片08](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片08.png)\n\n和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，下篇文章我们将进行讲解深度神经网络之中的正则化问题。\n\n参考\n\n> [知乎-神经网络激励函数的作用是什么？有没有形象的解释?-颜沁睿](https://www.zhihu.com/question/22334626/answer/21036590)\n>\n> [刘建平Pinard-深度神经网络(DNN)损失函数和激活函数的选择(BP)](https://www.cnblogs.com/pinard/p/6437495.html)\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](深度神经网络之损失函数和激活函数/推广.png)\n\n\n\n\n\n\n\n\n\n","source":"_posts/深度神经网络之损失函数和激活函数.md","raw":"---\ntitle: 深度神经网络之损失函数和激活函数\ndate: 2018-06-30 12:51:43\ntags: [深度学习,算法]\ncategories: 深度学习\nmathjax: true\n---\n\n### 1.损失函数和激活函数简介\n\n通过前面[深度神经网络之前向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483903&idx=1&sn=4e3f92578399013eba9f203d35afe972&chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f&scene=38#wechat_redirect)和[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)的学习，我们能够了解到损失函数是用来评估模型的预测值与真实值之间的差异程度。另外损失函数也是神经网络中优化的目标函数，神经网络训练或者优化的过程就是最小化损失函数的过程，损失函数越小，说明模型的预测值就越接近真实值，模型的准确性也就越好。前面我们已经学习过**平方损失函数**，**对数损失函数**、**交叉熵损失函数**等不同形式的损失函数，这里也就不做太多介绍。\n\n那么在深度神经网络之中，激活函数的作用又是什么呢？首先我们来看单层感知机模型，如下图所示，感知机可以利用分割线将平面分割开来。![深度神经网络之损失函数和激活函数图片01](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片01.png)\n\n现在我们利用多个感知机进行组合，获得更强的分类能力，模型分类效果如下图所示。\n\n![神经网络之损失函数和激活函数图片02](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片02.png)\n\n但无论怎样组合，模型输出的时候都只是线性模型，如何解决非线性分类呢？好吧，上面是我们没有增加激活函数的情况。那么现在我们在每一层迭代完之后，增加一个激活函数，如下图的y=σ(a)所示，这样模型的输出便能解决非线性情况。将多个有激活函数的神经元组合起来，我们就可以得到一个相当复杂的函数。\n\n![深度神经网络之损失函数和激活函数图片03](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片03.png)\n\n引入非线性激活函数之中，模型的表达能力增强，能够有效解决非线性情况。通过不同形式的激活函数，模型也就能够学习到不同形式的分类方式，比如**平滑分类平面**，方面我们解决各种问题。\n\n![深度神经网络之损失函数和激活函数图片04](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片04.png)\n\n通过上面的介绍，我们能够了解到神经网络之中损失函数和激活函数的作用，但实际上DNN可以使用的损失函数和激活函数有不少，这时我们应该如何去做选择呢？下面我们介绍一些DNN之中常见的损失函数和激活函数。\n\n### 2.交叉熵损失函数和Sigmoid激活函数\n\n在[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)之中，我们用的是均方差损失函数和Sigmoid激活函数，首先我们看看**均方差损失函数和Sigmoid激活函数**有什么问题。如下所示，是我们已经非常熟悉的Sigmoid激活函数表达式\n$$\n\\sigma(z)=\\frac{1}{1+e^{-z}}\n$$\n其中σ(z)的图像如下图所示，从图中可以看出，当z越来越大时，函数曲线也就变得越平缓，意味着此时导数σ′(z)也越小。同样，当z越来越小时，也会出现σ′(z)也越小。仅仅当z取值为0的附近时，导数σ′(z)取值较大。\n\n![深度神经网络之损失函数和激活函数图片05](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片05.png)\n\n在[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)之中，我们了解到每次反向迭代递推时，都要乘以σ′(z)得到梯度变化值。而Sigmoid的曲线意味着在大多数时候，DNN的梯度变化值较小，则会导致W,b更新到极值的速度很慢。那么有什么办法可以改变这种情况呢？\n\n常见的方法是选用交叉熵损失函数来代替均方差损失函数，首先来看看交叉熵损失函数的形式。其中 **∙** 为向量内积，我们在[机器学习之Logistic回归](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483814&idx=1&sn=16a56382d24e304a95ab2a2a028993c6&chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd)中便用到类似的交叉熵损失函数形式。\n$$\nJ(W,b,a,y)=-y \\cdot lna-(1-y)\\cdot ln(1-a)\n$$\n然后应用交叉熵损失函数之后，输出层 $\\delta ^L$的梯度变化情况如下所示。\n$$\n\\delta^L=\\frac{J(W,b,a^L,y)}{\\partial z^L}=-y\\frac{1}{a^L}(1-a^L)+(1-y)\\frac{1}{1-a^L}a^L(1-a^L)\n$$\n\n$$\n=-y(1-a^L)+(1-y)a^L=a^L-y\n$$\n\n使用交叉熵损失函数与[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)中使用均方差损失函数的$\\delta^L$有什么区别呢？我们发现使用均方差损失函数时$\\delta^L=(a^L-y)\\odot {\\sigma}'(z)$，对比发现使用交叉熵损失函数没有${\\sigma}'(z)$。这样求得的$w^l,b^l$便不包含${\\sigma}'(z)$，因此避免了反向传播收敛速度慢的问题。通常情况下，使用Sigmoid激活函数时，交叉熵损失函数比均方差损失函数好用。\n\n### 3.对数似然损失函数和softmax激活函数\n\n前面我们假设模型的输出都是连续可导的值，但如果是分类问题，输出的是不同类别，那么怎么用DNN解决呢？比如我们有三个类别的分类问题，这样DNN输出层对应的便是三个神经元，每个神经元分别代表类别1、类别2、类别3，这样我们的期望输出应该是(1,0,0)、(0,1,0)、(0,0,1)，即样本真实类别对应的神经元输出应该无限接近或等于1。或者说，我们希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即DNN模型对于输入值进行各类别的输出预测，同时这若干个概率之和为1。\n\n很明显，现在普通DNN无法满足目前要求，我们需要作出相应改变，来让DNN分类模型输出层的输出值在0到1之间，同时所有输出值之和为1。为此，我们定义输出层第i个神经元的激活函数如下所示\n$$\na_i^L=\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}}\n$$\n其中$n_L$为输出层的神经元个数，或者说分类问题的类别数。很容易看出，所有的$a_i^L$都是在(0,1)之间的数字，而$\\sum_{j=1}^{n_L}e^{z_j^L}$保证所有的$a_i^L$之和为1。\n\n下面我们通过例子来描述softmax激活函数在前向传播算法中的应用，假设输出层为三个神经元，未激活的输出为(3,1,-3)，求出各自的指数表达式为(20,2.7,0.05)，归一化后为22.75，求出三个类别的概率为(0.88,0.12,0)。\n\n![深度神经网络之损失函数和激活函数图片06](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片06.png)\n\n对于用作分类的softmax激活函数，对应的损失函数一般都是用对数似然函数，函数表达式如下所示。\n$$\nJ(W,b,a^L,y)=-\\sum_ky_klna_k^l\n$$\n其中$y_k$的取值为0或1，如果某一训练样本的输出为第i类，则$y_i=1$，其余的$j\\neq i$都有$y_j=0$。由于每个样本只属于一个类别，所以对数似然函数简化为下式，其中i即为训练样本的类别序号。\n$$\nJ(W,b,a^L,y)=-lna_i^L\n$$\n可见损失函数只是和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元梯度导数为0。对于真实类别第i类，所对应的梯度计算为\n$$\n\\frac{\\partial J(W,b,a^L,y)}{\\partial W_i^L}=\\frac{\\partial J(W,b,a^L,y)}{\\partial a_i^L}\\frac{\\partial a_i^L}{\\partial z_i^L}\\frac{\\partial z_i^L}{\\partial W_i^L}\n$$\n\n$$\n=-\\frac{1}{a_i^L}\\frac{(e^{z_i^L})(\\sum_{j=1}^{n_L}e^{z_j^L})-e^{z_i^L}e^{z_i^L}}{(\\sum_{j=1}^{n_L}e^{z_j^L})^2}a_i^{L-1}=-\\frac{1}{a_i^L}\\{ \\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}} -\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}}\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}} \\}a_i^{L-1}\n$$\n\n$$\n=-\\frac{1}{a_i^L}a_i^L(1-a_i^L) a_i^{L-1}=(a_i^L-1)a_i^L\n$$\n\n$$\n\\frac{\\partial J(W,b,a^L,y)}{\\partial b_i^L}=a_i^L-1\n$$\n\n可见，梯度计算相对较简单，也不会出现前面训练速度慢的问题。同样对于上面的例子，经过softmax函数激活后的概率输出为(0.88,0.12,0)，对第二类训练样本反向传播时，反向传播梯度的偏倚向量为(0.88,0.12-1,0)。\n\n### 4.DNN其他激活函数\n\n#### 4.1 ReLU激活函数\n\nReLU(Rectified Linear Unit)表达式如下所示，也就是说，大于等于0则激活后不变，小于0则激活后为0。这个函数有什么意义呢？ReLU激活函数在梯度爆炸和梯度消失方面有重要应用。\n$$\n\\sigma(z)=max(0,z)\n$$\n那什么是梯度爆炸和梯度消失呢？可以简单理解为，反向传播算法过程中，由于我们使用的是矩阵求导的链式法则，会有一系列连乘运算。如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，最后导致梯度爆炸。同理，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，最后导致梯度消失。\n\n例如在[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&scene=38#wechat_redirect)中介绍到的$\\delta^l$的计算，表达式如下所示。如果每层$\\frac{\\partial z^{l+1}}{\\partial z^{l}}$都小于1，则随着反向传播算法的进行，$\\delta ^l$会越来越小，甚至接近于0，导致梯度消失。进而导致隐含层中的W,b参数随着迭代的进行，几乎没什么改变，更谈不上收敛。对于梯度爆炸和梯度消失问题，有很多解决方法，这里不做详细介绍，后续再专门写一篇文章进行讲解。\n$$\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}} \\frac{\\partial z^{L-1}}{\\partial z^{L-2}}...\\frac{\\partial z^{l+1}}{\\partial z^{l}}\n$$\n\n#### 4.2 Tanh激活函数\n\nTanh激活函数是Sigmoid函数的变种，Tanh表达式如下所示。Tanh和Sigmoid函数的不同点是Tanh函数的输出值落在[-1,1]之间，因此Tanh输出可以进行标准化。同时Tanh自变量变化较大时，曲线变得平坦的幅度没有Sigmoid那么大，这样求梯度变化值有一些优势。当然，是使用Tanh函数还是使用Sigmoid函数需要根据具体问题而定。\n$$\ntanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\n$$\n\n$$\ntanh(z)=2Sigmoid(2z)-1\n$$\n\n#### 4.3 Softplus激活函数\n\nSoftplus激活函数是Sigmoid函数的原函数，表达式如下所示，Softplus函数和ReLU函数图像类似。\n$$\nsoftplus(z)=log(1+e^z)\n$$\n![深度神经网络之损失函数和激活函数图片07](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片07.png)\n\n#### 4.4 PReLU激活函数\n\nPReLU激活函数是ReLU的变种，特点是如果激活值小于0，激活值不是简单的变为0，而是逐渐的变化。\n\n![深度神经网络之损失函数和激活函数图片08](深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片08.png)\n\n和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，下篇文章我们将进行讲解深度神经网络之中的正则化问题。\n\n参考\n\n> [知乎-神经网络激励函数的作用是什么？有没有形象的解释?-颜沁睿](https://www.zhihu.com/question/22334626/answer/21036590)\n>\n> [刘建平Pinard-深度神经网络(DNN)损失函数和激活函数的选择(BP)](https://www.cnblogs.com/pinard/p/6437495.html)\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](深度神经网络之损失函数和激活函数/推广.png)\n\n\n\n\n\n\n\n\n\n","slug":"深度神经网络之损失函数和激活函数","published":1,"updated":"2018-06-30T10:45:53.662Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjktv5d51002cjiz5lfvsw44c","content":"<h3 id=\"1-损失函数和激活函数简介\"><a href=\"#1-损失函数和激活函数简介\" class=\"headerlink\" title=\"1.损失函数和激活函数简介\"></a>1.损失函数和激活函数简介</h3><p>通过前面<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483903&amp;idx=1&amp;sn=4e3f92578399013eba9f203d35afe972&amp;chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之前向传播算法</a>和<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>的学习，我们能够了解到损失函数是用来评估模型的预测值与真实值之间的差异程度。另外损失函数也是神经网络中优化的目标函数，神经网络训练或者优化的过程就是最小化损失函数的过程，损失函数越小，说明模型的预测值就越接近真实值，模型的准确性也就越好。前面我们已经学习过<strong>平方损失函数</strong>，<strong>对数损失函数</strong>、<strong>交叉熵损失函数</strong>等不同形式的损失函数，这里也就不做太多介绍。</p>\n<p>那么在深度神经网络之中，激活函数的作用又是什么呢？首先我们来看单层感知机模型，如下图所示，感知机可以利用分割线将平面分割开来。<img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片01.png\" alt=\"深度神经网络之损失函数和激活函数图片01\"></p>\n<p>现在我们利用多个感知机进行组合，获得更强的分类能力，模型分类效果如下图所示。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片02.png\" alt=\"神经网络之损失函数和激活函数图片02\"></p>\n<p>但无论怎样组合，模型输出的时候都只是线性模型，如何解决非线性分类呢？好吧，上面是我们没有增加激活函数的情况。那么现在我们在每一层迭代完之后，增加一个激活函数，如下图的y=σ(a)所示，这样模型的输出便能解决非线性情况。将多个有激活函数的神经元组合起来，我们就可以得到一个相当复杂的函数。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片03.png\" alt=\"深度神经网络之损失函数和激活函数图片03\"></p>\n<p>引入非线性激活函数之中，模型的表达能力增强，能够有效解决非线性情况。通过不同形式的激活函数，模型也就能够学习到不同形式的分类方式，比如<strong>平滑分类平面</strong>，方面我们解决各种问题。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片04.png\" alt=\"深度神经网络之损失函数和激活函数图片04\"></p>\n<p>通过上面的介绍，我们能够了解到神经网络之中损失函数和激活函数的作用，但实际上DNN可以使用的损失函数和激活函数有不少，这时我们应该如何去做选择呢？下面我们介绍一些DNN之中常见的损失函数和激活函数。</p>\n<h3 id=\"2-交叉熵损失函数和Sigmoid激活函数\"><a href=\"#2-交叉熵损失函数和Sigmoid激活函数\" class=\"headerlink\" title=\"2.交叉熵损失函数和Sigmoid激活函数\"></a>2.交叉熵损失函数和Sigmoid激活函数</h3><p>在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>之中，我们用的是均方差损失函数和Sigmoid激活函数，首先我们看看<strong>均方差损失函数和Sigmoid激活函数</strong>有什么问题。如下所示，是我们已经非常熟悉的Sigmoid激活函数表达式</p>\n<script type=\"math/tex; mode=display\">\n\\sigma(z)=\\frac{1}{1+e^{-z}}</script><p>其中σ(z)的图像如下图所示，从图中可以看出，当z越来越大时，函数曲线也就变得越平缓，意味着此时导数σ′(z)也越小。同样，当z越来越小时，也会出现σ′(z)也越小。仅仅当z取值为0的附近时，导数σ′(z)取值较大。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片05.png\" alt=\"深度神经网络之损失函数和激活函数图片05\"></p>\n<p>在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>之中，我们了解到每次反向迭代递推时，都要乘以σ′(z)得到梯度变化值。而Sigmoid的曲线意味着在大多数时候，DNN的梯度变化值较小，则会导致W,b更新到极值的速度很慢。那么有什么办法可以改变这种情况呢？</p>\n<p>常见的方法是选用交叉熵损失函数来代替均方差损失函数，首先来看看交叉熵损失函数的形式。其中 <strong>∙</strong> 为向量内积，我们在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483814&amp;idx=1&amp;sn=16a56382d24e304a95ab2a2a028993c6&amp;chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd\" target=\"_blank\" rel=\"noopener\">机器学习之Logistic回归</a>中便用到类似的交叉熵损失函数形式。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b,a,y)=-y \\cdot lna-(1-y)\\cdot ln(1-a)</script><p>然后应用交叉熵损失函数之后，输出层 $\\delta ^L$的梯度变化情况如下所示。</p>\n<script type=\"math/tex; mode=display\">\n\\delta^L=\\frac{J(W,b,a^L,y)}{\\partial z^L}=-y\\frac{1}{a^L}(1-a^L)+(1-y)\\frac{1}{1-a^L}a^L(1-a^L)</script><script type=\"math/tex; mode=display\">\n=-y(1-a^L)+(1-y)a^L=a^L-y</script><p>使用交叉熵损失函数与<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>中使用均方差损失函数的$\\delta^L$有什么区别呢？我们发现使用均方差损失函数时$\\delta^L=(a^L-y)\\odot {\\sigma}’(z)$，对比发现使用交叉熵损失函数没有${\\sigma}’(z)$。这样求得的$w^l,b^l$便不包含${\\sigma}’(z)$，因此避免了反向传播收敛速度慢的问题。通常情况下，使用Sigmoid激活函数时，交叉熵损失函数比均方差损失函数好用。</p>\n<h3 id=\"3-对数似然损失函数和softmax激活函数\"><a href=\"#3-对数似然损失函数和softmax激活函数\" class=\"headerlink\" title=\"3.对数似然损失函数和softmax激活函数\"></a>3.对数似然损失函数和softmax激活函数</h3><p>前面我们假设模型的输出都是连续可导的值，但如果是分类问题，输出的是不同类别，那么怎么用DNN解决呢？比如我们有三个类别的分类问题，这样DNN输出层对应的便是三个神经元，每个神经元分别代表类别1、类别2、类别3，这样我们的期望输出应该是(1,0,0)、(0,1,0)、(0,0,1)，即样本真实类别对应的神经元输出应该无限接近或等于1。或者说，我们希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即DNN模型对于输入值进行各类别的输出预测，同时这若干个概率之和为1。</p>\n<p>很明显，现在普通DNN无法满足目前要求，我们需要作出相应改变，来让DNN分类模型输出层的输出值在0到1之间，同时所有输出值之和为1。为此，我们定义输出层第i个神经元的激活函数如下所示</p>\n<script type=\"math/tex; mode=display\">\na_i^L=\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}}</script><p>其中$n_L$为输出层的神经元个数，或者说分类问题的类别数。很容易看出，所有的$a_i^L$都是在(0,1)之间的数字，而$\\sum_{j=1}^{n_L}e^{z_j^L}$保证所有的$a_i^L$之和为1。</p>\n<p>下面我们通过例子来描述softmax激活函数在前向传播算法中的应用，假设输出层为三个神经元，未激活的输出为(3,1,-3)，求出各自的指数表达式为(20,2.7,0.05)，归一化后为22.75，求出三个类别的概率为(0.88,0.12,0)。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片06.png\" alt=\"深度神经网络之损失函数和激活函数图片06\"></p>\n<p>对于用作分类的softmax激活函数，对应的损失函数一般都是用对数似然函数，函数表达式如下所示。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b,a^L,y)=-\\sum_ky_klna_k^l</script><p>其中$y_k$的取值为0或1，如果某一训练样本的输出为第i类，则$y_i=1$，其余的$j\\neq i$都有$y_j=0$。由于每个样本只属于一个类别，所以对数似然函数简化为下式，其中i即为训练样本的类别序号。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b,a^L,y)=-lna_i^L</script><p>可见损失函数只是和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元梯度导数为0。对于真实类别第i类，所对应的梯度计算为</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,a^L,y)}{\\partial W_i^L}=\\frac{\\partial J(W,b,a^L,y)}{\\partial a_i^L}\\frac{\\partial a_i^L}{\\partial z_i^L}\\frac{\\partial z_i^L}{\\partial W_i^L}</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{a_i^L}\\frac{(e^{z_i^L})(\\sum_{j=1}^{n_L}e^{z_j^L})-e^{z_i^L}e^{z_i^L}}{(\\sum_{j=1}^{n_L}e^{z_j^L})^2}a_i^{L-1}=-\\frac{1}{a_i^L}\\{ \\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}} -\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}}\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}} \\}a_i^{L-1}</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{a_i^L}a_i^L(1-a_i^L) a_i^{L-1}=(a_i^L-1)a_i^L</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,a^L,y)}{\\partial b_i^L}=a_i^L-1</script><p>可见，梯度计算相对较简单，也不会出现前面训练速度慢的问题。同样对于上面的例子，经过softmax函数激活后的概率输出为(0.88,0.12,0)，对第二类训练样本反向传播时，反向传播梯度的偏倚向量为(0.88,0.12-1,0)。</p>\n<h3 id=\"4-DNN其他激活函数\"><a href=\"#4-DNN其他激活函数\" class=\"headerlink\" title=\"4.DNN其他激活函数\"></a>4.DNN其他激活函数</h3><h4 id=\"4-1-ReLU激活函数\"><a href=\"#4-1-ReLU激活函数\" class=\"headerlink\" title=\"4.1 ReLU激活函数\"></a>4.1 ReLU激活函数</h4><p>ReLU(Rectified Linear Unit)表达式如下所示，也就是说，大于等于0则激活后不变，小于0则激活后为0。这个函数有什么意义呢？ReLU激活函数在梯度爆炸和梯度消失方面有重要应用。</p>\n<script type=\"math/tex; mode=display\">\n\\sigma(z)=max(0,z)</script><p>那什么是梯度爆炸和梯度消失呢？可以简单理解为，反向传播算法过程中，由于我们使用的是矩阵求导的链式法则，会有一系列连乘运算。如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，最后导致梯度爆炸。同理，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，最后导致梯度消失。</p>\n<p>例如在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>中介绍到的$\\delta^l$的计算，表达式如下所示。如果每层$\\frac{\\partial z^{l+1}}{\\partial z^{l}}$都小于1，则随着反向传播算法的进行，$\\delta ^l$会越来越小，甚至接近于0，导致梯度消失。进而导致隐含层中的W,b参数随着迭代的进行，几乎没什么改变，更谈不上收敛。对于梯度爆炸和梯度消失问题，有很多解决方法，这里不做详细介绍，后续再专门写一篇文章进行讲解。</p>\n<script type=\"math/tex; mode=display\">\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}} \\frac{\\partial z^{L-1}}{\\partial z^{L-2}}...\\frac{\\partial z^{l+1}}{\\partial z^{l}}</script><h4 id=\"4-2-Tanh激活函数\"><a href=\"#4-2-Tanh激活函数\" class=\"headerlink\" title=\"4.2 Tanh激活函数\"></a>4.2 Tanh激活函数</h4><p>Tanh激活函数是Sigmoid函数的变种，Tanh表达式如下所示。Tanh和Sigmoid函数的不同点是Tanh函数的输出值落在[-1,1]之间，因此Tanh输出可以进行标准化。同时Tanh自变量变化较大时，曲线变得平坦的幅度没有Sigmoid那么大，这样求梯度变化值有一些优势。当然，是使用Tanh函数还是使用Sigmoid函数需要根据具体问题而定。</p>\n<script type=\"math/tex; mode=display\">\ntanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}</script><script type=\"math/tex; mode=display\">\ntanh(z)=2Sigmoid(2z)-1</script><h4 id=\"4-3-Softplus激活函数\"><a href=\"#4-3-Softplus激活函数\" class=\"headerlink\" title=\"4.3 Softplus激活函数\"></a>4.3 Softplus激活函数</h4><p>Softplus激活函数是Sigmoid函数的原函数，表达式如下所示，Softplus函数和ReLU函数图像类似。</p>\n<script type=\"math/tex; mode=display\">\nsoftplus(z)=log(1+e^z)</script><p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片07.png\" alt=\"深度神经网络之损失函数和激活函数图片07\"></p>\n<h4 id=\"4-4-PReLU激活函数\"><a href=\"#4-4-PReLU激活函数\" class=\"headerlink\" title=\"4.4 PReLU激活函数\"></a>4.4 PReLU激活函数</h4><p>PReLU激活函数是ReLU的变种，特点是如果激活值小于0，激活值不是简单的变为0，而是逐渐的变化。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片08.png\" alt=\"深度神经网络之损失函数和激活函数图片08\"></p>\n<p>和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，下篇文章我们将进行讲解深度神经网络之中的正则化问题。</p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/question/22334626/answer/21036590\" target=\"_blank\" rel=\"noopener\">知乎-神经网络激励函数的作用是什么？有没有形象的解释?-颜沁睿</a></p>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6437495.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard-深度神经网络(DNN)损失函数和激活函数的选择(BP)</a></p>\n</blockquote>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-损失函数和激活函数简介\"><a href=\"#1-损失函数和激活函数简介\" class=\"headerlink\" title=\"1.损失函数和激活函数简介\"></a>1.损失函数和激活函数简介</h3><p>通过前面<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483903&amp;idx=1&amp;sn=4e3f92578399013eba9f203d35afe972&amp;chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之前向传播算法</a>和<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>的学习，我们能够了解到损失函数是用来评估模型的预测值与真实值之间的差异程度。另外损失函数也是神经网络中优化的目标函数，神经网络训练或者优化的过程就是最小化损失函数的过程，损失函数越小，说明模型的预测值就越接近真实值，模型的准确性也就越好。前面我们已经学习过<strong>平方损失函数</strong>，<strong>对数损失函数</strong>、<strong>交叉熵损失函数</strong>等不同形式的损失函数，这里也就不做太多介绍。</p>\n<p>那么在深度神经网络之中，激活函数的作用又是什么呢？首先我们来看单层感知机模型，如下图所示，感知机可以利用分割线将平面分割开来。<img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片01.png\" alt=\"深度神经网络之损失函数和激活函数图片01\"></p>\n<p>现在我们利用多个感知机进行组合，获得更强的分类能力，模型分类效果如下图所示。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片02.png\" alt=\"神经网络之损失函数和激活函数图片02\"></p>\n<p>但无论怎样组合，模型输出的时候都只是线性模型，如何解决非线性分类呢？好吧，上面是我们没有增加激活函数的情况。那么现在我们在每一层迭代完之后，增加一个激活函数，如下图的y=σ(a)所示，这样模型的输出便能解决非线性情况。将多个有激活函数的神经元组合起来，我们就可以得到一个相当复杂的函数。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片03.png\" alt=\"深度神经网络之损失函数和激活函数图片03\"></p>\n<p>引入非线性激活函数之中，模型的表达能力增强，能够有效解决非线性情况。通过不同形式的激活函数，模型也就能够学习到不同形式的分类方式，比如<strong>平滑分类平面</strong>，方面我们解决各种问题。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片04.png\" alt=\"深度神经网络之损失函数和激活函数图片04\"></p>\n<p>通过上面的介绍，我们能够了解到神经网络之中损失函数和激活函数的作用，但实际上DNN可以使用的损失函数和激活函数有不少，这时我们应该如何去做选择呢？下面我们介绍一些DNN之中常见的损失函数和激活函数。</p>\n<h3 id=\"2-交叉熵损失函数和Sigmoid激活函数\"><a href=\"#2-交叉熵损失函数和Sigmoid激活函数\" class=\"headerlink\" title=\"2.交叉熵损失函数和Sigmoid激活函数\"></a>2.交叉熵损失函数和Sigmoid激活函数</h3><p>在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>之中，我们用的是均方差损失函数和Sigmoid激活函数，首先我们看看<strong>均方差损失函数和Sigmoid激活函数</strong>有什么问题。如下所示，是我们已经非常熟悉的Sigmoid激活函数表达式</p>\n<script type=\"math/tex; mode=display\">\n\\sigma(z)=\\frac{1}{1+e^{-z}}</script><p>其中σ(z)的图像如下图所示，从图中可以看出，当z越来越大时，函数曲线也就变得越平缓，意味着此时导数σ′(z)也越小。同样，当z越来越小时，也会出现σ′(z)也越小。仅仅当z取值为0的附近时，导数σ′(z)取值较大。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片05.png\" alt=\"深度神经网络之损失函数和激活函数图片05\"></p>\n<p>在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>之中，我们了解到每次反向迭代递推时，都要乘以σ′(z)得到梯度变化值。而Sigmoid的曲线意味着在大多数时候，DNN的梯度变化值较小，则会导致W,b更新到极值的速度很慢。那么有什么办法可以改变这种情况呢？</p>\n<p>常见的方法是选用交叉熵损失函数来代替均方差损失函数，首先来看看交叉熵损失函数的形式。其中 <strong>∙</strong> 为向量内积，我们在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483814&amp;idx=1&amp;sn=16a56382d24e304a95ab2a2a028993c6&amp;chksm=fcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb#rd\" target=\"_blank\" rel=\"noopener\">机器学习之Logistic回归</a>中便用到类似的交叉熵损失函数形式。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b,a,y)=-y \\cdot lna-(1-y)\\cdot ln(1-a)</script><p>然后应用交叉熵损失函数之后，输出层 $\\delta ^L$的梯度变化情况如下所示。</p>\n<script type=\"math/tex; mode=display\">\n\\delta^L=\\frac{J(W,b,a^L,y)}{\\partial z^L}=-y\\frac{1}{a^L}(1-a^L)+(1-y)\\frac{1}{1-a^L}a^L(1-a^L)</script><script type=\"math/tex; mode=display\">\n=-y(1-a^L)+(1-y)a^L=a^L-y</script><p>使用交叉熵损失函数与<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>中使用均方差损失函数的$\\delta^L$有什么区别呢？我们发现使用均方差损失函数时$\\delta^L=(a^L-y)\\odot {\\sigma}’(z)$，对比发现使用交叉熵损失函数没有${\\sigma}’(z)$。这样求得的$w^l,b^l$便不包含${\\sigma}’(z)$，因此避免了反向传播收敛速度慢的问题。通常情况下，使用Sigmoid激活函数时，交叉熵损失函数比均方差损失函数好用。</p>\n<h3 id=\"3-对数似然损失函数和softmax激活函数\"><a href=\"#3-对数似然损失函数和softmax激活函数\" class=\"headerlink\" title=\"3.对数似然损失函数和softmax激活函数\"></a>3.对数似然损失函数和softmax激活函数</h3><p>前面我们假设模型的输出都是连续可导的值，但如果是分类问题，输出的是不同类别，那么怎么用DNN解决呢？比如我们有三个类别的分类问题，这样DNN输出层对应的便是三个神经元，每个神经元分别代表类别1、类别2、类别3，这样我们的期望输出应该是(1,0,0)、(0,1,0)、(0,0,1)，即样本真实类别对应的神经元输出应该无限接近或等于1。或者说，我们希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即DNN模型对于输入值进行各类别的输出预测，同时这若干个概率之和为1。</p>\n<p>很明显，现在普通DNN无法满足目前要求，我们需要作出相应改变，来让DNN分类模型输出层的输出值在0到1之间，同时所有输出值之和为1。为此，我们定义输出层第i个神经元的激活函数如下所示</p>\n<script type=\"math/tex; mode=display\">\na_i^L=\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}}</script><p>其中$n_L$为输出层的神经元个数，或者说分类问题的类别数。很容易看出，所有的$a_i^L$都是在(0,1)之间的数字，而$\\sum_{j=1}^{n_L}e^{z_j^L}$保证所有的$a_i^L$之和为1。</p>\n<p>下面我们通过例子来描述softmax激活函数在前向传播算法中的应用，假设输出层为三个神经元，未激活的输出为(3,1,-3)，求出各自的指数表达式为(20,2.7,0.05)，归一化后为22.75，求出三个类别的概率为(0.88,0.12,0)。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片06.png\" alt=\"深度神经网络之损失函数和激活函数图片06\"></p>\n<p>对于用作分类的softmax激活函数，对应的损失函数一般都是用对数似然函数，函数表达式如下所示。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b,a^L,y)=-\\sum_ky_klna_k^l</script><p>其中$y_k$的取值为0或1，如果某一训练样本的输出为第i类，则$y_i=1$，其余的$j\\neq i$都有$y_j=0$。由于每个样本只属于一个类别，所以对数似然函数简化为下式，其中i即为训练样本的类别序号。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b,a^L,y)=-lna_i^L</script><p>可见损失函数只是和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元梯度导数为0。对于真实类别第i类，所对应的梯度计算为</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,a^L,y)}{\\partial W_i^L}=\\frac{\\partial J(W,b,a^L,y)}{\\partial a_i^L}\\frac{\\partial a_i^L}{\\partial z_i^L}\\frac{\\partial z_i^L}{\\partial W_i^L}</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{a_i^L}\\frac{(e^{z_i^L})(\\sum_{j=1}^{n_L}e^{z_j^L})-e^{z_i^L}e^{z_i^L}}{(\\sum_{j=1}^{n_L}e^{z_j^L})^2}a_i^{L-1}=-\\frac{1}{a_i^L}\\{ \\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}} -\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}}\\frac{e^{z_i^L}}{\\sum_{j=1}^{n_L}e^{z_j^L}} \\}a_i^{L-1}</script><script type=\"math/tex; mode=display\">\n=-\\frac{1}{a_i^L}a_i^L(1-a_i^L) a_i^{L-1}=(a_i^L-1)a_i^L</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J(W,b,a^L,y)}{\\partial b_i^L}=a_i^L-1</script><p>可见，梯度计算相对较简单，也不会出现前面训练速度慢的问题。同样对于上面的例子，经过softmax函数激活后的概率输出为(0.88,0.12,0)，对第二类训练样本反向传播时，反向传播梯度的偏倚向量为(0.88,0.12-1,0)。</p>\n<h3 id=\"4-DNN其他激活函数\"><a href=\"#4-DNN其他激活函数\" class=\"headerlink\" title=\"4.DNN其他激活函数\"></a>4.DNN其他激活函数</h3><h4 id=\"4-1-ReLU激活函数\"><a href=\"#4-1-ReLU激活函数\" class=\"headerlink\" title=\"4.1 ReLU激活函数\"></a>4.1 ReLU激活函数</h4><p>ReLU(Rectified Linear Unit)表达式如下所示，也就是说，大于等于0则激活后不变，小于0则激活后为0。这个函数有什么意义呢？ReLU激活函数在梯度爆炸和梯度消失方面有重要应用。</p>\n<script type=\"math/tex; mode=display\">\n\\sigma(z)=max(0,z)</script><p>那什么是梯度爆炸和梯度消失呢？可以简单理解为，反向传播算法过程中，由于我们使用的是矩阵求导的链式法则，会有一系列连乘运算。如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，最后导致梯度爆炸。同理，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，最后导致梯度消失。</p>\n<p>例如在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a&amp;scene=38#wechat_redirect\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>中介绍到的$\\delta^l$的计算，表达式如下所示。如果每层$\\frac{\\partial z^{l+1}}{\\partial z^{l}}$都小于1，则随着反向传播算法的进行，$\\delta ^l$会越来越小，甚至接近于0，导致梯度消失。进而导致隐含层中的W,b参数随着迭代的进行，几乎没什么改变，更谈不上收敛。对于梯度爆炸和梯度消失问题，有很多解决方法，这里不做详细介绍，后续再专门写一篇文章进行讲解。</p>\n<script type=\"math/tex; mode=display\">\n\\delta^l=\\frac{\\partial J(W,b,x,y)}{\\partial z^l}=\\frac{\\partial J(W,b,x,y)}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}} \\frac{\\partial z^{L-1}}{\\partial z^{L-2}}...\\frac{\\partial z^{l+1}}{\\partial z^{l}}</script><h4 id=\"4-2-Tanh激活函数\"><a href=\"#4-2-Tanh激活函数\" class=\"headerlink\" title=\"4.2 Tanh激活函数\"></a>4.2 Tanh激活函数</h4><p>Tanh激活函数是Sigmoid函数的变种，Tanh表达式如下所示。Tanh和Sigmoid函数的不同点是Tanh函数的输出值落在[-1,1]之间，因此Tanh输出可以进行标准化。同时Tanh自变量变化较大时，曲线变得平坦的幅度没有Sigmoid那么大，这样求梯度变化值有一些优势。当然，是使用Tanh函数还是使用Sigmoid函数需要根据具体问题而定。</p>\n<script type=\"math/tex; mode=display\">\ntanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}</script><script type=\"math/tex; mode=display\">\ntanh(z)=2Sigmoid(2z)-1</script><h4 id=\"4-3-Softplus激活函数\"><a href=\"#4-3-Softplus激活函数\" class=\"headerlink\" title=\"4.3 Softplus激活函数\"></a>4.3 Softplus激活函数</h4><p>Softplus激活函数是Sigmoid函数的原函数，表达式如下所示，Softplus函数和ReLU函数图像类似。</p>\n<script type=\"math/tex; mode=display\">\nsoftplus(z)=log(1+e^z)</script><p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片07.png\" alt=\"深度神经网络之损失函数和激活函数图片07\"></p>\n<h4 id=\"4-4-PReLU激活函数\"><a href=\"#4-4-PReLU激活函数\" class=\"headerlink\" title=\"4.4 PReLU激活函数\"></a>4.4 PReLU激活函数</h4><p>PReLU激活函数是ReLU的变种，特点是如果激活值小于0，激活值不是简单的变为0，而是逐渐的变化。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片08.png\" alt=\"深度神经网络之损失函数和激活函数图片08\"></p>\n<p>和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，下篇文章我们将进行讲解深度神经网络之中的正则化问题。</p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/question/22334626/answer/21036590\" target=\"_blank\" rel=\"noopener\">知乎-神经网络激励函数的作用是什么？有没有形象的解释?-颜沁睿</a></p>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6437495.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard-深度神经网络(DNN)损失函数和激活函数的选择(BP)</a></p>\n</blockquote>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/06/30/深度神经网络之损失函数和激活函数/推广.png\" alt=\"推广\"></p>\n"},{"title":"深度神经网络之正则化","date":"2018-07-03T01:58:46.000Z","mathjax":true,"_content":"\n### 1.正则化\n\n之前介绍的[文章](https://mp.weixin.qq.com/s/UeEL1rOdjMpaKqTFpclS3A)之中，我们已多次接触到正则化方法，但没有详细的解释为什么要正则化，什么是正则化，以及L1正则化和L2正则化的区别。本次文章之中，我们将详解机器学习中正则化的概念和深度神经网络中的正则化方法。\n\n#### 1.1 为什么要正则化？\n\n讲到为什么需要正则化，就需要了解什么是过拟合问题。以下面图片为例，我们能够看到有两个类别，其中以X代表男生，O代表女生。\n\n![深度神经网络之正则化图片01](深度神经网络之正则化/深度神经网络之正则化图片01.png)\n\n我们想要通过学习来得到分类曲线，其中分类曲线能够有效区分男生和女生，现在来分析下上面的三种分类结果。\n\n+ **欠拟合：**图1分类明显欠缺，有些男生被分为女生，有些女生被分为男生。\n+ **正拟合：**图2虽然有两个男生被分类为女生，但能够理解，毕竟我们人类自己也有分类错误的情况，比如通过化妆，女装等方法。\n+ **过拟合：**图3虽然能够全部分类正确，但结果全部正确就一定好吗？不一定，我们能够看到分类曲线明显过于复杂，模型学习的时候学习了过多的参数项，但其中某些参数项是无用的特征，比如**眼睛大小**。当我们进行识别测试集数据时，就需要提供更多的特征，如果测试集包含海量的数据，模型的时间复杂度可想而知。\n\n#### 1.2 什么是正则化？\n\n既然我们已经知道什么是过拟合，那么怎么解决过拟合问题呢？上面有介绍到，模型出现过拟合，是在模型**特征**上过于复杂。而特征又包含在我们的目标函数f(x)之中，那么只能从目标函数f(x)中寻找解决问题的方法。假设目标函数f(x)和损失函数J0为\n$$\nf(X)=w_0x_0+w_1x_1+w_2x_3+...+w_nx_n\n$$\n\n$$\nJ_0=||f(x)-y||_2^2=||XW-y||^2_2\n$$\n\n对于上式，如果特征项$x_0,x_1,x_2,...,x_n$越多的话，自然$w_0,w_1,w_2,...,w_n$也就越多。想要减少特征的数量，自然减小**N**也就好了。而N影响的是$X=(x_0,x_1,x_2,...,x_n)$和$W=(w_0,w_1,w_2,...,w_n)$两项，那么是从**X**解决问题还是从**W**解决问题呢？\n\n如果从**X**入手解决问题，但训练过程中我们不知道下一个样本X是什么，会怎样的影响目标函数，所以此路不通。那么**W**如何呢？我们知道W系数是训练过程中通过学习历史数据得到的，和历史数据有关，所以应该可以。现在再回到我们原来的问题，希望减少N的数目，而让N最小化，其实就是让X向量或W向量中项的个数最小化，既然X不行，那么我们可以尝试让W向量中项的个数最小化。如何求解才能让W向量中项的个数最小，我们先简单介绍下0、1、2范数的概念。\n\n+ **L0范数：**向量中非零元素的个数，记为$||W||_0$。\n+ **L1范数：**绝对值之和，记为$||W||_1$。\n+ **L2范数：**通常意义上的模，记为$||W||_2$。\n\n所以为了防止过拟合，我们需要让$||W||_0$最小，同时让损失函数$J_0$最小，为了满足两项最小化，可以让$J_0$和$||W||_0$之和最小化。但因为$||W||_0$比较难求(NP难问题)，我们进而可以转化为求$||W||_1$。$||W||_1$是$||W||_0$的最优凸近似，都可以实现**稀疏**，比较容易求解，这也是为什么可以选用$||W||_1$的原因。最后损失函数后面添加的额外项$||W||_1$，也就是我们称作的L1正则化，$\\alpha$含义在后面进行讲解。\n$$\nJ=J_0+L1=||XW-y||^2_2+\\alpha||W||_1\n$$\n说完L0范数和L1范数，就不得不提L2范数。L2范数是指先求向量各元素的平方和，然后再进行求平方根，也就是通常意义上的模。同样，对于正则化问题，我们的目标是让W向量中的每个元素都很小，也就是让L2范数最小。L1范数和L2范数的不同点在于，L1范数会让其中某些元素等于0，而L2范数只是让其中元素接近0，这里有很大不同，我们在后面会进行详细讲解。最后损失函数后面添加的额外项||W||2，也就是我们称作的L2正则化。\n$$\nJ=J_0+L2=||XW-y||^2_2+\\alpha ||W||_2\n$$\n\n#### 1.3 L1正则化和L2正则化\n\n**L1正则化**可以产生稀疏值矩阵，即产生一个稀疏模型，可以用于特征选择和解决过拟合。那什么是稀疏值矩阵呢？稀疏矩阵是矩阵中很多元素为0，只有少数元素是非零值的矩阵，稀疏矩阵的好处就是能够帮助模型找到重要特征，而去掉无用特征或影响甚小的特征。\n\n比如在分类或预测时，很多特征难以选择，如果代入稀疏矩阵，能够筛选出少数对目标函数有贡献的特征，去掉绝大部分贡献很小或没有贡献的特征(因为稀疏矩阵很多值是0或是很小值)。因此我们只需要关注系数是非零值的特征，从而达到特征选择和解决过拟合的问题。那么为什么L1正则化可以产生稀疏模型呢？\n\n假如带有L1正则化的损失函数方程如下所示，其中$J_0$是原始损失函数，$\\alpha \\sum_{w}|w|$是正则化项，$\\alpha$是正则化系数。根据1.2节的介绍，可以采用一定的方法，比如梯度下降法，求出在L1的约束下$J_0$的最小值。\n$$\nJ=J_0+L_1=J_0+\\alpha \\sum_{w}|w|\n$$\n以上述损失函数为例，我们考虑二维的情况，即只有两个权值$w_1$和$w_2$，此时$L1=\\alpha(|w|_1+|w|_2)$。对于梯度下降法，求解$J_0$的过程，可以画出如下的等值线。同时L1正则化函数也可以在$w_1,w_2$二维平面上表示出来，即黑色直线所围成的菱形。\n\n![深度神经网络之正则化图片02](深度神经网络之正则化/深度神经网络之正则化图片02.png)\n\n从上图可以看出，当J0等值线与L1图形首次相交的点就是最优解，也就是上图中的(0,w)。而对于L1函数有许多突出的点(二维情况下是4个)，J0函数与这些顶点接触的概率远大于与L1其他部分接触的概率，恰好在这些顶点上会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。最后针对L1正则化再介绍下系数α，其目的是控制L1图形的大小。当α越小，L1的图形越大，α越大，L1图形也就越小。L1图形可以小到在原点附近，这也就是为什么w可以取到很小的原因。\n\n另外**L2正则化**也可以很好的解决过拟合问题。从上面得知，拟合过程中通常都倾向于让权值尽可能小，最后构造出一个让所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能够适应于不同的数据集，比如对于目标方程，若参数很大，那么数据只要偏倚一点点，那么对结果的影响就很大。如果参数很小的话，即使数据变化范围比较大，对结果影响也不是很大。相对来说，参数较小的话，对模型的抗扰动能力强。那么为什么L2正则化可以获得很小的参数值呢？我们假设带有L2正则化的损失函数方程如下所示，并对损失函数进行求导。\n$$\nJ=J_0+L_2=J_0+\\frac{\\lambda}{2n}\\sum_{w}w^2\n$$\n\n$$\n\\frac{\\partial J}{\\partial w}=\\frac{\\partial J_0}{\\partial w}+\\frac{\\lambda}{n}w\n$$\n\n$$\n\\frac{\\partial J}{\\partial b}=\\frac{\\partial J_0}{\\partial n}\n$$\n\n当利用梯度下降算法进行更新w时，w变化如下所示，其中α是学习速率。\n$$\nw\\rightarrow w-\\alpha \\frac{\\partial J_0}{\\partial w}-\\frac{\\alpha \\lambda}{n}w\n$$\n\n$$\n=(1-\\frac{\\alpha \\lambda}{n})w-\\alpha\\frac{\\partial J_0}{\\partial w}\n$$\n\n可以看到在梯度下降算法过程中，w是不断进行减小的，也就是权重衰减，这样也就能得到一个让所有参数都比较小的模型，也就能解决过拟合问题。最后再解释下为什么L2正则化不具有稀疏性的原因，如下图所示，二维平面下L2正则化的函数图形是圆，与L1图形相比，没有了菱角。因此J0与L2接触时，使w1或w2等于0的机率就小了很多，所以L2正则化不具有稀疏性。\n\n![深度神经网络之正则化图片03](深度神经网络之正则化/深度神经网络之正则化图片03.png)\n\n### 2.DNN之L1和L2正则化\n\n和普通机器学习算法一样，DNN也会遇到过拟合的问题，因此需要考虑泛化。结合我们上面讲到的L1和L2正则化，这里对深度神经网络中的正则化做个总结，其中L1正则化和L2正则化原理类似，这里主要介绍L2正则化方法。通过[深度神经网络之前向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483903&idx=1&sn=4e3f92578399013eba9f203d35afe972&chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f#rd)的学习，我们知道前向传播过程中损失函数为\n$$\nJ(W,b)=\\frac{1}{2m}\\sum _{i=1}^{m}||a^L-y||_2^2\n$$\n加入L2正则化后，损失函数如下所示。其中λ是正则化参数，实际使用时需要我们进行调参。\n$$\nJ(W,b)=\\frac{1}{2m}\\sum _{i=1}^{m}||a^L-y||_2^2+\\frac{\\lambda}{2m}\\sum_{l=2}^{L}||w||_2^2\n$$\n如果使用上式的损失函数，进行反向传播算法时，流程和没有正则化时的反向传播算法相同。区别在于进行梯度下降时，W更新公式会进行改变。在[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a#rd)中，W的梯度下降更新公式为\n$$\nW^l=W^l-\\alpha \\sum_{i=1}^{m} \\delta^{i,l}(a^{x,l-1})^T\n$$\n加入L2正则化后，W迭代更新公式如下所示\n$$\nW^l=W^l-\\alpha \\sum_{i=1}^{m} \\delta^{i,l}(a^{x,l-1})^T-\\frac{\\alpha }{m}\\lambda W^l\n$$\n类似的正则化方法，同样可以用于其他损失函数，在这里不再介绍。\n\n### 3.DNN之Dropout正则化\n\nDropout指的是在用前向传播算法和反向传播算法训练模型时，随机的从全连接DNN网络中去掉一部分隐含层的神经元。比如我们完整的DNN模型如下所示\n\n![深度神经网络之正则化图片04](深度神经网络之正则化/深度神经网络之正则化图片04.png)\n\n然后随机的去掉部分隐含层的神经元，利用数据进行训练模型，更新所有的W,b。\n\n![深度神经网络之正则化图片05](深度神经网络之正则化/深度神经网络之正则化图片05.png)\n\n总结下Dropout方法就是，每轮梯度下降迭代时，将训练数据分成若干批，然后分批进行迭代。每批数据迭代时，将原始的DNN模型随机去掉部分隐含层的神经元，然后用残缺的DNN模型来迭代更新W,b。每批数据迭代完成之后，将残缺的DNN模型恢复成原始的DNN模型，接着去训练模型，更新W,b。当然，运用Dropout正则化方法，需要有较大数据量支持，否则可能会出现欠拟合的情况。\n\n### 4.DNN之集成学习正则化\n\n在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)和[机器学习之梯度提升决策树](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483849&idx=1&sn=3cd3d40d26e600901cf3d72c43f7c696&chksm=fcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791#rd)之中，我们已经学习集成学习中的Bagging和Boosting方法，而DNN可以用Bagging方法来正则化。随机森林中，Bagging方法通过随机采样构建若干个相互独立的弱决策树学习器，最后通过采用加权平均法或者投票法决定集成的输出。\n\nDNN中我们采用的是若干个DNN的网络，首先对原始的训练样本进行有放回的随机采样，构建N组m个样本的数据集，然后分别用这N组数据集去训练我们的DNN。通过利用前向传播算法和反向传播算法得到N个DNN模型的W,b参数组合，然后对N个DNN模型的输出用加权平均法或者投票法决定最后的输出。最后，因为DNN模型比较复杂，通过Bagging后模型参数会增加N倍，从而导致训练模型需要花费较长的时间，因此一般N的取值不能太大，5-10个即可。\n\n### 5.DNN之增强数据集正则化\n\n增强模型泛化能力最好的方法，是有更多更好的训练数据，但实际情况之中，对于某些数据，我们很难能够得到。那么，我们不如去构造一些数据，来让模型得到更强的泛化能力。对于传统的机器学习算法，比如上面提到的[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)和[机器学习之梯度提升决策树](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483849&idx=1&sn=3cd3d40d26e600901cf3d72c43f7c696&chksm=fcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791#rd)算法，想要构造数据的话，能够很方便的构造输入数据，但是很难构造出对应的输出数据。\n\n但对于深度神经网络来说，比如图像识别领域，对于原始数据集的图像，我们可以偏倚或者旋转图像之后，得到新的数据集。显然原始数据和新构造的数据输入是不同的图像，但输出是相同的，因此通过训练后，模型的泛化便能够增强。对应的例子，比如利用DNN识别手写数字，数字5旋转15度之后，识别之后还是5。\n\n参考\n\n> [深度神经网络(DNN)的正则化-刘建平Pinard](https://www.cnblogs.com/pinard/p/6472666.html)\n>\n> [机器学习中正则化项L1和L2的直观理解-阿拉丁吃米粉](https://blog.csdn.net/jinping_shi/article/details/52433975)\n>\n> [机器学习中常常提到的正则化到底是什么意思？-陶轻松](https://www.zhihu.com/question/20924039)\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](深度神经网络之正则化/推广.png)","source":"_posts/深度神经网络之正则化.md","raw":"---\ntitle: 深度神经网络之正则化\ndate: 2018-07-03 09:58:46\ntags: [机器学习,深度学习,算法]\ncategories: 深度学习\nmathjax: true\n---\n\n### 1.正则化\n\n之前介绍的[文章](https://mp.weixin.qq.com/s/UeEL1rOdjMpaKqTFpclS3A)之中，我们已多次接触到正则化方法，但没有详细的解释为什么要正则化，什么是正则化，以及L1正则化和L2正则化的区别。本次文章之中，我们将详解机器学习中正则化的概念和深度神经网络中的正则化方法。\n\n#### 1.1 为什么要正则化？\n\n讲到为什么需要正则化，就需要了解什么是过拟合问题。以下面图片为例，我们能够看到有两个类别，其中以X代表男生，O代表女生。\n\n![深度神经网络之正则化图片01](深度神经网络之正则化/深度神经网络之正则化图片01.png)\n\n我们想要通过学习来得到分类曲线，其中分类曲线能够有效区分男生和女生，现在来分析下上面的三种分类结果。\n\n+ **欠拟合：**图1分类明显欠缺，有些男生被分为女生，有些女生被分为男生。\n+ **正拟合：**图2虽然有两个男生被分类为女生，但能够理解，毕竟我们人类自己也有分类错误的情况，比如通过化妆，女装等方法。\n+ **过拟合：**图3虽然能够全部分类正确，但结果全部正确就一定好吗？不一定，我们能够看到分类曲线明显过于复杂，模型学习的时候学习了过多的参数项，但其中某些参数项是无用的特征，比如**眼睛大小**。当我们进行识别测试集数据时，就需要提供更多的特征，如果测试集包含海量的数据，模型的时间复杂度可想而知。\n\n#### 1.2 什么是正则化？\n\n既然我们已经知道什么是过拟合，那么怎么解决过拟合问题呢？上面有介绍到，模型出现过拟合，是在模型**特征**上过于复杂。而特征又包含在我们的目标函数f(x)之中，那么只能从目标函数f(x)中寻找解决问题的方法。假设目标函数f(x)和损失函数J0为\n$$\nf(X)=w_0x_0+w_1x_1+w_2x_3+...+w_nx_n\n$$\n\n$$\nJ_0=||f(x)-y||_2^2=||XW-y||^2_2\n$$\n\n对于上式，如果特征项$x_0,x_1,x_2,...,x_n$越多的话，自然$w_0,w_1,w_2,...,w_n$也就越多。想要减少特征的数量，自然减小**N**也就好了。而N影响的是$X=(x_0,x_1,x_2,...,x_n)$和$W=(w_0,w_1,w_2,...,w_n)$两项，那么是从**X**解决问题还是从**W**解决问题呢？\n\n如果从**X**入手解决问题，但训练过程中我们不知道下一个样本X是什么，会怎样的影响目标函数，所以此路不通。那么**W**如何呢？我们知道W系数是训练过程中通过学习历史数据得到的，和历史数据有关，所以应该可以。现在再回到我们原来的问题，希望减少N的数目，而让N最小化，其实就是让X向量或W向量中项的个数最小化，既然X不行，那么我们可以尝试让W向量中项的个数最小化。如何求解才能让W向量中项的个数最小，我们先简单介绍下0、1、2范数的概念。\n\n+ **L0范数：**向量中非零元素的个数，记为$||W||_0$。\n+ **L1范数：**绝对值之和，记为$||W||_1$。\n+ **L2范数：**通常意义上的模，记为$||W||_2$。\n\n所以为了防止过拟合，我们需要让$||W||_0$最小，同时让损失函数$J_0$最小，为了满足两项最小化，可以让$J_0$和$||W||_0$之和最小化。但因为$||W||_0$比较难求(NP难问题)，我们进而可以转化为求$||W||_1$。$||W||_1$是$||W||_0$的最优凸近似，都可以实现**稀疏**，比较容易求解，这也是为什么可以选用$||W||_1$的原因。最后损失函数后面添加的额外项$||W||_1$，也就是我们称作的L1正则化，$\\alpha$含义在后面进行讲解。\n$$\nJ=J_0+L1=||XW-y||^2_2+\\alpha||W||_1\n$$\n说完L0范数和L1范数，就不得不提L2范数。L2范数是指先求向量各元素的平方和，然后再进行求平方根，也就是通常意义上的模。同样，对于正则化问题，我们的目标是让W向量中的每个元素都很小，也就是让L2范数最小。L1范数和L2范数的不同点在于，L1范数会让其中某些元素等于0，而L2范数只是让其中元素接近0，这里有很大不同，我们在后面会进行详细讲解。最后损失函数后面添加的额外项||W||2，也就是我们称作的L2正则化。\n$$\nJ=J_0+L2=||XW-y||^2_2+\\alpha ||W||_2\n$$\n\n#### 1.3 L1正则化和L2正则化\n\n**L1正则化**可以产生稀疏值矩阵，即产生一个稀疏模型，可以用于特征选择和解决过拟合。那什么是稀疏值矩阵呢？稀疏矩阵是矩阵中很多元素为0，只有少数元素是非零值的矩阵，稀疏矩阵的好处就是能够帮助模型找到重要特征，而去掉无用特征或影响甚小的特征。\n\n比如在分类或预测时，很多特征难以选择，如果代入稀疏矩阵，能够筛选出少数对目标函数有贡献的特征，去掉绝大部分贡献很小或没有贡献的特征(因为稀疏矩阵很多值是0或是很小值)。因此我们只需要关注系数是非零值的特征，从而达到特征选择和解决过拟合的问题。那么为什么L1正则化可以产生稀疏模型呢？\n\n假如带有L1正则化的损失函数方程如下所示，其中$J_0$是原始损失函数，$\\alpha \\sum_{w}|w|$是正则化项，$\\alpha$是正则化系数。根据1.2节的介绍，可以采用一定的方法，比如梯度下降法，求出在L1的约束下$J_0$的最小值。\n$$\nJ=J_0+L_1=J_0+\\alpha \\sum_{w}|w|\n$$\n以上述损失函数为例，我们考虑二维的情况，即只有两个权值$w_1$和$w_2$，此时$L1=\\alpha(|w|_1+|w|_2)$。对于梯度下降法，求解$J_0$的过程，可以画出如下的等值线。同时L1正则化函数也可以在$w_1,w_2$二维平面上表示出来，即黑色直线所围成的菱形。\n\n![深度神经网络之正则化图片02](深度神经网络之正则化/深度神经网络之正则化图片02.png)\n\n从上图可以看出，当J0等值线与L1图形首次相交的点就是最优解，也就是上图中的(0,w)。而对于L1函数有许多突出的点(二维情况下是4个)，J0函数与这些顶点接触的概率远大于与L1其他部分接触的概率，恰好在这些顶点上会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。最后针对L1正则化再介绍下系数α，其目的是控制L1图形的大小。当α越小，L1的图形越大，α越大，L1图形也就越小。L1图形可以小到在原点附近，这也就是为什么w可以取到很小的原因。\n\n另外**L2正则化**也可以很好的解决过拟合问题。从上面得知，拟合过程中通常都倾向于让权值尽可能小，最后构造出一个让所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能够适应于不同的数据集，比如对于目标方程，若参数很大，那么数据只要偏倚一点点，那么对结果的影响就很大。如果参数很小的话，即使数据变化范围比较大，对结果影响也不是很大。相对来说，参数较小的话，对模型的抗扰动能力强。那么为什么L2正则化可以获得很小的参数值呢？我们假设带有L2正则化的损失函数方程如下所示，并对损失函数进行求导。\n$$\nJ=J_0+L_2=J_0+\\frac{\\lambda}{2n}\\sum_{w}w^2\n$$\n\n$$\n\\frac{\\partial J}{\\partial w}=\\frac{\\partial J_0}{\\partial w}+\\frac{\\lambda}{n}w\n$$\n\n$$\n\\frac{\\partial J}{\\partial b}=\\frac{\\partial J_0}{\\partial n}\n$$\n\n当利用梯度下降算法进行更新w时，w变化如下所示，其中α是学习速率。\n$$\nw\\rightarrow w-\\alpha \\frac{\\partial J_0}{\\partial w}-\\frac{\\alpha \\lambda}{n}w\n$$\n\n$$\n=(1-\\frac{\\alpha \\lambda}{n})w-\\alpha\\frac{\\partial J_0}{\\partial w}\n$$\n\n可以看到在梯度下降算法过程中，w是不断进行减小的，也就是权重衰减，这样也就能得到一个让所有参数都比较小的模型，也就能解决过拟合问题。最后再解释下为什么L2正则化不具有稀疏性的原因，如下图所示，二维平面下L2正则化的函数图形是圆，与L1图形相比，没有了菱角。因此J0与L2接触时，使w1或w2等于0的机率就小了很多，所以L2正则化不具有稀疏性。\n\n![深度神经网络之正则化图片03](深度神经网络之正则化/深度神经网络之正则化图片03.png)\n\n### 2.DNN之L1和L2正则化\n\n和普通机器学习算法一样，DNN也会遇到过拟合的问题，因此需要考虑泛化。结合我们上面讲到的L1和L2正则化，这里对深度神经网络中的正则化做个总结，其中L1正则化和L2正则化原理类似，这里主要介绍L2正则化方法。通过[深度神经网络之前向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483903&idx=1&sn=4e3f92578399013eba9f203d35afe972&chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f#rd)的学习，我们知道前向传播过程中损失函数为\n$$\nJ(W,b)=\\frac{1}{2m}\\sum _{i=1}^{m}||a^L-y||_2^2\n$$\n加入L2正则化后，损失函数如下所示。其中λ是正则化参数，实际使用时需要我们进行调参。\n$$\nJ(W,b)=\\frac{1}{2m}\\sum _{i=1}^{m}||a^L-y||_2^2+\\frac{\\lambda}{2m}\\sum_{l=2}^{L}||w||_2^2\n$$\n如果使用上式的损失函数，进行反向传播算法时，流程和没有正则化时的反向传播算法相同。区别在于进行梯度下降时，W更新公式会进行改变。在[深度神经网络之反向传播算法](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483911&idx=1&sn=bcc0fe6a4a0c20a422f254b3264a5fb8&chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a#rd)中，W的梯度下降更新公式为\n$$\nW^l=W^l-\\alpha \\sum_{i=1}^{m} \\delta^{i,l}(a^{x,l-1})^T\n$$\n加入L2正则化后，W迭代更新公式如下所示\n$$\nW^l=W^l-\\alpha \\sum_{i=1}^{m} \\delta^{i,l}(a^{x,l-1})^T-\\frac{\\alpha }{m}\\lambda W^l\n$$\n类似的正则化方法，同样可以用于其他损失函数，在这里不再介绍。\n\n### 3.DNN之Dropout正则化\n\nDropout指的是在用前向传播算法和反向传播算法训练模型时，随机的从全连接DNN网络中去掉一部分隐含层的神经元。比如我们完整的DNN模型如下所示\n\n![深度神经网络之正则化图片04](深度神经网络之正则化/深度神经网络之正则化图片04.png)\n\n然后随机的去掉部分隐含层的神经元，利用数据进行训练模型，更新所有的W,b。\n\n![深度神经网络之正则化图片05](深度神经网络之正则化/深度神经网络之正则化图片05.png)\n\n总结下Dropout方法就是，每轮梯度下降迭代时，将训练数据分成若干批，然后分批进行迭代。每批数据迭代时，将原始的DNN模型随机去掉部分隐含层的神经元，然后用残缺的DNN模型来迭代更新W,b。每批数据迭代完成之后，将残缺的DNN模型恢复成原始的DNN模型，接着去训练模型，更新W,b。当然，运用Dropout正则化方法，需要有较大数据量支持，否则可能会出现欠拟合的情况。\n\n### 4.DNN之集成学习正则化\n\n在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)和[机器学习之梯度提升决策树](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483849&idx=1&sn=3cd3d40d26e600901cf3d72c43f7c696&chksm=fcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791#rd)之中，我们已经学习集成学习中的Bagging和Boosting方法，而DNN可以用Bagging方法来正则化。随机森林中，Bagging方法通过随机采样构建若干个相互独立的弱决策树学习器，最后通过采用加权平均法或者投票法决定集成的输出。\n\nDNN中我们采用的是若干个DNN的网络，首先对原始的训练样本进行有放回的随机采样，构建N组m个样本的数据集，然后分别用这N组数据集去训练我们的DNN。通过利用前向传播算法和反向传播算法得到N个DNN模型的W,b参数组合，然后对N个DNN模型的输出用加权平均法或者投票法决定最后的输出。最后，因为DNN模型比较复杂，通过Bagging后模型参数会增加N倍，从而导致训练模型需要花费较长的时间，因此一般N的取值不能太大，5-10个即可。\n\n### 5.DNN之增强数据集正则化\n\n增强模型泛化能力最好的方法，是有更多更好的训练数据，但实际情况之中，对于某些数据，我们很难能够得到。那么，我们不如去构造一些数据，来让模型得到更强的泛化能力。对于传统的机器学习算法，比如上面提到的[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)和[机器学习之梯度提升决策树](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483849&idx=1&sn=3cd3d40d26e600901cf3d72c43f7c696&chksm=fcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791#rd)算法，想要构造数据的话，能够很方便的构造输入数据，但是很难构造出对应的输出数据。\n\n但对于深度神经网络来说，比如图像识别领域，对于原始数据集的图像，我们可以偏倚或者旋转图像之后，得到新的数据集。显然原始数据和新构造的数据输入是不同的图像，但输出是相同的，因此通过训练后，模型的泛化便能够增强。对应的例子，比如利用DNN识别手写数字，数字5旋转15度之后，识别之后还是5。\n\n参考\n\n> [深度神经网络(DNN)的正则化-刘建平Pinard](https://www.cnblogs.com/pinard/p/6472666.html)\n>\n> [机器学习中正则化项L1和L2的直观理解-阿拉丁吃米粉](https://blog.csdn.net/jinping_shi/article/details/52433975)\n>\n> [机器学习中常常提到的正则化到底是什么意思？-陶轻松](https://www.zhihu.com/question/20924039)\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](深度神经网络之正则化/推广.png)","slug":"深度神经网络之正则化","published":1,"updated":"2018-07-04T03:06:45.367Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjktv5d54002fjiz5q8uygzb6","content":"<h3 id=\"1-正则化\"><a href=\"#1-正则化\" class=\"headerlink\" title=\"1.正则化\"></a>1.正则化</h3><p>之前介绍的<a href=\"https://mp.weixin.qq.com/s/UeEL1rOdjMpaKqTFpclS3A\" target=\"_blank\" rel=\"noopener\">文章</a>之中，我们已多次接触到正则化方法，但没有详细的解释为什么要正则化，什么是正则化，以及L1正则化和L2正则化的区别。本次文章之中，我们将详解机器学习中正则化的概念和深度神经网络中的正则化方法。</p>\n<h4 id=\"1-1-为什么要正则化？\"><a href=\"#1-1-为什么要正则化？\" class=\"headerlink\" title=\"1.1 为什么要正则化？\"></a>1.1 为什么要正则化？</h4><p>讲到为什么需要正则化，就需要了解什么是过拟合问题。以下面图片为例，我们能够看到有两个类别，其中以X代表男生，O代表女生。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片01.png\" alt=\"深度神经网络之正则化图片01\"></p>\n<p>我们想要通过学习来得到分类曲线，其中分类曲线能够有效区分男生和女生，现在来分析下上面的三种分类结果。</p>\n<ul>\n<li><strong>欠拟合：</strong>图1分类明显欠缺，有些男生被分为女生，有些女生被分为男生。</li>\n<li><strong>正拟合：</strong>图2虽然有两个男生被分类为女生，但能够理解，毕竟我们人类自己也有分类错误的情况，比如通过化妆，女装等方法。</li>\n<li><strong>过拟合：</strong>图3虽然能够全部分类正确，但结果全部正确就一定好吗？不一定，我们能够看到分类曲线明显过于复杂，模型学习的时候学习了过多的参数项，但其中某些参数项是无用的特征，比如<strong>眼睛大小</strong>。当我们进行识别测试集数据时，就需要提供更多的特征，如果测试集包含海量的数据，模型的时间复杂度可想而知。</li>\n</ul>\n<h4 id=\"1-2-什么是正则化？\"><a href=\"#1-2-什么是正则化？\" class=\"headerlink\" title=\"1.2 什么是正则化？\"></a>1.2 什么是正则化？</h4><p>既然我们已经知道什么是过拟合，那么怎么解决过拟合问题呢？上面有介绍到，模型出现过拟合，是在模型<strong>特征</strong>上过于复杂。而特征又包含在我们的目标函数f(x)之中，那么只能从目标函数f(x)中寻找解决问题的方法。假设目标函数f(x)和损失函数J0为</p>\n<script type=\"math/tex; mode=display\">\nf(X)=w_0x_0+w_1x_1+w_2x_3+...+w_nx_n</script><script type=\"math/tex; mode=display\">\nJ_0=||f(x)-y||_2^2=||XW-y||^2_2</script><p>对于上式，如果特征项$x_0,x_1,x_2,…,x_n$越多的话，自然$w_0,w_1,w_2,…,w_n$也就越多。想要减少特征的数量，自然减小<strong>N</strong>也就好了。而N影响的是$X=(x_0,x_1,x_2,…,x_n)$和$W=(w_0,w_1,w_2,…,w_n)$两项，那么是从<strong>X</strong>解决问题还是从<strong>W</strong>解决问题呢？</p>\n<p>如果从<strong>X</strong>入手解决问题，但训练过程中我们不知道下一个样本X是什么，会怎样的影响目标函数，所以此路不通。那么<strong>W</strong>如何呢？我们知道W系数是训练过程中通过学习历史数据得到的，和历史数据有关，所以应该可以。现在再回到我们原来的问题，希望减少N的数目，而让N最小化，其实就是让X向量或W向量中项的个数最小化，既然X不行，那么我们可以尝试让W向量中项的个数最小化。如何求解才能让W向量中项的个数最小，我们先简单介绍下0、1、2范数的概念。</p>\n<ul>\n<li><strong>L0范数：</strong>向量中非零元素的个数，记为$||W||_0$。</li>\n<li><strong>L1范数：</strong>绝对值之和，记为$||W||_1$。</li>\n<li><strong>L2范数：</strong>通常意义上的模，记为$||W||_2$。</li>\n</ul>\n<p>所以为了防止过拟合，我们需要让$||W||_0$最小，同时让损失函数$J_0$最小，为了满足两项最小化，可以让$J_0$和$||W||_0$之和最小化。但因为$||W||_0$比较难求(NP难问题)，我们进而可以转化为求$||W||_1$。$||W||_1$是$||W||_0$的最优凸近似，都可以实现<strong>稀疏</strong>，比较容易求解，这也是为什么可以选用$||W||_1$的原因。最后损失函数后面添加的额外项$||W||_1$，也就是我们称作的L1正则化，$\\alpha$含义在后面进行讲解。</p>\n<script type=\"math/tex; mode=display\">\nJ=J_0+L1=||XW-y||^2_2+\\alpha||W||_1</script><p>说完L0范数和L1范数，就不得不提L2范数。L2范数是指先求向量各元素的平方和，然后再进行求平方根，也就是通常意义上的模。同样，对于正则化问题，我们的目标是让W向量中的每个元素都很小，也就是让L2范数最小。L1范数和L2范数的不同点在于，L1范数会让其中某些元素等于0，而L2范数只是让其中元素接近0，这里有很大不同，我们在后面会进行详细讲解。最后损失函数后面添加的额外项||W||2，也就是我们称作的L2正则化。</p>\n<script type=\"math/tex; mode=display\">\nJ=J_0+L2=||XW-y||^2_2+\\alpha ||W||_2</script><h4 id=\"1-3-L1正则化和L2正则化\"><a href=\"#1-3-L1正则化和L2正则化\" class=\"headerlink\" title=\"1.3 L1正则化和L2正则化\"></a>1.3 L1正则化和L2正则化</h4><p><strong>L1正则化</strong>可以产生稀疏值矩阵，即产生一个稀疏模型，可以用于特征选择和解决过拟合。那什么是稀疏值矩阵呢？稀疏矩阵是矩阵中很多元素为0，只有少数元素是非零值的矩阵，稀疏矩阵的好处就是能够帮助模型找到重要特征，而去掉无用特征或影响甚小的特征。</p>\n<p>比如在分类或预测时，很多特征难以选择，如果代入稀疏矩阵，能够筛选出少数对目标函数有贡献的特征，去掉绝大部分贡献很小或没有贡献的特征(因为稀疏矩阵很多值是0或是很小值)。因此我们只需要关注系数是非零值的特征，从而达到特征选择和解决过拟合的问题。那么为什么L1正则化可以产生稀疏模型呢？</p>\n<p>假如带有L1正则化的损失函数方程如下所示，其中$J_0$是原始损失函数，$\\alpha \\sum_{w}|w|$是正则化项，$\\alpha$是正则化系数。根据1.2节的介绍，可以采用一定的方法，比如梯度下降法，求出在L1的约束下$J_0$的最小值。</p>\n<script type=\"math/tex; mode=display\">\nJ=J_0+L_1=J_0+\\alpha \\sum_{w}|w|</script><p>以上述损失函数为例，我们考虑二维的情况，即只有两个权值$w_1$和$w_2$，此时$L1=\\alpha(|w|_1+|w|_2)$。对于梯度下降法，求解$J_0$的过程，可以画出如下的等值线。同时L1正则化函数也可以在$w_1,w_2$二维平面上表示出来，即黑色直线所围成的菱形。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片02.png\" alt=\"深度神经网络之正则化图片02\"></p>\n<p>从上图可以看出，当J0等值线与L1图形首次相交的点就是最优解，也就是上图中的(0,w)。而对于L1函数有许多突出的点(二维情况下是4个)，J0函数与这些顶点接触的概率远大于与L1其他部分接触的概率，恰好在这些顶点上会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。最后针对L1正则化再介绍下系数α，其目的是控制L1图形的大小。当α越小，L1的图形越大，α越大，L1图形也就越小。L1图形可以小到在原点附近，这也就是为什么w可以取到很小的原因。</p>\n<p>另外<strong>L2正则化</strong>也可以很好的解决过拟合问题。从上面得知，拟合过程中通常都倾向于让权值尽可能小，最后构造出一个让所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能够适应于不同的数据集，比如对于目标方程，若参数很大，那么数据只要偏倚一点点，那么对结果的影响就很大。如果参数很小的话，即使数据变化范围比较大，对结果影响也不是很大。相对来说，参数较小的话，对模型的抗扰动能力强。那么为什么L2正则化可以获得很小的参数值呢？我们假设带有L2正则化的损失函数方程如下所示，并对损失函数进行求导。</p>\n<script type=\"math/tex; mode=display\">\nJ=J_0+L_2=J_0+\\frac{\\lambda}{2n}\\sum_{w}w^2</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial w}=\\frac{\\partial J_0}{\\partial w}+\\frac{\\lambda}{n}w</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial b}=\\frac{\\partial J_0}{\\partial n}</script><p>当利用梯度下降算法进行更新w时，w变化如下所示，其中α是学习速率。</p>\n<script type=\"math/tex; mode=display\">\nw\\rightarrow w-\\alpha \\frac{\\partial J_0}{\\partial w}-\\frac{\\alpha \\lambda}{n}w</script><script type=\"math/tex; mode=display\">\n=(1-\\frac{\\alpha \\lambda}{n})w-\\alpha\\frac{\\partial J_0}{\\partial w}</script><p>可以看到在梯度下降算法过程中，w是不断进行减小的，也就是权重衰减，这样也就能得到一个让所有参数都比较小的模型，也就能解决过拟合问题。最后再解释下为什么L2正则化不具有稀疏性的原因，如下图所示，二维平面下L2正则化的函数图形是圆，与L1图形相比，没有了菱角。因此J0与L2接触时，使w1或w2等于0的机率就小了很多，所以L2正则化不具有稀疏性。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片03.png\" alt=\"深度神经网络之正则化图片03\"></p>\n<h3 id=\"2-DNN之L1和L2正则化\"><a href=\"#2-DNN之L1和L2正则化\" class=\"headerlink\" title=\"2.DNN之L1和L2正则化\"></a>2.DNN之L1和L2正则化</h3><p>和普通机器学习算法一样，DNN也会遇到过拟合的问题，因此需要考虑泛化。结合我们上面讲到的L1和L2正则化，这里对深度神经网络中的正则化做个总结，其中L1正则化和L2正则化原理类似，这里主要介绍L2正则化方法。通过<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483903&amp;idx=1&amp;sn=4e3f92578399013eba9f203d35afe972&amp;chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f#rd\" target=\"_blank\" rel=\"noopener\">深度神经网络之前向传播算法</a>的学习，我们知道前向传播过程中损失函数为</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b)=\\frac{1}{2m}\\sum _{i=1}^{m}||a^L-y||_2^2</script><p>加入L2正则化后，损失函数如下所示。其中λ是正则化参数，实际使用时需要我们进行调参。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b)=\\frac{1}{2m}\\sum _{i=1}^{m}||a^L-y||_2^2+\\frac{\\lambda}{2m}\\sum_{l=2}^{L}||w||_2^2</script><p>如果使用上式的损失函数，进行反向传播算法时，流程和没有正则化时的反向传播算法相同。区别在于进行梯度下降时，W更新公式会进行改变。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a#rd\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>中，W的梯度下降更新公式为</p>\n<script type=\"math/tex; mode=display\">\nW^l=W^l-\\alpha \\sum_{i=1}^{m} \\delta^{i,l}(a^{x,l-1})^T</script><p>加入L2正则化后，W迭代更新公式如下所示</p>\n<script type=\"math/tex; mode=display\">\nW^l=W^l-\\alpha \\sum_{i=1}^{m} \\delta^{i,l}(a^{x,l-1})^T-\\frac{\\alpha }{m}\\lambda W^l</script><p>类似的正则化方法，同样可以用于其他损失函数，在这里不再介绍。</p>\n<h3 id=\"3-DNN之Dropout正则化\"><a href=\"#3-DNN之Dropout正则化\" class=\"headerlink\" title=\"3.DNN之Dropout正则化\"></a>3.DNN之Dropout正则化</h3><p>Dropout指的是在用前向传播算法和反向传播算法训练模型时，随机的从全连接DNN网络中去掉一部分隐含层的神经元。比如我们完整的DNN模型如下所示</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片04.png\" alt=\"深度神经网络之正则化图片04\"></p>\n<p>然后随机的去掉部分隐含层的神经元，利用数据进行训练模型，更新所有的W,b。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片05.png\" alt=\"深度神经网络之正则化图片05\"></p>\n<p>总结下Dropout方法就是，每轮梯度下降迭代时，将训练数据分成若干批，然后分批进行迭代。每批数据迭代时，将原始的DNN模型随机去掉部分隐含层的神经元，然后用残缺的DNN模型来迭代更新W,b。每批数据迭代完成之后，将残缺的DNN模型恢复成原始的DNN模型，接着去训练模型，更新W,b。当然，运用Dropout正则化方法，需要有较大数据量支持，否则可能会出现欠拟合的情况。</p>\n<h3 id=\"4-DNN之集成学习正则化\"><a href=\"#4-DNN之集成学习正则化\" class=\"headerlink\" title=\"4.DNN之集成学习正则化\"></a>4.DNN之集成学习正则化</h3><p>在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>和<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483849&amp;idx=1&amp;sn=3cd3d40d26e600901cf3d72c43f7c696&amp;chksm=fcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791#rd\" target=\"_blank\" rel=\"noopener\">机器学习之梯度提升决策树</a>之中，我们已经学习集成学习中的Bagging和Boosting方法，而DNN可以用Bagging方法来正则化。随机森林中，Bagging方法通过随机采样构建若干个相互独立的弱决策树学习器，最后通过采用加权平均法或者投票法决定集成的输出。</p>\n<p>DNN中我们采用的是若干个DNN的网络，首先对原始的训练样本进行有放回的随机采样，构建N组m个样本的数据集，然后分别用这N组数据集去训练我们的DNN。通过利用前向传播算法和反向传播算法得到N个DNN模型的W,b参数组合，然后对N个DNN模型的输出用加权平均法或者投票法决定最后的输出。最后，因为DNN模型比较复杂，通过Bagging后模型参数会增加N倍，从而导致训练模型需要花费较长的时间，因此一般N的取值不能太大，5-10个即可。</p>\n<h3 id=\"5-DNN之增强数据集正则化\"><a href=\"#5-DNN之增强数据集正则化\" class=\"headerlink\" title=\"5.DNN之增强数据集正则化\"></a>5.DNN之增强数据集正则化</h3><p>增强模型泛化能力最好的方法，是有更多更好的训练数据，但实际情况之中，对于某些数据，我们很难能够得到。那么，我们不如去构造一些数据，来让模型得到更强的泛化能力。对于传统的机器学习算法，比如上面提到的<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>和<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483849&amp;idx=1&amp;sn=3cd3d40d26e600901cf3d72c43f7c696&amp;chksm=fcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791#rd\" target=\"_blank\" rel=\"noopener\">机器学习之梯度提升决策树</a>算法，想要构造数据的话，能够很方便的构造输入数据，但是很难构造出对应的输出数据。</p>\n<p>但对于深度神经网络来说，比如图像识别领域，对于原始数据集的图像，我们可以偏倚或者旋转图像之后，得到新的数据集。显然原始数据和新构造的数据输入是不同的图像，但输出是相同的，因此通过训练后，模型的泛化便能够增强。对应的例子，比如利用DNN识别手写数字，数字5旋转15度之后，识别之后还是5。</p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6472666.html\" target=\"_blank\" rel=\"noopener\">深度神经网络(DNN)的正则化-刘建平Pinard</a></p>\n<p><a href=\"https://blog.csdn.net/jinping_shi/article/details/52433975\" target=\"_blank\" rel=\"noopener\">机器学习中正则化项L1和L2的直观理解-阿拉丁吃米粉</a></p>\n<p><a href=\"https://www.zhihu.com/question/20924039\" target=\"_blank\" rel=\"noopener\">机器学习中常常提到的正则化到底是什么意思？-陶轻松</a></p>\n</blockquote>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-正则化\"><a href=\"#1-正则化\" class=\"headerlink\" title=\"1.正则化\"></a>1.正则化</h3><p>之前介绍的<a href=\"https://mp.weixin.qq.com/s/UeEL1rOdjMpaKqTFpclS3A\" target=\"_blank\" rel=\"noopener\">文章</a>之中，我们已多次接触到正则化方法，但没有详细的解释为什么要正则化，什么是正则化，以及L1正则化和L2正则化的区别。本次文章之中，我们将详解机器学习中正则化的概念和深度神经网络中的正则化方法。</p>\n<h4 id=\"1-1-为什么要正则化？\"><a href=\"#1-1-为什么要正则化？\" class=\"headerlink\" title=\"1.1 为什么要正则化？\"></a>1.1 为什么要正则化？</h4><p>讲到为什么需要正则化，就需要了解什么是过拟合问题。以下面图片为例，我们能够看到有两个类别，其中以X代表男生，O代表女生。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片01.png\" alt=\"深度神经网络之正则化图片01\"></p>\n<p>我们想要通过学习来得到分类曲线，其中分类曲线能够有效区分男生和女生，现在来分析下上面的三种分类结果。</p>\n<ul>\n<li><strong>欠拟合：</strong>图1分类明显欠缺，有些男生被分为女生，有些女生被分为男生。</li>\n<li><strong>正拟合：</strong>图2虽然有两个男生被分类为女生，但能够理解，毕竟我们人类自己也有分类错误的情况，比如通过化妆，女装等方法。</li>\n<li><strong>过拟合：</strong>图3虽然能够全部分类正确，但结果全部正确就一定好吗？不一定，我们能够看到分类曲线明显过于复杂，模型学习的时候学习了过多的参数项，但其中某些参数项是无用的特征，比如<strong>眼睛大小</strong>。当我们进行识别测试集数据时，就需要提供更多的特征，如果测试集包含海量的数据，模型的时间复杂度可想而知。</li>\n</ul>\n<h4 id=\"1-2-什么是正则化？\"><a href=\"#1-2-什么是正则化？\" class=\"headerlink\" title=\"1.2 什么是正则化？\"></a>1.2 什么是正则化？</h4><p>既然我们已经知道什么是过拟合，那么怎么解决过拟合问题呢？上面有介绍到，模型出现过拟合，是在模型<strong>特征</strong>上过于复杂。而特征又包含在我们的目标函数f(x)之中，那么只能从目标函数f(x)中寻找解决问题的方法。假设目标函数f(x)和损失函数J0为</p>\n<script type=\"math/tex; mode=display\">\nf(X)=w_0x_0+w_1x_1+w_2x_3+...+w_nx_n</script><script type=\"math/tex; mode=display\">\nJ_0=||f(x)-y||_2^2=||XW-y||^2_2</script><p>对于上式，如果特征项$x_0,x_1,x_2,…,x_n$越多的话，自然$w_0,w_1,w_2,…,w_n$也就越多。想要减少特征的数量，自然减小<strong>N</strong>也就好了。而N影响的是$X=(x_0,x_1,x_2,…,x_n)$和$W=(w_0,w_1,w_2,…,w_n)$两项，那么是从<strong>X</strong>解决问题还是从<strong>W</strong>解决问题呢？</p>\n<p>如果从<strong>X</strong>入手解决问题，但训练过程中我们不知道下一个样本X是什么，会怎样的影响目标函数，所以此路不通。那么<strong>W</strong>如何呢？我们知道W系数是训练过程中通过学习历史数据得到的，和历史数据有关，所以应该可以。现在再回到我们原来的问题，希望减少N的数目，而让N最小化，其实就是让X向量或W向量中项的个数最小化，既然X不行，那么我们可以尝试让W向量中项的个数最小化。如何求解才能让W向量中项的个数最小，我们先简单介绍下0、1、2范数的概念。</p>\n<ul>\n<li><strong>L0范数：</strong>向量中非零元素的个数，记为$||W||_0$。</li>\n<li><strong>L1范数：</strong>绝对值之和，记为$||W||_1$。</li>\n<li><strong>L2范数：</strong>通常意义上的模，记为$||W||_2$。</li>\n</ul>\n<p>所以为了防止过拟合，我们需要让$||W||_0$最小，同时让损失函数$J_0$最小，为了满足两项最小化，可以让$J_0$和$||W||_0$之和最小化。但因为$||W||_0$比较难求(NP难问题)，我们进而可以转化为求$||W||_1$。$||W||_1$是$||W||_0$的最优凸近似，都可以实现<strong>稀疏</strong>，比较容易求解，这也是为什么可以选用$||W||_1$的原因。最后损失函数后面添加的额外项$||W||_1$，也就是我们称作的L1正则化，$\\alpha$含义在后面进行讲解。</p>\n<script type=\"math/tex; mode=display\">\nJ=J_0+L1=||XW-y||^2_2+\\alpha||W||_1</script><p>说完L0范数和L1范数，就不得不提L2范数。L2范数是指先求向量各元素的平方和，然后再进行求平方根，也就是通常意义上的模。同样，对于正则化问题，我们的目标是让W向量中的每个元素都很小，也就是让L2范数最小。L1范数和L2范数的不同点在于，L1范数会让其中某些元素等于0，而L2范数只是让其中元素接近0，这里有很大不同，我们在后面会进行详细讲解。最后损失函数后面添加的额外项||W||2，也就是我们称作的L2正则化。</p>\n<script type=\"math/tex; mode=display\">\nJ=J_0+L2=||XW-y||^2_2+\\alpha ||W||_2</script><h4 id=\"1-3-L1正则化和L2正则化\"><a href=\"#1-3-L1正则化和L2正则化\" class=\"headerlink\" title=\"1.3 L1正则化和L2正则化\"></a>1.3 L1正则化和L2正则化</h4><p><strong>L1正则化</strong>可以产生稀疏值矩阵，即产生一个稀疏模型，可以用于特征选择和解决过拟合。那什么是稀疏值矩阵呢？稀疏矩阵是矩阵中很多元素为0，只有少数元素是非零值的矩阵，稀疏矩阵的好处就是能够帮助模型找到重要特征，而去掉无用特征或影响甚小的特征。</p>\n<p>比如在分类或预测时，很多特征难以选择，如果代入稀疏矩阵，能够筛选出少数对目标函数有贡献的特征，去掉绝大部分贡献很小或没有贡献的特征(因为稀疏矩阵很多值是0或是很小值)。因此我们只需要关注系数是非零值的特征，从而达到特征选择和解决过拟合的问题。那么为什么L1正则化可以产生稀疏模型呢？</p>\n<p>假如带有L1正则化的损失函数方程如下所示，其中$J_0$是原始损失函数，$\\alpha \\sum_{w}|w|$是正则化项，$\\alpha$是正则化系数。根据1.2节的介绍，可以采用一定的方法，比如梯度下降法，求出在L1的约束下$J_0$的最小值。</p>\n<script type=\"math/tex; mode=display\">\nJ=J_0+L_1=J_0+\\alpha \\sum_{w}|w|</script><p>以上述损失函数为例，我们考虑二维的情况，即只有两个权值$w_1$和$w_2$，此时$L1=\\alpha(|w|_1+|w|_2)$。对于梯度下降法，求解$J_0$的过程，可以画出如下的等值线。同时L1正则化函数也可以在$w_1,w_2$二维平面上表示出来，即黑色直线所围成的菱形。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片02.png\" alt=\"深度神经网络之正则化图片02\"></p>\n<p>从上图可以看出，当J0等值线与L1图形首次相交的点就是最优解，也就是上图中的(0,w)。而对于L1函数有许多突出的点(二维情况下是4个)，J0函数与这些顶点接触的概率远大于与L1其他部分接触的概率，恰好在这些顶点上会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。最后针对L1正则化再介绍下系数α，其目的是控制L1图形的大小。当α越小，L1的图形越大，α越大，L1图形也就越小。L1图形可以小到在原点附近，这也就是为什么w可以取到很小的原因。</p>\n<p>另外<strong>L2正则化</strong>也可以很好的解决过拟合问题。从上面得知，拟合过程中通常都倾向于让权值尽可能小，最后构造出一个让所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能够适应于不同的数据集，比如对于目标方程，若参数很大，那么数据只要偏倚一点点，那么对结果的影响就很大。如果参数很小的话，即使数据变化范围比较大，对结果影响也不是很大。相对来说，参数较小的话，对模型的抗扰动能力强。那么为什么L2正则化可以获得很小的参数值呢？我们假设带有L2正则化的损失函数方程如下所示，并对损失函数进行求导。</p>\n<script type=\"math/tex; mode=display\">\nJ=J_0+L_2=J_0+\\frac{\\lambda}{2n}\\sum_{w}w^2</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial w}=\\frac{\\partial J_0}{\\partial w}+\\frac{\\lambda}{n}w</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial b}=\\frac{\\partial J_0}{\\partial n}</script><p>当利用梯度下降算法进行更新w时，w变化如下所示，其中α是学习速率。</p>\n<script type=\"math/tex; mode=display\">\nw\\rightarrow w-\\alpha \\frac{\\partial J_0}{\\partial w}-\\frac{\\alpha \\lambda}{n}w</script><script type=\"math/tex; mode=display\">\n=(1-\\frac{\\alpha \\lambda}{n})w-\\alpha\\frac{\\partial J_0}{\\partial w}</script><p>可以看到在梯度下降算法过程中，w是不断进行减小的，也就是权重衰减，这样也就能得到一个让所有参数都比较小的模型，也就能解决过拟合问题。最后再解释下为什么L2正则化不具有稀疏性的原因，如下图所示，二维平面下L2正则化的函数图形是圆，与L1图形相比，没有了菱角。因此J0与L2接触时，使w1或w2等于0的机率就小了很多，所以L2正则化不具有稀疏性。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片03.png\" alt=\"深度神经网络之正则化图片03\"></p>\n<h3 id=\"2-DNN之L1和L2正则化\"><a href=\"#2-DNN之L1和L2正则化\" class=\"headerlink\" title=\"2.DNN之L1和L2正则化\"></a>2.DNN之L1和L2正则化</h3><p>和普通机器学习算法一样，DNN也会遇到过拟合的问题，因此需要考虑泛化。结合我们上面讲到的L1和L2正则化，这里对深度神经网络中的正则化做个总结，其中L1正则化和L2正则化原理类似，这里主要介绍L2正则化方法。通过<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483903&amp;idx=1&amp;sn=4e3f92578399013eba9f203d35afe972&amp;chksm=fcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f#rd\" target=\"_blank\" rel=\"noopener\">深度神经网络之前向传播算法</a>的学习，我们知道前向传播过程中损失函数为</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b)=\\frac{1}{2m}\\sum _{i=1}^{m}||a^L-y||_2^2</script><p>加入L2正则化后，损失函数如下所示。其中λ是正则化参数，实际使用时需要我们进行调参。</p>\n<script type=\"math/tex; mode=display\">\nJ(W,b)=\\frac{1}{2m}\\sum _{i=1}^{m}||a^L-y||_2^2+\\frac{\\lambda}{2m}\\sum_{l=2}^{L}||w||_2^2</script><p>如果使用上式的损失函数，进行反向传播算法时，流程和没有正则化时的反向传播算法相同。区别在于进行梯度下降时，W更新公式会进行改变。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483911&amp;idx=1&amp;sn=bcc0fe6a4a0c20a422f254b3264a5fb8&amp;chksm=fcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a#rd\" target=\"_blank\" rel=\"noopener\">深度神经网络之反向传播算法</a>中，W的梯度下降更新公式为</p>\n<script type=\"math/tex; mode=display\">\nW^l=W^l-\\alpha \\sum_{i=1}^{m} \\delta^{i,l}(a^{x,l-1})^T</script><p>加入L2正则化后，W迭代更新公式如下所示</p>\n<script type=\"math/tex; mode=display\">\nW^l=W^l-\\alpha \\sum_{i=1}^{m} \\delta^{i,l}(a^{x,l-1})^T-\\frac{\\alpha }{m}\\lambda W^l</script><p>类似的正则化方法，同样可以用于其他损失函数，在这里不再介绍。</p>\n<h3 id=\"3-DNN之Dropout正则化\"><a href=\"#3-DNN之Dropout正则化\" class=\"headerlink\" title=\"3.DNN之Dropout正则化\"></a>3.DNN之Dropout正则化</h3><p>Dropout指的是在用前向传播算法和反向传播算法训练模型时，随机的从全连接DNN网络中去掉一部分隐含层的神经元。比如我们完整的DNN模型如下所示</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片04.png\" alt=\"深度神经网络之正则化图片04\"></p>\n<p>然后随机的去掉部分隐含层的神经元，利用数据进行训练模型，更新所有的W,b。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/深度神经网络之正则化图片05.png\" alt=\"深度神经网络之正则化图片05\"></p>\n<p>总结下Dropout方法就是，每轮梯度下降迭代时，将训练数据分成若干批，然后分批进行迭代。每批数据迭代时，将原始的DNN模型随机去掉部分隐含层的神经元，然后用残缺的DNN模型来迭代更新W,b。每批数据迭代完成之后，将残缺的DNN模型恢复成原始的DNN模型，接着去训练模型，更新W,b。当然，运用Dropout正则化方法，需要有较大数据量支持，否则可能会出现欠拟合的情况。</p>\n<h3 id=\"4-DNN之集成学习正则化\"><a href=\"#4-DNN之集成学习正则化\" class=\"headerlink\" title=\"4.DNN之集成学习正则化\"></a>4.DNN之集成学习正则化</h3><p>在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>和<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483849&amp;idx=1&amp;sn=3cd3d40d26e600901cf3d72c43f7c696&amp;chksm=fcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791#rd\" target=\"_blank\" rel=\"noopener\">机器学习之梯度提升决策树</a>之中，我们已经学习集成学习中的Bagging和Boosting方法，而DNN可以用Bagging方法来正则化。随机森林中，Bagging方法通过随机采样构建若干个相互独立的弱决策树学习器，最后通过采用加权平均法或者投票法决定集成的输出。</p>\n<p>DNN中我们采用的是若干个DNN的网络，首先对原始的训练样本进行有放回的随机采样，构建N组m个样本的数据集，然后分别用这N组数据集去训练我们的DNN。通过利用前向传播算法和反向传播算法得到N个DNN模型的W,b参数组合，然后对N个DNN模型的输出用加权平均法或者投票法决定最后的输出。最后，因为DNN模型比较复杂，通过Bagging后模型参数会增加N倍，从而导致训练模型需要花费较长的时间，因此一般N的取值不能太大，5-10个即可。</p>\n<h3 id=\"5-DNN之增强数据集正则化\"><a href=\"#5-DNN之增强数据集正则化\" class=\"headerlink\" title=\"5.DNN之增强数据集正则化\"></a>5.DNN之增强数据集正则化</h3><p>增强模型泛化能力最好的方法，是有更多更好的训练数据，但实际情况之中，对于某些数据，我们很难能够得到。那么，我们不如去构造一些数据，来让模型得到更强的泛化能力。对于传统的机器学习算法，比如上面提到的<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>和<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483849&amp;idx=1&amp;sn=3cd3d40d26e600901cf3d72c43f7c696&amp;chksm=fcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791#rd\" target=\"_blank\" rel=\"noopener\">机器学习之梯度提升决策树</a>算法，想要构造数据的话，能够很方便的构造输入数据，但是很难构造出对应的输出数据。</p>\n<p>但对于深度神经网络来说，比如图像识别领域，对于原始数据集的图像，我们可以偏倚或者旋转图像之后，得到新的数据集。显然原始数据和新构造的数据输入是不同的图像，但输出是相同的，因此通过训练后，模型的泛化便能够增强。对应的例子，比如利用DNN识别手写数字，数字5旋转15度之后，识别之后还是5。</p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6472666.html\" target=\"_blank\" rel=\"noopener\">深度神经网络(DNN)的正则化-刘建平Pinard</a></p>\n<p><a href=\"https://blog.csdn.net/jinping_shi/article/details/52433975\" target=\"_blank\" rel=\"noopener\">机器学习中正则化项L1和L2的直观理解-阿拉丁吃米粉</a></p>\n<p><a href=\"https://www.zhihu.com/question/20924039\" target=\"_blank\" rel=\"noopener\">机器学习中常常提到的正则化到底是什么意思？-陶轻松</a></p>\n</blockquote>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/07/03/深度神经网络之正则化/推广.png\" alt=\"推广\"></p>\n"},{"title":"网店工商信息图片文字提取","date":"2018-06-10T13:00:24.000Z","comments":1,"_content":"首先非常抱歉，最近一段时间由于学校课程作业较多，外加个人较懒，所以一直没有更新文章，以后一定会勤奋点，多加更新。正如前几天在stormzhang**(张哥)**的公众号里看到的一样，写作其实并不难，每个人都可以做到，但是长期坚持写作就非常难，这也是写作者想要长期创作遇到的第一个大问题，所以贵在坚持。另外长期写作的第二大问题是什么呢？你猜猜看，看看我们认为的是不是一样。\n\n最近主要是完成专业内的一些课程作业，比如Oracle数据库、JaveEE、搜索引擎等作业。国内大学总是会学很多课程，其实对多数学生来说，一些课程都不知道学着有什么意义。这点国外做的较是不错，在英国UWS当交换生的时候，可以选择自己喜欢的课程，这样也就有很大的兴趣去学习这些知识点。\n\n在解决这些课程作业之中，有件事感觉可以和大家分享一下。我们都知道计算机行业技术更新非常快，然而JavaEE老师教的知识点还是10多年前的内容，每次课程结束之中还需完成一个实验。但就是这样一个简单实验，却需要我们学生花费2天或者3天时间去完成。花这么长时间，按理来说应该很难吧，恰恰相反，实验很简单，那为什么还要花这么长时间呢。其实多数时间都是用在各种环境配置、参数设置、寻找各种jar包中，实在不需要写多少代码。比如我需要调用某个jar包，版本太高不行，版本太低不行，来来回回换个好几个，遇到问题想去查一些博客，竟然都是10年前的资料。完成一次实验之后，至此JaveEE的实验我再也没有去做，每次要交的时候，都是借用同学的电脑给老师展示一下，然后拿个分数就走。不是说我懒，没有什么探索、钻研精神，全然是因为学习这种东西实在没有什么用处，还浪费很多时间，不如利用这些时间去完成一些自己比较感兴趣的事情。\n\n另外需要声明一点的是，我的意思并不是旧的东西就没有用，而是强调在实用性和意义方面。比如数据结构、网络原理、操作系统，这样原理性的知识点，沉淀起来才是精华。但对于JavaEE这种实际开发技术来说，我认为过于陈旧的东西实在没有必要去学习。另外针对JaveEE开发这门课，任课老师为什么就不能更新一下知识点，来教一些更新的技术呢。既然如此，我的目标又不是追求多高多高的GPA，那么不如利用这些时间来解决一些自己比较感兴趣的问题，做一些有意义的事情较好。\n\n专业课程作业之外，还有一个实训作业，也就是从[中软杯](http://www.cnsoftbei.com/)12个题目之中选出来一个完成，然后进行答辩，由指导老师进行评分。这个我感觉还是比较有意思的，所以选了个网店工商信息图片文字提取的题目，然后花四天时间完成，下面主要和大家分享一下问题的解决思路。\n\n### 1.网店工商信息图片文字提取\n\n图片内容如下所示，但每张图片中信息出现的位置不尽相同，题目要求所写的程序能够完成如下几个功能点。\n\n+ 程序能够识别不同格式的图片，并能够提取所要求的信息。\n\n- 从图片之中提取企业注册号和企业名称信息，并保存到Excel表格之中。\n\n- 程序能够自动读取企业工商信息图片所在的文件夹路径。\n- 识别速度保持在60秒识别50张图片，识别正确率保证在95%以上。\n\n![网店工商信息图片文字提取01](http://p66yyzg4i.bkt.clouddn.com/%E7%BD%91%E5%BA%97%E5%B7%A5%E5%95%86%E4%BF%A1%E6%81%AF%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E6%8F%90%E5%8F%9601.png)\n\n### 2.Tess4j\n\n了解题目要求之后，我们便开始来解决问题。首先明确一点的是，肯定不能从头去写文字识别算法或者文字识别程序，OCR(Optical Character Recognition , 光学字符识别)发展这么多年来，开源的库肯定不少，只需找到适合中文识别的类库或者项目即可。\n\n个人采用的是Tess4j开源库，其中Tess4j是由Tesseract扩展而来，Tesseract是HP实验室开发由Google维护的开源OCR引擎，Tess4j支持Tiff,jpeg,gif,png,pdf等多种格式识别。我们只需要在[https://sourceforge.net/projects/tess4j/](https://sourceforge.net/projects/tess4j/)下载类库，然后编写下述代码便可实现文字识别，使用方法很简单。如果你要使用的话，请注意package,imageFile,instance的位置。\n\n```java\npackage net.sourceforge.tess4j.example;\n\nimport java.io.File;\nimport net.sourceforge.tess4j.*;\n\npublic class TesseractExample1 {\n    public static void main(String[] args) {        \n    \t\n        File imageFile = new File(\"/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao/1.png\");\n        ITesseract instance = new Tesseract();  // JNA Interface Mapping\n        // ITesseract instance = new Tesseract1(); // JNA Direct Mapping\n        instance.setDatapath(\"/Users/zhenhai/Downloads/SoftwareCup/Tess4J\");\n\t\tinstance.setLanguage(\"chi_sim\");\n\n        try {\n            String result = instance.doOCR(imageFile);\n            System.out.println(result);\n        } catch (TesseractException e) {\n            System.err.println(e.getMessage());\n        }\n    }\n}\n```\n\n### 3.网店工商信息图片文字提取\n\n下载的tess4j项目自带英文字体库，而我们需要识别中文信息，所以需下载中文简体字体库。字体库下载完成之后，将题目提供给我们的图片进行识别，识别之后发现准确率很低，而且识别时间过长，所以需要对图片进行处理。\n\n> 企业注册号 : 913302055612570鄄7 ′\n>\n> 企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬\n>\n> 类 型 霉嫣膘占辆 虫资) 趴辕~蓼唧 `\n>\n> 住惑7妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇\n>\n> 法 人: 杨禾口荣\n>\n> 成立时间:2010-08-26 甬 甬\n>\n> 注册资本 : 1000万人民币元 / /\n>\n> 营业I言【j目〖艮:2010-08洲:i墅o碾言壹 鹏 莹鬓、′墓示簪\n>\n> 经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵签稻昊信息的咨询 ; 服装i氦十犹撕{\n>\n> 菖〈茵珥跨止\\ 懦牌苣理 广告服务、 企业苣癫颧琨蓼 扩〈喔圃蓼′\n>\n> 登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局\n>\n> 核准时间 : 2015-12-24 __日q __日辄\n\n#### 3.1去除水印\n\n首先能够看到，提供的图片带有**天猫营业执照信息公示专用**水印，所以我们需要进行去水印处理。花了很长时间在网上找去水印的开源代码，但多数都需要先提供水印模版，然后才能进行去水印处理。水印模版不是问题，我们直接截取水印图片即可，但重点是去水印处理之后，水印去除效果并不是很理想。观察一番之后，发现所有的图片水印都是同一个颜色，那么我们是不是可以把水印的rgb值改为和背景色相同，实验之后发现果然可以，由于代码比较简单，此处就不再贴出代码，可以自行尝试一下。然后重新对图片进行识别，发现准确率还是很低，那是什么原因呢？\n\n> 企业注册号 : 913302055612570鄄7 ′\n>\n> 企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬\n>\n> 类 型 霉裴章膘占辆 虫资) 趴辕~苜趴 `\n>\n> 住惑)妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇\n>\n> 法 人: 杨禾口荣\n>\n> 成立时间:2010-08-26 甬 甬\n>\n> 注册资本 : 1000万人民币元 / /\n>\n> 营业I言【j目〖艮:2010-08洲:i墅o蔺言壹 鹏 莹鬓、′墓示簪\n>\n> 经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵垦稻昊信息的咨询 ; 眼装i氦十犹撕{\n>\n> 菖〈茵珥跨止\\ 懦牌苣理 广告目艮务、 企业苣癫颧寰蓼 扩〈喔圃蓼′\n>\n> 登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局\n>\n> 核准时间 : 2015-12-24 __日q __日辄\n\n#### 3.2图片格式转换\n\n将图片放大之后，发现图片处于全黑的状态，完全看不到任何字。然后尝试将图片格式转换为其他格式，比如jpg，当然这里不是直接改后缀名，而是利用在线图片格式转换工具。当然你也可以转换成其他格式，看看效果如何，转换完成之后，再次进行图片文字识别，发现准确率有较大提升。\n\n> 企业注丹舟号 : 9133020……612…70177\n>\n> 企业名称 : 宁波中哲慕尚电子商务有限公司\n>\n> 类 型 : 有限责任公司〈法人独资)\n>\n> 住 所 二 宁波市江才匕长兴路689弄22号11瞳A112室\n>\n> 法定代表人: 杨禾口荣\n>\n> 成立时间 : 2010-08-26\n>\n> 注册资本 : 1000万人民币元\n>\n> 营业期限 : 2010-08-26至2020-08-25\n>\n> 经菖范围 : 服装、 箱包、 鞋帽` 眼饰的批发` 零售、 网上批发` 零售及棺关信息的咨询 : 眼装i毓十\n>\n> 、 企业品牌营王里、 广 告眼务、 企业盲理咨询。\n>\n> 登记机关 : 浙江雀宁波市麦工才匕区工商肴壬政苣王里局\n>\n> 核准时间 : 2015-12-24\n\n#### 3.3分区域识别\n\n图片识别准确率有一定程度提升之后，但是时间还是很高，大概15s左右，所以需要进一步优化。由于题目只需要我们识别企业注册号和企业名称，所以没有必要识别整张图片。但我们又不知道企业名称和企业注册号处于图片的什么位置，所以只能扫描着进行文字图片识别。我这里采用的是每次识别图片高度的18%，例第一次识别区域为0-18%，如果没有识别到我们所需的文字信息，下次识别图片15%-32%，这样就不会遇到文字刚好被识别区域切割的问题。\n\n那这样识别会识别到很多重复区域，时间怎么会提升呢？其实不然，观察题目所给的50张图片，其中有46张图片的信息都是在头部，那么第一次扫描便能得到所需的信息，综合来看时间有很大程度提升。另外如果长时间未能识别到某张图片信息，那么则自动放弃识别。更改程序后重新识别图片，时间有很大程度提升，每张图片的识别速度在3s左右。\n\n> 企业注册号 : 913302055612570177\n>\n> 企业名称 : 宁波中哲票尚电子商务有眼公司\n\n对于本张图片来说，企业注册号已经能够正确识别，但是企业名称还是有个别字错误，比如将**幕**识别成**票**，将**限**识别成**眼**，因此需要进一步优化。\n\n#### 3.4 图片二值化\n\n为进一步提高准确率，我们将图片二值化，然后再对图片放大10倍，其实放大倍数越高，识别准确率也应该越高。这里为了在时间和准确度之间做个平衡，对图片只放大10倍。\n\n```java\nBufferedImage textImage = ImageHelper.convertImageToGrayscale(ImageHelper.getSubImage(image, 0, startHeight, resetWidth, resetHeight));\n\ntextImage = ImageHelper.convertImageToBinary(textImage);\n\ntextImage = ImageHelper.getScaledInstance(textImage, textImage.getWidth() * 10, textImage.getHeight() * 10);\n\n```\n\n图片放大10倍之后，我们再次对图片进行识别，发现企业注册号和企业名称完全正确。\n\n> 企业注册号 : 913302055612570177\n>\n> 企业名称 : 宁波中哲幕尚电子商务有限公司\n\n#### 3.5图片模糊寻找和结果导出\n\n图片模糊寻找的意思也就是，给出图片文件夹的大致路径，然后程序能够找到正确的图片路径，并能够正确进行文字识别。比如给定/Users/zhenhai/Downloads/SoftwareCup/Tess4J路径，程序能够找到/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao1/1.jpg路径。然后将识别到的结果导出到Excel表格，问题也很简单，这里也就不给出相应代码。\n\n![网店工商信息图片文字提取02](http://p66yyzg4i.bkt.clouddn.com/%E7%BD%91%E5%BA%97%E5%B7%A5%E5%95%86%E4%BF%A1%E6%81%AF%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E6%8F%90%E5%8F%9602.png)\n\n至此已经能够识别图片，而且准确率挺不错，准确率在95%左右，识别成功之后也能够正常导出至Excel表格。但时间依旧不理想，目前识别50张图片大概在2分钟30s左右。\n\n### 4.待优化\n\n图片文字识别方面只做了4天，之前也没有做过相关问题，所以还是有很大的优化空间。\n\n+ 利用多线程，识别时间应该能够减少1分钟，达到1分钟30s识别50张(猜测)。\n+ 由于我们直接利用网上的字库，没有对字库做任何训练。比如可以将出现频率较高的词设置更高的优先级，这样不仅能够提高准确率，而且能够进一步降低时间，比如上述的**限**不会再识别成**眼**。\n+ 图片大小不一，可以将图片设置为平均宽度和高度，然后再进行分区域识别。而且每次识别时候不是识别企业注册号和企业名称的完整信息，而只是试探识别这几个字，如果识别成功之后，然后再扩大识别宽度，提取所需要的完整信息。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/网店工商信息图片文字提取.md","raw":"---\ntitle: 网店工商信息图片文字提取\ndate: 2018-06-10 21:00:24\ntags: [文字识别]\ncategories: 文字识别\ncomments: true\n---\n首先非常抱歉，最近一段时间由于学校课程作业较多，外加个人较懒，所以一直没有更新文章，以后一定会勤奋点，多加更新。正如前几天在stormzhang**(张哥)**的公众号里看到的一样，写作其实并不难，每个人都可以做到，但是长期坚持写作就非常难，这也是写作者想要长期创作遇到的第一个大问题，所以贵在坚持。另外长期写作的第二大问题是什么呢？你猜猜看，看看我们认为的是不是一样。\n\n最近主要是完成专业内的一些课程作业，比如Oracle数据库、JaveEE、搜索引擎等作业。国内大学总是会学很多课程，其实对多数学生来说，一些课程都不知道学着有什么意义。这点国外做的较是不错，在英国UWS当交换生的时候，可以选择自己喜欢的课程，这样也就有很大的兴趣去学习这些知识点。\n\n在解决这些课程作业之中，有件事感觉可以和大家分享一下。我们都知道计算机行业技术更新非常快，然而JavaEE老师教的知识点还是10多年前的内容，每次课程结束之中还需完成一个实验。但就是这样一个简单实验，却需要我们学生花费2天或者3天时间去完成。花这么长时间，按理来说应该很难吧，恰恰相反，实验很简单，那为什么还要花这么长时间呢。其实多数时间都是用在各种环境配置、参数设置、寻找各种jar包中，实在不需要写多少代码。比如我需要调用某个jar包，版本太高不行，版本太低不行，来来回回换个好几个，遇到问题想去查一些博客，竟然都是10年前的资料。完成一次实验之后，至此JaveEE的实验我再也没有去做，每次要交的时候，都是借用同学的电脑给老师展示一下，然后拿个分数就走。不是说我懒，没有什么探索、钻研精神，全然是因为学习这种东西实在没有什么用处，还浪费很多时间，不如利用这些时间去完成一些自己比较感兴趣的事情。\n\n另外需要声明一点的是，我的意思并不是旧的东西就没有用，而是强调在实用性和意义方面。比如数据结构、网络原理、操作系统，这样原理性的知识点，沉淀起来才是精华。但对于JavaEE这种实际开发技术来说，我认为过于陈旧的东西实在没有必要去学习。另外针对JaveEE开发这门课，任课老师为什么就不能更新一下知识点，来教一些更新的技术呢。既然如此，我的目标又不是追求多高多高的GPA，那么不如利用这些时间来解决一些自己比较感兴趣的问题，做一些有意义的事情较好。\n\n专业课程作业之外，还有一个实训作业，也就是从[中软杯](http://www.cnsoftbei.com/)12个题目之中选出来一个完成，然后进行答辩，由指导老师进行评分。这个我感觉还是比较有意思的，所以选了个网店工商信息图片文字提取的题目，然后花四天时间完成，下面主要和大家分享一下问题的解决思路。\n\n### 1.网店工商信息图片文字提取\n\n图片内容如下所示，但每张图片中信息出现的位置不尽相同，题目要求所写的程序能够完成如下几个功能点。\n\n+ 程序能够识别不同格式的图片，并能够提取所要求的信息。\n\n- 从图片之中提取企业注册号和企业名称信息，并保存到Excel表格之中。\n\n- 程序能够自动读取企业工商信息图片所在的文件夹路径。\n- 识别速度保持在60秒识别50张图片，识别正确率保证在95%以上。\n\n![网店工商信息图片文字提取01](http://p66yyzg4i.bkt.clouddn.com/%E7%BD%91%E5%BA%97%E5%B7%A5%E5%95%86%E4%BF%A1%E6%81%AF%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E6%8F%90%E5%8F%9601.png)\n\n### 2.Tess4j\n\n了解题目要求之后，我们便开始来解决问题。首先明确一点的是，肯定不能从头去写文字识别算法或者文字识别程序，OCR(Optical Character Recognition , 光学字符识别)发展这么多年来，开源的库肯定不少，只需找到适合中文识别的类库或者项目即可。\n\n个人采用的是Tess4j开源库，其中Tess4j是由Tesseract扩展而来，Tesseract是HP实验室开发由Google维护的开源OCR引擎，Tess4j支持Tiff,jpeg,gif,png,pdf等多种格式识别。我们只需要在[https://sourceforge.net/projects/tess4j/](https://sourceforge.net/projects/tess4j/)下载类库，然后编写下述代码便可实现文字识别，使用方法很简单。如果你要使用的话，请注意package,imageFile,instance的位置。\n\n```java\npackage net.sourceforge.tess4j.example;\n\nimport java.io.File;\nimport net.sourceforge.tess4j.*;\n\npublic class TesseractExample1 {\n    public static void main(String[] args) {        \n    \t\n        File imageFile = new File(\"/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao/1.png\");\n        ITesseract instance = new Tesseract();  // JNA Interface Mapping\n        // ITesseract instance = new Tesseract1(); // JNA Direct Mapping\n        instance.setDatapath(\"/Users/zhenhai/Downloads/SoftwareCup/Tess4J\");\n\t\tinstance.setLanguage(\"chi_sim\");\n\n        try {\n            String result = instance.doOCR(imageFile);\n            System.out.println(result);\n        } catch (TesseractException e) {\n            System.err.println(e.getMessage());\n        }\n    }\n}\n```\n\n### 3.网店工商信息图片文字提取\n\n下载的tess4j项目自带英文字体库，而我们需要识别中文信息，所以需下载中文简体字体库。字体库下载完成之后，将题目提供给我们的图片进行识别，识别之后发现准确率很低，而且识别时间过长，所以需要对图片进行处理。\n\n> 企业注册号 : 913302055612570鄄7 ′\n>\n> 企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬\n>\n> 类 型 霉嫣膘占辆 虫资) 趴辕~蓼唧 `\n>\n> 住惑7妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇\n>\n> 法 人: 杨禾口荣\n>\n> 成立时间:2010-08-26 甬 甬\n>\n> 注册资本 : 1000万人民币元 / /\n>\n> 营业I言【j目〖艮:2010-08洲:i墅o碾言壹 鹏 莹鬓、′墓示簪\n>\n> 经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵签稻昊信息的咨询 ; 服装i氦十犹撕{\n>\n> 菖〈茵珥跨止\\ 懦牌苣理 广告服务、 企业苣癫颧琨蓼 扩〈喔圃蓼′\n>\n> 登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局\n>\n> 核准时间 : 2015-12-24 __日q __日辄\n\n#### 3.1去除水印\n\n首先能够看到，提供的图片带有**天猫营业执照信息公示专用**水印，所以我们需要进行去水印处理。花了很长时间在网上找去水印的开源代码，但多数都需要先提供水印模版，然后才能进行去水印处理。水印模版不是问题，我们直接截取水印图片即可，但重点是去水印处理之后，水印去除效果并不是很理想。观察一番之后，发现所有的图片水印都是同一个颜色，那么我们是不是可以把水印的rgb值改为和背景色相同，实验之后发现果然可以，由于代码比较简单，此处就不再贴出代码，可以自行尝试一下。然后重新对图片进行识别，发现准确率还是很低，那是什么原因呢？\n\n> 企业注册号 : 913302055612570鄄7 ′\n>\n> 企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬\n>\n> 类 型 霉裴章膘占辆 虫资) 趴辕~苜趴 `\n>\n> 住惑)妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇\n>\n> 法 人: 杨禾口荣\n>\n> 成立时间:2010-08-26 甬 甬\n>\n> 注册资本 : 1000万人民币元 / /\n>\n> 营业I言【j目〖艮:2010-08洲:i墅o蔺言壹 鹏 莹鬓、′墓示簪\n>\n> 经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵垦稻昊信息的咨询 ; 眼装i氦十犹撕{\n>\n> 菖〈茵珥跨止\\ 懦牌苣理 广告目艮务、 企业苣癫颧寰蓼 扩〈喔圃蓼′\n>\n> 登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局\n>\n> 核准时间 : 2015-12-24 __日q __日辄\n\n#### 3.2图片格式转换\n\n将图片放大之后，发现图片处于全黑的状态，完全看不到任何字。然后尝试将图片格式转换为其他格式，比如jpg，当然这里不是直接改后缀名，而是利用在线图片格式转换工具。当然你也可以转换成其他格式，看看效果如何，转换完成之后，再次进行图片文字识别，发现准确率有较大提升。\n\n> 企业注丹舟号 : 9133020……612…70177\n>\n> 企业名称 : 宁波中哲慕尚电子商务有限公司\n>\n> 类 型 : 有限责任公司〈法人独资)\n>\n> 住 所 二 宁波市江才匕长兴路689弄22号11瞳A112室\n>\n> 法定代表人: 杨禾口荣\n>\n> 成立时间 : 2010-08-26\n>\n> 注册资本 : 1000万人民币元\n>\n> 营业期限 : 2010-08-26至2020-08-25\n>\n> 经菖范围 : 服装、 箱包、 鞋帽` 眼饰的批发` 零售、 网上批发` 零售及棺关信息的咨询 : 眼装i毓十\n>\n> 、 企业品牌营王里、 广 告眼务、 企业盲理咨询。\n>\n> 登记机关 : 浙江雀宁波市麦工才匕区工商肴壬政苣王里局\n>\n> 核准时间 : 2015-12-24\n\n#### 3.3分区域识别\n\n图片识别准确率有一定程度提升之后，但是时间还是很高，大概15s左右，所以需要进一步优化。由于题目只需要我们识别企业注册号和企业名称，所以没有必要识别整张图片。但我们又不知道企业名称和企业注册号处于图片的什么位置，所以只能扫描着进行文字图片识别。我这里采用的是每次识别图片高度的18%，例第一次识别区域为0-18%，如果没有识别到我们所需的文字信息，下次识别图片15%-32%，这样就不会遇到文字刚好被识别区域切割的问题。\n\n那这样识别会识别到很多重复区域，时间怎么会提升呢？其实不然，观察题目所给的50张图片，其中有46张图片的信息都是在头部，那么第一次扫描便能得到所需的信息，综合来看时间有很大程度提升。另外如果长时间未能识别到某张图片信息，那么则自动放弃识别。更改程序后重新识别图片，时间有很大程度提升，每张图片的识别速度在3s左右。\n\n> 企业注册号 : 913302055612570177\n>\n> 企业名称 : 宁波中哲票尚电子商务有眼公司\n\n对于本张图片来说，企业注册号已经能够正确识别，但是企业名称还是有个别字错误，比如将**幕**识别成**票**，将**限**识别成**眼**，因此需要进一步优化。\n\n#### 3.4 图片二值化\n\n为进一步提高准确率，我们将图片二值化，然后再对图片放大10倍，其实放大倍数越高，识别准确率也应该越高。这里为了在时间和准确度之间做个平衡，对图片只放大10倍。\n\n```java\nBufferedImage textImage = ImageHelper.convertImageToGrayscale(ImageHelper.getSubImage(image, 0, startHeight, resetWidth, resetHeight));\n\ntextImage = ImageHelper.convertImageToBinary(textImage);\n\ntextImage = ImageHelper.getScaledInstance(textImage, textImage.getWidth() * 10, textImage.getHeight() * 10);\n\n```\n\n图片放大10倍之后，我们再次对图片进行识别，发现企业注册号和企业名称完全正确。\n\n> 企业注册号 : 913302055612570177\n>\n> 企业名称 : 宁波中哲幕尚电子商务有限公司\n\n#### 3.5图片模糊寻找和结果导出\n\n图片模糊寻找的意思也就是，给出图片文件夹的大致路径，然后程序能够找到正确的图片路径，并能够正确进行文字识别。比如给定/Users/zhenhai/Downloads/SoftwareCup/Tess4J路径，程序能够找到/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao1/1.jpg路径。然后将识别到的结果导出到Excel表格，问题也很简单，这里也就不给出相应代码。\n\n![网店工商信息图片文字提取02](http://p66yyzg4i.bkt.clouddn.com/%E7%BD%91%E5%BA%97%E5%B7%A5%E5%95%86%E4%BF%A1%E6%81%AF%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E6%8F%90%E5%8F%9602.png)\n\n至此已经能够识别图片，而且准确率挺不错，准确率在95%左右，识别成功之后也能够正常导出至Excel表格。但时间依旧不理想，目前识别50张图片大概在2分钟30s左右。\n\n### 4.待优化\n\n图片文字识别方面只做了4天，之前也没有做过相关问题，所以还是有很大的优化空间。\n\n+ 利用多线程，识别时间应该能够减少1分钟，达到1分钟30s识别50张(猜测)。\n+ 由于我们直接利用网上的字库，没有对字库做任何训练。比如可以将出现频率较高的词设置更高的优先级，这样不仅能够提高准确率，而且能够进一步降低时间，比如上述的**限**不会再识别成**眼**。\n+ 图片大小不一，可以将图片设置为平均宽度和高度，然后再进行分区域识别。而且每次识别时候不是识别企业注册号和企业名称的完整信息，而只是试探识别这几个字，如果识别成功之后，然后再扩大识别宽度，提取所需要的完整信息。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"网店工商信息图片文字提取","published":1,"updated":"2018-06-11T12:57:07.000Z","layout":"post","photos":[],"link":"","_id":"cjktv5d57002ijiz5hufi4ixt","content":"<p>首先非常抱歉，最近一段时间由于学校课程作业较多，外加个人较懒，所以一直没有更新文章，以后一定会勤奋点，多加更新。正如前几天在stormzhang<strong>(张哥)</strong>的公众号里看到的一样，写作其实并不难，每个人都可以做到，但是长期坚持写作就非常难，这也是写作者想要长期创作遇到的第一个大问题，所以贵在坚持。另外长期写作的第二大问题是什么呢？你猜猜看，看看我们认为的是不是一样。</p>\n<p>最近主要是完成专业内的一些课程作业，比如Oracle数据库、JaveEE、搜索引擎等作业。国内大学总是会学很多课程，其实对多数学生来说，一些课程都不知道学着有什么意义。这点国外做的较是不错，在英国UWS当交换生的时候，可以选择自己喜欢的课程，这样也就有很大的兴趣去学习这些知识点。</p>\n<p>在解决这些课程作业之中，有件事感觉可以和大家分享一下。我们都知道计算机行业技术更新非常快，然而JavaEE老师教的知识点还是10多年前的内容，每次课程结束之中还需完成一个实验。但就是这样一个简单实验，却需要我们学生花费2天或者3天时间去完成。花这么长时间，按理来说应该很难吧，恰恰相反，实验很简单，那为什么还要花这么长时间呢。其实多数时间都是用在各种环境配置、参数设置、寻找各种jar包中，实在不需要写多少代码。比如我需要调用某个jar包，版本太高不行，版本太低不行，来来回回换个好几个，遇到问题想去查一些博客，竟然都是10年前的资料。完成一次实验之后，至此JaveEE的实验我再也没有去做，每次要交的时候，都是借用同学的电脑给老师展示一下，然后拿个分数就走。不是说我懒，没有什么探索、钻研精神，全然是因为学习这种东西实在没有什么用处，还浪费很多时间，不如利用这些时间去完成一些自己比较感兴趣的事情。</p>\n<p>另外需要声明一点的是，我的意思并不是旧的东西就没有用，而是强调在实用性和意义方面。比如数据结构、网络原理、操作系统，这样原理性的知识点，沉淀起来才是精华。但对于JavaEE这种实际开发技术来说，我认为过于陈旧的东西实在没有必要去学习。另外针对JaveEE开发这门课，任课老师为什么就不能更新一下知识点，来教一些更新的技术呢。既然如此，我的目标又不是追求多高多高的GPA，那么不如利用这些时间来解决一些自己比较感兴趣的问题，做一些有意义的事情较好。</p>\n<p>专业课程作业之外，还有一个实训作业，也就是从<a href=\"http://www.cnsoftbei.com/\" target=\"_blank\" rel=\"noopener\">中软杯</a>12个题目之中选出来一个完成，然后进行答辩，由指导老师进行评分。这个我感觉还是比较有意思的，所以选了个网店工商信息图片文字提取的题目，然后花四天时间完成，下面主要和大家分享一下问题的解决思路。</p>\n<h3 id=\"1-网店工商信息图片文字提取\"><a href=\"#1-网店工商信息图片文字提取\" class=\"headerlink\" title=\"1.网店工商信息图片文字提取\"></a>1.网店工商信息图片文字提取</h3><p>图片内容如下所示，但每张图片中信息出现的位置不尽相同，题目要求所写的程序能够完成如下几个功能点。</p>\n<ul>\n<li>程序能够识别不同格式的图片，并能够提取所要求的信息。</li>\n</ul>\n<ul>\n<li><p>从图片之中提取企业注册号和企业名称信息，并保存到Excel表格之中。</p>\n</li>\n<li><p>程序能够自动读取企业工商信息图片所在的文件夹路径。</p>\n</li>\n<li>识别速度保持在60秒识别50张图片，识别正确率保证在95%以上。</li>\n</ul>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BD%91%E5%BA%97%E5%B7%A5%E5%95%86%E4%BF%A1%E6%81%AF%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E6%8F%90%E5%8F%9601.png\" alt=\"网店工商信息图片文字提取01\"></p>\n<h3 id=\"2-Tess4j\"><a href=\"#2-Tess4j\" class=\"headerlink\" title=\"2.Tess4j\"></a>2.Tess4j</h3><p>了解题目要求之后，我们便开始来解决问题。首先明确一点的是，肯定不能从头去写文字识别算法或者文字识别程序，OCR(Optical Character Recognition , 光学字符识别)发展这么多年来，开源的库肯定不少，只需找到适合中文识别的类库或者项目即可。</p>\n<p>个人采用的是Tess4j开源库，其中Tess4j是由Tesseract扩展而来，Tesseract是HP实验室开发由Google维护的开源OCR引擎，Tess4j支持Tiff,jpeg,gif,png,pdf等多种格式识别。我们只需要在<a href=\"https://sourceforge.net/projects/tess4j/\" target=\"_blank\" rel=\"noopener\">https://sourceforge.net/projects/tess4j/</a>下载类库，然后编写下述代码便可实现文字识别，使用方法很简单。如果你要使用的话，请注意package,imageFile,instance的位置。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> net.sourceforge.tess4j.example;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.File;</span><br><span class=\"line\"><span class=\"keyword\">import</span> net.sourceforge.tess4j.*;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TesseractExample1</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;        </span><br><span class=\"line\">    \t</span><br><span class=\"line\">        File imageFile = <span class=\"keyword\">new</span> File(<span class=\"string\">\"/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao/1.png\"</span>);</span><br><span class=\"line\">        ITesseract instance = <span class=\"keyword\">new</span> Tesseract();  <span class=\"comment\">// JNA Interface Mapping</span></span><br><span class=\"line\">        <span class=\"comment\">// ITesseract instance = new Tesseract1(); // JNA Direct Mapping</span></span><br><span class=\"line\">        instance.setDatapath(<span class=\"string\">\"/Users/zhenhai/Downloads/SoftwareCup/Tess4J\"</span>);</span><br><span class=\"line\">\t\tinstance.setLanguage(<span class=\"string\">\"chi_sim\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            String result = instance.doOCR(imageFile);</span><br><span class=\"line\">            System.out.println(result);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (TesseractException e) &#123;</span><br><span class=\"line\">            System.err.println(e.getMessage());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-网店工商信息图片文字提取\"><a href=\"#3-网店工商信息图片文字提取\" class=\"headerlink\" title=\"3.网店工商信息图片文字提取\"></a>3.网店工商信息图片文字提取</h3><p>下载的tess4j项目自带英文字体库，而我们需要识别中文信息，所以需下载中文简体字体库。字体库下载完成之后，将题目提供给我们的图片进行识别，识别之后发现准确率很低，而且识别时间过长，所以需要对图片进行处理。</p>\n<blockquote>\n<p>企业注册号 : 913302055612570鄄7 ′</p>\n<p>企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬</p>\n<p>类 型 霉嫣膘占辆 虫资) 趴辕~蓼唧 `</p>\n<p>住惑7妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇</p>\n<p>法 人: 杨禾口荣</p>\n<p>成立时间:2010-08-26 甬 甬</p>\n<p>注册资本 : 1000万人民币元 / /</p>\n<p>营业I言【j目〖艮:2010-08洲:i墅o碾言壹 鹏 莹鬓、′墓示簪</p>\n<p>经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵签稻昊信息的咨询 ; 服装i氦十犹撕{</p>\n<p>菖〈茵珥跨止\\ 懦牌苣理 广告服务、 企业苣癫颧琨蓼 扩〈喔圃蓼′</p>\n<p>登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局</p>\n<p>核准时间 : 2015-12-24 <strong>日q </strong>日辄</p>\n</blockquote>\n<h4 id=\"3-1去除水印\"><a href=\"#3-1去除水印\" class=\"headerlink\" title=\"3.1去除水印\"></a>3.1去除水印</h4><p>首先能够看到，提供的图片带有<strong>天猫营业执照信息公示专用</strong>水印，所以我们需要进行去水印处理。花了很长时间在网上找去水印的开源代码，但多数都需要先提供水印模版，然后才能进行去水印处理。水印模版不是问题，我们直接截取水印图片即可，但重点是去水印处理之后，水印去除效果并不是很理想。观察一番之后，发现所有的图片水印都是同一个颜色，那么我们是不是可以把水印的rgb值改为和背景色相同，实验之后发现果然可以，由于代码比较简单，此处就不再贴出代码，可以自行尝试一下。然后重新对图片进行识别，发现准确率还是很低，那是什么原因呢？</p>\n<blockquote>\n<p>企业注册号 : 913302055612570鄄7 ′</p>\n<p>企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬</p>\n<p>类 型 霉裴章膘占辆 虫资) 趴辕~苜趴 `</p>\n<p>住惑)妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇</p>\n<p>法 人: 杨禾口荣</p>\n<p>成立时间:2010-08-26 甬 甬</p>\n<p>注册资本 : 1000万人民币元 / /</p>\n<p>营业I言【j目〖艮:2010-08洲:i墅o蔺言壹 鹏 莹鬓、′墓示簪</p>\n<p>经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵垦稻昊信息的咨询 ; 眼装i氦十犹撕{</p>\n<p>菖〈茵珥跨止\\ 懦牌苣理 广告目艮务、 企业苣癫颧寰蓼 扩〈喔圃蓼′</p>\n<p>登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局</p>\n<p>核准时间 : 2015-12-24 <strong>日q </strong>日辄</p>\n</blockquote>\n<h4 id=\"3-2图片格式转换\"><a href=\"#3-2图片格式转换\" class=\"headerlink\" title=\"3.2图片格式转换\"></a>3.2图片格式转换</h4><p>将图片放大之后，发现图片处于全黑的状态，完全看不到任何字。然后尝试将图片格式转换为其他格式，比如jpg，当然这里不是直接改后缀名，而是利用在线图片格式转换工具。当然你也可以转换成其他格式，看看效果如何，转换完成之后，再次进行图片文字识别，发现准确率有较大提升。</p>\n<blockquote>\n<p>企业注丹舟号 : 9133020……612…70177</p>\n<p>企业名称 : 宁波中哲慕尚电子商务有限公司</p>\n<p>类 型 : 有限责任公司〈法人独资)</p>\n<p>住 所 二 宁波市江才匕长兴路689弄22号11瞳A112室</p>\n<p>法定代表人: 杨禾口荣</p>\n<p>成立时间 : 2010-08-26</p>\n<p>注册资本 : 1000万人民币元</p>\n<p>营业期限 : 2010-08-26至2020-08-25</p>\n<p>经菖范围 : 服装、 箱包、 鞋帽<code>眼饰的批发</code> 零售、 网上批发` 零售及棺关信息的咨询 : 眼装i毓十</p>\n<p>、 企业品牌营王里、 广 告眼务、 企业盲理咨询。</p>\n<p>登记机关 : 浙江雀宁波市麦工才匕区工商肴壬政苣王里局</p>\n<p>核准时间 : 2015-12-24</p>\n</blockquote>\n<h4 id=\"3-3分区域识别\"><a href=\"#3-3分区域识别\" class=\"headerlink\" title=\"3.3分区域识别\"></a>3.3分区域识别</h4><p>图片识别准确率有一定程度提升之后，但是时间还是很高，大概15s左右，所以需要进一步优化。由于题目只需要我们识别企业注册号和企业名称，所以没有必要识别整张图片。但我们又不知道企业名称和企业注册号处于图片的什么位置，所以只能扫描着进行文字图片识别。我这里采用的是每次识别图片高度的18%，例第一次识别区域为0-18%，如果没有识别到我们所需的文字信息，下次识别图片15%-32%，这样就不会遇到文字刚好被识别区域切割的问题。</p>\n<p>那这样识别会识别到很多重复区域，时间怎么会提升呢？其实不然，观察题目所给的50张图片，其中有46张图片的信息都是在头部，那么第一次扫描便能得到所需的信息，综合来看时间有很大程度提升。另外如果长时间未能识别到某张图片信息，那么则自动放弃识别。更改程序后重新识别图片，时间有很大程度提升，每张图片的识别速度在3s左右。</p>\n<blockquote>\n<p>企业注册号 : 913302055612570177</p>\n<p>企业名称 : 宁波中哲票尚电子商务有眼公司</p>\n</blockquote>\n<p>对于本张图片来说，企业注册号已经能够正确识别，但是企业名称还是有个别字错误，比如将<strong>幕</strong>识别成<strong>票</strong>，将<strong>限</strong>识别成<strong>眼</strong>，因此需要进一步优化。</p>\n<h4 id=\"3-4-图片二值化\"><a href=\"#3-4-图片二值化\" class=\"headerlink\" title=\"3.4 图片二值化\"></a>3.4 图片二值化</h4><p>为进一步提高准确率，我们将图片二值化，然后再对图片放大10倍，其实放大倍数越高，识别准确率也应该越高。这里为了在时间和准确度之间做个平衡，对图片只放大10倍。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BufferedImage textImage = ImageHelper.convertImageToGrayscale(ImageHelper.getSubImage(image, <span class=\"number\">0</span>, startHeight, resetWidth, resetHeight));</span><br><span class=\"line\"></span><br><span class=\"line\">textImage = ImageHelper.convertImageToBinary(textImage);</span><br><span class=\"line\"></span><br><span class=\"line\">textImage = ImageHelper.getScaledInstance(textImage, textImage.getWidth() * <span class=\"number\">10</span>, textImage.getHeight() * <span class=\"number\">10</span>);</span><br></pre></td></tr></table></figure>\n<p>图片放大10倍之后，我们再次对图片进行识别，发现企业注册号和企业名称完全正确。</p>\n<blockquote>\n<p>企业注册号 : 913302055612570177</p>\n<p>企业名称 : 宁波中哲幕尚电子商务有限公司</p>\n</blockquote>\n<h4 id=\"3-5图片模糊寻找和结果导出\"><a href=\"#3-5图片模糊寻找和结果导出\" class=\"headerlink\" title=\"3.5图片模糊寻找和结果导出\"></a>3.5图片模糊寻找和结果导出</h4><p>图片模糊寻找的意思也就是，给出图片文件夹的大致路径，然后程序能够找到正确的图片路径，并能够正确进行文字识别。比如给定/Users/zhenhai/Downloads/SoftwareCup/Tess4J路径，程序能够找到/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao1/1.jpg路径。然后将识别到的结果导出到Excel表格，问题也很简单，这里也就不给出相应代码。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BD%91%E5%BA%97%E5%B7%A5%E5%95%86%E4%BF%A1%E6%81%AF%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E6%8F%90%E5%8F%9602.png\" alt=\"网店工商信息图片文字提取02\"></p>\n<p>至此已经能够识别图片，而且准确率挺不错，准确率在95%左右，识别成功之后也能够正常导出至Excel表格。但时间依旧不理想，目前识别50张图片大概在2分钟30s左右。</p>\n<h3 id=\"4-待优化\"><a href=\"#4-待优化\" class=\"headerlink\" title=\"4.待优化\"></a>4.待优化</h3><p>图片文字识别方面只做了4天，之前也没有做过相关问题，所以还是有很大的优化空间。</p>\n<ul>\n<li>利用多线程，识别时间应该能够减少1分钟，达到1分钟30s识别50张(猜测)。</li>\n<li>由于我们直接利用网上的字库，没有对字库做任何训练。比如可以将出现频率较高的词设置更高的优先级，这样不仅能够提高准确率，而且能够进一步降低时间，比如上述的<strong>限</strong>不会再识别成<strong>眼</strong>。</li>\n<li>图片大小不一，可以将图片设置为平均宽度和高度，然后再进行分区域识别。而且每次识别时候不是识别企业注册号和企业名称的完整信息，而只是试探识别这几个字，如果识别成功之后，然后再扩大识别宽度，提取所需要的完整信息。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>首先非常抱歉，最近一段时间由于学校课程作业较多，外加个人较懒，所以一直没有更新文章，以后一定会勤奋点，多加更新。正如前几天在stormzhang<strong>(张哥)</strong>的公众号里看到的一样，写作其实并不难，每个人都可以做到，但是长期坚持写作就非常难，这也是写作者想要长期创作遇到的第一个大问题，所以贵在坚持。另外长期写作的第二大问题是什么呢？你猜猜看，看看我们认为的是不是一样。</p>\n<p>最近主要是完成专业内的一些课程作业，比如Oracle数据库、JaveEE、搜索引擎等作业。国内大学总是会学很多课程，其实对多数学生来说，一些课程都不知道学着有什么意义。这点国外做的较是不错，在英国UWS当交换生的时候，可以选择自己喜欢的课程，这样也就有很大的兴趣去学习这些知识点。</p>\n<p>在解决这些课程作业之中，有件事感觉可以和大家分享一下。我们都知道计算机行业技术更新非常快，然而JavaEE老师教的知识点还是10多年前的内容，每次课程结束之中还需完成一个实验。但就是这样一个简单实验，却需要我们学生花费2天或者3天时间去完成。花这么长时间，按理来说应该很难吧，恰恰相反，实验很简单，那为什么还要花这么长时间呢。其实多数时间都是用在各种环境配置、参数设置、寻找各种jar包中，实在不需要写多少代码。比如我需要调用某个jar包，版本太高不行，版本太低不行，来来回回换个好几个，遇到问题想去查一些博客，竟然都是10年前的资料。完成一次实验之后，至此JaveEE的实验我再也没有去做，每次要交的时候，都是借用同学的电脑给老师展示一下，然后拿个分数就走。不是说我懒，没有什么探索、钻研精神，全然是因为学习这种东西实在没有什么用处，还浪费很多时间，不如利用这些时间去完成一些自己比较感兴趣的事情。</p>\n<p>另外需要声明一点的是，我的意思并不是旧的东西就没有用，而是强调在实用性和意义方面。比如数据结构、网络原理、操作系统，这样原理性的知识点，沉淀起来才是精华。但对于JavaEE这种实际开发技术来说，我认为过于陈旧的东西实在没有必要去学习。另外针对JaveEE开发这门课，任课老师为什么就不能更新一下知识点，来教一些更新的技术呢。既然如此，我的目标又不是追求多高多高的GPA，那么不如利用这些时间来解决一些自己比较感兴趣的问题，做一些有意义的事情较好。</p>\n<p>专业课程作业之外，还有一个实训作业，也就是从<a href=\"http://www.cnsoftbei.com/\" target=\"_blank\" rel=\"noopener\">中软杯</a>12个题目之中选出来一个完成，然后进行答辩，由指导老师进行评分。这个我感觉还是比较有意思的，所以选了个网店工商信息图片文字提取的题目，然后花四天时间完成，下面主要和大家分享一下问题的解决思路。</p>\n<h3 id=\"1-网店工商信息图片文字提取\"><a href=\"#1-网店工商信息图片文字提取\" class=\"headerlink\" title=\"1.网店工商信息图片文字提取\"></a>1.网店工商信息图片文字提取</h3><p>图片内容如下所示，但每张图片中信息出现的位置不尽相同，题目要求所写的程序能够完成如下几个功能点。</p>\n<ul>\n<li>程序能够识别不同格式的图片，并能够提取所要求的信息。</li>\n</ul>\n<ul>\n<li><p>从图片之中提取企业注册号和企业名称信息，并保存到Excel表格之中。</p>\n</li>\n<li><p>程序能够自动读取企业工商信息图片所在的文件夹路径。</p>\n</li>\n<li>识别速度保持在60秒识别50张图片，识别正确率保证在95%以上。</li>\n</ul>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BD%91%E5%BA%97%E5%B7%A5%E5%95%86%E4%BF%A1%E6%81%AF%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E6%8F%90%E5%8F%9601.png\" alt=\"网店工商信息图片文字提取01\"></p>\n<h3 id=\"2-Tess4j\"><a href=\"#2-Tess4j\" class=\"headerlink\" title=\"2.Tess4j\"></a>2.Tess4j</h3><p>了解题目要求之后，我们便开始来解决问题。首先明确一点的是，肯定不能从头去写文字识别算法或者文字识别程序，OCR(Optical Character Recognition , 光学字符识别)发展这么多年来，开源的库肯定不少，只需找到适合中文识别的类库或者项目即可。</p>\n<p>个人采用的是Tess4j开源库，其中Tess4j是由Tesseract扩展而来，Tesseract是HP实验室开发由Google维护的开源OCR引擎，Tess4j支持Tiff,jpeg,gif,png,pdf等多种格式识别。我们只需要在<a href=\"https://sourceforge.net/projects/tess4j/\" target=\"_blank\" rel=\"noopener\">https://sourceforge.net/projects/tess4j/</a>下载类库，然后编写下述代码便可实现文字识别，使用方法很简单。如果你要使用的话，请注意package,imageFile,instance的位置。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> net.sourceforge.tess4j.example;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.File;</span><br><span class=\"line\"><span class=\"keyword\">import</span> net.sourceforge.tess4j.*;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TesseractExample1</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;        </span><br><span class=\"line\">    \t</span><br><span class=\"line\">        File imageFile = <span class=\"keyword\">new</span> File(<span class=\"string\">\"/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao/1.png\"</span>);</span><br><span class=\"line\">        ITesseract instance = <span class=\"keyword\">new</span> Tesseract();  <span class=\"comment\">// JNA Interface Mapping</span></span><br><span class=\"line\">        <span class=\"comment\">// ITesseract instance = new Tesseract1(); // JNA Direct Mapping</span></span><br><span class=\"line\">        instance.setDatapath(<span class=\"string\">\"/Users/zhenhai/Downloads/SoftwareCup/Tess4J\"</span>);</span><br><span class=\"line\">\t\tinstance.setLanguage(<span class=\"string\">\"chi_sim\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            String result = instance.doOCR(imageFile);</span><br><span class=\"line\">            System.out.println(result);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (TesseractException e) &#123;</span><br><span class=\"line\">            System.err.println(e.getMessage());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-网店工商信息图片文字提取\"><a href=\"#3-网店工商信息图片文字提取\" class=\"headerlink\" title=\"3.网店工商信息图片文字提取\"></a>3.网店工商信息图片文字提取</h3><p>下载的tess4j项目自带英文字体库，而我们需要识别中文信息，所以需下载中文简体字体库。字体库下载完成之后，将题目提供给我们的图片进行识别，识别之后发现准确率很低，而且识别时间过长，所以需要对图片进行处理。</p>\n<blockquote>\n<p>企业注册号 : 913302055612570鄄7 ′</p>\n<p>企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬</p>\n<p>类 型 霉嫣膘占辆 虫资) 趴辕~蓼唧 `</p>\n<p>住惑7妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇</p>\n<p>法 人: 杨禾口荣</p>\n<p>成立时间:2010-08-26 甬 甬</p>\n<p>注册资本 : 1000万人民币元 / /</p>\n<p>营业I言【j目〖艮:2010-08洲:i墅o碾言壹 鹏 莹鬓、′墓示簪</p>\n<p>经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵签稻昊信息的咨询 ; 服装i氦十犹撕{</p>\n<p>菖〈茵珥跨止\\ 懦牌苣理 广告服务、 企业苣癫颧琨蓼 扩〈喔圃蓼′</p>\n<p>登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局</p>\n<p>核准时间 : 2015-12-24 <strong>日q </strong>日辄</p>\n</blockquote>\n<h4 id=\"3-1去除水印\"><a href=\"#3-1去除水印\" class=\"headerlink\" title=\"3.1去除水印\"></a>3.1去除水印</h4><p>首先能够看到，提供的图片带有<strong>天猫营业执照信息公示专用</strong>水印，所以我们需要进行去水印处理。花了很长时间在网上找去水印的开源代码，但多数都需要先提供水印模版，然后才能进行去水印处理。水印模版不是问题，我们直接截取水印图片即可，但重点是去水印处理之后，水印去除效果并不是很理想。观察一番之后，发现所有的图片水印都是同一个颜色，那么我们是不是可以把水印的rgb值改为和背景色相同，实验之后发现果然可以，由于代码比较简单，此处就不再贴出代码，可以自行尝试一下。然后重新对图片进行识别，发现准确率还是很低，那是什么原因呢？</p>\n<blockquote>\n<p>企业注册号 : 913302055612570鄄7 ′</p>\n<p>企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬</p>\n<p>类 型 霉裴章膘占辆 虫资) 趴辕~苜趴 `</p>\n<p>住惑)妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇</p>\n<p>法 人: 杨禾口荣</p>\n<p>成立时间:2010-08-26 甬 甬</p>\n<p>注册资本 : 1000万人民币元 / /</p>\n<p>营业I言【j目〖艮:2010-08洲:i墅o蔺言壹 鹏 莹鬓、′墓示簪</p>\n<p>经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵垦稻昊信息的咨询 ; 眼装i氦十犹撕{</p>\n<p>菖〈茵珥跨止\\ 懦牌苣理 广告目艮务、 企业苣癫颧寰蓼 扩〈喔圃蓼′</p>\n<p>登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局</p>\n<p>核准时间 : 2015-12-24 <strong>日q </strong>日辄</p>\n</blockquote>\n<h4 id=\"3-2图片格式转换\"><a href=\"#3-2图片格式转换\" class=\"headerlink\" title=\"3.2图片格式转换\"></a>3.2图片格式转换</h4><p>将图片放大之后，发现图片处于全黑的状态，完全看不到任何字。然后尝试将图片格式转换为其他格式，比如jpg，当然这里不是直接改后缀名，而是利用在线图片格式转换工具。当然你也可以转换成其他格式，看看效果如何，转换完成之后，再次进行图片文字识别，发现准确率有较大提升。</p>\n<blockquote>\n<p>企业注丹舟号 : 9133020……612…70177</p>\n<p>企业名称 : 宁波中哲慕尚电子商务有限公司</p>\n<p>类 型 : 有限责任公司〈法人独资)</p>\n<p>住 所 二 宁波市江才匕长兴路689弄22号11瞳A112室</p>\n<p>法定代表人: 杨禾口荣</p>\n<p>成立时间 : 2010-08-26</p>\n<p>注册资本 : 1000万人民币元</p>\n<p>营业期限 : 2010-08-26至2020-08-25</p>\n<p>经菖范围 : 服装、 箱包、 鞋帽<code>眼饰的批发</code> 零售、 网上批发` 零售及棺关信息的咨询 : 眼装i毓十</p>\n<p>、 企业品牌营王里、 广 告眼务、 企业盲理咨询。</p>\n<p>登记机关 : 浙江雀宁波市麦工才匕区工商肴壬政苣王里局</p>\n<p>核准时间 : 2015-12-24</p>\n</blockquote>\n<h4 id=\"3-3分区域识别\"><a href=\"#3-3分区域识别\" class=\"headerlink\" title=\"3.3分区域识别\"></a>3.3分区域识别</h4><p>图片识别准确率有一定程度提升之后，但是时间还是很高，大概15s左右，所以需要进一步优化。由于题目只需要我们识别企业注册号和企业名称，所以没有必要识别整张图片。但我们又不知道企业名称和企业注册号处于图片的什么位置，所以只能扫描着进行文字图片识别。我这里采用的是每次识别图片高度的18%，例第一次识别区域为0-18%，如果没有识别到我们所需的文字信息，下次识别图片15%-32%，这样就不会遇到文字刚好被识别区域切割的问题。</p>\n<p>那这样识别会识别到很多重复区域，时间怎么会提升呢？其实不然，观察题目所给的50张图片，其中有46张图片的信息都是在头部，那么第一次扫描便能得到所需的信息，综合来看时间有很大程度提升。另外如果长时间未能识别到某张图片信息，那么则自动放弃识别。更改程序后重新识别图片，时间有很大程度提升，每张图片的识别速度在3s左右。</p>\n<blockquote>\n<p>企业注册号 : 913302055612570177</p>\n<p>企业名称 : 宁波中哲票尚电子商务有眼公司</p>\n</blockquote>\n<p>对于本张图片来说，企业注册号已经能够正确识别，但是企业名称还是有个别字错误，比如将<strong>幕</strong>识别成<strong>票</strong>，将<strong>限</strong>识别成<strong>眼</strong>，因此需要进一步优化。</p>\n<h4 id=\"3-4-图片二值化\"><a href=\"#3-4-图片二值化\" class=\"headerlink\" title=\"3.4 图片二值化\"></a>3.4 图片二值化</h4><p>为进一步提高准确率，我们将图片二值化，然后再对图片放大10倍，其实放大倍数越高，识别准确率也应该越高。这里为了在时间和准确度之间做个平衡，对图片只放大10倍。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BufferedImage textImage = ImageHelper.convertImageToGrayscale(ImageHelper.getSubImage(image, <span class=\"number\">0</span>, startHeight, resetWidth, resetHeight));</span><br><span class=\"line\"></span><br><span class=\"line\">textImage = ImageHelper.convertImageToBinary(textImage);</span><br><span class=\"line\"></span><br><span class=\"line\">textImage = ImageHelper.getScaledInstance(textImage, textImage.getWidth() * <span class=\"number\">10</span>, textImage.getHeight() * <span class=\"number\">10</span>);</span><br></pre></td></tr></table></figure>\n<p>图片放大10倍之后，我们再次对图片进行识别，发现企业注册号和企业名称完全正确。</p>\n<blockquote>\n<p>企业注册号 : 913302055612570177</p>\n<p>企业名称 : 宁波中哲幕尚电子商务有限公司</p>\n</blockquote>\n<h4 id=\"3-5图片模糊寻找和结果导出\"><a href=\"#3-5图片模糊寻找和结果导出\" class=\"headerlink\" title=\"3.5图片模糊寻找和结果导出\"></a>3.5图片模糊寻找和结果导出</h4><p>图片模糊寻找的意思也就是，给出图片文件夹的大致路径，然后程序能够找到正确的图片路径，并能够正确进行文字识别。比如给定/Users/zhenhai/Downloads/SoftwareCup/Tess4J路径，程序能够找到/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao1/1.jpg路径。然后将识别到的结果导出到Excel表格，问题也很简单，这里也就不给出相应代码。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BD%91%E5%BA%97%E5%B7%A5%E5%95%86%E4%BF%A1%E6%81%AF%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E6%8F%90%E5%8F%9602.png\" alt=\"网店工商信息图片文字提取02\"></p>\n<p>至此已经能够识别图片，而且准确率挺不错，准确率在95%左右，识别成功之后也能够正常导出至Excel表格。但时间依旧不理想，目前识别50张图片大概在2分钟30s左右。</p>\n<h3 id=\"4-待优化\"><a href=\"#4-待优化\" class=\"headerlink\" title=\"4.待优化\"></a>4.待优化</h3><p>图片文字识别方面只做了4天，之前也没有做过相关问题，所以还是有很大的优化空间。</p>\n<ul>\n<li>利用多线程，识别时间应该能够减少1分钟，达到1分钟30s识别50张(猜测)。</li>\n<li>由于我们直接利用网上的字库，没有对字库做任何训练。比如可以将出现频率较高的词设置更高的优先级，这样不仅能够提高准确率，而且能够进一步降低时间，比如上述的<strong>限</strong>不会再识别成<strong>眼</strong>。</li>\n<li>图片大小不一，可以将图片设置为平均宽度和高度，然后再进行分区域识别。而且每次识别时候不是识别企业注册号和企业名称的完整信息，而只是试探识别这几个字，如果识别成功之后，然后再扩大识别宽度，提取所需要的完整信息。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"《剑指Offer》Python版","date":"2018-07-29T07:02:01.000Z","mathjax":true,"_content":"\n### 1.二维数组中的查找\n\n**题目：** 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n\n**思路：**遍历每一行，查找该元素是否在该行之中。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # array 二维列表\n    def Find(self, target, array):\n        # write code here\n        for line in array:\n            if target in line:\n                return True\n        return False\n\nif __name__=='__main__':\n    target=2\n    array=[[1,2,3,4],[2,3,4,5],[3,4,5,6],[4,5,6,7]]\n    solution=Solution()\n    ans=solution.Find(target,array)\n    print(ans)\n```\n\n### 2.替换空格\n\n**题目：** 请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。\n\n**思路：**利用字符串中的replace直接替换即可。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # s 源字符串\n    def replaceSpace(self, s):\n        # write code here\n        temp = s.replace(\" \", \"%20\")\n        return temp\n\nif __name__=='__main__':\n    s='We Are Happy'\n    solution=Solution()\n    ans=solution.replaceSpace(s)\n    print(ans)\n```\n\n### 3.从尾到头打印链表\n\n**题目：**输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。\n\n**思路：**将链表中的值记录到list之中，然后进行翻转list。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    # 返回从尾部到头部的列表值序列，例如[1,2,3]\n    def printListFromTailToHead(self, listNode):\n        # write code here\n        l=[]\n        while listNode:\n            l.append(listNode.val)\n            listNode=listNode.next\n        return l[::-1]\n\nif __name__=='__main__':\n    A1 = ListNode(1)\n    A2 = ListNode(2)\n    A3 = ListNode(3)\n    A4 = ListNode(4)\n    A5 = ListNode(5)\n\n    A1.next=A2\n    A2.next=A3\n    A3.next=A4\n    A4.next=A5\n\n    solution=Solution()\n    ans=solution.printListFromTailToHead(A1)\n    print(ans)\n```\n\n### 4.重建二叉树\n\n**题目：**输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。\n\n**题解：**首先前序遍历的第一个元素为二叉树的根结点，那么便能够在中序遍历之中找到根节点，那么在根结点左侧则是左子树，假设长度为M.在根结点右侧，便是右子树,假设长度为N。然后在前序遍历根节点后面M长度的便是左子树的前序遍历序列，再后面的N个长度便是右子树的后序遍历的长度。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    # 返回构造的TreeNode根节点\n    def reConstructBinaryTree(self, pre, tin):\n        # write code here\n        if len(pre)==0:\n            return None\n        if len(pre)==1:\n            return TreeNode(pre[0])\n        else:\n            flag=TreeNode(pre[0])\n            flag.left=self.reConstructBinaryTree(pre[1:tin.index(pre[0])+1],tin[:tin.index(pre[0])])\n            flag.right=self.reConstructBinaryTree(pre[tin.index(pre[0])+1:],tin[tin.index(pre[0])+1:])\n        return flag\n\nif __name__=='__main__':\n    solution=Solution()\n    pre=list(map(int,input().split(',')))\n    tin=list(map(int,input().split(',')))\n    ans=solution.reConstructBinaryTree(pre,tin)\n    print(ans.val)\n```\n\n### 5.用两个栈实现队列\n\n**题目：**用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。\n\n**题解：**申请两个栈Stack1和Stack2，Stack1当作输入，Stack2当作pop。当Stack2空的时候，将Stack1进行反转，并且输入到Stack2。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def __init__(self):\n        self.Stack1=[]\n        self.Stack2=[]\n    def push(self, node):\n        # write code here\n        self.Stack1.append(node)\n    def pop(self):\n        # return xx\n        if self.Stack2==[]:\n            while self.Stack1:\n                self.Stack2.append(self.Stack1.pop())\n            return self.Stack2.pop()\n        return self.Stack2.pop()\n\nif __name__=='__main__':\n    solution = Solution()\n    solution.push(1)\n    solution.push(2)\n    solution.push(3)\n    print(solution.pop())\n```\n\n### 6.旋转数组的最小数字\n\n**题目：**把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。\n\n**题解：**遍历数组寻找数组最小值。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def minNumberInRotateArray(self, rotateArray):\n        # write code here\n        minnum=999999\n        for i in range(0,len(rotateArray)):\n            if minnum>rotateArray[i]:\n                minnum=rotateArray[i]\n        if minnum:\n            return minnum\n        else:\n            return 0\n\nif __name__=='__main__':\n    solution=Solution()\n    rotateArray=list(map(int,input().split(',')))\n    ans=solution.minNumberInRotateArray(rotateArray)\n    print(ans)\n```\n\n### 7.斐波那契数列\n\n**题目：**大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n<=39。\n\n**题解：**递归和非递归方法。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Fibonacci(self, n):\n        # write code here\n        if n==0:\n            return 0\n        if n==1:\n            return 1\n        Fib=[0 for i in range(0,n+1)]\n        Fib[0],Fib[1]=0,1\n        for i in range(2,n+1):\n            Fib[i]=Fib[i-1]+Fib[i-2]\n        return Fib[n]\n    def Fibonacci1(self,n):\n        if n==0:\n            return 0\n        if n==1 or n==2:\n            return 1\n        else:\n            return self.Fibonacci1(n-1)+self.Fibonacci1(n-2)\n\nif __name__=='__main__':\n    solution=Solution()\n    n=int(input())\n    ans=solution.Fibonacci1(n)\n    print(ans)\n```\n\n### 8.跳台阶\n\n**题目：**一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。\n\n**题解：**ans[n]=ans[n-1]+ans[n-2]\n\n```python\nclass Solution:\n    def jumpFloor(self, number):\n        # write code here\n        if number==0:\n            return 0\n        if number==1:\n            return 1\n        if number==2:\n            return 2\n        ans=[0 for i in range(0,number+1)]\n        ans[1],ans[2]=1,2\n        for i in range(3,number+1):\n            ans[i]=ans[i-1]+ans[i-2]\n        return ans[number]\n\n\nif __name__ == '__main__':\n    solution = Solution()\n    n=int(input())\n    ans=solution.jumpFloor(n)\n    print(ans)\n```\n\n### 9.变态跳台阶\n\n**题目：**一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。\n\n**题解：**ans[n]=ans[n-1]+ans[n-2]+ans[n-3]+...+ans[n-n]，ans[n-1]=ans[n-2]+ans[n-3]+...+ans[n-n]，ans[n]=2*ans[n-1]。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def jumpFloorII(self, number):\n        # write code here\n        if number==1:\n            return 1\n        if number==2:\n            return 2\n        return 2*self.jumpFloorII(number-1)\n\nif __name__=='__main__':\n    solution=Solution()\n    n=int(input())\n    ans=solution.jumpFloorII(n)\n    print(ans)\n```\n\n### 10.矩形覆盖\n\n**题目：**我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？\n\n**题解：**新增加的小矩阵竖着放，则方法与n-1时相同，新增加的小矩阵横着放，则方法与n-2时相同，于是f(n)=f(n-1)+f(n-2)。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def rectCover(self, number):\n        # write code here\n        if number==0:\n            return 0\n        if number==1:\n            return 1\n        Fib=[0 for i in range(0,number+1)]\n        Fib[1],Fib[2]=1,2\n        for i in range(3,number+1):\n            Fib[i]=Fib[i-1]+Fib[i-2]\n        return Fib[number]\n\nif __name__=='__main__':\n    solution=Solution()\n    n=int(input())\n    ans=solution.rectCover(n)\n    print(ans)\n```\n\n### 11.二进制中1的个数\n\n**题目：**输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。\n\n**题解：**每次进行左移一位，然后与1进行相与，如果是1则进行加1。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def NumberOf1(self, n):\n        # write code here\n        count = 0\n        for i in range(32):\n            count += (n >> i) & 1\n        return count\n\nif __name__=='__main__':\n    solution=Solution()\n    n=int(input())\n    ans=solution.NumberOf1(n)\n    print(ans)\n```\n\n### 12.数值的整次方\n\n**题目：**给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Power(self, base, exponent):\n        # write code here\n        ans=1\n        for i in range(0,abs(exponent)):\n            ans=ans*base\n        if exponent>0:\n            return ans\n        else:\n            return 1/ans\n\nif __name__=='__main__':\n    solution=Solution()\n    base=float(input())\n    exponent=int(input())\n    ans=solution.Power(base,exponent)\n    print(ans)\n```\n\n### 13.调整数组顺序使奇数位于偶数前面\n\n**题目：**输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。\n\n**题解：**申请奇数数组和偶数数组，分别存放奇数值和偶数值，数组相加便为结果。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def reOrderArray(self, array):\n        # write code here\n        array1=[]#奇数\n        array2=[]#偶数\n\n        for i in range(0,len(array)):\n            if array[i]%2!=0:\n                array1.append(array[i])\n            else:\n                array2.append(array[i])\n        ans=array1+array2\n        return ans\n\nif __name__=='__main__':\n    solution=Solution()\n    array=list(map(int,input().split(',')))\n    ans=solution.reOrderArray(array)\n    print(ans)\n```\n\n### 14.链表中倒数第K个节点\n\n**题目：**输入一个链表，输出该链表中倒数第k个结点。\n\n**题解：**反转链表，寻找第K个节点。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    def FindKthToTail(self, head, k):\n        # write code here\n        #反转链表\n        if head is None or head.next is None:\n            return head\n        pre=None #指向上一个节点\n        while head:\n            #先用temp保存当前节点的下一个节点信息\n            temp=head.next\n            #保存好next之后，便可以指向上一个节点\n            head.next=pre\n            #让pre,head指向下一个移动的节点\n            pre=head\n            head=temp\n        # 寻找第K个元素的位置\n        for i in range(1,k):\n            pre=pre.next\n        temp=pre\n        return temp\n\nif __name__=='__main__':\n    solution=Solution()\n    k=3\n    p1=ListNode(1)\n    p2=ListNode(2)\n    p3=ListNode(3)\n    p4=ListNode(4)\n    p1.next=p2\n    p2.next=p3\n    p3.next=p4\n\n    ans=solution.FindKthToTail(p1,k)\n    print(ans.val)\n```\n\n### 15.反转链表\n\n**题目：**输入一个链表，反转链表后，输出新链表的表头。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    # 返回ListNode\n    def ReverseList(self, pHead):\n        # write code here\n        if pHead is None or pHead.next is None:\n            return pHead\n        pre=None\n        while pHead:\n            #暂存当前节点的下一个节点信息\n            temp=pHead.next\n            #反转节点\n            pHead.next=pre\n            #进行下一个节点\n            pre = pHead\n            pHead=temp\n        return pre\n\nif __name__=='__main__':\n    solution=Solution()\n    p1=ListNode(1)\n    p2=ListNode(2)\n    p3=ListNode(3)\n    p1.next=p2\n    p2.next=p3\n    ans=solution.ReverseList(p1)\n    print(ans.val)\n```\n\n### 16.合并两个排序的列表\n\n**题目：**输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。\n\n**题解：**将两个链表之中的数值转换到列表之中，并进行排序，将排序后的列表构造成链表。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    # 返回合并后列表\n    def Merge(self,pHead1,pHead2):\n        # write code here\n        if pHead1 is None and pHead2 is None:\n            return None\n        num1,num2=[],[]\n        while pHead1:\n            num1.append(pHead1.val)\n            pHead1=pHead1.next\n        while pHead2:\n            num2.append(pHead2.val)\n            pHead2=pHead2.next\n        ans=num1+num2\n        ans.sort()\n        head=ListNode(ans[0])\n        pre=head\n        for i in range(1,len(ans)):\n            node=ListNode(ans[i])\n            pre.next=node\n            pre=pre.next\n        return head\n\nif __name__=='__main__':\n    solution=Solution()\n    pHead1_1 = ListNode(1)\n    pHead1_2 = ListNode(3)\n    pHead1_3 = ListNode(5)\n    pHead1_1.next=pHead1_2\n    pHead1_2.next=pHead1_3\n\n    pHead2_1 = ListNode(2)\n    pHead2_2 = ListNode(4)\n    pHead2_3 = ListNode(6)\n    pHead2_1.next=pHead2_2\n    pHead2_2.next=pHead2_3\n    ans=solution.Merge(pHead1_1,pHead2_1)\n    print(ans)\n```\n\n### 17.树的子结构\n\n**题目：**输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）。\n\n**题解：**将树转变为中序序列，然后转变为str类型，最后判断是否包含。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    def HasSubtree(self, pRoot1, pRoot2):\n        # write code here\n        if pRoot1 is None or pRoot2 is None:\n            return False\n        pRoot1_result,pRoot2_result=[],[]\n        self.order_traversal(pRoot1,pRoot1_result)\n        self.order_traversal(pRoot2,pRoot2_result)\n        str1=''.join(str(i) for i in pRoot1_result)\n        str2=''.join(str(i) for i in pRoot2_result)\n        print(str1,str2)\n        if str2 in str1:\n            return True\n        else:\n            return False\n\n    def order_traversal(self,root,result):\n        if not root:\n            return\n        self.order_traversal(root.left,result)\n        result.append(root.val)\n        self.order_traversal(root.right,result)\n\nif __name__=='__main__':\n    solution=Solution()\n    pRootA1 = TreeNode(1)\n    pRootA2 = TreeNode(2)\n    pRootA3 = TreeNode(3)\n    pRootA4 = TreeNode(4)\n    pRootA5 = TreeNode(5)\n    pRootA1.left=pRootA2\n    pRootA1.right=pRootA3\n    pRootA2.left=pRootA4\n    pRootA2.right=pRootA5\n\n    pRootB2 = TreeNode(2)\n    pRootB4 = TreeNode(4)\n    pRootB5 = TreeNode(5)\n    pRootB2.left=pRootB4\n    pRootB2.right = pRootB5\n    ans=solution.HasSubtree(pRootA1,pRootB2)\n    print(ans)\n```\n\n### 18.二叉树的镜像\n\n**题目：** 操作给定的二叉树，将其变换为源二叉树的镜像。\n\n**输入描述：**\n\n​\t源二叉树\n          8\n         /  \\\n        6   10\n       / \\  / \\\n      5  7 9 11\n      镜像二叉树\n          8\n         /  \\\n        10   6\n       / \\  / \\\n      11 9 7  5\n\n**思路：**递归实现反转每个子节点\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n        \nclass Solution:\n    # 返回镜像树的根节点\n    def Mirror(self, root):\n        # write code here\n        # A1_order_result=[]\n        # self.order_traversal(A1,A1_order_result)\n        if root is None:\n            return\n        if root.left is None and root.right is None:\n            return\n        temp=root.left\n        root.left=root.right\n        root.right=temp\n\n        if root is not None:\n            self.Mirror(root.left)\n        if root is not None:\n            self.Mirror(root.right)\n\n    def order_traversal(self,root,result):\n        if not root:\n            return\n        self.order_traversal(root.left,result)\n        result.append(root.val)\n        self.order_traversal(root.right,result)\n\nif __name__=='__main__':\n    A1 = TreeNode(8)\n    A2 = TreeNode(6)\n    A3 = TreeNode(10)\n    A4 = TreeNode(5)\n    A5 = TreeNode(7)\n    A6 = TreeNode(9)\n    A7 = TreeNode(11)\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    temp1=[]\n    solution=Solution()\n    solution.order_traversal(A1,temp1)\n    print(temp1)\n    solution.Mirror(A1)\n    solution.order_traversal(A1,temp1)\n    print(temp1)\n```\n\n### 19.顺时针打印矩阵\n\n**题目：**\n\n> ```python\n> 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，\n> 例如，如果输入如下矩阵：\n>  1 2 3 4\n>  5 6 7 8\n>  9 10 11 12\n>  13 14 15 16\n> 则依次打印出数字\n> 1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10.\n> \n> \n> ```\n\n**思路：**每次打印圈，但要判断最后一次是打印横还是竖，另外判断数据是否已存在。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # matrix类型为二维列表，需要返回列表\n    def printMatrix(self, matrix):\n        # write code here\n        m,n=len(matrix),len(matrix[0])\n        res = []\n        if n==1 and m==1:\n            res.append(matrix[0][0])\n            return res\n        for k in range(0,(min(m,n)+1)//2):\n            [res.append(matrix[k][i]) for i in range(k, n - k)]\n            [res.append(matrix[j][n-k-1]) for j in range(k,m-k) if matrix[j][n-k-1] not in res]\n            [res.append(matrix[m-k-1][j]) for j in range(n-k-1,k-1,-1) if matrix[m-k-1][j] not in res]\n            [res.append(matrix[j][k]) for j in range(m-1-k,k-1,-1) if matrix[j][k] not in res]\n        return res\n\nif __name__=='__main__':\n    solution=Solution()\n    m,n=1,5\n    matrix=[]\n    for i in range(0,m):\n        matrix.append(list(map(int,input().split(' '))))\n    print(matrix)\n    ans=solution.printMatrix(matrix)\n    print(ans)\n```\n\n### 20.包含Min函数的栈\n\n**题目：**定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def __init__(self):\n        self.num=[]\n    def push(self, node):\n        # write code here\n        self.num.append(node)\n    def pop(self):\n        # write code here\n        self.num.pop()\n    def top(self):\n        # write code here\n        numlen = len(self.num)\n        return self.num[numlen-1]\n    def min(self):\n        # write code here\n        return min(self.num)\n\nif __name__=='__main__':\n    solution = Solution()\n    solution.push(1)\n    solution.push(2)\n    solution.push(3)\n    solution.push(4)\n    solution.pop()\n    print(solution.top())\n    print(solution.min())\t\t\t\n```\n\n### 21.栈的压入弹出序列\n\n**题目：**输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）。\n\n**题解：**新构建一个中间栈，来模拟栈的输入和栈的输出，比对输入结果和输出结果是否相等。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def IsPopOrder(self, pushV, popV):\n        # write code here\n        if len(pushV)==1 and len(popV)==1 and pushV[0]!=popV[0]:\n            return False\n\n        helpV=[]\n        pushV.reverse()\n        popV.reverse()\n        #模拟给定栈的压入和压出\n        helpV.append(pushV[len(pushV)-1])\n        pushV.pop()\n        while True:\n            if helpV[len(helpV)-1]!=popV[len(popV)-1]:\n                helpV.append(pushV[len(pushV)-1])\n                pushV.pop()\n\n            if helpV[len(helpV)-1]==popV[len(popV)-1]:\n                helpV.pop()\n                popV.pop()\n\n            if pushV==[] and popV==[] and helpV==[]:\n                return True\n\n            if pushV==[] and popV[len(popV)-1]!=helpV[len(helpV)-1]:\n                return False\n\n\nif __name__=='__main__':\n    solution=Solution()\n    push=list(map(int,input().split(' ')))\n    pop=list(map(int,input().split(' ')))\n    ans=solution.IsPopOrder(push,pop)\n    print(ans)\n```\n\n### 22.从上往下打印二叉树\n\n**题目：**从上往下打印出二叉树的每个节点，同层节点从左至右打印。\n\n**思路：**递归，每次将左子树结果和右子树结果存到结果集之中。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    # 返回从上到下每个节点值列表，例：[1,2,3]\n    def PrintFromTopToBottom(self, root):\n        # write code here\n        if root is None:\n            return []\n        ans=[]\n        ans.append(root.val)\n        self.orderans(root,ans)\n        return ans\n\n    def orderans(self,root,ans):\n        if not root:\n            return\n        if root.left:\n            ans.append(root.left.val)\n        if root.right:\n            ans.append(root.right.val)\n\n        self.orderans(root.left, ans)\n        self.orderans(root.right,ans)\n\nif __name__=='__main__':\n    solution=Solution()\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    ans=solution.PrintFromTopToBottom(A1)\n    print(ans)\n```\n\n### 23.二叉树的后续遍历序列\n\n**题目：**输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。\n\n**思路：**二叉搜索树的特性是所有左子树值都小于中节点，所有右子树的值都大于中节点，递归遍历左子树和右子树的值。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def VerifySquenceOfBST(self, sequence):\n        # write code here\n        if not sequence:\n            return False\n        if len(sequence)==1:\n            return True\n        i=0\n        while sequence[i]<sequence[-1]:\n            i=i+1\n        k=i\n        for j in range(i,len(sequence)-1):\n            if sequence[j]<sequence[-1]:\n                return False\n            \n        leftsequence=sequence[:k]\n        rightsequence=sequence[k:len(sequence)-1]\n\n        leftans=True\n        rightans=True\n\n        if len(leftsequence)>0:\n            self.VerifySquenceOfBST(leftsequence)\n        if len(rightsequence)>0:\n            self.VerifySquenceOfBST(rightsequence)\n\n        return leftans and rightans\n\nif __name__=='__main__':\n    solution=Solution()\n    num=list(map(int,input().split(' ')))\n    ans=solution.VerifySquenceOfBST(num)\n    print(ans)\n```\n\n### 24.二叉树中和为某一值的路径\n\n**题目：**输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前)。\n\n**思路：**利用递归的方法，计算加左子树和右子树之后的值，当参数较多是，可以将结果添加到函数变量之中。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    # 返回二维列表，内部每个列表表示找到的路径\n    def FindPath(self, root, expectNumber):\n        # write code here\n        if not root:\n            return []\n        ans=[]\n        path=[]\n        self.dfs(root,expectNumber,ans,path)\n        ans.sort()\n        return ans\n\n    def dfs(self,root,target,ans,path):\n        if not root:\n            return\n\n        path.append(root.val)\n        if root.left is None and root.right is None and target==root.val:\n            ans.append(path[:])\n\n        if root.left:\n            self.dfs(root.left,target-root.val,ans,path)\n        if root.right:\n            self.dfs(root.right,target-root.val,ans,path)\n\n        path.pop()\n\n\nif __name__=='__main__':\n    A1=TreeNode(10)\n    A2=TreeNode(8)\n    A3=TreeNode(12)\n    A4=TreeNode(4)\n    A5=TreeNode(2)\n    A6=TreeNode(2)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A5.left=A6\n\n    expectNumber=22\n    solution=Solution()\n    ans=solution.FindPath(A1,expectNumber)\n    print(ans)\n```\n\n### 25.复杂链表的复制\n\n**题目：**输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）。\n\n**思路：**将大问题转变为小问题，每次都进行复制头部节点，然后进行递归，每次同样处理头部节点。\n\n```python\n# -*- coding:utf-8 -*-\nclass RandomListNode:\n    def __init__(self, x):\n        self.label = x\n        self.next = None\n        self.random = None\n\nclass Solution:\n    # 返回 RandomListNode\n    def Clone(self, pHead):\n        # write code here\n        # 复制头部节点\n        if pHead is None:\n            return None\n\n        newHead=RandomListNode(pHead.label)\n        newHead.next=pHead.next\n        newHead.random=pHead.random\n\n        # 递归其他节点\n        newHead.next=self.Clone(pHead.next)\n\n        return newHead\n\n\nif __name__=='__main__':\n    A1=RandomListNode(2)\n    A2=RandomListNode(3)\n    A3=RandomListNode(4)\n    A4=RandomListNode(5)\n    A5=RandomListNode(6)\n\n    A1.next=A2\n    A1.random=A3\n\n    A2.next=A3\n    A2.random=A4\n\n    A3.next=A4\n    A3.random=A5\n\n    A4.next=A5\n    A4.random=A3\n\n    solution=Solution()\n    ans=solution.Clone(A1)\n```\n\n### 26.二叉搜索树与双向列表\n\n**题目：**输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。\n\n**思路：**递归将根结点和左子树的最右节点和右子树的最左节点进行连接起来。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    def Convert(self, pRootOfTree):\n        # write code here\n        if pRootOfTree is None:\n            return pRootOfTree\n        if pRootOfTree.left is None and pRootOfTree.right is None:\n            return pRootOfTree\n\n        #处理左子树\n        self.Convert(pRootOfTree.left)\n        left=pRootOfTree.left\n\n        if left:\n            while left.right:\n                left=left.right\n            pRootOfTree.left,left.right=left,pRootOfTree\n\n        #处理右子树\n        self.Convert(pRootOfTree.right)\n        right=pRootOfTree.right\n\n        if right:\n            while right.left:\n                right=right.left\n            pRootOfTree.right,right.left=right,pRootOfTree\n\n        while pRootOfTree.left:\n            pRootOfTree=pRootOfTree.left\n        return pRootOfTree\n\n\nif __name__=='__main__':\n    A1 = TreeNode(7)\n    A2 = TreeNode(5)\n    A3 = TreeNode(15)\n    A4 = TreeNode(2)\n    A5 = TreeNode(6)\n    A6 = TreeNode(8)\n    A7 = TreeNode(19)\n    A8 = TreeNode(24)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n    A7.right=A8\n\n    solution=Solution()\n    solution.Convert(A1)\n```\n\n### 27.字符串的排列\n\n**题目：**输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。\n\n**输入：**输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。\n\n**思路：**通过将第k位的字符提取到最前面，然后进行和后面的每个字符进行交换，得到所有结果集。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Permutation(self, ss):\n        # write code here\n        if not ss:\n            return []\n        res=[]\n        self.helper(ss,res,'')\n        return sorted(list(set(res)))\n\n    def helper(self,ss,res,path):\n        if not ss:\n            res.append(path)\n        else:\n            for i in range(0,len(ss)):\n                self.helper(ss[:i]+ss[i+1:],res,path+ss[i])\n\nif __name__=='__main__':\n    str='abbcDeefg'\n    str1='abbc'\n    solution=Solution()\n    ans=solution.Permutation(str1)\n    print(ans)\n```\n\n### 28.数组中出现次数超过一般的数字\n\n**题目：**数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0\n\n**题解：**利用list列表来存放每个数出现的次数ans[numbers[i]]=ans[numbers[i]]+1。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def MoreThanHalfNum_Solution(self, numbers):\n        # write code here\n        numlen=len(numbers)\n        halflen=numlen//2\n        maxans=0\n        ans=[0 for i in range(0,1000)]\n        for i in range(0,len(numbers)):\n            ans[numbers[i]]=ans[numbers[i]]+1\n            if ans[numbers[i]]>maxans:\n                maxans=numbers[i]\n        ans.sort()\n        ans.reverse()\n        res=ans[0]\n        if res>halflen:\n            return maxans\n        else:\n            return 0\n\n\nif __name__=='__main__':\n    num=list(map(int,input().split(',')))\n    solution=Solution()\n    ans=solution.MoreThanHalfNum_Solution(num)\n    print(ans)\n```\n\n### 29.最小的K个数\n\n**题目：**输入n个整数，找出其中最小的K个数，例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def GetLeastNumbers_Solution(self, tinput, k):\n        # write code here\n        if k>len(tinput):\n            return []\n        tinput.sort()\n        return tinput[:k]\n\nif __name__=='__main__':\n    num=list(map(int,input().split(',')))\n    k=int(input())\n    solution=Solution()\n    ans=solution.GetLeastNumbers_Solution(num,k)\n    print(ans)\n```\n\n### 30.连续子数组的最大和\n\n**题目：**HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。你会不会被他忽悠住？(子向量的长度至少是1)\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def FindGreatestSumOfSubArray(self, array):\n        # write code here\n        maxsum,tempsum=array[0],array[0]\n        for i in range(1,len(array)):\n            if tempsum<0:\n                tempsum=array[i]\n            else:\n                tempsum = tempsum + array[i]\n            if tempsum>maxsum:\n                maxsum=tempsum\n        return maxsum\n\nif __name__=='__main__':\n    array=list(map(int,input().split(',')))\n    solution=Solution()\n    ans=solution.FindGreatestSumOfSubArray(array)\n    print(ans)\n```\n\n### 31.整数中1出现的次数\n\n**题目：**求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。\n\n**思路：**对每个数字的每位进行分解，含有1则结果加1。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def NumberOf1Between1AndN_Solution(self, n):\n        # write code here\n        ans=0\n        for i in range(1,n+1):\n            tempans=0\n            while i!=0:\n                eachnum=i%10\n                i=i//10\n                if eachnum==1:\n                    tempans=tempans+1\n            ans=ans+tempans\n        return ans\n\nif __name__=='__main__':\n    n=130\n    solution=Solution()\n    ans=solution.NumberOf1Between1AndN_Solution(n)\n    print(ans)\n```\n\n### 32.把数组排成最小的数\n\n**题目：**输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。\n\n**思路：**将数组转换成字符串之后，进行两两比较字符串的大小，比如3,32的大小由332和323确定，即3+32和32+3确定。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def PrintMinNumber(self, numbers):\n        # write code here\n        if not numbers:\n            return \"\"\n        num = map(str, numbers)\n        for i in range(0,len(numbers)):\n            for j in range(i,len(numbers)):\n                if int(str(numbers[i])+str(numbers[j]))>int(str(numbers[j])+str(numbers[i])):\n                    numbers[i],numbers[j]=numbers[j],numbers[i]\n        ans=''\n        for i in range(0,len(numbers)):\n            ans=ans+str(numbers[i])\n        return ans\n\nif __name__=='__main__':\n    numbers=[3,32,321]\n    solution=Solution()\n    ans=solution.PrintMinNumber(numbers)\n    print(ans)\n```\n\n### 33.丑数\n\n**题目：**把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。\n\n**思路：**每一个丑数必然是由之前的某个丑数与2，3或5的乘积得到的，这样下一个丑数就用之前的丑数分别乘以2，3，5，找出这三这种最小的并且大于当前最大丑数的值，即为下一个要求的丑数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def GetUglyNumber_Solution(self, index):\n        # write code here\n        if (index <= 0):\n            return 0\n        uglyList = [1]\n        indexTwo = 0\n        indexThree = 0\n        indexFive = 0\n        for i in range(index-1):\n            newUgly = min(uglyList[indexTwo]*2, uglyList[indexThree]*3, uglyList[indexFive]*5)\n            uglyList.append(newUgly)\n            if (newUgly % 2 == 0):\n                indexTwo += 1\n            if (newUgly % 3 == 0):\n                indexThree += 1\n            if (newUgly % 5 == 0):\n                indexFive += 1\n        return uglyList[-1]\n\nif __name__=='__main__':\n    solution=Solution()\n    index=200\n    ans=solution.GetUglyNumber_Solution(index)\n    print(ans)\n```\n\n### 34.第一个只出现一次的字符\n\n**题目：**在一个字符串(0<=字符串长度<=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1。\n\n**思路：**找出所有出现一次的字符，然后进行遍历找到第一次出现字符的位置。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def FirstNotRepeatingChar(self, s):\n        # write code here\n        if not s:\n            return -1\n        sset=set(s)\n        dict={}\n        for c in sset:\n            dict[c]=0\n        for i in range(0,len(s)):\n            dict[s[i]]=dict[s[i]]+1\n        onetime=[]\n        for c in dict:\n            if dict[c]==1:\n                onetime.append(c)\n\n        if onetime is None:\n            return -1\n        else:\n            index=0\n            for i in range(0,len(s)):\n                if s[i] in onetime:\n                    index=i\n                    break\n            return index\n\nif __name__=='__main__':\n    s='abbddebbac'\n    solution=Solution()\n    ans=solution.FirstNotRepeatingChar(s)\n    print(ans)\n```\n\n### 35.数组中的逆序对\n\n**题目描述：**在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007。\n\n**输入描述：**题目保证输入的数组中没有的相同的数字。\n\n**数据范围：**\n   对于%50的数据,size<=10^4\n   对于%75的数据,size<=10^5\n   对于%100的数据,size<=2*10^5\n\n> 示例1\n>\n> 输入 1,2,3,4,5,6,7,0\n>\n> 输出 7\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def InversePairs(self, data):\n        # write code here\n        global count\n        count = 0\n\n        def A(array):\n            global count\n            if len(array) <= 1:\n                return array\n            k = int(len(array) / 2)\n            left = A(array[:k])\n            right = A(array[k:])\n            l = 0\n            r = 0\n            result = []\n            while l < len(left) and r < len(right):\n                if left[l] < right[r]:\n                    result.append(left[l])\n                    l += 1\n                else:\n                    result.append(right[r])\n                    r += 1\n                    count += len(left) - l\n            result += left[l:]\n            result += right[r:]\n            return result\n\n        A(data)\n        return count % 1000000007\n\nif __name__=='__main__':\n    data=[1,2,3,4,5,6,7,0]\n    solution=Solution()\n    ans=solution.InversePairs(data)\n    print(ans)\n```\n\n### 36.两个链表的第一个公共节点\n\n**题目：**输入两个链表，找出它们的第一个公共结点。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\nclass Solution:\n    def FindFirstCommonNode(self, pHead1, pHead2):\n        # write code here\n        list1 = []\n        list2 = []\n        node1 = pHead1\n        node2 = pHead2\n        while node1:\n            list1.append(node1.val)\n            node1 = node1.next\n        while node2:\n            if node2.val in list1:\n                return node2\n            else:\n                node2 = node2.next\n\nif __name__=='__main__':\n    A1 = ListNode(1)\n    A2 = ListNode(2)\n    A3 = ListNode(3)\n    A1.next=A2\n    A2.next=A3\n\n    B4 = ListNode(4)\n    B5 = ListNode(5)\n    B4.next=B5\n\n    C6=ListNode(6)\n    C7=ListNode(7)\n\n    A3.next=C6\n    B5.next=C6\n    C6.next=C7\n\n    solution=Solution()\n    ans=solution.FindFirstCommonNode(A1,B4)\n    print(ans.val)\n```\n\n### 37.数字在排序数组中出现的次数\n\n**题目：**统计一个数字在排序数组中出现的次数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def GetNumberOfK(self, data, k):\n        # write code here\n        ans=0\n        for i in range(0,len(data)):\n            if data[i]==k:\n                ans=ans+1\n            if data[i]>k:\n                break\n        return ans\n\nif __name__=='__main__':\n    data=[1,2,3,3,3,4,4,5]\n    k=3\n    solution=Solution()\n    ans=solution.GetNumberOfK(data,k)\n    print(ans)\n```\n\n### 38.二叉树的深度\n\n**题目：**输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    def TreeDepth(self, pRoot):\n        # write code here\n        if pRoot is None:\n            return 0\n        left=self.TreeDepth(pRoot.left)\n        right=self.TreeDepth(pRoot.right)\n        print(left,right)\n        return max(left,right)+1\n\nif __name__=='__main__':\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A4.left=A6\n\n    solution=Solution()\n    ans=solution.TreeDepth(A1)\n    print('ans=',ans)\n```\n\n### 39.平衡二叉树\n\n**题目：**输入一棵二叉树，判断该二叉树是否是平衡二叉树。\n\n**题解：**平衡二叉树是左右子数的距离不能大于1，因此递归左右子树，判断子树距离是否大于1。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    def IsBalanced_Solution(self, pRoot):\n        # write code here\n        if pRoot is None:\n            return True\n        if abs(self.TreeDepth(pRoot.left)-self.TreeDepth(pRoot.right))>1:\n            return False\n        return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right)\n\n    def TreeDepth(self,root):\n        if root is None:\n            return 0\n        left=self.TreeDepth(root.left)\n        right=self.TreeDepth(root.right)\n        return max(left+1,right+1)\n\nif __name__=='__main__':\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    #A4.left=A6\n\n    solution=Solution()\n    ans=solution.IsBalanced_Solution(A1)\n    print(ans)\n```\n\n### 40.数组中只出现一次的数字\n\n**题目：**一个整型数组里除了两个数字之外，其他的数字都出现了偶数次。请写程序找出这两个只出现一次的数字。\n\n**题解：**将数组中数转到set之中，然后利用dict存储每个数字出现的次数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # 返回[a,b] 其中ab是出现一次的两个数字\n    def FindNumsAppearOnce(self, array):\n        # write code here\n        arrayset=set(array)\n        dict={}\n        for num in arrayset:\n            dict[num]=0\n        for i in range(0,len(array)):\n            dict[array[i]]=dict[array[i]]+1\n        ans=[]\n        for num in arrayset:\n            if dict[num]==1:\n                ans.append(num)\n        return ans\n\n\nif __name__=='__main__':\n    array=[1,1,2,2,3,3,4,5,5,6,7,7]\n    solution=Solution()\n    ans=solution.FindNumsAppearOnce(array)\n    print(ans)\n```\n### 41.和为S的连续正整数序列\n\n**题目：**小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck!\n\n**输出描述：**输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序。\n\n**思路：**首项加尾项*2等于和，那么只要遍历项的开始和长度即可。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def FindContinuousSequence(self, tsum):\n        # write code here\n        ans=[]\n        for i in range(1,tsum//2+1):\n            oneans=[]\n            for k in range(1,tsum):\n                tempsum=((i+i+k-1)*k)//2\n                if tempsum==tsum:\n                    for j in range(i,i+k):\n                        oneans.append(j)\n                    break\n            if oneans !=[]:\n                ans.append(oneans)\n        return ans\n\nif __name__=='__main__':\n    tsum=15\n    solution=Solution()\n    ans=solution.FindContinuousSequence(tsum)\n    print(ans)\n```\n\n### 42.和为S的两个数字\n\n**题目：**输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。\n\n**输出描述：**对应每个测试案例，输出两个数，小的先输出。\n\n**思路：**利用i和j从后面进行扫描结果，选取最小的乘积放入到结果集之中。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def FindNumbersWithSum(self, array, tsum):\n        # write code here\n        ans=[]\n        i,j,minres=0,len(array)-1,1000000\n        for i in range(0,len(array)-1):\n            j=len(array)-1\n            while True:\n                tempsum = array[i] + array[j]\n                if tempsum == tsum:\n                    if array[i]*array[j]<minres:\n                        ans=[]\n                        ans.append(array[i])\n                        ans.append(array[j])\n                        minres=array[i]*array[j]\n                    break\n                else:\n                    j = j - 1\n                if tempsum<tsum:\n                    break\n                if j<=i:\n                    break\n        return ans\n\nif __name__=='__main__':\n    array=[1,2,4,7,11,15]\n    tsum=15\n    solution=Solution()\n    ans=solution.FindNumbersWithSum(array,tsum)\n    print(ans)\n```\n\n### 43.左旋字符子串\n\n**题目：**汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def LeftRotateString(self, s, n):\n        # write code here\n        if s=='' and n==0:\n            return ''\n        ans=''\n        ans=s[n:]+s[0:n]\n        return ans\n\nif __name__=='__main__':\n    s='abcdefg'\n    n=2\n    solution=Solution()\n    ans=solution.LeftRotateString(s,n)\n    print(ans)\n```\n\n### 44.反转单词顺序\n\n**题目：**牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def ReverseSentence(self, s):\n        # write code here\n        ans,word=[],''\n        for i in range(0,len(s)):\n            word = word + s[i]\n            if s[i]==' ':\n                ans.append(word)\n                word=''\n            if i==len(s)-1:\n                word=word+' '\n                ans.append(word)\n        ans.reverse()\n        res=''\n        for c in ans:\n            res=res+c\n        return res[:len(res)-1]\n\nif __name__=='__main__':\n    solution=Solution()\n    s='I am a student.'\n    ans=solution.ReverseSentence(s)\n    print(ans)\n```\n\n### 45.扑克牌顺序\n\n**题目：**LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)...他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子.....LL不高兴了,他想了想,决定大\\小王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def IsContinuous(self, numbers):\n        # write code here\n        if numbers==[]:\n            return False\n        numbers.sort()\n        zero=0\n        for i in range(0,len(numbers)):\n            if numbers[i]==0:\n                zero=zero+1\n        for i in range(zero+1,len(numbers)):\n            if numbers[i]==numbers[i-1]:\n                return False\n            if numbers[i]-numbers[i-1]==1:\n                continue\n            else:\n                diff=numbers[i]-numbers[i-1]-1\n                zero=zero-diff\n\n        if zero<0:\n            return False\n        return True\n    \nif __name__=='__main__':\n    numbers=[1,0,0,1,0]\n    solution=Solution()\n    ans=solution.IsContinuous(numbers)\n    print(ans)\n```\n\n### 46.孩子们的圈圈(圈圈中最后剩下的数)\n\n**题目：**每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)。\n\n**思路：**约瑟夫环问题。\n\n```python\n# 题目\n# 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。\n# 其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。\n# 每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,\n# 从他的下一个小朋友开始,继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,\n# 并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)\n\n# 思路\n# 约瑟夫环问题\n\n# -*- coding:utf-8 -*-\nclass Solution:\n    def LastRemaining_Solution(self, n, m):\n        # write code here\n        if n<1 or m<1:\n            return -1\n        last=0\n        for i in range(2,n+1):\n            last=(last+m)%i\n        return last\n\nif __name__=='__main__':\n    n,m=8,4\n    solution=Solution()\n    ans=solution.LastRemaining_Solution(n,m)\n    print(ans)\n```\n\n### 47.求1+2+3+...+n\n\n**题目：**求1+2+3+...+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。\n\n**思路：**利用递归当作计算结果。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Sum_Solution(self, n):\n        # write code here\n        if n==0:\n            return 0\n        return self.Sum_Solution(n-1)+n\n\nif __name__=='__main__':\n    n=6\n    solution=Solution()\n    ans=solution.Sum_Solution(n)\n    print(ans)\n```\n\n### 48.不用加减乘除做加法\n\n**题目：**写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。\n\n**思路：**二进制异或进位。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Add(self, num1, num2):\n        # write code here\n        while num2!=0:\n            sum=num1^num2\n            carry=(num1&num2)<<1\n            num1=sum\n            num2=carry\n        return num1\n\nif __name__=='__main__':\n    num1,num2=10,500000\n    solution=Solution()\n    ans=solution.Add(num1,num2)\n    print(ans)\n```\n\n### 49.把字符串转换成整数\n\n**题目：**将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。\n\n**输入描述：**输入一个字符串,包括数字字母符号,可以为空输出描述:如果是合法的数值表达则返回该数字，否则返回0。\n\n```python\n示例\n+2147483647\n    1a33\n2147483647\n    0\n```\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def StrToInt(self, s):\n        # write code here\n        if len(s) == 0:\n            return 0\n        else:\n            if s[0] > '9' or s[0] < '0':\n                a = 0\n            else:\n                a = int(s[0]) * 10 ** (len(s) - 1)\n            if len(s) > 1:\n                for i in range(1, len(s)):\n                    if s[i] >= '0' and s[i] <= '9':\n                        a = a + int(s[i]) * 10 ** (len(s) - 1 - i)\n                    else:\n                        return 0\n        if s[0] == '+':\n            return a\n        if s[0] == '-':\n            return -a\n        return a\n\nif __name__=='__main__':\n    s='115'\n    solution=Solution()\n    ans=solution.StrToInt(s)\n    print(ans)\n```\n\n### 50.数组中重复的数字\n\n**题目：**在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。\n\n**思路：**利用dict计算重复数字。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0]\n    # 函数返回True/False\n    def duplicate(self, numbers, duplication):\n        # write code here\n        numset=set(numbers)\n        dict={}\n        duplication.append(0)\n        for val in numbers:\n            dict[val]=0\n        for i in range(0,len(numbers)):\n            dict[numbers[i]]=dict[numbers[i]]+1\n        for val in numset:\n            if dict[val]>1:\n                duplication[0]=val\n                return True\n        return False\n\nif __name__=='__main__':\n    numbers=[2,1,3,1,4]\n    solution=Solution()\n    duplication=[]\n    ans=solution.duplicate(numbers,duplication)\n    print(ans)\n```\n\n### 51.构建乘积数组\n\n```python\n# 题目\n# 给定一个数组A[0,1,...,n-1],请构建一个数组B[0,1,...,n-1],\n# 其中B中的元素B[i]=A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]。不能使用除法。\n\n# 思路\n# 审题仔细 没有A[i]\n\n# -*- coding:utf-8 -*-\nclass Solution:\n    def multiply(self, A):\n        # write code here\n        B=[]\n        for i in range(0,len(A)):\n            temp=1\n            for j in range(0,len(A)):\n                if j==i:\n                    continue\n                temp=temp*A[j]\n            B.append(temp)\n        return B\n\nif __name__=='__main__':\n    solution=Solution()\n    A=[1,2,3,4,5]\n    ans=solution.multiply(A)\n    print(ans)\n```\n\n### 52.正则表达式匹配\n\n**题目：**请实现一个函数用来匹配包括'.'和'\\*'的正则表达式。模式中的字符'.'表示任意一个字符，而'\\*'表示它前面的字符可以出现任意次（包含0次）。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串\"aaa\"与模式\"a.a\"和\"ab\\*ac\\*a\"匹配，但是与\"aa.a\"和\"ab*a\"均不匹配。\n\n**思路：**\n\n> 当模式中的第二个字符不是`*`时： \n>\n> - 如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 \n> - 如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。\n\n> 当模式中的第二个字符是`*`时：\n>\n> + 如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。\n> + 如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式。\n>   + 模式后移2字符，相当于`x*`被忽略。即模式串中*与他前面的字符和字符串匹配0次。 \n>   +  字符串后移1字符，模式后移2字符。即模式串中*与他前面的字符和字符串匹配1次。\n>   + 字符串后移1字符，模式不变，即继续匹配字符下一位，因为`*`可以匹配多位。即模式串中*与他前面的字符和字符串匹配多次。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # s, pattern都是字符串\n    def match(self, s, pattern):\n        if s == pattern:\n            return True\n        if not pattern:\n            return False\n        if len(pattern) > 1 and pattern[1] == '*':\n            if (s and s[0] == pattern[0]) or (s and pattern[0] == '.'):\n                return self.match(s, pattern[2:]) \\\n                       or self.match(s[1:], pattern) \\\n                       or self.match(s[1:], pattern[2:])\n            else:\n                return self.match(s, pattern[2:])\n        elif s and (s[0] == pattern[0] or pattern[0] == '.'):\n            return self.match(s[1:], pattern[1:])\n        return False\n\nif __name__=='__main__':\n    solution=Solution()\n    s='aaa'\n    pattern='a*a.a'\n    ans=solution.match(s,pattern)\n    print(ans)\n```\n\n### 53.表示数值的字符串\n\n**题目：**请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串\"+100\",\"5e2\",\"-123\",\"3.1416\"和\"-1E-16\"都表示数值。 但是\"12e\",\"1a3.14\",\"1.2.3\",\"+-5\"和\"12e+4.3\"都不是。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # s字符串\n    def isNumeric(self, s):\n        # write code here\n        # 标记符号、小数点、e是否出现过\n        sign,decimal,hasE=False,False,False\n        for i in range(0,len(s)):\n            if s[i]=='e' or s[i]=='E':\n                if i==len(s)-1:# e后面一定要接数字\n                    return False\n                if hasE==True:# 不能出现两次e\n                    return False\n                hasE=True\n            elif s[i]=='+' or s[i]=='-':\n                #第二次出现+或-一定要在e之后\n                if sign and s[i-1]!='e' and s[i-1]!='E':\n                    return False\n                # 第一次出现+或-，如果不是出现在字符最前面，那么就要出现在e或者E后面\n                if sign==False and i>0 and s[i-1]!='e' and s[i-1]!='E':\n                    return False\n                sign=True\n            elif s[i]=='.':\n                # e后面不能出现小数点，小数点不能出现两次\n                if decimal or hasE:\n                    return False\n                decimal=True\n            elif s[i]>'9' or s[i]<'0':\n                return False\n        return True\n\nif __name__=='__main__':\n    solution=Solution()\n    s='123e.1416'\n    ans=solution.isNumeric(s)\n    print(ans)\n```\n\n### 54.字符流中第一个不重复的字符\n\n**题目：**请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符\"go\"时，第一个只出现一次的字符是\"g\"。当从该字符流中读出前六个字符“google\"时，第一个只出现一次的字符是\"l\"。\n\n**输出描述：**如果当前字符流没有存在出现一次的字符，返回#字符。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # 返回对应char\n    def __init__(self):\n        self.all={}\n        self.ch=[]\n    def FirstAppearingOnce(self):\n        # write code here\n        if self.all is None:\n            return '#'\n        for c in self.ch:\n            if self.all[c]==1:\n                return c\n        return '#'\n\n    def Insert(self, char):\n        # write code here\n        self.ch.append(char)\n        if char in self.all:\n            self.all[char]=self.all[char]+1\n        else:\n            self.all[char]=1\n\nif __name__=='__main__':\n    solution=Solution()\n    solution.Insert('g')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('o')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('o')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('g')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('l')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('e')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n```\n\n### 55.链表中环的入口节点\n\n**题目：**给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。\n\n**思路：**把链表中节点值放到dict数组中，并记录出现的次数，如果出现次数超过一次，则为环的入口节点。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\nclass Solution:\n    def EntryNodeOfLoop(self, pHead):\n        # write code here\n        if pHead is None:\n            return None\n        num,dict,flag=[],{},True\n        tempans=0\n        while pHead and flag==True:\n            num.append(pHead.val)\n            numset=set(num)\n            for c in numset:\n                dict[c]=0\n            for c in num:\n                dict[c]=dict[c]+1\n            for c in num:\n                if dict[c]>1:\n                    flag=False\n                    tempans=c\n            pHead=pHead.next\n        while pHead:\n            if pHead.val==tempans:\n                return pHead\n            pHead=pHead.next\n        return None\n\nif __name__=='__main__':\n    pHead1 = ListNode(1)\n    pHead2 = ListNode(2)\n    pHead3 = ListNode(3)\n    pHead4 = ListNode(4)\n    pHead5 = ListNode(5)\n\n    pHead1.next=pHead2\n    pHead2.next=pHead3\n    pHead3.next=pHead4\n    pHead4.next=pHead5\n    pHead5.next=pHead1\n\n    solution=Solution()\n    ans=solution.EntryNodeOfLoop(pHead1)\n    print(ans.val)\n```\n\n### 56.删除链表中重复的节点\n\n**题目：**在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1->2->3->3->4->4->5 处理后为 1->2->5。\n\n**思路：**记录链表中出现的数字，然后构建新链表。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\nclass Solution:\n    def deleteDuplication(self, pHead):\n        # write code here\n        num=[]\n        tempnum1=pHead\n        while tempnum1:\n            num.append(tempnum1.val)\n            tempnum1=tempnum1.next\n        dict={}\n        for c in num:\n            dict[c]=0\n        for c in num:\n            dict[c]=dict[c]+1\n        newnum=[]\n        for c in num:\n            if dict[c]==1:\n                newnum.append(c)\n        if newnum==[]:\n            return None\n        head=ListNode(newnum[0])\n        temphead=head\n        for i in range(1,len(newnum)):\n            tempnode=ListNode(newnum[i])\n            temphead.next=tempnode\n            temphead=tempnode\n        # while head:\n        #     print(head.val)\n        #     head=head.next\n        return head\n\nif __name__=='__main__':\n    pHead1 = ListNode(1)\n    pHead2 = ListNode(1)\n    pHead3 = ListNode(1)\n    pHead4 = ListNode(1)\n    pHead5 = ListNode(1)\n    pHead6 = ListNode(1)\n    pHead7 = ListNode(1)\n\n    pHead1.next=pHead2\n    pHead2.next=pHead3\n    pHead3.next=pHead4\n    pHead4.next=pHead5\n    pHead5.next=pHead6\n    pHead6.next=pHead7\n\n    solution=Solution()\n    ans=solution.deleteDuplication(pHead1)\n    print(ans)\n```\n\n### 57. 二叉树中的下一个节点\n\n**题目：**给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。\n\n**思路：**分析二叉树的下一个节点，一共有以下情况：1.二叉树为空，则返回空；2.节点右孩子存在，则设置一个指针从该节点的右孩子出发，一直沿着指向左子结点的指针找到的叶子节点即为下一个节点；3.节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点；否则继续向上遍历其父节点的父节点，重复之前的判断，返回结果。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeLinkNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n        self.next = None\nclass Solution:\n    def GetNext(self, pNode):\n        # write code here\n        if not pNode:\n            return pNode\n        if pNode.right:\n            left1=pNode.right\n            while left1.left:\n                   left1=left1.left\n            return left1\n\n        while pNode.next:\n            tmp=pNode.next\n            if tmp.left==pNode:\n                return tmp\n            pNode=tmp\n\nif __name__=='__main__':\n    solution=Solution()\n```\n\n### 58.对称的二叉树\n\n**题目：**请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。\n\n**思路：**采用递归的方法来判断两数是否相同。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    def isSymmetrical(self, pRoot):\n        # write code here\n        if not pRoot:\n            return True\n        result=self.same(pRoot,pRoot)\n        return result\n    def same(self,root1,root2):\n        if not root1 and not root2:\n            return True\n        if root1 and not root2:\n            return False\n        if not root1 and root2:\n            return False\n        if root1.val!= root2.val:\n            return False\n\n        left=self.same(root1.left,root2.right)\n        if not left:\n            return False\n        right=self.same(root1.right,root2.left)\n        if not right:\n            return False\n        return True\n\nif __name__=='__main__':\n\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(2)\n    A4 = TreeNode(3)\n    A5 = TreeNode(4)\n    A6 = TreeNode(4)\n    A7 = TreeNode(3)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n\n    solution = Solution()\n    ans=solution.isSymmetrical(A1)\n    print(ans)\n```\n\n### 59.按之字形顺序打印二叉树\n\n**题目：**请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。\n\n**思路：** 把当前列结果存放到list之中，设置翻转变量，依次从左到右打印和从右到左打印。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    def Print(self, pRoot):\n        # write code here\n        root=pRoot\n        if not root:\n            return []\n        level=[root]\n        result=[]\n        righttoleft=False\n        while level:\n            curvalues=[]\n            nextlevel=[]\n            for i in level:\n                curvalues.append(i.val)\n                if i.left:\n                    nextlevel.append(i.left)\n                if i.right:\n                    nextlevel.append(i.right)\n            if righttoleft:\n                    curvalues.reverse()\n            if curvalues:\n                    result.append(curvalues)\n            level = nextlevel\n            righttoleft = not righttoleft\n        return result\n\nif __name__=='__main__':\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n    A7 = TreeNode(7)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    solution = Solution()\n    ans=solution.Print(A1)\n    print(ans)\n```\n\n### 60.把二叉树打印成多行\n\n**题目：**从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    # 返回二维列表[[1,2],[4,5]]\n    def Print(self, pRoot):\n        # write code here\n        root=pRoot\n        if not root:\n            return []\n        level=[root]\n        result=[]\n        while level:\n            curvalues=[]\n            nextlevel=[]\n            for i in level:\n                curvalues.append(i.val)\n                if i.left:\n                    nextlevel.append(i.left)\n                if i.right:\n                    nextlevel.append(i.right)\n            if curvalues:\n                    result.append(curvalues)\n            level = nextlevel\n        return result\n\nif __name__=='__main__':\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n    A7 = TreeNode(7)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    solution = Solution()\n    ans=solution.Print(A1)\n    print(ans)\n```\n\n### 61.序列化二叉树\n\n**题目：**请实现两个函数，分别用来序列化和反序列化二叉树。\n\n**思路：**转变成前序遍历，空元素利用\"#\"代替，然后进行解序列。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nimport collections\nclass Solution:\n    def Serialize(self, root):\n        # write code here\n        if not root:\n            return None\n        res=[]\n        self.pre(root,res)\n        return res\n\n    def pre(self,root,res):\n        if not root:\n            return\n        res.append(root.val)\n        if root.left:\n            self.pre(root.left, res)\n        else:\n            res.append('#')\n        if root.right:\n            self.pre(root.right,res)\n        else:\n            res.append('#')\n    def Deserialize(self, s):\n        if s=='':\n            return None\n        vals=[]\n        for i in range(0,len(s)):\n            vals.append(s[i])\n        vals=collections.deque(vals)\n        ans=self.build(vals)\n        return ans\n\n    def build(self,vals):\n        if vals:\n            val = vals.popleft()\n            if val == '#':\n                return None\n            root = TreeNode(int(val))\n            root.left = self.build(vals)\n            root.right = self.build(vals)\n            return root\n        return self.build(vals)\n\n# [1, ',', 2, ',', 4, ',', ',', ',', 5, ',', ',', ',', 3, ',', 6, ',', ',', ',', 7, ',', ',']\nif __name__==\"__main__\":\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n    A7 = TreeNode(7)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    solution = Solution()\n    ans=solution.Serialize(A1)\n    print(ans)\n    root=solution.Deserialize(ans)\n    res=solution.Serialize(root)\n    print(res)\n```\n\n### 62.二叉搜索树中的第K个节点\n\n**题目：**给定一棵二叉搜索树，请找出其中的第k小的结点。例如（5，3，7，2，4，6，8）中，按结点数值大小顺序第三小结点的值为4。\n\n**思路：**中序遍历后，返回第K个节点值。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    # 返回对应节点TreeNode\n    def KthNode(self, pRoot, k):\n        # write code here\n        res=[]\n        if not pRoot:\n            return None\n        self.order(pRoot,res)\n        if len(res)<k or k<=0:\n            return None\n        else:\n            return res[k-1]\n\n    def order(self,root,res):\n        if not root:\n            return\n        self.order(root.left,res)\n        res.append(root)\n        self.order(root.right,res)\n\nif __name__=='__main__':\n    A1 = TreeNode(5)\n    A2 = TreeNode(3)\n    A3 = TreeNode(7)\n    A4 = TreeNode(2)\n    A5 = TreeNode(4)\n    A6 = TreeNode(6)\n    A7 = TreeNode(8)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    k=3\n    solution = Solution()\n    ans=solution.KthNode(A1,k)\n    print(ans)\n```\n\n### 63.数据流中的中位数\n\n**题目：**如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def __init__(self):\n        self.data=[]\n    def Insert(self, num):\n        # write code here\n        self.data.append(num)\n        self.data.sort()\n    def GetMedian(self):\n        # write code here\n        length=len(self.data)\n        if length%2==0:\n            return (self.data[length//2]+self.data[length//2-1])/2.0\n        else:\n            return self.data[int(length//2)]\n\n\nif __name__==\"__main__\":\n    solution=Solution()\n    solution.Insert(5)\n    ans = solution.GetMedian()\n    print(ans)\n    solution.Insert(2)\n    ans = solution.GetMedian()\n    print(ans)\n    solution.Insert(3)\n    ans = solution.GetMedian()\n    print(ans)\n    solution.Insert(4)\n    ans = solution.GetMedian()\n    print(ans)\n    solution.Insert(1)\n    ans = solution.GetMedian()\n    print(ans)\n```\n\n### 64.滑动窗口的最大值\n\n**题目：**给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}，{2,3,4,2,6,[2,5,1]}。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def maxInWindows(self, num, size):\n        # write code here\n        if size==0 or num==[]:\n            return []\n        res=[]\n        for i in range(0,len(num)-size+1):\n            tempnum=[]\n            for j in range(i,i+size):\n                tempnum.append(num[j])\n            res.append(max(tempnum))\n        return res\n\nif __name__==\"__main__\":\n    solution=Solution()\n    num=[2,3,4,2,6,2,5,1]\n    size=3\n    ans=solution.maxInWindows(num,size)\n    print(ans)\n```\n\n### 66.矩阵中的路径\n\n**题目：**请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串\"bcced\"的路径，但是矩阵中不包含\"abcb\"路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。\n\n**思路：**当起点第一个字符相同时，开始进行递归搜索，设计搜索函数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def hasPath(self, matrix, rows, cols, path):\n        # write code here\n        for i in range(0,rows):\n            for j in range(0,cols):\n                if matrix[i*rows+j]==path[0]:\n                    if self.find_path(list(matrix),rows,cols,path[1:],i,j):\n                        return True\n        return False\n\n    def find_path(self,matrix,rows,cols,path,i,j):\n        if not path:\n            return True\n        matrix[i*cols+j]=0\n        if j+1<cols and matrix[i*cols+j+1]==path[0]:\n            return self.find_path(matrix,rows,cols,path[1:],i,j+1)\n        elif j-1>=0 and matrix[i*cols+j-1]==path[0]:\n            return self.find_path(matrix, rows, cols, path[1:], i, j - 1)\n        elif i+1<rows and matrix[(i+1)*cols+j]==path[0]:\n            return self.find_path(matrix, rows, cols, path[1:], i+1, j)\n        elif i-1>=0 and matrix[(i-1)*cols+j]==path[0]:\n            return self.find_path(matrix, rows, cols, path[1:], i-1, j)\n        else:\n            return False\n\nif __name__=='__main__':\n    solution=Solution()\n    matrix='ABCEHJIGSFCSLOPQADEEMNOEADIDEJFMVCEIFGGS'\n    rows=5\n    cols=8\n    path='SGGFIECVAASABCEHJIGQEMS'\n    ans=solution.hasPath(matrix,rows,cols,path)\n    print(ans)\n```\n\n### 66.机器人的运动范围\n\n**题目：**地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？\n\n**思路：**对未走过的路径进行遍历，搜索所有的路径值。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def __init__(self):\n        self.vis = {}\n\n    def movingCount(self, threshold, rows, cols):\n        # write code here\n        return self.moving(threshold, rows, cols, 0, 0)\n\n    def moving(self, threshold, rows, cols, row, col):\n        rowans,colans=0,0\n        rowtemp,coltemp=row,col\n        while rowtemp>0:\n            rowans=rowans+rowtemp%10\n            rowtemp=rowtemp//10\n        while coltemp>0:\n            colans=colans+coltemp%10\n            coltemp=coltemp//10\n\n        if rowans+colans>threshold:\n            return 0\n        if row >= rows or col >= cols or row < 0 or col < 0:\n            return 0\n        if (row, col) in self.vis:\n            return 0\n        self.vis[(row, col)] = 1\n\n        return 1 + self.moving(threshold, rows, cols, row - 1, col) +\\\n               self.moving(threshold, rows, cols, row + 1,col) + \\\n               self.moving(threshold, rows,cols, row,col - 1) + \\\n               self.moving(threshold, rows, cols, row, col + 1)\n\n\nif __name__=='__main__':\n    solution=Solution()\n    threshold=10\n    rows,cols=1,100\n    ans=solution.movingCount(threshold,rows,cols)\n    print(ans)\n```\n### 67.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](《剑指Offer》Python版/推广.png)","source":"_posts/《剑指Offer》Python版.md","raw":"---\ntitle: 《剑指Offer》Python版\ndate: 2018-07-29 15:02:01\ntags: [算法]\ncategories: 算法\nmathjax: true\n---\n\n### 1.二维数组中的查找\n\n**题目：** 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n\n**思路：**遍历每一行，查找该元素是否在该行之中。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # array 二维列表\n    def Find(self, target, array):\n        # write code here\n        for line in array:\n            if target in line:\n                return True\n        return False\n\nif __name__=='__main__':\n    target=2\n    array=[[1,2,3,4],[2,3,4,5],[3,4,5,6],[4,5,6,7]]\n    solution=Solution()\n    ans=solution.Find(target,array)\n    print(ans)\n```\n\n### 2.替换空格\n\n**题目：** 请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。\n\n**思路：**利用字符串中的replace直接替换即可。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # s 源字符串\n    def replaceSpace(self, s):\n        # write code here\n        temp = s.replace(\" \", \"%20\")\n        return temp\n\nif __name__=='__main__':\n    s='We Are Happy'\n    solution=Solution()\n    ans=solution.replaceSpace(s)\n    print(ans)\n```\n\n### 3.从尾到头打印链表\n\n**题目：**输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。\n\n**思路：**将链表中的值记录到list之中，然后进行翻转list。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    # 返回从尾部到头部的列表值序列，例如[1,2,3]\n    def printListFromTailToHead(self, listNode):\n        # write code here\n        l=[]\n        while listNode:\n            l.append(listNode.val)\n            listNode=listNode.next\n        return l[::-1]\n\nif __name__=='__main__':\n    A1 = ListNode(1)\n    A2 = ListNode(2)\n    A3 = ListNode(3)\n    A4 = ListNode(4)\n    A5 = ListNode(5)\n\n    A1.next=A2\n    A2.next=A3\n    A3.next=A4\n    A4.next=A5\n\n    solution=Solution()\n    ans=solution.printListFromTailToHead(A1)\n    print(ans)\n```\n\n### 4.重建二叉树\n\n**题目：**输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。\n\n**题解：**首先前序遍历的第一个元素为二叉树的根结点，那么便能够在中序遍历之中找到根节点，那么在根结点左侧则是左子树，假设长度为M.在根结点右侧，便是右子树,假设长度为N。然后在前序遍历根节点后面M长度的便是左子树的前序遍历序列，再后面的N个长度便是右子树的后序遍历的长度。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    # 返回构造的TreeNode根节点\n    def reConstructBinaryTree(self, pre, tin):\n        # write code here\n        if len(pre)==0:\n            return None\n        if len(pre)==1:\n            return TreeNode(pre[0])\n        else:\n            flag=TreeNode(pre[0])\n            flag.left=self.reConstructBinaryTree(pre[1:tin.index(pre[0])+1],tin[:tin.index(pre[0])])\n            flag.right=self.reConstructBinaryTree(pre[tin.index(pre[0])+1:],tin[tin.index(pre[0])+1:])\n        return flag\n\nif __name__=='__main__':\n    solution=Solution()\n    pre=list(map(int,input().split(',')))\n    tin=list(map(int,input().split(',')))\n    ans=solution.reConstructBinaryTree(pre,tin)\n    print(ans.val)\n```\n\n### 5.用两个栈实现队列\n\n**题目：**用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。\n\n**题解：**申请两个栈Stack1和Stack2，Stack1当作输入，Stack2当作pop。当Stack2空的时候，将Stack1进行反转，并且输入到Stack2。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def __init__(self):\n        self.Stack1=[]\n        self.Stack2=[]\n    def push(self, node):\n        # write code here\n        self.Stack1.append(node)\n    def pop(self):\n        # return xx\n        if self.Stack2==[]:\n            while self.Stack1:\n                self.Stack2.append(self.Stack1.pop())\n            return self.Stack2.pop()\n        return self.Stack2.pop()\n\nif __name__=='__main__':\n    solution = Solution()\n    solution.push(1)\n    solution.push(2)\n    solution.push(3)\n    print(solution.pop())\n```\n\n### 6.旋转数组的最小数字\n\n**题目：**把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。\n\n**题解：**遍历数组寻找数组最小值。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def minNumberInRotateArray(self, rotateArray):\n        # write code here\n        minnum=999999\n        for i in range(0,len(rotateArray)):\n            if minnum>rotateArray[i]:\n                minnum=rotateArray[i]\n        if minnum:\n            return minnum\n        else:\n            return 0\n\nif __name__=='__main__':\n    solution=Solution()\n    rotateArray=list(map(int,input().split(',')))\n    ans=solution.minNumberInRotateArray(rotateArray)\n    print(ans)\n```\n\n### 7.斐波那契数列\n\n**题目：**大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n<=39。\n\n**题解：**递归和非递归方法。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Fibonacci(self, n):\n        # write code here\n        if n==0:\n            return 0\n        if n==1:\n            return 1\n        Fib=[0 for i in range(0,n+1)]\n        Fib[0],Fib[1]=0,1\n        for i in range(2,n+1):\n            Fib[i]=Fib[i-1]+Fib[i-2]\n        return Fib[n]\n    def Fibonacci1(self,n):\n        if n==0:\n            return 0\n        if n==1 or n==2:\n            return 1\n        else:\n            return self.Fibonacci1(n-1)+self.Fibonacci1(n-2)\n\nif __name__=='__main__':\n    solution=Solution()\n    n=int(input())\n    ans=solution.Fibonacci1(n)\n    print(ans)\n```\n\n### 8.跳台阶\n\n**题目：**一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。\n\n**题解：**ans[n]=ans[n-1]+ans[n-2]\n\n```python\nclass Solution:\n    def jumpFloor(self, number):\n        # write code here\n        if number==0:\n            return 0\n        if number==1:\n            return 1\n        if number==2:\n            return 2\n        ans=[0 for i in range(0,number+1)]\n        ans[1],ans[2]=1,2\n        for i in range(3,number+1):\n            ans[i]=ans[i-1]+ans[i-2]\n        return ans[number]\n\n\nif __name__ == '__main__':\n    solution = Solution()\n    n=int(input())\n    ans=solution.jumpFloor(n)\n    print(ans)\n```\n\n### 9.变态跳台阶\n\n**题目：**一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。\n\n**题解：**ans[n]=ans[n-1]+ans[n-2]+ans[n-3]+...+ans[n-n]，ans[n-1]=ans[n-2]+ans[n-3]+...+ans[n-n]，ans[n]=2*ans[n-1]。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def jumpFloorII(self, number):\n        # write code here\n        if number==1:\n            return 1\n        if number==2:\n            return 2\n        return 2*self.jumpFloorII(number-1)\n\nif __name__=='__main__':\n    solution=Solution()\n    n=int(input())\n    ans=solution.jumpFloorII(n)\n    print(ans)\n```\n\n### 10.矩形覆盖\n\n**题目：**我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？\n\n**题解：**新增加的小矩阵竖着放，则方法与n-1时相同，新增加的小矩阵横着放，则方法与n-2时相同，于是f(n)=f(n-1)+f(n-2)。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def rectCover(self, number):\n        # write code here\n        if number==0:\n            return 0\n        if number==1:\n            return 1\n        Fib=[0 for i in range(0,number+1)]\n        Fib[1],Fib[2]=1,2\n        for i in range(3,number+1):\n            Fib[i]=Fib[i-1]+Fib[i-2]\n        return Fib[number]\n\nif __name__=='__main__':\n    solution=Solution()\n    n=int(input())\n    ans=solution.rectCover(n)\n    print(ans)\n```\n\n### 11.二进制中1的个数\n\n**题目：**输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。\n\n**题解：**每次进行左移一位，然后与1进行相与，如果是1则进行加1。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def NumberOf1(self, n):\n        # write code here\n        count = 0\n        for i in range(32):\n            count += (n >> i) & 1\n        return count\n\nif __name__=='__main__':\n    solution=Solution()\n    n=int(input())\n    ans=solution.NumberOf1(n)\n    print(ans)\n```\n\n### 12.数值的整次方\n\n**题目：**给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Power(self, base, exponent):\n        # write code here\n        ans=1\n        for i in range(0,abs(exponent)):\n            ans=ans*base\n        if exponent>0:\n            return ans\n        else:\n            return 1/ans\n\nif __name__=='__main__':\n    solution=Solution()\n    base=float(input())\n    exponent=int(input())\n    ans=solution.Power(base,exponent)\n    print(ans)\n```\n\n### 13.调整数组顺序使奇数位于偶数前面\n\n**题目：**输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。\n\n**题解：**申请奇数数组和偶数数组，分别存放奇数值和偶数值，数组相加便为结果。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def reOrderArray(self, array):\n        # write code here\n        array1=[]#奇数\n        array2=[]#偶数\n\n        for i in range(0,len(array)):\n            if array[i]%2!=0:\n                array1.append(array[i])\n            else:\n                array2.append(array[i])\n        ans=array1+array2\n        return ans\n\nif __name__=='__main__':\n    solution=Solution()\n    array=list(map(int,input().split(',')))\n    ans=solution.reOrderArray(array)\n    print(ans)\n```\n\n### 14.链表中倒数第K个节点\n\n**题目：**输入一个链表，输出该链表中倒数第k个结点。\n\n**题解：**反转链表，寻找第K个节点。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    def FindKthToTail(self, head, k):\n        # write code here\n        #反转链表\n        if head is None or head.next is None:\n            return head\n        pre=None #指向上一个节点\n        while head:\n            #先用temp保存当前节点的下一个节点信息\n            temp=head.next\n            #保存好next之后，便可以指向上一个节点\n            head.next=pre\n            #让pre,head指向下一个移动的节点\n            pre=head\n            head=temp\n        # 寻找第K个元素的位置\n        for i in range(1,k):\n            pre=pre.next\n        temp=pre\n        return temp\n\nif __name__=='__main__':\n    solution=Solution()\n    k=3\n    p1=ListNode(1)\n    p2=ListNode(2)\n    p3=ListNode(3)\n    p4=ListNode(4)\n    p1.next=p2\n    p2.next=p3\n    p3.next=p4\n\n    ans=solution.FindKthToTail(p1,k)\n    print(ans.val)\n```\n\n### 15.反转链表\n\n**题目：**输入一个链表，反转链表后，输出新链表的表头。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    # 返回ListNode\n    def ReverseList(self, pHead):\n        # write code here\n        if pHead is None or pHead.next is None:\n            return pHead\n        pre=None\n        while pHead:\n            #暂存当前节点的下一个节点信息\n            temp=pHead.next\n            #反转节点\n            pHead.next=pre\n            #进行下一个节点\n            pre = pHead\n            pHead=temp\n        return pre\n\nif __name__=='__main__':\n    solution=Solution()\n    p1=ListNode(1)\n    p2=ListNode(2)\n    p3=ListNode(3)\n    p1.next=p2\n    p2.next=p3\n    ans=solution.ReverseList(p1)\n    print(ans.val)\n```\n\n### 16.合并两个排序的列表\n\n**题目：**输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。\n\n**题解：**将两个链表之中的数值转换到列表之中，并进行排序，将排序后的列表构造成链表。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    # 返回合并后列表\n    def Merge(self,pHead1,pHead2):\n        # write code here\n        if pHead1 is None and pHead2 is None:\n            return None\n        num1,num2=[],[]\n        while pHead1:\n            num1.append(pHead1.val)\n            pHead1=pHead1.next\n        while pHead2:\n            num2.append(pHead2.val)\n            pHead2=pHead2.next\n        ans=num1+num2\n        ans.sort()\n        head=ListNode(ans[0])\n        pre=head\n        for i in range(1,len(ans)):\n            node=ListNode(ans[i])\n            pre.next=node\n            pre=pre.next\n        return head\n\nif __name__=='__main__':\n    solution=Solution()\n    pHead1_1 = ListNode(1)\n    pHead1_2 = ListNode(3)\n    pHead1_3 = ListNode(5)\n    pHead1_1.next=pHead1_2\n    pHead1_2.next=pHead1_3\n\n    pHead2_1 = ListNode(2)\n    pHead2_2 = ListNode(4)\n    pHead2_3 = ListNode(6)\n    pHead2_1.next=pHead2_2\n    pHead2_2.next=pHead2_3\n    ans=solution.Merge(pHead1_1,pHead2_1)\n    print(ans)\n```\n\n### 17.树的子结构\n\n**题目：**输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）。\n\n**题解：**将树转变为中序序列，然后转变为str类型，最后判断是否包含。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    def HasSubtree(self, pRoot1, pRoot2):\n        # write code here\n        if pRoot1 is None or pRoot2 is None:\n            return False\n        pRoot1_result,pRoot2_result=[],[]\n        self.order_traversal(pRoot1,pRoot1_result)\n        self.order_traversal(pRoot2,pRoot2_result)\n        str1=''.join(str(i) for i in pRoot1_result)\n        str2=''.join(str(i) for i in pRoot2_result)\n        print(str1,str2)\n        if str2 in str1:\n            return True\n        else:\n            return False\n\n    def order_traversal(self,root,result):\n        if not root:\n            return\n        self.order_traversal(root.left,result)\n        result.append(root.val)\n        self.order_traversal(root.right,result)\n\nif __name__=='__main__':\n    solution=Solution()\n    pRootA1 = TreeNode(1)\n    pRootA2 = TreeNode(2)\n    pRootA3 = TreeNode(3)\n    pRootA4 = TreeNode(4)\n    pRootA5 = TreeNode(5)\n    pRootA1.left=pRootA2\n    pRootA1.right=pRootA3\n    pRootA2.left=pRootA4\n    pRootA2.right=pRootA5\n\n    pRootB2 = TreeNode(2)\n    pRootB4 = TreeNode(4)\n    pRootB5 = TreeNode(5)\n    pRootB2.left=pRootB4\n    pRootB2.right = pRootB5\n    ans=solution.HasSubtree(pRootA1,pRootB2)\n    print(ans)\n```\n\n### 18.二叉树的镜像\n\n**题目：** 操作给定的二叉树，将其变换为源二叉树的镜像。\n\n**输入描述：**\n\n​\t源二叉树\n          8\n         /  \\\n        6   10\n       / \\  / \\\n      5  7 9 11\n      镜像二叉树\n          8\n         /  \\\n        10   6\n       / \\  / \\\n      11 9 7  5\n\n**思路：**递归实现反转每个子节点\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n        \nclass Solution:\n    # 返回镜像树的根节点\n    def Mirror(self, root):\n        # write code here\n        # A1_order_result=[]\n        # self.order_traversal(A1,A1_order_result)\n        if root is None:\n            return\n        if root.left is None and root.right is None:\n            return\n        temp=root.left\n        root.left=root.right\n        root.right=temp\n\n        if root is not None:\n            self.Mirror(root.left)\n        if root is not None:\n            self.Mirror(root.right)\n\n    def order_traversal(self,root,result):\n        if not root:\n            return\n        self.order_traversal(root.left,result)\n        result.append(root.val)\n        self.order_traversal(root.right,result)\n\nif __name__=='__main__':\n    A1 = TreeNode(8)\n    A2 = TreeNode(6)\n    A3 = TreeNode(10)\n    A4 = TreeNode(5)\n    A5 = TreeNode(7)\n    A6 = TreeNode(9)\n    A7 = TreeNode(11)\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    temp1=[]\n    solution=Solution()\n    solution.order_traversal(A1,temp1)\n    print(temp1)\n    solution.Mirror(A1)\n    solution.order_traversal(A1,temp1)\n    print(temp1)\n```\n\n### 19.顺时针打印矩阵\n\n**题目：**\n\n> ```python\n> 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，\n> 例如，如果输入如下矩阵：\n>  1 2 3 4\n>  5 6 7 8\n>  9 10 11 12\n>  13 14 15 16\n> 则依次打印出数字\n> 1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10.\n> \n> \n> ```\n\n**思路：**每次打印圈，但要判断最后一次是打印横还是竖，另外判断数据是否已存在。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # matrix类型为二维列表，需要返回列表\n    def printMatrix(self, matrix):\n        # write code here\n        m,n=len(matrix),len(matrix[0])\n        res = []\n        if n==1 and m==1:\n            res.append(matrix[0][0])\n            return res\n        for k in range(0,(min(m,n)+1)//2):\n            [res.append(matrix[k][i]) for i in range(k, n - k)]\n            [res.append(matrix[j][n-k-1]) for j in range(k,m-k) if matrix[j][n-k-1] not in res]\n            [res.append(matrix[m-k-1][j]) for j in range(n-k-1,k-1,-1) if matrix[m-k-1][j] not in res]\n            [res.append(matrix[j][k]) for j in range(m-1-k,k-1,-1) if matrix[j][k] not in res]\n        return res\n\nif __name__=='__main__':\n    solution=Solution()\n    m,n=1,5\n    matrix=[]\n    for i in range(0,m):\n        matrix.append(list(map(int,input().split(' '))))\n    print(matrix)\n    ans=solution.printMatrix(matrix)\n    print(ans)\n```\n\n### 20.包含Min函数的栈\n\n**题目：**定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def __init__(self):\n        self.num=[]\n    def push(self, node):\n        # write code here\n        self.num.append(node)\n    def pop(self):\n        # write code here\n        self.num.pop()\n    def top(self):\n        # write code here\n        numlen = len(self.num)\n        return self.num[numlen-1]\n    def min(self):\n        # write code here\n        return min(self.num)\n\nif __name__=='__main__':\n    solution = Solution()\n    solution.push(1)\n    solution.push(2)\n    solution.push(3)\n    solution.push(4)\n    solution.pop()\n    print(solution.top())\n    print(solution.min())\t\t\t\n```\n\n### 21.栈的压入弹出序列\n\n**题目：**输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）。\n\n**题解：**新构建一个中间栈，来模拟栈的输入和栈的输出，比对输入结果和输出结果是否相等。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def IsPopOrder(self, pushV, popV):\n        # write code here\n        if len(pushV)==1 and len(popV)==1 and pushV[0]!=popV[0]:\n            return False\n\n        helpV=[]\n        pushV.reverse()\n        popV.reverse()\n        #模拟给定栈的压入和压出\n        helpV.append(pushV[len(pushV)-1])\n        pushV.pop()\n        while True:\n            if helpV[len(helpV)-1]!=popV[len(popV)-1]:\n                helpV.append(pushV[len(pushV)-1])\n                pushV.pop()\n\n            if helpV[len(helpV)-1]==popV[len(popV)-1]:\n                helpV.pop()\n                popV.pop()\n\n            if pushV==[] and popV==[] and helpV==[]:\n                return True\n\n            if pushV==[] and popV[len(popV)-1]!=helpV[len(helpV)-1]:\n                return False\n\n\nif __name__=='__main__':\n    solution=Solution()\n    push=list(map(int,input().split(' ')))\n    pop=list(map(int,input().split(' ')))\n    ans=solution.IsPopOrder(push,pop)\n    print(ans)\n```\n\n### 22.从上往下打印二叉树\n\n**题目：**从上往下打印出二叉树的每个节点，同层节点从左至右打印。\n\n**思路：**递归，每次将左子树结果和右子树结果存到结果集之中。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    # 返回从上到下每个节点值列表，例：[1,2,3]\n    def PrintFromTopToBottom(self, root):\n        # write code here\n        if root is None:\n            return []\n        ans=[]\n        ans.append(root.val)\n        self.orderans(root,ans)\n        return ans\n\n    def orderans(self,root,ans):\n        if not root:\n            return\n        if root.left:\n            ans.append(root.left.val)\n        if root.right:\n            ans.append(root.right.val)\n\n        self.orderans(root.left, ans)\n        self.orderans(root.right,ans)\n\nif __name__=='__main__':\n    solution=Solution()\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    ans=solution.PrintFromTopToBottom(A1)\n    print(ans)\n```\n\n### 23.二叉树的后续遍历序列\n\n**题目：**输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。\n\n**思路：**二叉搜索树的特性是所有左子树值都小于中节点，所有右子树的值都大于中节点，递归遍历左子树和右子树的值。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def VerifySquenceOfBST(self, sequence):\n        # write code here\n        if not sequence:\n            return False\n        if len(sequence)==1:\n            return True\n        i=0\n        while sequence[i]<sequence[-1]:\n            i=i+1\n        k=i\n        for j in range(i,len(sequence)-1):\n            if sequence[j]<sequence[-1]:\n                return False\n            \n        leftsequence=sequence[:k]\n        rightsequence=sequence[k:len(sequence)-1]\n\n        leftans=True\n        rightans=True\n\n        if len(leftsequence)>0:\n            self.VerifySquenceOfBST(leftsequence)\n        if len(rightsequence)>0:\n            self.VerifySquenceOfBST(rightsequence)\n\n        return leftans and rightans\n\nif __name__=='__main__':\n    solution=Solution()\n    num=list(map(int,input().split(' ')))\n    ans=solution.VerifySquenceOfBST(num)\n    print(ans)\n```\n\n### 24.二叉树中和为某一值的路径\n\n**题目：**输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前)。\n\n**思路：**利用递归的方法，计算加左子树和右子树之后的值，当参数较多是，可以将结果添加到函数变量之中。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    # 返回二维列表，内部每个列表表示找到的路径\n    def FindPath(self, root, expectNumber):\n        # write code here\n        if not root:\n            return []\n        ans=[]\n        path=[]\n        self.dfs(root,expectNumber,ans,path)\n        ans.sort()\n        return ans\n\n    def dfs(self,root,target,ans,path):\n        if not root:\n            return\n\n        path.append(root.val)\n        if root.left is None and root.right is None and target==root.val:\n            ans.append(path[:])\n\n        if root.left:\n            self.dfs(root.left,target-root.val,ans,path)\n        if root.right:\n            self.dfs(root.right,target-root.val,ans,path)\n\n        path.pop()\n\n\nif __name__=='__main__':\n    A1=TreeNode(10)\n    A2=TreeNode(8)\n    A3=TreeNode(12)\n    A4=TreeNode(4)\n    A5=TreeNode(2)\n    A6=TreeNode(2)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A5.left=A6\n\n    expectNumber=22\n    solution=Solution()\n    ans=solution.FindPath(A1,expectNumber)\n    print(ans)\n```\n\n### 25.复杂链表的复制\n\n**题目：**输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）。\n\n**思路：**将大问题转变为小问题，每次都进行复制头部节点，然后进行递归，每次同样处理头部节点。\n\n```python\n# -*- coding:utf-8 -*-\nclass RandomListNode:\n    def __init__(self, x):\n        self.label = x\n        self.next = None\n        self.random = None\n\nclass Solution:\n    # 返回 RandomListNode\n    def Clone(self, pHead):\n        # write code here\n        # 复制头部节点\n        if pHead is None:\n            return None\n\n        newHead=RandomListNode(pHead.label)\n        newHead.next=pHead.next\n        newHead.random=pHead.random\n\n        # 递归其他节点\n        newHead.next=self.Clone(pHead.next)\n\n        return newHead\n\n\nif __name__=='__main__':\n    A1=RandomListNode(2)\n    A2=RandomListNode(3)\n    A3=RandomListNode(4)\n    A4=RandomListNode(5)\n    A5=RandomListNode(6)\n\n    A1.next=A2\n    A1.random=A3\n\n    A2.next=A3\n    A2.random=A4\n\n    A3.next=A4\n    A3.random=A5\n\n    A4.next=A5\n    A4.random=A3\n\n    solution=Solution()\n    ans=solution.Clone(A1)\n```\n\n### 26.二叉搜索树与双向列表\n\n**题目：**输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。\n\n**思路：**递归将根结点和左子树的最右节点和右子树的最左节点进行连接起来。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    def Convert(self, pRootOfTree):\n        # write code here\n        if pRootOfTree is None:\n            return pRootOfTree\n        if pRootOfTree.left is None and pRootOfTree.right is None:\n            return pRootOfTree\n\n        #处理左子树\n        self.Convert(pRootOfTree.left)\n        left=pRootOfTree.left\n\n        if left:\n            while left.right:\n                left=left.right\n            pRootOfTree.left,left.right=left,pRootOfTree\n\n        #处理右子树\n        self.Convert(pRootOfTree.right)\n        right=pRootOfTree.right\n\n        if right:\n            while right.left:\n                right=right.left\n            pRootOfTree.right,right.left=right,pRootOfTree\n\n        while pRootOfTree.left:\n            pRootOfTree=pRootOfTree.left\n        return pRootOfTree\n\n\nif __name__=='__main__':\n    A1 = TreeNode(7)\n    A2 = TreeNode(5)\n    A3 = TreeNode(15)\n    A4 = TreeNode(2)\n    A5 = TreeNode(6)\n    A6 = TreeNode(8)\n    A7 = TreeNode(19)\n    A8 = TreeNode(24)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n    A7.right=A8\n\n    solution=Solution()\n    solution.Convert(A1)\n```\n\n### 27.字符串的排列\n\n**题目：**输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。\n\n**输入：**输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。\n\n**思路：**通过将第k位的字符提取到最前面，然后进行和后面的每个字符进行交换，得到所有结果集。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Permutation(self, ss):\n        # write code here\n        if not ss:\n            return []\n        res=[]\n        self.helper(ss,res,'')\n        return sorted(list(set(res)))\n\n    def helper(self,ss,res,path):\n        if not ss:\n            res.append(path)\n        else:\n            for i in range(0,len(ss)):\n                self.helper(ss[:i]+ss[i+1:],res,path+ss[i])\n\nif __name__=='__main__':\n    str='abbcDeefg'\n    str1='abbc'\n    solution=Solution()\n    ans=solution.Permutation(str1)\n    print(ans)\n```\n\n### 28.数组中出现次数超过一般的数字\n\n**题目：**数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0\n\n**题解：**利用list列表来存放每个数出现的次数ans[numbers[i]]=ans[numbers[i]]+1。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def MoreThanHalfNum_Solution(self, numbers):\n        # write code here\n        numlen=len(numbers)\n        halflen=numlen//2\n        maxans=0\n        ans=[0 for i in range(0,1000)]\n        for i in range(0,len(numbers)):\n            ans[numbers[i]]=ans[numbers[i]]+1\n            if ans[numbers[i]]>maxans:\n                maxans=numbers[i]\n        ans.sort()\n        ans.reverse()\n        res=ans[0]\n        if res>halflen:\n            return maxans\n        else:\n            return 0\n\n\nif __name__=='__main__':\n    num=list(map(int,input().split(',')))\n    solution=Solution()\n    ans=solution.MoreThanHalfNum_Solution(num)\n    print(ans)\n```\n\n### 29.最小的K个数\n\n**题目：**输入n个整数，找出其中最小的K个数，例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def GetLeastNumbers_Solution(self, tinput, k):\n        # write code here\n        if k>len(tinput):\n            return []\n        tinput.sort()\n        return tinput[:k]\n\nif __name__=='__main__':\n    num=list(map(int,input().split(',')))\n    k=int(input())\n    solution=Solution()\n    ans=solution.GetLeastNumbers_Solution(num,k)\n    print(ans)\n```\n\n### 30.连续子数组的最大和\n\n**题目：**HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。你会不会被他忽悠住？(子向量的长度至少是1)\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def FindGreatestSumOfSubArray(self, array):\n        # write code here\n        maxsum,tempsum=array[0],array[0]\n        for i in range(1,len(array)):\n            if tempsum<0:\n                tempsum=array[i]\n            else:\n                tempsum = tempsum + array[i]\n            if tempsum>maxsum:\n                maxsum=tempsum\n        return maxsum\n\nif __name__=='__main__':\n    array=list(map(int,input().split(',')))\n    solution=Solution()\n    ans=solution.FindGreatestSumOfSubArray(array)\n    print(ans)\n```\n\n### 31.整数中1出现的次数\n\n**题目：**求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。\n\n**思路：**对每个数字的每位进行分解，含有1则结果加1。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def NumberOf1Between1AndN_Solution(self, n):\n        # write code here\n        ans=0\n        for i in range(1,n+1):\n            tempans=0\n            while i!=0:\n                eachnum=i%10\n                i=i//10\n                if eachnum==1:\n                    tempans=tempans+1\n            ans=ans+tempans\n        return ans\n\nif __name__=='__main__':\n    n=130\n    solution=Solution()\n    ans=solution.NumberOf1Between1AndN_Solution(n)\n    print(ans)\n```\n\n### 32.把数组排成最小的数\n\n**题目：**输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。\n\n**思路：**将数组转换成字符串之后，进行两两比较字符串的大小，比如3,32的大小由332和323确定，即3+32和32+3确定。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def PrintMinNumber(self, numbers):\n        # write code here\n        if not numbers:\n            return \"\"\n        num = map(str, numbers)\n        for i in range(0,len(numbers)):\n            for j in range(i,len(numbers)):\n                if int(str(numbers[i])+str(numbers[j]))>int(str(numbers[j])+str(numbers[i])):\n                    numbers[i],numbers[j]=numbers[j],numbers[i]\n        ans=''\n        for i in range(0,len(numbers)):\n            ans=ans+str(numbers[i])\n        return ans\n\nif __name__=='__main__':\n    numbers=[3,32,321]\n    solution=Solution()\n    ans=solution.PrintMinNumber(numbers)\n    print(ans)\n```\n\n### 33.丑数\n\n**题目：**把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。\n\n**思路：**每一个丑数必然是由之前的某个丑数与2，3或5的乘积得到的，这样下一个丑数就用之前的丑数分别乘以2，3，5，找出这三这种最小的并且大于当前最大丑数的值，即为下一个要求的丑数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def GetUglyNumber_Solution(self, index):\n        # write code here\n        if (index <= 0):\n            return 0\n        uglyList = [1]\n        indexTwo = 0\n        indexThree = 0\n        indexFive = 0\n        for i in range(index-1):\n            newUgly = min(uglyList[indexTwo]*2, uglyList[indexThree]*3, uglyList[indexFive]*5)\n            uglyList.append(newUgly)\n            if (newUgly % 2 == 0):\n                indexTwo += 1\n            if (newUgly % 3 == 0):\n                indexThree += 1\n            if (newUgly % 5 == 0):\n                indexFive += 1\n        return uglyList[-1]\n\nif __name__=='__main__':\n    solution=Solution()\n    index=200\n    ans=solution.GetUglyNumber_Solution(index)\n    print(ans)\n```\n\n### 34.第一个只出现一次的字符\n\n**题目：**在一个字符串(0<=字符串长度<=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1。\n\n**思路：**找出所有出现一次的字符，然后进行遍历找到第一次出现字符的位置。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def FirstNotRepeatingChar(self, s):\n        # write code here\n        if not s:\n            return -1\n        sset=set(s)\n        dict={}\n        for c in sset:\n            dict[c]=0\n        for i in range(0,len(s)):\n            dict[s[i]]=dict[s[i]]+1\n        onetime=[]\n        for c in dict:\n            if dict[c]==1:\n                onetime.append(c)\n\n        if onetime is None:\n            return -1\n        else:\n            index=0\n            for i in range(0,len(s)):\n                if s[i] in onetime:\n                    index=i\n                    break\n            return index\n\nif __name__=='__main__':\n    s='abbddebbac'\n    solution=Solution()\n    ans=solution.FirstNotRepeatingChar(s)\n    print(ans)\n```\n\n### 35.数组中的逆序对\n\n**题目描述：**在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007。\n\n**输入描述：**题目保证输入的数组中没有的相同的数字。\n\n**数据范围：**\n   对于%50的数据,size<=10^4\n   对于%75的数据,size<=10^5\n   对于%100的数据,size<=2*10^5\n\n> 示例1\n>\n> 输入 1,2,3,4,5,6,7,0\n>\n> 输出 7\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def InversePairs(self, data):\n        # write code here\n        global count\n        count = 0\n\n        def A(array):\n            global count\n            if len(array) <= 1:\n                return array\n            k = int(len(array) / 2)\n            left = A(array[:k])\n            right = A(array[k:])\n            l = 0\n            r = 0\n            result = []\n            while l < len(left) and r < len(right):\n                if left[l] < right[r]:\n                    result.append(left[l])\n                    l += 1\n                else:\n                    result.append(right[r])\n                    r += 1\n                    count += len(left) - l\n            result += left[l:]\n            result += right[r:]\n            return result\n\n        A(data)\n        return count % 1000000007\n\nif __name__=='__main__':\n    data=[1,2,3,4,5,6,7,0]\n    solution=Solution()\n    ans=solution.InversePairs(data)\n    print(ans)\n```\n\n### 36.两个链表的第一个公共节点\n\n**题目：**输入两个链表，找出它们的第一个公共结点。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\nclass Solution:\n    def FindFirstCommonNode(self, pHead1, pHead2):\n        # write code here\n        list1 = []\n        list2 = []\n        node1 = pHead1\n        node2 = pHead2\n        while node1:\n            list1.append(node1.val)\n            node1 = node1.next\n        while node2:\n            if node2.val in list1:\n                return node2\n            else:\n                node2 = node2.next\n\nif __name__=='__main__':\n    A1 = ListNode(1)\n    A2 = ListNode(2)\n    A3 = ListNode(3)\n    A1.next=A2\n    A2.next=A3\n\n    B4 = ListNode(4)\n    B5 = ListNode(5)\n    B4.next=B5\n\n    C6=ListNode(6)\n    C7=ListNode(7)\n\n    A3.next=C6\n    B5.next=C6\n    C6.next=C7\n\n    solution=Solution()\n    ans=solution.FindFirstCommonNode(A1,B4)\n    print(ans.val)\n```\n\n### 37.数字在排序数组中出现的次数\n\n**题目：**统计一个数字在排序数组中出现的次数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def GetNumberOfK(self, data, k):\n        # write code here\n        ans=0\n        for i in range(0,len(data)):\n            if data[i]==k:\n                ans=ans+1\n            if data[i]>k:\n                break\n        return ans\n\nif __name__=='__main__':\n    data=[1,2,3,3,3,4,4,5]\n    k=3\n    solution=Solution()\n    ans=solution.GetNumberOfK(data,k)\n    print(ans)\n```\n\n### 38.二叉树的深度\n\n**题目：**输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nclass Solution:\n    def TreeDepth(self, pRoot):\n        # write code here\n        if pRoot is None:\n            return 0\n        left=self.TreeDepth(pRoot.left)\n        right=self.TreeDepth(pRoot.right)\n        print(left,right)\n        return max(left,right)+1\n\nif __name__=='__main__':\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A4.left=A6\n\n    solution=Solution()\n    ans=solution.TreeDepth(A1)\n    print('ans=',ans)\n```\n\n### 39.平衡二叉树\n\n**题目：**输入一棵二叉树，判断该二叉树是否是平衡二叉树。\n\n**题解：**平衡二叉树是左右子数的距离不能大于1，因此递归左右子树，判断子树距离是否大于1。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    def IsBalanced_Solution(self, pRoot):\n        # write code here\n        if pRoot is None:\n            return True\n        if abs(self.TreeDepth(pRoot.left)-self.TreeDepth(pRoot.right))>1:\n            return False\n        return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right)\n\n    def TreeDepth(self,root):\n        if root is None:\n            return 0\n        left=self.TreeDepth(root.left)\n        right=self.TreeDepth(root.right)\n        return max(left+1,right+1)\n\nif __name__=='__main__':\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    #A4.left=A6\n\n    solution=Solution()\n    ans=solution.IsBalanced_Solution(A1)\n    print(ans)\n```\n\n### 40.数组中只出现一次的数字\n\n**题目：**一个整型数组里除了两个数字之外，其他的数字都出现了偶数次。请写程序找出这两个只出现一次的数字。\n\n**题解：**将数组中数转到set之中，然后利用dict存储每个数字出现的次数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # 返回[a,b] 其中ab是出现一次的两个数字\n    def FindNumsAppearOnce(self, array):\n        # write code here\n        arrayset=set(array)\n        dict={}\n        for num in arrayset:\n            dict[num]=0\n        for i in range(0,len(array)):\n            dict[array[i]]=dict[array[i]]+1\n        ans=[]\n        for num in arrayset:\n            if dict[num]==1:\n                ans.append(num)\n        return ans\n\n\nif __name__=='__main__':\n    array=[1,1,2,2,3,3,4,5,5,6,7,7]\n    solution=Solution()\n    ans=solution.FindNumsAppearOnce(array)\n    print(ans)\n```\n### 41.和为S的连续正整数序列\n\n**题目：**小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck!\n\n**输出描述：**输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序。\n\n**思路：**首项加尾项*2等于和，那么只要遍历项的开始和长度即可。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def FindContinuousSequence(self, tsum):\n        # write code here\n        ans=[]\n        for i in range(1,tsum//2+1):\n            oneans=[]\n            for k in range(1,tsum):\n                tempsum=((i+i+k-1)*k)//2\n                if tempsum==tsum:\n                    for j in range(i,i+k):\n                        oneans.append(j)\n                    break\n            if oneans !=[]:\n                ans.append(oneans)\n        return ans\n\nif __name__=='__main__':\n    tsum=15\n    solution=Solution()\n    ans=solution.FindContinuousSequence(tsum)\n    print(ans)\n```\n\n### 42.和为S的两个数字\n\n**题目：**输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。\n\n**输出描述：**对应每个测试案例，输出两个数，小的先输出。\n\n**思路：**利用i和j从后面进行扫描结果，选取最小的乘积放入到结果集之中。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def FindNumbersWithSum(self, array, tsum):\n        # write code here\n        ans=[]\n        i,j,minres=0,len(array)-1,1000000\n        for i in range(0,len(array)-1):\n            j=len(array)-1\n            while True:\n                tempsum = array[i] + array[j]\n                if tempsum == tsum:\n                    if array[i]*array[j]<minres:\n                        ans=[]\n                        ans.append(array[i])\n                        ans.append(array[j])\n                        minres=array[i]*array[j]\n                    break\n                else:\n                    j = j - 1\n                if tempsum<tsum:\n                    break\n                if j<=i:\n                    break\n        return ans\n\nif __name__=='__main__':\n    array=[1,2,4,7,11,15]\n    tsum=15\n    solution=Solution()\n    ans=solution.FindNumbersWithSum(array,tsum)\n    print(ans)\n```\n\n### 43.左旋字符子串\n\n**题目：**汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def LeftRotateString(self, s, n):\n        # write code here\n        if s=='' and n==0:\n            return ''\n        ans=''\n        ans=s[n:]+s[0:n]\n        return ans\n\nif __name__=='__main__':\n    s='abcdefg'\n    n=2\n    solution=Solution()\n    ans=solution.LeftRotateString(s,n)\n    print(ans)\n```\n\n### 44.反转单词顺序\n\n**题目：**牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def ReverseSentence(self, s):\n        # write code here\n        ans,word=[],''\n        for i in range(0,len(s)):\n            word = word + s[i]\n            if s[i]==' ':\n                ans.append(word)\n                word=''\n            if i==len(s)-1:\n                word=word+' '\n                ans.append(word)\n        ans.reverse()\n        res=''\n        for c in ans:\n            res=res+c\n        return res[:len(res)-1]\n\nif __name__=='__main__':\n    solution=Solution()\n    s='I am a student.'\n    ans=solution.ReverseSentence(s)\n    print(ans)\n```\n\n### 45.扑克牌顺序\n\n**题目：**LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)...他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子.....LL不高兴了,他想了想,决定大\\小王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def IsContinuous(self, numbers):\n        # write code here\n        if numbers==[]:\n            return False\n        numbers.sort()\n        zero=0\n        for i in range(0,len(numbers)):\n            if numbers[i]==0:\n                zero=zero+1\n        for i in range(zero+1,len(numbers)):\n            if numbers[i]==numbers[i-1]:\n                return False\n            if numbers[i]-numbers[i-1]==1:\n                continue\n            else:\n                diff=numbers[i]-numbers[i-1]-1\n                zero=zero-diff\n\n        if zero<0:\n            return False\n        return True\n    \nif __name__=='__main__':\n    numbers=[1,0,0,1,0]\n    solution=Solution()\n    ans=solution.IsContinuous(numbers)\n    print(ans)\n```\n\n### 46.孩子们的圈圈(圈圈中最后剩下的数)\n\n**题目：**每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)。\n\n**思路：**约瑟夫环问题。\n\n```python\n# 题目\n# 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。\n# 其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。\n# 每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,\n# 从他的下一个小朋友开始,继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,\n# 并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)\n\n# 思路\n# 约瑟夫环问题\n\n# -*- coding:utf-8 -*-\nclass Solution:\n    def LastRemaining_Solution(self, n, m):\n        # write code here\n        if n<1 or m<1:\n            return -1\n        last=0\n        for i in range(2,n+1):\n            last=(last+m)%i\n        return last\n\nif __name__=='__main__':\n    n,m=8,4\n    solution=Solution()\n    ans=solution.LastRemaining_Solution(n,m)\n    print(ans)\n```\n\n### 47.求1+2+3+...+n\n\n**题目：**求1+2+3+...+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。\n\n**思路：**利用递归当作计算结果。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Sum_Solution(self, n):\n        # write code here\n        if n==0:\n            return 0\n        return self.Sum_Solution(n-1)+n\n\nif __name__=='__main__':\n    n=6\n    solution=Solution()\n    ans=solution.Sum_Solution(n)\n    print(ans)\n```\n\n### 48.不用加减乘除做加法\n\n**题目：**写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。\n\n**思路：**二进制异或进位。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def Add(self, num1, num2):\n        # write code here\n        while num2!=0:\n            sum=num1^num2\n            carry=(num1&num2)<<1\n            num1=sum\n            num2=carry\n        return num1\n\nif __name__=='__main__':\n    num1,num2=10,500000\n    solution=Solution()\n    ans=solution.Add(num1,num2)\n    print(ans)\n```\n\n### 49.把字符串转换成整数\n\n**题目：**将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。\n\n**输入描述：**输入一个字符串,包括数字字母符号,可以为空输出描述:如果是合法的数值表达则返回该数字，否则返回0。\n\n```python\n示例\n+2147483647\n    1a33\n2147483647\n    0\n```\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def StrToInt(self, s):\n        # write code here\n        if len(s) == 0:\n            return 0\n        else:\n            if s[0] > '9' or s[0] < '0':\n                a = 0\n            else:\n                a = int(s[0]) * 10 ** (len(s) - 1)\n            if len(s) > 1:\n                for i in range(1, len(s)):\n                    if s[i] >= '0' and s[i] <= '9':\n                        a = a + int(s[i]) * 10 ** (len(s) - 1 - i)\n                    else:\n                        return 0\n        if s[0] == '+':\n            return a\n        if s[0] == '-':\n            return -a\n        return a\n\nif __name__=='__main__':\n    s='115'\n    solution=Solution()\n    ans=solution.StrToInt(s)\n    print(ans)\n```\n\n### 50.数组中重复的数字\n\n**题目：**在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。\n\n**思路：**利用dict计算重复数字。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0]\n    # 函数返回True/False\n    def duplicate(self, numbers, duplication):\n        # write code here\n        numset=set(numbers)\n        dict={}\n        duplication.append(0)\n        for val in numbers:\n            dict[val]=0\n        for i in range(0,len(numbers)):\n            dict[numbers[i]]=dict[numbers[i]]+1\n        for val in numset:\n            if dict[val]>1:\n                duplication[0]=val\n                return True\n        return False\n\nif __name__=='__main__':\n    numbers=[2,1,3,1,4]\n    solution=Solution()\n    duplication=[]\n    ans=solution.duplicate(numbers,duplication)\n    print(ans)\n```\n\n### 51.构建乘积数组\n\n```python\n# 题目\n# 给定一个数组A[0,1,...,n-1],请构建一个数组B[0,1,...,n-1],\n# 其中B中的元素B[i]=A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]。不能使用除法。\n\n# 思路\n# 审题仔细 没有A[i]\n\n# -*- coding:utf-8 -*-\nclass Solution:\n    def multiply(self, A):\n        # write code here\n        B=[]\n        for i in range(0,len(A)):\n            temp=1\n            for j in range(0,len(A)):\n                if j==i:\n                    continue\n                temp=temp*A[j]\n            B.append(temp)\n        return B\n\nif __name__=='__main__':\n    solution=Solution()\n    A=[1,2,3,4,5]\n    ans=solution.multiply(A)\n    print(ans)\n```\n\n### 52.正则表达式匹配\n\n**题目：**请实现一个函数用来匹配包括'.'和'\\*'的正则表达式。模式中的字符'.'表示任意一个字符，而'\\*'表示它前面的字符可以出现任意次（包含0次）。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串\"aaa\"与模式\"a.a\"和\"ab\\*ac\\*a\"匹配，但是与\"aa.a\"和\"ab*a\"均不匹配。\n\n**思路：**\n\n> 当模式中的第二个字符不是`*`时： \n>\n> - 如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 \n> - 如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。\n\n> 当模式中的第二个字符是`*`时：\n>\n> + 如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。\n> + 如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式。\n>   + 模式后移2字符，相当于`x*`被忽略。即模式串中*与他前面的字符和字符串匹配0次。 \n>   +  字符串后移1字符，模式后移2字符。即模式串中*与他前面的字符和字符串匹配1次。\n>   + 字符串后移1字符，模式不变，即继续匹配字符下一位，因为`*`可以匹配多位。即模式串中*与他前面的字符和字符串匹配多次。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # s, pattern都是字符串\n    def match(self, s, pattern):\n        if s == pattern:\n            return True\n        if not pattern:\n            return False\n        if len(pattern) > 1 and pattern[1] == '*':\n            if (s and s[0] == pattern[0]) or (s and pattern[0] == '.'):\n                return self.match(s, pattern[2:]) \\\n                       or self.match(s[1:], pattern) \\\n                       or self.match(s[1:], pattern[2:])\n            else:\n                return self.match(s, pattern[2:])\n        elif s and (s[0] == pattern[0] or pattern[0] == '.'):\n            return self.match(s[1:], pattern[1:])\n        return False\n\nif __name__=='__main__':\n    solution=Solution()\n    s='aaa'\n    pattern='a*a.a'\n    ans=solution.match(s,pattern)\n    print(ans)\n```\n\n### 53.表示数值的字符串\n\n**题目：**请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串\"+100\",\"5e2\",\"-123\",\"3.1416\"和\"-1E-16\"都表示数值。 但是\"12e\",\"1a3.14\",\"1.2.3\",\"+-5\"和\"12e+4.3\"都不是。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # s字符串\n    def isNumeric(self, s):\n        # write code here\n        # 标记符号、小数点、e是否出现过\n        sign,decimal,hasE=False,False,False\n        for i in range(0,len(s)):\n            if s[i]=='e' or s[i]=='E':\n                if i==len(s)-1:# e后面一定要接数字\n                    return False\n                if hasE==True:# 不能出现两次e\n                    return False\n                hasE=True\n            elif s[i]=='+' or s[i]=='-':\n                #第二次出现+或-一定要在e之后\n                if sign and s[i-1]!='e' and s[i-1]!='E':\n                    return False\n                # 第一次出现+或-，如果不是出现在字符最前面，那么就要出现在e或者E后面\n                if sign==False and i>0 and s[i-1]!='e' and s[i-1]!='E':\n                    return False\n                sign=True\n            elif s[i]=='.':\n                # e后面不能出现小数点，小数点不能出现两次\n                if decimal or hasE:\n                    return False\n                decimal=True\n            elif s[i]>'9' or s[i]<'0':\n                return False\n        return True\n\nif __name__=='__main__':\n    solution=Solution()\n    s='123e.1416'\n    ans=solution.isNumeric(s)\n    print(ans)\n```\n\n### 54.字符流中第一个不重复的字符\n\n**题目：**请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符\"go\"时，第一个只出现一次的字符是\"g\"。当从该字符流中读出前六个字符“google\"时，第一个只出现一次的字符是\"l\"。\n\n**输出描述：**如果当前字符流没有存在出现一次的字符，返回#字符。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    # 返回对应char\n    def __init__(self):\n        self.all={}\n        self.ch=[]\n    def FirstAppearingOnce(self):\n        # write code here\n        if self.all is None:\n            return '#'\n        for c in self.ch:\n            if self.all[c]==1:\n                return c\n        return '#'\n\n    def Insert(self, char):\n        # write code here\n        self.ch.append(char)\n        if char in self.all:\n            self.all[char]=self.all[char]+1\n        else:\n            self.all[char]=1\n\nif __name__=='__main__':\n    solution=Solution()\n    solution.Insert('g')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('o')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('o')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('g')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('l')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n    solution.Insert('e')\n    ans = solution.FirstAppearingOnce()\n    print(ans)\n```\n\n### 55.链表中环的入口节点\n\n**题目：**给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。\n\n**思路：**把链表中节点值放到dict数组中，并记录出现的次数，如果出现次数超过一次，则为环的入口节点。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\nclass Solution:\n    def EntryNodeOfLoop(self, pHead):\n        # write code here\n        if pHead is None:\n            return None\n        num,dict,flag=[],{},True\n        tempans=0\n        while pHead and flag==True:\n            num.append(pHead.val)\n            numset=set(num)\n            for c in numset:\n                dict[c]=0\n            for c in num:\n                dict[c]=dict[c]+1\n            for c in num:\n                if dict[c]>1:\n                    flag=False\n                    tempans=c\n            pHead=pHead.next\n        while pHead:\n            if pHead.val==tempans:\n                return pHead\n            pHead=pHead.next\n        return None\n\nif __name__=='__main__':\n    pHead1 = ListNode(1)\n    pHead2 = ListNode(2)\n    pHead3 = ListNode(3)\n    pHead4 = ListNode(4)\n    pHead5 = ListNode(5)\n\n    pHead1.next=pHead2\n    pHead2.next=pHead3\n    pHead3.next=pHead4\n    pHead4.next=pHead5\n    pHead5.next=pHead1\n\n    solution=Solution()\n    ans=solution.EntryNodeOfLoop(pHead1)\n    print(ans.val)\n```\n\n### 56.删除链表中重复的节点\n\n**题目：**在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1->2->3->3->4->4->5 处理后为 1->2->5。\n\n**思路：**记录链表中出现的数字，然后构建新链表。\n\n```python\n# -*- coding:utf-8 -*-\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\nclass Solution:\n    def deleteDuplication(self, pHead):\n        # write code here\n        num=[]\n        tempnum1=pHead\n        while tempnum1:\n            num.append(tempnum1.val)\n            tempnum1=tempnum1.next\n        dict={}\n        for c in num:\n            dict[c]=0\n        for c in num:\n            dict[c]=dict[c]+1\n        newnum=[]\n        for c in num:\n            if dict[c]==1:\n                newnum.append(c)\n        if newnum==[]:\n            return None\n        head=ListNode(newnum[0])\n        temphead=head\n        for i in range(1,len(newnum)):\n            tempnode=ListNode(newnum[i])\n            temphead.next=tempnode\n            temphead=tempnode\n        # while head:\n        #     print(head.val)\n        #     head=head.next\n        return head\n\nif __name__=='__main__':\n    pHead1 = ListNode(1)\n    pHead2 = ListNode(1)\n    pHead3 = ListNode(1)\n    pHead4 = ListNode(1)\n    pHead5 = ListNode(1)\n    pHead6 = ListNode(1)\n    pHead7 = ListNode(1)\n\n    pHead1.next=pHead2\n    pHead2.next=pHead3\n    pHead3.next=pHead4\n    pHead4.next=pHead5\n    pHead5.next=pHead6\n    pHead6.next=pHead7\n\n    solution=Solution()\n    ans=solution.deleteDuplication(pHead1)\n    print(ans)\n```\n\n### 57. 二叉树中的下一个节点\n\n**题目：**给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。\n\n**思路：**分析二叉树的下一个节点，一共有以下情况：1.二叉树为空，则返回空；2.节点右孩子存在，则设置一个指针从该节点的右孩子出发，一直沿着指向左子结点的指针找到的叶子节点即为下一个节点；3.节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点；否则继续向上遍历其父节点的父节点，重复之前的判断，返回结果。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeLinkNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n        self.next = None\nclass Solution:\n    def GetNext(self, pNode):\n        # write code here\n        if not pNode:\n            return pNode\n        if pNode.right:\n            left1=pNode.right\n            while left1.left:\n                   left1=left1.left\n            return left1\n\n        while pNode.next:\n            tmp=pNode.next\n            if tmp.left==pNode:\n                return tmp\n            pNode=tmp\n\nif __name__=='__main__':\n    solution=Solution()\n```\n\n### 58.对称的二叉树\n\n**题目：**请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。\n\n**思路：**采用递归的方法来判断两数是否相同。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    def isSymmetrical(self, pRoot):\n        # write code here\n        if not pRoot:\n            return True\n        result=self.same(pRoot,pRoot)\n        return result\n    def same(self,root1,root2):\n        if not root1 and not root2:\n            return True\n        if root1 and not root2:\n            return False\n        if not root1 and root2:\n            return False\n        if root1.val!= root2.val:\n            return False\n\n        left=self.same(root1.left,root2.right)\n        if not left:\n            return False\n        right=self.same(root1.right,root2.left)\n        if not right:\n            return False\n        return True\n\nif __name__=='__main__':\n\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(2)\n    A4 = TreeNode(3)\n    A5 = TreeNode(4)\n    A6 = TreeNode(4)\n    A7 = TreeNode(3)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n\n    solution = Solution()\n    ans=solution.isSymmetrical(A1)\n    print(ans)\n```\n\n### 59.按之字形顺序打印二叉树\n\n**题目：**请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。\n\n**思路：** 把当前列结果存放到list之中，设置翻转变量，依次从左到右打印和从右到左打印。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    def Print(self, pRoot):\n        # write code here\n        root=pRoot\n        if not root:\n            return []\n        level=[root]\n        result=[]\n        righttoleft=False\n        while level:\n            curvalues=[]\n            nextlevel=[]\n            for i in level:\n                curvalues.append(i.val)\n                if i.left:\n                    nextlevel.append(i.left)\n                if i.right:\n                    nextlevel.append(i.right)\n            if righttoleft:\n                    curvalues.reverse()\n            if curvalues:\n                    result.append(curvalues)\n            level = nextlevel\n            righttoleft = not righttoleft\n        return result\n\nif __name__=='__main__':\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n    A7 = TreeNode(7)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    solution = Solution()\n    ans=solution.Print(A1)\n    print(ans)\n```\n\n### 60.把二叉树打印成多行\n\n**题目：**从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    # 返回二维列表[[1,2],[4,5]]\n    def Print(self, pRoot):\n        # write code here\n        root=pRoot\n        if not root:\n            return []\n        level=[root]\n        result=[]\n        while level:\n            curvalues=[]\n            nextlevel=[]\n            for i in level:\n                curvalues.append(i.val)\n                if i.left:\n                    nextlevel.append(i.left)\n                if i.right:\n                    nextlevel.append(i.right)\n            if curvalues:\n                    result.append(curvalues)\n            level = nextlevel\n        return result\n\nif __name__=='__main__':\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n    A7 = TreeNode(7)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    solution = Solution()\n    ans=solution.Print(A1)\n    print(ans)\n```\n\n### 61.序列化二叉树\n\n**题目：**请实现两个函数，分别用来序列化和反序列化二叉树。\n\n**思路：**转变成前序遍历，空元素利用\"#\"代替，然后进行解序列。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\n\nimport collections\nclass Solution:\n    def Serialize(self, root):\n        # write code here\n        if not root:\n            return None\n        res=[]\n        self.pre(root,res)\n        return res\n\n    def pre(self,root,res):\n        if not root:\n            return\n        res.append(root.val)\n        if root.left:\n            self.pre(root.left, res)\n        else:\n            res.append('#')\n        if root.right:\n            self.pre(root.right,res)\n        else:\n            res.append('#')\n    def Deserialize(self, s):\n        if s=='':\n            return None\n        vals=[]\n        for i in range(0,len(s)):\n            vals.append(s[i])\n        vals=collections.deque(vals)\n        ans=self.build(vals)\n        return ans\n\n    def build(self,vals):\n        if vals:\n            val = vals.popleft()\n            if val == '#':\n                return None\n            root = TreeNode(int(val))\n            root.left = self.build(vals)\n            root.right = self.build(vals)\n            return root\n        return self.build(vals)\n\n# [1, ',', 2, ',', 4, ',', ',', ',', 5, ',', ',', ',', 3, ',', 6, ',', ',', ',', 7, ',', ',']\nif __name__==\"__main__\":\n    A1 = TreeNode(1)\n    A2 = TreeNode(2)\n    A3 = TreeNode(3)\n    A4 = TreeNode(4)\n    A5 = TreeNode(5)\n    A6 = TreeNode(6)\n    A7 = TreeNode(7)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    solution = Solution()\n    ans=solution.Serialize(A1)\n    print(ans)\n    root=solution.Deserialize(ans)\n    res=solution.Serialize(root)\n    print(res)\n```\n\n### 62.二叉搜索树中的第K个节点\n\n**题目：**给定一棵二叉搜索树，请找出其中的第k小的结点。例如（5，3，7，2，4，6，8）中，按结点数值大小顺序第三小结点的值为4。\n\n**思路：**中序遍历后，返回第K个节点值。\n\n```python\n# -*- coding:utf-8 -*-\nclass TreeNode:\n    def __init__(self, x):\n        self.val = x\n        self.left = None\n        self.right = None\nclass Solution:\n    # 返回对应节点TreeNode\n    def KthNode(self, pRoot, k):\n        # write code here\n        res=[]\n        if not pRoot:\n            return None\n        self.order(pRoot,res)\n        if len(res)<k or k<=0:\n            return None\n        else:\n            return res[k-1]\n\n    def order(self,root,res):\n        if not root:\n            return\n        self.order(root.left,res)\n        res.append(root)\n        self.order(root.right,res)\n\nif __name__=='__main__':\n    A1 = TreeNode(5)\n    A2 = TreeNode(3)\n    A3 = TreeNode(7)\n    A4 = TreeNode(2)\n    A5 = TreeNode(4)\n    A6 = TreeNode(6)\n    A7 = TreeNode(8)\n\n    A1.left=A2\n    A1.right=A3\n    A2.left=A4\n    A2.right=A5\n    A3.left=A6\n    A3.right=A7\n\n    k=3\n    solution = Solution()\n    ans=solution.KthNode(A1,k)\n    print(ans)\n```\n\n### 63.数据流中的中位数\n\n**题目：**如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def __init__(self):\n        self.data=[]\n    def Insert(self, num):\n        # write code here\n        self.data.append(num)\n        self.data.sort()\n    def GetMedian(self):\n        # write code here\n        length=len(self.data)\n        if length%2==0:\n            return (self.data[length//2]+self.data[length//2-1])/2.0\n        else:\n            return self.data[int(length//2)]\n\n\nif __name__==\"__main__\":\n    solution=Solution()\n    solution.Insert(5)\n    ans = solution.GetMedian()\n    print(ans)\n    solution.Insert(2)\n    ans = solution.GetMedian()\n    print(ans)\n    solution.Insert(3)\n    ans = solution.GetMedian()\n    print(ans)\n    solution.Insert(4)\n    ans = solution.GetMedian()\n    print(ans)\n    solution.Insert(1)\n    ans = solution.GetMedian()\n    print(ans)\n```\n\n### 64.滑动窗口的最大值\n\n**题目：**给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}，{2,3,4,2,6,[2,5,1]}。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def maxInWindows(self, num, size):\n        # write code here\n        if size==0 or num==[]:\n            return []\n        res=[]\n        for i in range(0,len(num)-size+1):\n            tempnum=[]\n            for j in range(i,i+size):\n                tempnum.append(num[j])\n            res.append(max(tempnum))\n        return res\n\nif __name__==\"__main__\":\n    solution=Solution()\n    num=[2,3,4,2,6,2,5,1]\n    size=3\n    ans=solution.maxInWindows(num,size)\n    print(ans)\n```\n\n### 66.矩阵中的路径\n\n**题目：**请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串\"bcced\"的路径，但是矩阵中不包含\"abcb\"路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。\n\n**思路：**当起点第一个字符相同时，开始进行递归搜索，设计搜索函数。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def hasPath(self, matrix, rows, cols, path):\n        # write code here\n        for i in range(0,rows):\n            for j in range(0,cols):\n                if matrix[i*rows+j]==path[0]:\n                    if self.find_path(list(matrix),rows,cols,path[1:],i,j):\n                        return True\n        return False\n\n    def find_path(self,matrix,rows,cols,path,i,j):\n        if not path:\n            return True\n        matrix[i*cols+j]=0\n        if j+1<cols and matrix[i*cols+j+1]==path[0]:\n            return self.find_path(matrix,rows,cols,path[1:],i,j+1)\n        elif j-1>=0 and matrix[i*cols+j-1]==path[0]:\n            return self.find_path(matrix, rows, cols, path[1:], i, j - 1)\n        elif i+1<rows and matrix[(i+1)*cols+j]==path[0]:\n            return self.find_path(matrix, rows, cols, path[1:], i+1, j)\n        elif i-1>=0 and matrix[(i-1)*cols+j]==path[0]:\n            return self.find_path(matrix, rows, cols, path[1:], i-1, j)\n        else:\n            return False\n\nif __name__=='__main__':\n    solution=Solution()\n    matrix='ABCEHJIGSFCSLOPQADEEMNOEADIDEJFMVCEIFGGS'\n    rows=5\n    cols=8\n    path='SGGFIECVAASABCEHJIGQEMS'\n    ans=solution.hasPath(matrix,rows,cols,path)\n    print(ans)\n```\n\n### 66.机器人的运动范围\n\n**题目：**地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？\n\n**思路：**对未走过的路径进行遍历，搜索所有的路径值。\n\n```python\n# -*- coding:utf-8 -*-\nclass Solution:\n    def __init__(self):\n        self.vis = {}\n\n    def movingCount(self, threshold, rows, cols):\n        # write code here\n        return self.moving(threshold, rows, cols, 0, 0)\n\n    def moving(self, threshold, rows, cols, row, col):\n        rowans,colans=0,0\n        rowtemp,coltemp=row,col\n        while rowtemp>0:\n            rowans=rowans+rowtemp%10\n            rowtemp=rowtemp//10\n        while coltemp>0:\n            colans=colans+coltemp%10\n            coltemp=coltemp//10\n\n        if rowans+colans>threshold:\n            return 0\n        if row >= rows or col >= cols or row < 0 or col < 0:\n            return 0\n        if (row, col) in self.vis:\n            return 0\n        self.vis[(row, col)] = 1\n\n        return 1 + self.moving(threshold, rows, cols, row - 1, col) +\\\n               self.moving(threshold, rows, cols, row + 1,col) + \\\n               self.moving(threshold, rows,cols, row,col - 1) + \\\n               self.moving(threshold, rows, cols, row, col + 1)\n\n\nif __name__=='__main__':\n    solution=Solution()\n    threshold=10\n    rows,cols=1,100\n    ans=solution.movingCount(threshold,rows,cols)\n    print(ans)\n```\n### 67.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](《剑指Offer》Python版/推广.png)","slug":"《剑指Offer》Python版","published":1,"updated":"2018-08-06T03:56:23.764Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjktv5ddx005vjiz5pjqvwuqh","content":"<h3 id=\"1-二维数组中的查找\"><a href=\"#1-二维数组中的查找\" class=\"headerlink\" title=\"1.二维数组中的查找\"></a>1.二维数组中的查找</h3><p><strong>题目：</strong> 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p>\n<p><strong>思路：</strong>遍历每一行，查找该元素是否在该行之中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># array 二维列表</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Find</span><span class=\"params\">(self, target, array)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> array:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> target <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    target=<span class=\"number\">2</span></span><br><span class=\"line\">    array=[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>],[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],[<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>]]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Find(target,array)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-替换空格\"><a href=\"#2-替换空格\" class=\"headerlink\" title=\"2.替换空格\"></a>2.替换空格</h3><p><strong>题目：</strong> 请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。</p>\n<p><strong>思路：</strong>利用字符串中的replace直接替换即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># s 源字符串</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">replaceSpace</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        temp = s.replace(<span class=\"string\">\" \"</span>, <span class=\"string\">\"%20\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> temp</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=<span class=\"string\">'We Are Happy'</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.replaceSpace(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-从尾到头打印链表\"><a href=\"#3-从尾到头打印链表\" class=\"headerlink\" title=\"3.从尾到头打印链表\"></a>3.从尾到头打印链表</h3><p><strong>题目：</strong>输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。</p>\n<p><strong>思路：</strong>将链表中的值记录到list之中，然后进行翻转list。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回从尾部到头部的列表值序列，例如[1,2,3]</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printListFromTailToHead</span><span class=\"params\">(self, listNode)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        l=[]</span><br><span class=\"line\">        <span class=\"keyword\">while</span> listNode:</span><br><span class=\"line\">            l.append(listNode.val)</span><br><span class=\"line\">            listNode=listNode.next</span><br><span class=\"line\">        <span class=\"keyword\">return</span> l[::<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = ListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.next=A2</span><br><span class=\"line\">    A2.next=A3</span><br><span class=\"line\">    A3.next=A4</span><br><span class=\"line\">    A4.next=A5</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.printListFromTailToHead(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-重建二叉树\"><a href=\"#4-重建二叉树\" class=\"headerlink\" title=\"4.重建二叉树\"></a>4.重建二叉树</h3><p><strong>题目：</strong>输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。</p>\n<p><strong>题解：</strong>首先前序遍历的第一个元素为二叉树的根结点，那么便能够在中序遍历之中找到根节点，那么在根结点左侧则是左子树，假设长度为M.在根结点右侧，便是右子树,假设长度为N。然后在前序遍历根节点后面M长度的便是左子树的前序遍历序列，再后面的N个长度便是右子树的后序遍历的长度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回构造的TreeNode根节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reConstructBinaryTree</span><span class=\"params\">(self, pre, tin)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(pre)==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(pre)==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> TreeNode(pre[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            flag=TreeNode(pre[<span class=\"number\">0</span>])</span><br><span class=\"line\">            flag.left=self.reConstructBinaryTree(pre[<span class=\"number\">1</span>:tin.index(pre[<span class=\"number\">0</span>])+<span class=\"number\">1</span>],tin[:tin.index(pre[<span class=\"number\">0</span>])])</span><br><span class=\"line\">            flag.right=self.reConstructBinaryTree(pre[tin.index(pre[<span class=\"number\">0</span>])+<span class=\"number\">1</span>:],tin[tin.index(pre[<span class=\"number\">0</span>])+<span class=\"number\">1</span>:])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> flag</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    pre=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    tin=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    ans=solution.reConstructBinaryTree(pre,tin)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-用两个栈实现队列\"><a href=\"#5-用两个栈实现队列\" class=\"headerlink\" title=\"5.用两个栈实现队列\"></a>5.用两个栈实现队列</h3><p><strong>题目：</strong>用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。</p>\n<p><strong>题解：</strong>申请两个栈Stack1和Stack2，Stack1当作输入，Stack2当作pop。当Stack2空的时候，将Stack1进行反转，并且输入到Stack2。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.Stack1=[]</span><br><span class=\"line\">        self.Stack2=[]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">push</span><span class=\"params\">(self, node)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.Stack1.append(node)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pop</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># return xx</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.Stack2==[]:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> self.Stack1:</span><br><span class=\"line\">                self.Stack2.append(self.Stack1.pop())</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.Stack2.pop()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.Stack2.pop()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    solution.push(<span class=\"number\">1</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">2</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">3</span>)</span><br><span class=\"line\">    print(solution.pop())</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-旋转数组的最小数字\"><a href=\"#6-旋转数组的最小数字\" class=\"headerlink\" title=\"6.旋转数组的最小数字\"></a>6.旋转数组的最小数字</h3><p><strong>题目：</strong>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。</p>\n<p><strong>题解：</strong>遍历数组寻找数组最小值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">minNumberInRotateArray</span><span class=\"params\">(self, rotateArray)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        minnum=<span class=\"number\">999999</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(rotateArray)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> minnum&gt;rotateArray[i]:</span><br><span class=\"line\">                minnum=rotateArray[i]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> minnum:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> minnum</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    rotateArray=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    ans=solution.minNumberInRotateArray(rotateArray)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"7-斐波那契数列\"><a href=\"#7-斐波那契数列\" class=\"headerlink\" title=\"7.斐波那契数列\"></a>7.斐波那契数列</h3><p><strong>题目：</strong>大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39。</p>\n<p><strong>题解：</strong>递归和非递归方法。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Fibonacci</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        Fib=[<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,n+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        Fib[<span class=\"number\">0</span>],Fib[<span class=\"number\">1</span>]=<span class=\"number\">0</span>,<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>,n+<span class=\"number\">1</span>):</span><br><span class=\"line\">            Fib[i]=Fib[i<span class=\"number\">-1</span>]+Fib[i<span class=\"number\">-2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Fib[n]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Fibonacci1</span><span class=\"params\">(self,n)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">1</span> <span class=\"keyword\">or</span> n==<span class=\"number\">2</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.Fibonacci1(n<span class=\"number\">-1</span>)+self.Fibonacci1(n<span class=\"number\">-2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.Fibonacci1(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-跳台阶\"><a href=\"#8-跳台阶\" class=\"headerlink\" title=\"8.跳台阶\"></a>8.跳台阶</h3><p><strong>题目：</strong>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。</p>\n<p><strong>题解：</strong>ans[n]=ans[n-1]+ans[n-2]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">jumpFloor</span><span class=\"params\">(self, number)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">2</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span></span><br><span class=\"line\">        ans=[<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,number+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        ans[<span class=\"number\">1</span>],ans[<span class=\"number\">2</span>]=<span class=\"number\">1</span>,<span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>,number+<span class=\"number\">1</span>):</span><br><span class=\"line\">            ans[i]=ans[i<span class=\"number\">-1</span>]+ans[i<span class=\"number\">-2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans[number]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.jumpFloor(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"9-变态跳台阶\"><a href=\"#9-变态跳台阶\" class=\"headerlink\" title=\"9.变态跳台阶\"></a>9.变态跳台阶</h3><p><strong>题目：</strong>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p>\n<p><strong>题解：</strong>ans[n]=ans[n-1]+ans[n-2]+ans[n-3]+…+ans[n-n]，ans[n-1]=ans[n-2]+ans[n-3]+…+ans[n-n]，ans[n]=2*ans[n-1]。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">jumpFloorII</span><span class=\"params\">(self, number)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">2</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">2</span>*self.jumpFloorII(number<span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.jumpFloorII(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"10-矩形覆盖\"><a href=\"#10-矩形覆盖\" class=\"headerlink\" title=\"10.矩形覆盖\"></a>10.矩形覆盖</h3><p><strong>题目：</strong>我们可以用2<em>1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2</em>1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？</p>\n<p><strong>题解：</strong>新增加的小矩阵竖着放，则方法与n-1时相同，新增加的小矩阵横着放，则方法与n-2时相同，于是f(n)=f(n-1)+f(n-2)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rectCover</span><span class=\"params\">(self, number)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        Fib=[<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,number+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        Fib[<span class=\"number\">1</span>],Fib[<span class=\"number\">2</span>]=<span class=\"number\">1</span>,<span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>,number+<span class=\"number\">1</span>):</span><br><span class=\"line\">            Fib[i]=Fib[i<span class=\"number\">-1</span>]+Fib[i<span class=\"number\">-2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Fib[number]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.rectCover(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"11-二进制中1的个数\"><a href=\"#11-二进制中1的个数\" class=\"headerlink\" title=\"11.二进制中1的个数\"></a>11.二进制中1的个数</h3><p><strong>题目：</strong>输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。</p>\n<p><strong>题解：</strong>每次进行左移一位，然后与1进行相与，如果是1则进行加1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">NumberOf1</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        count = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">32</span>):</span><br><span class=\"line\">            count += (n &gt;&gt; i) &amp; <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> count</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.NumberOf1(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"12-数值的整次方\"><a href=\"#12-数值的整次方\" class=\"headerlink\" title=\"12.数值的整次方\"></a>12.数值的整次方</h3><p><strong>题目：</strong>给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Power</span><span class=\"params\">(self, base, exponent)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,abs(exponent)):</span><br><span class=\"line\">            ans=ans*base</span><br><span class=\"line\">        <span class=\"keyword\">if</span> exponent&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> ans</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span>/ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    base=float(input())</span><br><span class=\"line\">    exponent=int(input())</span><br><span class=\"line\">    ans=solution.Power(base,exponent)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"13-调整数组顺序使奇数位于偶数前面\"><a href=\"#13-调整数组顺序使奇数位于偶数前面\" class=\"headerlink\" title=\"13.调整数组顺序使奇数位于偶数前面\"></a>13.调整数组顺序使奇数位于偶数前面</h3><p><strong>题目：</strong>输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。</p>\n<p><strong>题解：</strong>申请奇数数组和偶数数组，分别存放奇数值和偶数值，数组相加便为结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reOrderArray</span><span class=\"params\">(self, array)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        array1=[]<span class=\"comment\">#奇数</span></span><br><span class=\"line\">        array2=[]<span class=\"comment\">#偶数</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(array)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> array[i]%<span class=\"number\">2</span>!=<span class=\"number\">0</span>:</span><br><span class=\"line\">                array1.append(array[i])</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                array2.append(array[i])</span><br><span class=\"line\">        ans=array1+array2</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    array=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    ans=solution.reOrderArray(array)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"14-链表中倒数第K个节点\"><a href=\"#14-链表中倒数第K个节点\" class=\"headerlink\" title=\"14.链表中倒数第K个节点\"></a>14.链表中倒数第K个节点</h3><p><strong>题目：</strong>输入一个链表，输出该链表中倒数第k个结点。</p>\n<p><strong>题解：</strong>反转链表，寻找第K个节点。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindKthToTail</span><span class=\"params\">(self, head, k)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"comment\">#反转链表</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> head <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">or</span> head.next <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> head</span><br><span class=\"line\">        pre=<span class=\"keyword\">None</span> <span class=\"comment\">#指向上一个节点</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> head:</span><br><span class=\"line\">            <span class=\"comment\">#先用temp保存当前节点的下一个节点信息</span></span><br><span class=\"line\">            temp=head.next</span><br><span class=\"line\">            <span class=\"comment\">#保存好next之后，便可以指向上一个节点</span></span><br><span class=\"line\">            head.next=pre</span><br><span class=\"line\">            <span class=\"comment\">#让pre,head指向下一个移动的节点</span></span><br><span class=\"line\">            pre=head</span><br><span class=\"line\">            head=temp</span><br><span class=\"line\">        <span class=\"comment\"># 寻找第K个元素的位置</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,k):</span><br><span class=\"line\">            pre=pre.next</span><br><span class=\"line\">        temp=pre</span><br><span class=\"line\">        <span class=\"keyword\">return</span> temp</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    k=<span class=\"number\">3</span></span><br><span class=\"line\">    p1=ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    p2=ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    p3=ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    p4=ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    p1.next=p2</span><br><span class=\"line\">    p2.next=p3</span><br><span class=\"line\">    p3.next=p4</span><br><span class=\"line\"></span><br><span class=\"line\">    ans=solution.FindKthToTail(p1,k)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"15-反转链表\"><a href=\"#15-反转链表\" class=\"headerlink\" title=\"15.反转链表\"></a>15.反转链表</h3><p><strong>题目：</strong>输入一个链表，反转链表后，输出新链表的表头。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回ListNode</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ReverseList</span><span class=\"params\">(self, pHead)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pHead <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">or</span> pHead.next <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pHead</span><br><span class=\"line\">        pre=<span class=\"keyword\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead:</span><br><span class=\"line\">            <span class=\"comment\">#暂存当前节点的下一个节点信息</span></span><br><span class=\"line\">            temp=pHead.next</span><br><span class=\"line\">            <span class=\"comment\">#反转节点</span></span><br><span class=\"line\">            pHead.next=pre</span><br><span class=\"line\">            <span class=\"comment\">#进行下一个节点</span></span><br><span class=\"line\">            pre = pHead</span><br><span class=\"line\">            pHead=temp</span><br><span class=\"line\">        <span class=\"keyword\">return</span> pre</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    p1=ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    p2=ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    p3=ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    p1.next=p2</span><br><span class=\"line\">    p2.next=p3</span><br><span class=\"line\">    ans=solution.ReverseList(p1)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"16-合并两个排序的列表\"><a href=\"#16-合并两个排序的列表\" class=\"headerlink\" title=\"16.合并两个排序的列表\"></a>16.合并两个排序的列表</h3><p><strong>题目：</strong>输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。</p>\n<p><strong>题解：</strong>将两个链表之中的数值转换到列表之中，并进行排序，将排序后的列表构造成链表。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回合并后列表</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Merge</span><span class=\"params\">(self,pHead1,pHead2)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pHead1 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> pHead2 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        num1,num2=[],[]</span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead1:</span><br><span class=\"line\">            num1.append(pHead1.val)</span><br><span class=\"line\">            pHead1=pHead1.next</span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead2:</span><br><span class=\"line\">            num2.append(pHead2.val)</span><br><span class=\"line\">            pHead2=pHead2.next</span><br><span class=\"line\">        ans=num1+num2</span><br><span class=\"line\">        ans.sort()</span><br><span class=\"line\">        head=ListNode(ans[<span class=\"number\">0</span>])</span><br><span class=\"line\">        pre=head</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(ans)):</span><br><span class=\"line\">            node=ListNode(ans[i])</span><br><span class=\"line\">            pre.next=node</span><br><span class=\"line\">            pre=pre.next</span><br><span class=\"line\">        <span class=\"keyword\">return</span> head</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    pHead1_1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead1_2 = ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    pHead1_3 = ListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    pHead1_1.next=pHead1_2</span><br><span class=\"line\">    pHead1_2.next=pHead1_3</span><br><span class=\"line\"></span><br><span class=\"line\">    pHead2_1 = ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    pHead2_2 = ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    pHead2_3 = ListNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    pHead2_1.next=pHead2_2</span><br><span class=\"line\">    pHead2_2.next=pHead2_3</span><br><span class=\"line\">    ans=solution.Merge(pHead1_1,pHead2_1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"17-树的子结构\"><a href=\"#17-树的子结构\" class=\"headerlink\" title=\"17.树的子结构\"></a>17.树的子结构</h3><p><strong>题目：</strong>输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）。</p>\n<p><strong>题解：</strong>将树转变为中序序列，然后转变为str类型，最后判断是否包含。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">HasSubtree</span><span class=\"params\">(self, pRoot1, pRoot2)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRoot1 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">or</span> pRoot2 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        pRoot1_result,pRoot2_result=[],[]</span><br><span class=\"line\">        self.order_traversal(pRoot1,pRoot1_result)</span><br><span class=\"line\">        self.order_traversal(pRoot2,pRoot2_result)</span><br><span class=\"line\">        str1=<span class=\"string\">''</span>.join(str(i) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> pRoot1_result)</span><br><span class=\"line\">        str2=<span class=\"string\">''</span>.join(str(i) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> pRoot2_result)</span><br><span class=\"line\">        print(str1,str2)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> str2 <span class=\"keyword\">in</span> str1:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">order_traversal</span><span class=\"params\">(self,root,result)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        self.order_traversal(root.left,result)</span><br><span class=\"line\">        result.append(root.val)</span><br><span class=\"line\">        self.order_traversal(root.right,result)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    pRootA1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pRootA2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    pRootA3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    pRootA4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    pRootA5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    pRootA1.left=pRootA2</span><br><span class=\"line\">    pRootA1.right=pRootA3</span><br><span class=\"line\">    pRootA2.left=pRootA4</span><br><span class=\"line\">    pRootA2.right=pRootA5</span><br><span class=\"line\"></span><br><span class=\"line\">    pRootB2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    pRootB4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    pRootB5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    pRootB2.left=pRootB4</span><br><span class=\"line\">    pRootB2.right = pRootB5</span><br><span class=\"line\">    ans=solution.HasSubtree(pRootA1,pRootB2)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"18-二叉树的镜像\"><a href=\"#18-二叉树的镜像\" class=\"headerlink\" title=\"18.二叉树的镜像\"></a>18.二叉树的镜像</h3><p><strong>题目：</strong> 操作给定的二叉树，将其变换为源二叉树的镜像。</p>\n<p><strong>输入描述：</strong></p>\n<p>​    源二叉树<br>          8<br>         /  \\<br>        6   10<br>       / \\  / \\<br>      5  7 9 11<br>      镜像二叉树<br>          8<br>         /  \\<br>        10   6<br>       / \\  / \\<br>      11 9 7  5</p>\n<p><strong>思路：</strong>递归实现反转每个子节点</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回镜像树的根节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Mirror</span><span class=\"params\">(self, root)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"comment\"># A1_order_result=[]</span></span><br><span class=\"line\">        <span class=\"comment\"># self.order_traversal(A1,A1_order_result)</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> root.right <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        temp=root.left</span><br><span class=\"line\">        root.left=root.right</span><br><span class=\"line\">        root.right=temp</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            self.Mirror(root.left)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            self.Mirror(root.right)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">order_traversal</span><span class=\"params\">(self,root,result)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        self.order_traversal(root.left,result)</span><br><span class=\"line\">        result.append(root.val)</span><br><span class=\"line\">        self.order_traversal(root.right,result)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">8</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">10</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">9</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">11</span>)</span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    temp1=[]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    solution.order_traversal(A1,temp1)</span><br><span class=\"line\">    print(temp1)</span><br><span class=\"line\">    solution.Mirror(A1)</span><br><span class=\"line\">    solution.order_traversal(A1,temp1)</span><br><span class=\"line\">    print(temp1)</span><br></pre></td></tr></table></figure>\n<h3 id=\"19-顺时针打印矩阵\"><a href=\"#19-顺时针打印矩阵\" class=\"headerlink\" title=\"19.顺时针打印矩阵\"></a>19.顺时针打印矩阵</h3><p><strong>题目：</strong></p>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，</span><br><span class=\"line\">&gt; 例如，如果输入如下矩阵：</span><br><span class=\"line\">&gt;  <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span></span><br><span class=\"line\">&gt;  <span class=\"number\">5</span> <span class=\"number\">6</span> <span class=\"number\">7</span> <span class=\"number\">8</span></span><br><span class=\"line\">&gt;  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span> <span class=\"number\">12</span></span><br><span class=\"line\">&gt;  <span class=\"number\">13</span> <span class=\"number\">14</span> <span class=\"number\">15</span> <span class=\"number\">16</span></span><br><span class=\"line\">&gt; 则依次打印出数字</span><br><span class=\"line\">&gt; <span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">12</span>,<span class=\"number\">16</span>,<span class=\"number\">15</span>,<span class=\"number\">14</span>,<span class=\"number\">13</span>,<span class=\"number\">9</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">11</span>,<span class=\"number\">10.</span></span><br><span class=\"line\">&gt; </span><br><span class=\"line\">&gt; </span><br><span class=\"line\">&gt;</span><br></pre></td></tr></table></figure>\n</blockquote>\n<p><strong>思路：</strong>每次打印圈，但要判断最后一次是打印横还是竖，另外判断数据是否已存在。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># matrix类型为二维列表，需要返回列表</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printMatrix</span><span class=\"params\">(self, matrix)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        m,n=len(matrix),len(matrix[<span class=\"number\">0</span>])</span><br><span class=\"line\">        res = []</span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">1</span> <span class=\"keyword\">and</span> m==<span class=\"number\">1</span>:</span><br><span class=\"line\">            res.append(matrix[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\">            <span class=\"keyword\">return</span> res</span><br><span class=\"line\">        <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,(min(m,n)+<span class=\"number\">1</span>)//<span class=\"number\">2</span>):</span><br><span class=\"line\">            [res.append(matrix[k][i]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(k, n - k)]</span><br><span class=\"line\">            [res.append(matrix[j][n-k<span class=\"number\">-1</span>]) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(k,m-k) <span class=\"keyword\">if</span> matrix[j][n-k<span class=\"number\">-1</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> res]</span><br><span class=\"line\">            [res.append(matrix[m-k<span class=\"number\">-1</span>][j]) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(n-k<span class=\"number\">-1</span>,k<span class=\"number\">-1</span>,<span class=\"number\">-1</span>) <span class=\"keyword\">if</span> matrix[m-k<span class=\"number\">-1</span>][j] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> res]</span><br><span class=\"line\">            [res.append(matrix[j][k]) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(m<span class=\"number\">-1</span>-k,k<span class=\"number\">-1</span>,<span class=\"number\">-1</span>) <span class=\"keyword\">if</span> matrix[j][k] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> res]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    m,n=<span class=\"number\">1</span>,<span class=\"number\">5</span></span><br><span class=\"line\">    matrix=[]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,m):</span><br><span class=\"line\">        matrix.append(list(map(int,input().split(<span class=\"string\">' '</span>))))</span><br><span class=\"line\">    print(matrix)</span><br><span class=\"line\">    ans=solution.printMatrix(matrix)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"20-包含Min函数的栈\"><a href=\"#20-包含Min函数的栈\" class=\"headerlink\" title=\"20.包含Min函数的栈\"></a>20.包含Min函数的栈</h3><p><strong>题目：</strong>定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.num=[]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">push</span><span class=\"params\">(self, node)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.num.append(node)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pop</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.num.pop()</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">top</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        numlen = len(self.num)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.num[numlen<span class=\"number\">-1</span>]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">min</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> min(self.num)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    solution.push(<span class=\"number\">1</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">2</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">3</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">4</span>)</span><br><span class=\"line\">    solution.pop()</span><br><span class=\"line\">    print(solution.top())</span><br><span class=\"line\">    print(solution.min())</span><br></pre></td></tr></table></figure>\n<h3 id=\"21-栈的压入弹出序列\"><a href=\"#21-栈的压入弹出序列\" class=\"headerlink\" title=\"21.栈的压入弹出序列\"></a>21.栈的压入弹出序列</h3><p><strong>题目：</strong>输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）。</p>\n<p><strong>题解：</strong>新构建一个中间栈，来模拟栈的输入和栈的输出，比对输入结果和输出结果是否相等。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">IsPopOrder</span><span class=\"params\">(self, pushV, popV)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(pushV)==<span class=\"number\">1</span> <span class=\"keyword\">and</span> len(popV)==<span class=\"number\">1</span> <span class=\"keyword\">and</span> pushV[<span class=\"number\">0</span>]!=popV[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">        helpV=[]</span><br><span class=\"line\">        pushV.reverse()</span><br><span class=\"line\">        popV.reverse()</span><br><span class=\"line\">        <span class=\"comment\">#模拟给定栈的压入和压出</span></span><br><span class=\"line\">        helpV.append(pushV[len(pushV)<span class=\"number\">-1</span>])</span><br><span class=\"line\">        pushV.pop()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> helpV[len(helpV)<span class=\"number\">-1</span>]!=popV[len(popV)<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                helpV.append(pushV[len(pushV)<span class=\"number\">-1</span>])</span><br><span class=\"line\">                pushV.pop()</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> helpV[len(helpV)<span class=\"number\">-1</span>]==popV[len(popV)<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                helpV.pop()</span><br><span class=\"line\">                popV.pop()</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> pushV==[] <span class=\"keyword\">and</span> popV==[] <span class=\"keyword\">and</span> helpV==[]:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> pushV==[] <span class=\"keyword\">and</span> popV[len(popV)<span class=\"number\">-1</span>]!=helpV[len(helpV)<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    push=list(map(int,input().split(<span class=\"string\">' '</span>)))</span><br><span class=\"line\">    pop=list(map(int,input().split(<span class=\"string\">' '</span>)))</span><br><span class=\"line\">    ans=solution.IsPopOrder(push,pop)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"22-从上往下打印二叉树\"><a href=\"#22-从上往下打印二叉树\" class=\"headerlink\" title=\"22.从上往下打印二叉树\"></a>22.从上往下打印二叉树</h3><p><strong>题目：</strong>从上往下打印出二叉树的每个节点，同层节点从左至右打印。</p>\n<p><strong>思路：</strong>递归，每次将左子树结果和右子树结果存到结果集之中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回从上到下每个节点值列表，例：[1,2,3]</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">PrintFromTopToBottom</span><span class=\"params\">(self, root)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        ans.append(root.val)</span><br><span class=\"line\">        self.orderans(root,ans)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">orderans</span><span class=\"params\">(self,root,ans)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left:</span><br><span class=\"line\">            ans.append(root.left.val)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.right:</span><br><span class=\"line\">            ans.append(root.right.val)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.orderans(root.left, ans)</span><br><span class=\"line\">        self.orderans(root.right,ans)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    ans=solution.PrintFromTopToBottom(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"23-二叉树的后续遍历序列\"><a href=\"#23-二叉树的后续遍历序列\" class=\"headerlink\" title=\"23.二叉树的后续遍历序列\"></a>23.二叉树的后续遍历序列</h3><p><strong>题目：</strong>输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。</p>\n<p><strong>思路：</strong>二叉搜索树的特性是所有左子树值都小于中节点，所有右子树的值都大于中节点，递归遍历左子树和右子树的值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">VerifySquenceOfBST</span><span class=\"params\">(self, sequence)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> sequence:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(sequence)==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        i=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> sequence[i]&lt;sequence[<span class=\"number\">-1</span>]:</span><br><span class=\"line\">            i=i+<span class=\"number\">1</span></span><br><span class=\"line\">        k=i</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i,len(sequence)<span class=\"number\">-1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> sequence[j]&lt;sequence[<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">            </span><br><span class=\"line\">        leftsequence=sequence[:k]</span><br><span class=\"line\">        rightsequence=sequence[k:len(sequence)<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        leftans=<span class=\"keyword\">True</span></span><br><span class=\"line\">        rightans=<span class=\"keyword\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(leftsequence)&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            self.VerifySquenceOfBST(leftsequence)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(rightsequence)&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            self.VerifySquenceOfBST(rightsequence)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> leftans <span class=\"keyword\">and</span> rightans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    num=list(map(int,input().split(<span class=\"string\">' '</span>)))</span><br><span class=\"line\">    ans=solution.VerifySquenceOfBST(num)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"24-二叉树中和为某一值的路径\"><a href=\"#24-二叉树中和为某一值的路径\" class=\"headerlink\" title=\"24.二叉树中和为某一值的路径\"></a>24.二叉树中和为某一值的路径</h3><p><strong>题目：</strong>输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前)。</p>\n<p><strong>思路：</strong>利用递归的方法，计算加左子树和右子树之后的值，当参数较多是，可以将结果添加到函数变量之中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回二维列表，内部每个列表表示找到的路径</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindPath</span><span class=\"params\">(self, root, expectNumber)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        path=[]</span><br><span class=\"line\">        self.dfs(root,expectNumber,ans,path)</span><br><span class=\"line\">        ans.sort()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dfs</span><span class=\"params\">(self,root,target,ans,path)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\">        path.append(root.val)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> root.right <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> target==root.val:</span><br><span class=\"line\">            ans.append(path[:])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left:</span><br><span class=\"line\">            self.dfs(root.left,target-root.val,ans,path)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.right:</span><br><span class=\"line\">            self.dfs(root.right,target-root.val,ans,path)</span><br><span class=\"line\"></span><br><span class=\"line\">        path.pop()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1=TreeNode(<span class=\"number\">10</span>)</span><br><span class=\"line\">    A2=TreeNode(<span class=\"number\">8</span>)</span><br><span class=\"line\">    A3=TreeNode(<span class=\"number\">12</span>)</span><br><span class=\"line\">    A4=TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5=TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A6=TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A5.left=A6</span><br><span class=\"line\"></span><br><span class=\"line\">    expectNumber=<span class=\"number\">22</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindPath(A1,expectNumber)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"25-复杂链表的复制\"><a href=\"#25-复杂链表的复制\" class=\"headerlink\" title=\"25.复杂链表的复制\"></a>25.复杂链表的复制</h3><p><strong>题目：</strong>输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）。</p>\n<p><strong>思路：</strong>将大问题转变为小问题，每次都进行复制头部节点，然后进行递归，每次同样处理头部节点。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.label = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.random = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回 RandomListNode</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Clone</span><span class=\"params\">(self, pHead)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"comment\"># 复制头部节点</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pHead <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        newHead=RandomListNode(pHead.label)</span><br><span class=\"line\">        newHead.next=pHead.next</span><br><span class=\"line\">        newHead.random=pHead.random</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 递归其他节点</span></span><br><span class=\"line\">        newHead.next=self.Clone(pHead.next)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> newHead</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1=RandomListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A2=RandomListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A3=RandomListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A4=RandomListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A5=RandomListNode(<span class=\"number\">6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.next=A2</span><br><span class=\"line\">    A1.random=A3</span><br><span class=\"line\"></span><br><span class=\"line\">    A2.next=A3</span><br><span class=\"line\">    A2.random=A4</span><br><span class=\"line\"></span><br><span class=\"line\">    A3.next=A4</span><br><span class=\"line\">    A3.random=A5</span><br><span class=\"line\"></span><br><span class=\"line\">    A4.next=A5</span><br><span class=\"line\">    A4.random=A3</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Clone(A1)</span><br></pre></td></tr></table></figure>\n<h3 id=\"26-二叉搜索树与双向列表\"><a href=\"#26-二叉搜索树与双向列表\" class=\"headerlink\" title=\"26.二叉搜索树与双向列表\"></a>26.二叉搜索树与双向列表</h3><p><strong>题目：</strong>输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。</p>\n<p><strong>思路：</strong>递归将根结点和左子树的最右节点和右子树的最左节点进行连接起来。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Convert</span><span class=\"params\">(self, pRootOfTree)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRootOfTree <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pRootOfTree</span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRootOfTree.left <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> pRootOfTree.right <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pRootOfTree</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#处理左子树</span></span><br><span class=\"line\">        self.Convert(pRootOfTree.left)</span><br><span class=\"line\">        left=pRootOfTree.left</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> left:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> left.right:</span><br><span class=\"line\">                left=left.right</span><br><span class=\"line\">            pRootOfTree.left,left.right=left,pRootOfTree</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#处理右子树</span></span><br><span class=\"line\">        self.Convert(pRootOfTree.right)</span><br><span class=\"line\">        right=pRootOfTree.right</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> right:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> right.left:</span><br><span class=\"line\">                right=right.left</span><br><span class=\"line\">            pRootOfTree.right,right.left=right,pRootOfTree</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pRootOfTree.left:</span><br><span class=\"line\">            pRootOfTree=pRootOfTree.left</span><br><span class=\"line\">        <span class=\"keyword\">return</span> pRootOfTree</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">15</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">8</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">19</span>)</span><br><span class=\"line\">    A8 = TreeNode(<span class=\"number\">24</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\">    A7.right=A8</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    solution.Convert(A1)</span><br></pre></td></tr></table></figure>\n<h3 id=\"27-字符串的排列\"><a href=\"#27-字符串的排列\" class=\"headerlink\" title=\"27.字符串的排列\"></a>27.字符串的排列</h3><p><strong>题目：</strong>输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。</p>\n<p><strong>输入：</strong>输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。</p>\n<p><strong>思路：</strong>通过将第k位的字符提取到最前面，然后进行和后面的每个字符进行交换，得到所有结果集。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Permutation</span><span class=\"params\">(self, ss)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> ss:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        res=[]</span><br><span class=\"line\">        self.helper(ss,res,<span class=\"string\">''</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sorted(list(set(res)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">helper</span><span class=\"params\">(self,ss,res,path)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> ss:</span><br><span class=\"line\">            res.append(path)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(ss)):</span><br><span class=\"line\">                self.helper(ss[:i]+ss[i+<span class=\"number\">1</span>:],res,path+ss[i])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    str=<span class=\"string\">'abbcDeefg'</span></span><br><span class=\"line\">    str1=<span class=\"string\">'abbc'</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Permutation(str1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"28-数组中出现次数超过一般的数字\"><a href=\"#28-数组中出现次数超过一般的数字\" class=\"headerlink\" title=\"28.数组中出现次数超过一般的数字\"></a>28.数组中出现次数超过一般的数字</h3><p><strong>题目：</strong>数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0</p>\n<p><strong>题解：</strong>利用list列表来存放每个数出现的次数ans[numbers[i]]=ans[numbers[i]]+1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">MoreThanHalfNum_Solution</span><span class=\"params\">(self, numbers)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        numlen=len(numbers)</span><br><span class=\"line\">        halflen=numlen//<span class=\"number\">2</span></span><br><span class=\"line\">        maxans=<span class=\"number\">0</span></span><br><span class=\"line\">        ans=[<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,<span class=\"number\">1000</span>)]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            ans[numbers[i]]=ans[numbers[i]]+<span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> ans[numbers[i]]&gt;maxans:</span><br><span class=\"line\">                maxans=numbers[i]</span><br><span class=\"line\">        ans.sort()</span><br><span class=\"line\">        ans.reverse()</span><br><span class=\"line\">        res=ans[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> res&gt;halflen:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> maxans</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    num=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.MoreThanHalfNum_Solution(num)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"29-最小的K个数\"><a href=\"#29-最小的K个数\" class=\"headerlink\" title=\"29.最小的K个数\"></a>29.最小的K个数</h3><p><strong>题目：</strong>输入n个整数，找出其中最小的K个数，例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetLeastNumbers_Solution</span><span class=\"params\">(self, tinput, k)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> k&gt;len(tinput):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        tinput.sort()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tinput[:k]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    num=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    k=int(input())</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.GetLeastNumbers_Solution(num,k)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"30-连续子数组的最大和\"><a href=\"#30-连续子数组的最大和\" class=\"headerlink\" title=\"30.连续子数组的最大和\"></a>30.连续子数组的最大和</h3><p><strong>题目：</strong>HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。你会不会被他忽悠住？(子向量的长度至少是1)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindGreatestSumOfSubArray</span><span class=\"params\">(self, array)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        maxsum,tempsum=array[<span class=\"number\">0</span>],array[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(array)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> tempsum&lt;<span class=\"number\">0</span>:</span><br><span class=\"line\">                tempsum=array[i]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                tempsum = tempsum + array[i]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> tempsum&gt;maxsum:</span><br><span class=\"line\">                maxsum=tempsum</span><br><span class=\"line\">        <span class=\"keyword\">return</span> maxsum</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    array=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindGreatestSumOfSubArray(array)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"31-整数中1出现的次数\"><a href=\"#31-整数中1出现的次数\" class=\"headerlink\" title=\"31.整数中1出现的次数\"></a>31.整数中1出现的次数</h3><p><strong>题目：</strong>求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。</p>\n<p><strong>思路：</strong>对每个数字的每位进行分解，含有1则结果加1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">NumberOf1Between1AndN_Solution</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,n+<span class=\"number\">1</span>):</span><br><span class=\"line\">            tempans=<span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> i!=<span class=\"number\">0</span>:</span><br><span class=\"line\">                eachnum=i%<span class=\"number\">10</span></span><br><span class=\"line\">                i=i//<span class=\"number\">10</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> eachnum==<span class=\"number\">1</span>:</span><br><span class=\"line\">                    tempans=tempans+<span class=\"number\">1</span></span><br><span class=\"line\">            ans=ans+tempans</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    n=<span class=\"number\">130</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.NumberOf1Between1AndN_Solution(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"32-把数组排成最小的数\"><a href=\"#32-把数组排成最小的数\" class=\"headerlink\" title=\"32.把数组排成最小的数\"></a>32.把数组排成最小的数</h3><p><strong>题目：</strong>输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。</p>\n<p><strong>思路：</strong>将数组转换成字符串之后，进行两两比较字符串的大小，比如3,32的大小由332和323确定，即3+32和32+3确定。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">PrintMinNumber</span><span class=\"params\">(self, numbers)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> numbers:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">\"\"</span></span><br><span class=\"line\">        num = map(str, numbers)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i,len(numbers)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> int(str(numbers[i])+str(numbers[j]))&gt;int(str(numbers[j])+str(numbers[i])):</span><br><span class=\"line\">                    numbers[i],numbers[j]=numbers[j],numbers[i]</span><br><span class=\"line\">        ans=<span class=\"string\">''</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            ans=ans+str(numbers[i])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    numbers=[<span class=\"number\">3</span>,<span class=\"number\">32</span>,<span class=\"number\">321</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.PrintMinNumber(numbers)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"33-丑数\"><a href=\"#33-丑数\" class=\"headerlink\" title=\"33.丑数\"></a>33.丑数</h3><p><strong>题目：</strong>把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。</p>\n<p><strong>思路：</strong>每一个丑数必然是由之前的某个丑数与2，3或5的乘积得到的，这样下一个丑数就用之前的丑数分别乘以2，3，5，找出这三这种最小的并且大于当前最大丑数的值，即为下一个要求的丑数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetUglyNumber_Solution</span><span class=\"params\">(self, index)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (index &lt;= <span class=\"number\">0</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        uglyList = [<span class=\"number\">1</span>]</span><br><span class=\"line\">        indexTwo = <span class=\"number\">0</span></span><br><span class=\"line\">        indexThree = <span class=\"number\">0</span></span><br><span class=\"line\">        indexFive = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(index<span class=\"number\">-1</span>):</span><br><span class=\"line\">            newUgly = min(uglyList[indexTwo]*<span class=\"number\">2</span>, uglyList[indexThree]*<span class=\"number\">3</span>, uglyList[indexFive]*<span class=\"number\">5</span>)</span><br><span class=\"line\">            uglyList.append(newUgly)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (newUgly % <span class=\"number\">2</span> == <span class=\"number\">0</span>):</span><br><span class=\"line\">                indexTwo += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (newUgly % <span class=\"number\">3</span> == <span class=\"number\">0</span>):</span><br><span class=\"line\">                indexThree += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (newUgly % <span class=\"number\">5</span> == <span class=\"number\">0</span>):</span><br><span class=\"line\">                indexFive += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> uglyList[<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    index=<span class=\"number\">200</span></span><br><span class=\"line\">    ans=solution.GetUglyNumber_Solution(index)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"34-第一个只出现一次的字符\"><a href=\"#34-第一个只出现一次的字符\" class=\"headerlink\" title=\"34.第一个只出现一次的字符\"></a>34.第一个只出现一次的字符</h3><p><strong>题目：</strong>在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1。</p>\n<p><strong>思路：</strong>找出所有出现一次的字符，然后进行遍历找到第一次出现字符的位置。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FirstNotRepeatingChar</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> s:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\">        sset=set(s)</span><br><span class=\"line\">        dict=&#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> sset:</span><br><span class=\"line\">            dict[c]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">            dict[s[i]]=dict[s[i]]+<span class=\"number\">1</span></span><br><span class=\"line\">        onetime=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> dict:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> dict[c]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                onetime.append(c)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> onetime <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            index=<span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> s[i] <span class=\"keyword\">in</span> onetime:</span><br><span class=\"line\">                    index=i</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> index</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=<span class=\"string\">'abbddebbac'</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FirstNotRepeatingChar(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"35-数组中的逆序对\"><a href=\"#35-数组中的逆序对\" class=\"headerlink\" title=\"35.数组中的逆序对\"></a>35.数组中的逆序对</h3><p><strong>题目描述：</strong>在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007。</p>\n<p><strong>输入描述：</strong>题目保证输入的数组中没有的相同的数字。</p>\n<p><strong>数据范围：</strong><br>   对于%50的数据,size&lt;=10^4<br>   对于%75的数据,size&lt;=10^5<br>   对于%100的数据,size&lt;=2*10^5</p>\n<blockquote>\n<p>示例1</p>\n<p>输入 1,2,3,4,5,6,7,0</p>\n<p>输出 7</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">InversePairs</span><span class=\"params\">(self, data)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">global</span> count</span><br><span class=\"line\">        count = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">A</span><span class=\"params\">(array)</span>:</span></span><br><span class=\"line\">            <span class=\"keyword\">global</span> count</span><br><span class=\"line\">            <span class=\"keyword\">if</span> len(array) &lt;= <span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> array</span><br><span class=\"line\">            k = int(len(array) / <span class=\"number\">2</span>)</span><br><span class=\"line\">            left = A(array[:k])</span><br><span class=\"line\">            right = A(array[k:])</span><br><span class=\"line\">            l = <span class=\"number\">0</span></span><br><span class=\"line\">            r = <span class=\"number\">0</span></span><br><span class=\"line\">            result = []</span><br><span class=\"line\">            <span class=\"keyword\">while</span> l &lt; len(left) <span class=\"keyword\">and</span> r &lt; len(right):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> left[l] &lt; right[r]:</span><br><span class=\"line\">                    result.append(left[l])</span><br><span class=\"line\">                    l += <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    result.append(right[r])</span><br><span class=\"line\">                    r += <span class=\"number\">1</span></span><br><span class=\"line\">                    count += len(left) - l</span><br><span class=\"line\">            result += left[l:]</span><br><span class=\"line\">            result += right[r:]</span><br><span class=\"line\">            <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\">        A(data)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> count % <span class=\"number\">1000000007</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    data=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.InversePairs(data)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"36-两个链表的第一个公共节点\"><a href=\"#36-两个链表的第一个公共节点\" class=\"headerlink\" title=\"36.两个链表的第一个公共节点\"></a>36.两个链表的第一个公共节点</h3><p><strong>题目：</strong>输入两个链表，找出它们的第一个公共结点。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindFirstCommonNode</span><span class=\"params\">(self, pHead1, pHead2)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        list1 = []</span><br><span class=\"line\">        list2 = []</span><br><span class=\"line\">        node1 = pHead1</span><br><span class=\"line\">        node2 = pHead2</span><br><span class=\"line\">        <span class=\"keyword\">while</span> node1:</span><br><span class=\"line\">            list1.append(node1.val)</span><br><span class=\"line\">            node1 = node1.next</span><br><span class=\"line\">        <span class=\"keyword\">while</span> node2:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> node2.val <span class=\"keyword\">in</span> list1:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> node2</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                node2 = node2.next</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A1.next=A2</span><br><span class=\"line\">    A2.next=A3</span><br><span class=\"line\"></span><br><span class=\"line\">    B4 = ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    B5 = ListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    B4.next=B5</span><br><span class=\"line\"></span><br><span class=\"line\">    C6=ListNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    C7=ListNode(<span class=\"number\">7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A3.next=C6</span><br><span class=\"line\">    B5.next=C6</span><br><span class=\"line\">    C6.next=C7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindFirstCommonNode(A1,B4)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"37-数字在排序数组中出现的次数\"><a href=\"#37-数字在排序数组中出现的次数\" class=\"headerlink\" title=\"37.数字在排序数组中出现的次数\"></a>37.数字在排序数组中出现的次数</h3><p><strong>题目：</strong>统计一个数字在排序数组中出现的次数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetNumberOfK</span><span class=\"params\">(self, data, k)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(data)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> data[i]==k:</span><br><span class=\"line\">                ans=ans+<span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> data[i]&gt;k:</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    data=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">    k=<span class=\"number\">3</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.GetNumberOfK(data,k)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"38-二叉树的深度\"><a href=\"#38-二叉树的深度\" class=\"headerlink\" title=\"38.二叉树的深度\"></a>38.二叉树的深度</h3><p><strong>题目：</strong>输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">TreeDepth</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRoot <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        left=self.TreeDepth(pRoot.left)</span><br><span class=\"line\">        right=self.TreeDepth(pRoot.right)</span><br><span class=\"line\">        print(left,right)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> max(left,right)+<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A4.left=A6</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.TreeDepth(A1)</span><br><span class=\"line\">    print(<span class=\"string\">'ans='</span>,ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"39-平衡二叉树\"><a href=\"#39-平衡二叉树\" class=\"headerlink\" title=\"39.平衡二叉树\"></a>39.平衡二叉树</h3><p><strong>题目：</strong>输入一棵二叉树，判断该二叉树是否是平衡二叉树。</p>\n<p><strong>题解：</strong>平衡二叉树是左右子数的距离不能大于1，因此递归左右子树，判断子树距离是否大于1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">IsBalanced_Solution</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRoot <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> abs(self.TreeDepth(pRoot.left)-self.TreeDepth(pRoot.right))&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.IsBalanced_Solution(pRoot.left) <span class=\"keyword\">and</span> self.IsBalanced_Solution(pRoot.right)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">TreeDepth</span><span class=\"params\">(self,root)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        left=self.TreeDepth(root.left)</span><br><span class=\"line\">        right=self.TreeDepth(root.right)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> max(left+<span class=\"number\">1</span>,right+<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    <span class=\"comment\">#A4.left=A6</span></span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.IsBalanced_Solution(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"40-数组中只出现一次的数字\"><a href=\"#40-数组中只出现一次的数字\" class=\"headerlink\" title=\"40.数组中只出现一次的数字\"></a>40.数组中只出现一次的数字</h3><p><strong>题目：</strong>一个整型数组里除了两个数字之外，其他的数字都出现了偶数次。请写程序找出这两个只出现一次的数字。</p>\n<p><strong>题解：</strong>将数组中数转到set之中，然后利用dict存储每个数字出现的次数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回[a,b] 其中ab是出现一次的两个数字</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindNumsAppearOnce</span><span class=\"params\">(self, array)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        arrayset=set(array)</span><br><span class=\"line\">        dict=&#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> arrayset:</span><br><span class=\"line\">            dict[num]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(array)):</span><br><span class=\"line\">            dict[array[i]]=dict[array[i]]+<span class=\"number\">1</span></span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> arrayset:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> dict[num]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                ans.append(num)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    array=[<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">7</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindNumsAppearOnce(array)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"41-和为S的连续正整数序列\"><a href=\"#41-和为S的连续正整数序列\" class=\"headerlink\" title=\"41.和为S的连续正整数序列\"></a>41.和为S的连续正整数序列</h3><p><strong>题目：</strong>小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck!</p>\n<p><strong>输出描述：</strong>输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序。</p>\n<p><strong>思路：</strong>首项加尾项*2等于和，那么只要遍历项的开始和长度即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindContinuousSequence</span><span class=\"params\">(self, tsum)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,tsum//<span class=\"number\">2</span>+<span class=\"number\">1</span>):</span><br><span class=\"line\">            oneans=[]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,tsum):</span><br><span class=\"line\">                tempsum=((i+i+k<span class=\"number\">-1</span>)*k)//<span class=\"number\">2</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> tempsum==tsum:</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i,i+k):</span><br><span class=\"line\">                        oneans.append(j)</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> oneans !=[]:</span><br><span class=\"line\">                ans.append(oneans)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    tsum=<span class=\"number\">15</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindContinuousSequence(tsum)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"42-和为S的两个数字\"><a href=\"#42-和为S的两个数字\" class=\"headerlink\" title=\"42.和为S的两个数字\"></a>42.和为S的两个数字</h3><p><strong>题目：</strong>输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。</p>\n<p><strong>输出描述：</strong>对应每个测试案例，输出两个数，小的先输出。</p>\n<p><strong>思路：</strong>利用i和j从后面进行扫描结果，选取最小的乘积放入到结果集之中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindNumbersWithSum</span><span class=\"params\">(self, array, tsum)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        i,j,minres=<span class=\"number\">0</span>,len(array)<span class=\"number\">-1</span>,<span class=\"number\">1000000</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(array)<span class=\"number\">-1</span>):</span><br><span class=\"line\">            j=len(array)<span class=\"number\">-1</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">                tempsum = array[i] + array[j]</span><br><span class=\"line\">                <span class=\"keyword\">if</span> tempsum == tsum:</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> array[i]*array[j]&lt;minres:</span><br><span class=\"line\">                        ans=[]</span><br><span class=\"line\">                        ans.append(array[i])</span><br><span class=\"line\">                        ans.append(array[j])</span><br><span class=\"line\">                        minres=array[i]*array[j]</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    j = j - <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> tempsum&lt;tsum:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> j&lt;=i:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    array=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>,<span class=\"number\">7</span>,<span class=\"number\">11</span>,<span class=\"number\">15</span>]</span><br><span class=\"line\">    tsum=<span class=\"number\">15</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindNumbersWithSum(array,tsum)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"43-左旋字符子串\"><a href=\"#43-左旋字符子串\" class=\"headerlink\" title=\"43.左旋字符子串\"></a>43.左旋字符子串</h3><p><strong>题目：</strong>汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">LeftRotateString</span><span class=\"params\">(self, s, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> s==<span class=\"string\">''</span> <span class=\"keyword\">and</span> n==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">''</span></span><br><span class=\"line\">        ans=<span class=\"string\">''</span></span><br><span class=\"line\">        ans=s[n:]+s[<span class=\"number\">0</span>:n]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=<span class=\"string\">'abcdefg'</span></span><br><span class=\"line\">    n=<span class=\"number\">2</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.LeftRotateString(s,n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"44-反转单词顺序\"><a href=\"#44-反转单词顺序\" class=\"headerlink\" title=\"44.反转单词顺序\"></a>44.反转单词顺序</h3><p><strong>题目：</strong>牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ReverseSentence</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans,word=[],<span class=\"string\">''</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">            word = word + s[i]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> s[i]==<span class=\"string\">' '</span>:</span><br><span class=\"line\">                ans.append(word)</span><br><span class=\"line\">                word=<span class=\"string\">''</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> i==len(s)<span class=\"number\">-1</span>:</span><br><span class=\"line\">                word=word+<span class=\"string\">' '</span></span><br><span class=\"line\">                ans.append(word)</span><br><span class=\"line\">        ans.reverse()</span><br><span class=\"line\">        res=<span class=\"string\">''</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> ans:</span><br><span class=\"line\">            res=res+c</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res[:len(res)<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    s=<span class=\"string\">'I am a student.'</span></span><br><span class=\"line\">    ans=solution.ReverseSentence(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"45-扑克牌顺序\"><a href=\"#45-扑克牌顺序\" class=\"headerlink\" title=\"45.扑克牌顺序\"></a>45.扑克牌顺序</h3><p><strong>题目：</strong>LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\\小王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">IsContinuous</span><span class=\"params\">(self, numbers)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> numbers==[]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        numbers.sort()</span><br><span class=\"line\">        zero=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> numbers[i]==<span class=\"number\">0</span>:</span><br><span class=\"line\">                zero=zero+<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(zero+<span class=\"number\">1</span>,len(numbers)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> numbers[i]==numbers[i<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> numbers[i]-numbers[i<span class=\"number\">-1</span>]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                diff=numbers[i]-numbers[i<span class=\"number\">-1</span>]<span class=\"number\">-1</span></span><br><span class=\"line\">                zero=zero-diff</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> zero&lt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    numbers=[<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.IsContinuous(numbers)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"46-孩子们的圈圈-圈圈中最后剩下的数\"><a href=\"#46-孩子们的圈圈-圈圈中最后剩下的数\" class=\"headerlink\" title=\"46.孩子们的圈圈(圈圈中最后剩下的数)\"></a>46.孩子们的圈圈(圈圈中最后剩下的数)</h3><p><strong>题目：</strong>每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)。</p>\n<p><strong>思路：</strong>约瑟夫环问题。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 题目</span></span><br><span class=\"line\"><span class=\"comment\"># 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。</span></span><br><span class=\"line\"><span class=\"comment\"># 其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。</span></span><br><span class=\"line\"><span class=\"comment\"># 每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,</span></span><br><span class=\"line\"><span class=\"comment\"># 从他的下一个小朋友开始,继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,</span></span><br><span class=\"line\"><span class=\"comment\"># 并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 思路</span></span><br><span class=\"line\"><span class=\"comment\"># 约瑟夫环问题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">LastRemaining_Solution</span><span class=\"params\">(self, n, m)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n&lt;<span class=\"number\">1</span> <span class=\"keyword\">or</span> m&lt;<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\">        last=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>,n+<span class=\"number\">1</span>):</span><br><span class=\"line\">            last=(last+m)%i</span><br><span class=\"line\">        <span class=\"keyword\">return</span> last</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    n,m=<span class=\"number\">8</span>,<span class=\"number\">4</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.LastRemaining_Solution(n,m)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"47-求1-2-3-…-n\"><a href=\"#47-求1-2-3-…-n\" class=\"headerlink\" title=\"47.求1+2+3+…+n\"></a>47.求1+2+3+…+n</h3><p><strong>题目：</strong>求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。</p>\n<p><strong>思路：</strong>利用递归当作计算结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Sum_Solution</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.Sum_Solution(n<span class=\"number\">-1</span>)+n</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    n=<span class=\"number\">6</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Sum_Solution(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"48-不用加减乘除做加法\"><a href=\"#48-不用加减乘除做加法\" class=\"headerlink\" title=\"48.不用加减乘除做加法\"></a>48.不用加减乘除做加法</h3><p><strong>题目：</strong>写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。</p>\n<p><strong>思路：</strong>二进制异或进位。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Add</span><span class=\"params\">(self, num1, num2)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> num2!=<span class=\"number\">0</span>:</span><br><span class=\"line\">            sum=num1^num2</span><br><span class=\"line\">            carry=(num1&amp;num2)&lt;&lt;<span class=\"number\">1</span></span><br><span class=\"line\">            num1=sum</span><br><span class=\"line\">            num2=carry</span><br><span class=\"line\">        <span class=\"keyword\">return</span> num1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    num1,num2=<span class=\"number\">10</span>,<span class=\"number\">500000</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Add(num1,num2)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"49-把字符串转换成整数\"><a href=\"#49-把字符串转换成整数\" class=\"headerlink\" title=\"49.把字符串转换成整数\"></a>49.把字符串转换成整数</h3><p><strong>题目：</strong>将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。</p>\n<p><strong>输入描述：</strong>输入一个字符串,包括数字字母符号,可以为空输出描述:如果是合法的数值表达则返回该数字，否则返回0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">示例</span><br><span class=\"line\">+<span class=\"number\">2147483647</span></span><br><span class=\"line\">    <span class=\"number\">1</span>a33</span><br><span class=\"line\"><span class=\"number\">2147483647</span></span><br><span class=\"line\">    <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">StrToInt</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(s) == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> s[<span class=\"number\">0</span>] &gt; <span class=\"string\">'9'</span> <span class=\"keyword\">or</span> s[<span class=\"number\">0</span>] &lt; <span class=\"string\">'0'</span>:</span><br><span class=\"line\">                a = <span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                a = int(s[<span class=\"number\">0</span>]) * <span class=\"number\">10</span> ** (len(s) - <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> len(s) &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, len(s)):</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> s[i] &gt;= <span class=\"string\">'0'</span> <span class=\"keyword\">and</span> s[i] &lt;= <span class=\"string\">'9'</span>:</span><br><span class=\"line\">                        a = a + int(s[i]) * <span class=\"number\">10</span> ** (len(s) - <span class=\"number\">1</span> - i)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> s[<span class=\"number\">0</span>] == <span class=\"string\">'+'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> a</span><br><span class=\"line\">        <span class=\"keyword\">if</span> s[<span class=\"number\">0</span>] == <span class=\"string\">'-'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> -a</span><br><span class=\"line\">        <span class=\"keyword\">return</span> a</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=<span class=\"string\">'115'</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.StrToInt(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"50-数组中重复的数字\"><a href=\"#50-数组中重复的数字\" class=\"headerlink\" title=\"50.数组中重复的数字\"></a>50.数组中重复的数字</h3><p><strong>题目：</strong>在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。</p>\n<p><strong>思路：</strong>利用dict计算重复数字。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 这里要特别注意~找到任意重复的一个值并赋值到duplication[0]</span></span><br><span class=\"line\">    <span class=\"comment\"># 函数返回True/False</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">duplicate</span><span class=\"params\">(self, numbers, duplication)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        numset=set(numbers)</span><br><span class=\"line\">        dict=&#123;&#125;</span><br><span class=\"line\">        duplication.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> val <span class=\"keyword\">in</span> numbers:</span><br><span class=\"line\">            dict[val]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            dict[numbers[i]]=dict[numbers[i]]+<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> val <span class=\"keyword\">in</span> numset:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> dict[val]&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">                duplication[<span class=\"number\">0</span>]=val</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    numbers=[<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    duplication=[]</span><br><span class=\"line\">    ans=solution.duplicate(numbers,duplication)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"51-构建乘积数组\"><a href=\"#51-构建乘积数组\" class=\"headerlink\" title=\"51.构建乘积数组\"></a>51.构建乘积数组</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 题目</span></span><br><span class=\"line\"><span class=\"comment\"># 给定一个数组A[0,1,...,n-1],请构建一个数组B[0,1,...,n-1],</span></span><br><span class=\"line\"><span class=\"comment\"># 其中B中的元素B[i]=A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]。不能使用除法。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 思路</span></span><br><span class=\"line\"><span class=\"comment\"># 审题仔细 没有A[i]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">multiply</span><span class=\"params\">(self, A)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        B=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(A)):</span><br><span class=\"line\">            temp=<span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(A)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> j==i:</span><br><span class=\"line\">                    <span class=\"keyword\">continue</span></span><br><span class=\"line\">                temp=temp*A[j]</span><br><span class=\"line\">            B.append(temp)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> B</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    A=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">    ans=solution.multiply(A)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"52-正则表达式匹配\"><a href=\"#52-正则表达式匹配\" class=\"headerlink\" title=\"52.正则表达式匹配\"></a>52.正则表达式匹配</h3><p><strong>题目：</strong>请实现一个函数用来匹配包括’.’和’*‘的正则表达式。模式中的字符’.’表示任意一个字符，而’*‘表示它前面的字符可以出现任意次（包含0次）。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”ab*ac*a”匹配，但是与”aa.a”和”ab*a”均不匹配。</p>\n<p><strong>思路：</strong></p>\n<blockquote>\n<p>当模式中的第二个字符不是<code>*</code>时： </p>\n<ul>\n<li>如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 </li>\n<li>如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。</li>\n</ul>\n<p>当模式中的第二个字符是<code>*</code>时：</p>\n<ul>\n<li>如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。</li>\n<li>如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式。<ul>\n<li>模式后移2字符，相当于<code>x*</code>被忽略。即模式串中*与他前面的字符和字符串匹配0次。 </li>\n<li>字符串后移1字符，模式后移2字符。即模式串中*与他前面的字符和字符串匹配1次。</li>\n<li>字符串后移1字符，模式不变，即继续匹配字符下一位，因为<code>*</code>可以匹配多位。即模式串中*与他前面的字符和字符串匹配多次。</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># s, pattern都是字符串</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">match</span><span class=\"params\">(self, s, pattern)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> s == pattern:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> pattern:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(pattern) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> pattern[<span class=\"number\">1</span>] == <span class=\"string\">'*'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (s <span class=\"keyword\">and</span> s[<span class=\"number\">0</span>] == pattern[<span class=\"number\">0</span>]) <span class=\"keyword\">or</span> (s <span class=\"keyword\">and</span> pattern[<span class=\"number\">0</span>] == <span class=\"string\">'.'</span>):</span><br><span class=\"line\">                <span class=\"keyword\">return</span> self.match(s, pattern[<span class=\"number\">2</span>:]) \\</span><br><span class=\"line\">                       <span class=\"keyword\">or</span> self.match(s[<span class=\"number\">1</span>:], pattern) \\</span><br><span class=\"line\">                       <span class=\"keyword\">or</span> self.match(s[<span class=\"number\">1</span>:], pattern[<span class=\"number\">2</span>:])</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> self.match(s, pattern[<span class=\"number\">2</span>:])</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> s <span class=\"keyword\">and</span> (s[<span class=\"number\">0</span>] == pattern[<span class=\"number\">0</span>] <span class=\"keyword\">or</span> pattern[<span class=\"number\">0</span>] == <span class=\"string\">'.'</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.match(s[<span class=\"number\">1</span>:], pattern[<span class=\"number\">1</span>:])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    s=<span class=\"string\">'aaa'</span></span><br><span class=\"line\">    pattern=<span class=\"string\">'a*a.a'</span></span><br><span class=\"line\">    ans=solution.match(s,pattern)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"53-表示数值的字符串\"><a href=\"#53-表示数值的字符串\" class=\"headerlink\" title=\"53.表示数值的字符串\"></a>53.表示数值的字符串</h3><p><strong>题目：</strong>请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># s字符串</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isNumeric</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"comment\"># 标记符号、小数点、e是否出现过</span></span><br><span class=\"line\">        sign,decimal,hasE=<span class=\"keyword\">False</span>,<span class=\"keyword\">False</span>,<span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> s[i]==<span class=\"string\">'e'</span> <span class=\"keyword\">or</span> s[i]==<span class=\"string\">'E'</span>:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i==len(s)<span class=\"number\">-1</span>:<span class=\"comment\"># e后面一定要接数字</span></span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> hasE==<span class=\"keyword\">True</span>:<span class=\"comment\"># 不能出现两次e</span></span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                hasE=<span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> s[i]==<span class=\"string\">'+'</span> <span class=\"keyword\">or</span> s[i]==<span class=\"string\">'-'</span>:</span><br><span class=\"line\">                <span class=\"comment\">#第二次出现+或-一定要在e之后</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> sign <span class=\"keyword\">and</span> s[i<span class=\"number\">-1</span>]!=<span class=\"string\">'e'</span> <span class=\"keyword\">and</span> s[i<span class=\"number\">-1</span>]!=<span class=\"string\">'E'</span>:</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                <span class=\"comment\"># 第一次出现+或-，如果不是出现在字符最前面，那么就要出现在e或者E后面</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> sign==<span class=\"keyword\">False</span> <span class=\"keyword\">and</span> i&gt;<span class=\"number\">0</span> <span class=\"keyword\">and</span> s[i<span class=\"number\">-1</span>]!=<span class=\"string\">'e'</span> <span class=\"keyword\">and</span> s[i<span class=\"number\">-1</span>]!=<span class=\"string\">'E'</span>:</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                sign=<span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> s[i]==<span class=\"string\">'.'</span>:</span><br><span class=\"line\">                <span class=\"comment\"># e后面不能出现小数点，小数点不能出现两次</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> decimal <span class=\"keyword\">or</span> hasE:</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                decimal=<span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> s[i]&gt;<span class=\"string\">'9'</span> <span class=\"keyword\">or</span> s[i]&lt;<span class=\"string\">'0'</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    s=<span class=\"string\">'123e.1416'</span></span><br><span class=\"line\">    ans=solution.isNumeric(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"54-字符流中第一个不重复的字符\"><a href=\"#54-字符流中第一个不重复的字符\" class=\"headerlink\" title=\"54.字符流中第一个不重复的字符\"></a>54.字符流中第一个不重复的字符</h3><p><strong>题目：</strong>请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。</p>\n<p><strong>输出描述：</strong>如果当前字符流没有存在出现一次的字符，返回#字符。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回对应char</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.all=&#123;&#125;</span><br><span class=\"line\">        self.ch=[]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FirstAppearingOnce</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.all <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">'#'</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> self.ch:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.all[c]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> c</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">'#'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Insert</span><span class=\"params\">(self, char)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.ch.append(char)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> char <span class=\"keyword\">in</span> self.all:</span><br><span class=\"line\">            self.all[char]=self.all[char]+<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.all[char]=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'g'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'o'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'o'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'g'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'l'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'e'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"55-链表中环的入口节点\"><a href=\"#55-链表中环的入口节点\" class=\"headerlink\" title=\"55.链表中环的入口节点\"></a>55.链表中环的入口节点</h3><p><strong>题目：</strong>给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。</p>\n<p><strong>思路：</strong>把链表中节点值放到dict数组中，并记录出现的次数，如果出现次数超过一次，则为环的入口节点。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">EntryNodeOfLoop</span><span class=\"params\">(self, pHead)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pHead <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        num,dict,flag=[],&#123;&#125;,<span class=\"keyword\">True</span></span><br><span class=\"line\">        tempans=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead <span class=\"keyword\">and</span> flag==<span class=\"keyword\">True</span>:</span><br><span class=\"line\">            num.append(pHead.val)</span><br><span class=\"line\">            numset=set(num)</span><br><span class=\"line\">            <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> numset:</span><br><span class=\"line\">                dict[c]=<span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">                dict[c]=dict[c]+<span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> dict[c]&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">                    flag=<span class=\"keyword\">False</span></span><br><span class=\"line\">                    tempans=c</span><br><span class=\"line\">            pHead=pHead.next</span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> pHead.val==tempans:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> pHead</span><br><span class=\"line\">            pHead=pHead.next</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    pHead1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead2 = ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    pHead3 = ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    pHead4 = ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    pHead5 = ListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    pHead1.next=pHead2</span><br><span class=\"line\">    pHead2.next=pHead3</span><br><span class=\"line\">    pHead3.next=pHead4</span><br><span class=\"line\">    pHead4.next=pHead5</span><br><span class=\"line\">    pHead5.next=pHead1</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.EntryNodeOfLoop(pHead1)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"56-删除链表中重复的节点\"><a href=\"#56-删除链表中重复的节点\" class=\"headerlink\" title=\"56.删除链表中重复的节点\"></a>56.删除链表中重复的节点</h3><p><strong>题目：</strong>在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5。</p>\n<p><strong>思路：</strong>记录链表中出现的数字，然后构建新链表。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deleteDuplication</span><span class=\"params\">(self, pHead)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        num=[]</span><br><span class=\"line\">        tempnum1=pHead</span><br><span class=\"line\">        <span class=\"keyword\">while</span> tempnum1:</span><br><span class=\"line\">            num.append(tempnum1.val)</span><br><span class=\"line\">            tempnum1=tempnum1.next</span><br><span class=\"line\">        dict=&#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">            dict[c]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">            dict[c]=dict[c]+<span class=\"number\">1</span></span><br><span class=\"line\">        newnum=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> dict[c]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                newnum.append(c)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> newnum==[]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        head=ListNode(newnum[<span class=\"number\">0</span>])</span><br><span class=\"line\">        temphead=head</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(newnum)):</span><br><span class=\"line\">            tempnode=ListNode(newnum[i])</span><br><span class=\"line\">            temphead.next=tempnode</span><br><span class=\"line\">            temphead=tempnode</span><br><span class=\"line\">        <span class=\"comment\"># while head:</span></span><br><span class=\"line\">        <span class=\"comment\">#     print(head.val)</span></span><br><span class=\"line\">        <span class=\"comment\">#     head=head.next</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> head</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    pHead1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead2 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead3 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead4 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead5 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead6 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead7 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    pHead1.next=pHead2</span><br><span class=\"line\">    pHead2.next=pHead3</span><br><span class=\"line\">    pHead3.next=pHead4</span><br><span class=\"line\">    pHead4.next=pHead5</span><br><span class=\"line\">    pHead5.next=pHead6</span><br><span class=\"line\">    pHead6.next=pHead7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.deleteDuplication(pHead1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"57-二叉树中的下一个节点\"><a href=\"#57-二叉树中的下一个节点\" class=\"headerlink\" title=\"57. 二叉树中的下一个节点\"></a>57. 二叉树中的下一个节点</h3><p><strong>题目：</strong>给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。</p>\n<p><strong>思路：</strong>分析二叉树的下一个节点，一共有以下情况：1.二叉树为空，则返回空；2.节点右孩子存在，则设置一个指针从该节点的右孩子出发，一直沿着指向左子结点的指针找到的叶子节点即为下一个节点；3.节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点；否则继续向上遍历其父节点的父节点，重复之前的判断，返回结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeLinkNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetNext</span><span class=\"params\">(self, pNode)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> pNode:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pNode</span><br><span class=\"line\">        <span class=\"keyword\">if</span> pNode.right:</span><br><span class=\"line\">            left1=pNode.right</span><br><span class=\"line\">            <span class=\"keyword\">while</span> left1.left:</span><br><span class=\"line\">                   left1=left1.left</span><br><span class=\"line\">            <span class=\"keyword\">return</span> left1</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pNode.next:</span><br><span class=\"line\">            tmp=pNode.next</span><br><span class=\"line\">            <span class=\"keyword\">if</span> tmp.left==pNode:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> tmp</span><br><span class=\"line\">            pNode=tmp</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br></pre></td></tr></table></figure>\n<h3 id=\"58-对称的二叉树\"><a href=\"#58-对称的二叉树\" class=\"headerlink\" title=\"58.对称的二叉树\"></a>58.对称的二叉树</h3><p><strong>题目：</strong>请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。</p>\n<p><strong>思路：</strong>采用递归的方法来判断两数是否相同。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isSymmetrical</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> pRoot:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        result=self.same(pRoot,pRoot)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">same</span><span class=\"params\">(self,root1,root2)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root1 <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> root2:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root1 <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> root2:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root1 <span class=\"keyword\">and</span> root2:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root1.val!= root2.val:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">        left=self.same(root1.left,root2.right)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> left:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        right=self.same(root1.right,root2.left)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> right:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.isSymmetrical(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"59-按之字形顺序打印二叉树\"><a href=\"#59-按之字形顺序打印二叉树\" class=\"headerlink\" title=\"59.按之字形顺序打印二叉树\"></a>59.按之字形顺序打印二叉树</h3><p><strong>题目：</strong>请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。</p>\n<p><strong>思路：</strong> 把当前列结果存放到list之中，设置翻转变量，依次从左到右打印和从右到左打印。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Print</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        root=pRoot</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        level=[root]</span><br><span class=\"line\">        result=[]</span><br><span class=\"line\">        righttoleft=<span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> level:</span><br><span class=\"line\">            curvalues=[]</span><br><span class=\"line\">            nextlevel=[]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> level:</span><br><span class=\"line\">                curvalues.append(i.val)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i.left:</span><br><span class=\"line\">                    nextlevel.append(i.left)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i.right:</span><br><span class=\"line\">                    nextlevel.append(i.right)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> righttoleft:</span><br><span class=\"line\">                    curvalues.reverse()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> curvalues:</span><br><span class=\"line\">                    result.append(curvalues)</span><br><span class=\"line\">            level = nextlevel</span><br><span class=\"line\">            righttoleft = <span class=\"keyword\">not</span> righttoleft</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.Print(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"60-把二叉树打印成多行\"><a href=\"#60-把二叉树打印成多行\" class=\"headerlink\" title=\"60.把二叉树打印成多行\"></a>60.把二叉树打印成多行</h3><p><strong>题目：</strong>从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回二维列表[[1,2],[4,5]]</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Print</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        root=pRoot</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        level=[root]</span><br><span class=\"line\">        result=[]</span><br><span class=\"line\">        <span class=\"keyword\">while</span> level:</span><br><span class=\"line\">            curvalues=[]</span><br><span class=\"line\">            nextlevel=[]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> level:</span><br><span class=\"line\">                curvalues.append(i.val)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i.left:</span><br><span class=\"line\">                    nextlevel.append(i.left)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i.right:</span><br><span class=\"line\">                    nextlevel.append(i.right)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> curvalues:</span><br><span class=\"line\">                    result.append(curvalues)</span><br><span class=\"line\">            level = nextlevel</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.Print(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"61-序列化二叉树\"><a href=\"#61-序列化二叉树\" class=\"headerlink\" title=\"61.序列化二叉树\"></a>61.序列化二叉树</h3><p><strong>题目：</strong>请实现两个函数，分别用来序列化和反序列化二叉树。</p>\n<p><strong>思路：</strong>转变成前序遍历，空元素利用”#”代替，然后进行解序列。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> collections</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Serialize</span><span class=\"params\">(self, root)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        res=[]</span><br><span class=\"line\">        self.pre(root,res)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pre</span><span class=\"params\">(self,root,res)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        res.append(root.val)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left:</span><br><span class=\"line\">            self.pre(root.left, res)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            res.append(<span class=\"string\">'#'</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.right:</span><br><span class=\"line\">            self.pre(root.right,res)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            res.append(<span class=\"string\">'#'</span>)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Deserialize</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> s==<span class=\"string\">''</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        vals=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">            vals.append(s[i])</span><br><span class=\"line\">        vals=collections.deque(vals)</span><br><span class=\"line\">        ans=self.build(vals)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">build</span><span class=\"params\">(self,vals)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> vals:</span><br><span class=\"line\">            val = vals.popleft()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> val == <span class=\"string\">'#'</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">            root = TreeNode(int(val))</span><br><span class=\"line\">            root.left = self.build(vals)</span><br><span class=\"line\">            root.right = self.build(vals)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> root</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.build(vals)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># [1, ',', 2, ',', 4, ',', ',', ',', 5, ',', ',', ',', 3, ',', 6, ',', ',', ',', 7, ',', ',']</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.Serialize(A1)</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    root=solution.Deserialize(ans)</span><br><span class=\"line\">    res=solution.Serialize(root)</span><br><span class=\"line\">    print(res)</span><br></pre></td></tr></table></figure>\n<h3 id=\"62-二叉搜索树中的第K个节点\"><a href=\"#62-二叉搜索树中的第K个节点\" class=\"headerlink\" title=\"62.二叉搜索树中的第K个节点\"></a>62.二叉搜索树中的第K个节点</h3><p><strong>题目：</strong>给定一棵二叉搜索树，请找出其中的第k小的结点。例如（5，3，7，2，4，6，8）中，按结点数值大小顺序第三小结点的值为4。</p>\n<p><strong>思路：</strong>中序遍历后，返回第K个节点值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回对应节点TreeNode</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">KthNode</span><span class=\"params\">(self, pRoot, k)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        res=[]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> pRoot:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.order(pRoot,res)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(res)&lt;k <span class=\"keyword\">or</span> k&lt;=<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> res[k<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">order</span><span class=\"params\">(self,root,res)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        self.order(root.left,res)</span><br><span class=\"line\">        res.append(root)</span><br><span class=\"line\">        self.order(root.right,res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    k=<span class=\"number\">3</span></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.KthNode(A1,k)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"63-数据流中的中位数\"><a href=\"#63-数据流中的中位数\" class=\"headerlink\" title=\"63.数据流中的中位数\"></a>63.数据流中的中位数</h3><p><strong>题目：</strong>如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.data=[]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Insert</span><span class=\"params\">(self, num)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.data.append(num)</span><br><span class=\"line\">        self.data.sort()</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetMedian</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        length=len(self.data)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> length%<span class=\"number\">2</span>==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> (self.data[length//<span class=\"number\">2</span>]+self.data[length//<span class=\"number\">2</span><span class=\"number\">-1</span>])/<span class=\"number\">2.0</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.data[int(length//<span class=\"number\">2</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">5</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">2</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">3</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">4</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">1</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"64-滑动窗口的最大值\"><a href=\"#64-滑动窗口的最大值\" class=\"headerlink\" title=\"64.滑动窗口的最大值\"></a>64.滑动窗口的最大值</h3><p><strong>题目：</strong>给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}，{2,3,4,2,6,[2,5,1]}。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maxInWindows</span><span class=\"params\">(self, num, size)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> size==<span class=\"number\">0</span> <span class=\"keyword\">or</span> num==[]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        res=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(num)-size+<span class=\"number\">1</span>):</span><br><span class=\"line\">            tempnum=[]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i,i+size):</span><br><span class=\"line\">                tempnum.append(num[j])</span><br><span class=\"line\">            res.append(max(tempnum))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    num=[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">6</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">    size=<span class=\"number\">3</span></span><br><span class=\"line\">    ans=solution.maxInWindows(num,size)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"66-矩阵中的路径\"><a href=\"#66-矩阵中的路径\" class=\"headerlink\" title=\"66.矩阵中的路径\"></a>66.矩阵中的路径</h3><p><strong>题目：</strong>请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。</p>\n<p><strong>思路：</strong>当起点第一个字符相同时，开始进行递归搜索，设计搜索函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasPath</span><span class=\"params\">(self, matrix, rows, cols, path)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,rows):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,cols):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> matrix[i*rows+j]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> self.find_path(list(matrix),rows,cols,path[<span class=\"number\">1</span>:],i,j):</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">find_path</span><span class=\"params\">(self,matrix,rows,cols,path,i,j)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> path:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        matrix[i*cols+j]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> j+<span class=\"number\">1</span>&lt;cols <span class=\"keyword\">and</span> matrix[i*cols+j+<span class=\"number\">1</span>]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.find_path(matrix,rows,cols,path[<span class=\"number\">1</span>:],i,j+<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> j<span class=\"number\">-1</span>&gt;=<span class=\"number\">0</span> <span class=\"keyword\">and</span> matrix[i*cols+j<span class=\"number\">-1</span>]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.find_path(matrix, rows, cols, path[<span class=\"number\">1</span>:], i, j - <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> i+<span class=\"number\">1</span>&lt;rows <span class=\"keyword\">and</span> matrix[(i+<span class=\"number\">1</span>)*cols+j]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.find_path(matrix, rows, cols, path[<span class=\"number\">1</span>:], i+<span class=\"number\">1</span>, j)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> i<span class=\"number\">-1</span>&gt;=<span class=\"number\">0</span> <span class=\"keyword\">and</span> matrix[(i<span class=\"number\">-1</span>)*cols+j]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.find_path(matrix, rows, cols, path[<span class=\"number\">1</span>:], i<span class=\"number\">-1</span>, j)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    matrix=<span class=\"string\">'ABCEHJIGSFCSLOPQADEEMNOEADIDEJFMVCEIFGGS'</span></span><br><span class=\"line\">    rows=<span class=\"number\">5</span></span><br><span class=\"line\">    cols=<span class=\"number\">8</span></span><br><span class=\"line\">    path=<span class=\"string\">'SGGFIECVAASABCEHJIGQEMS'</span></span><br><span class=\"line\">    ans=solution.hasPath(matrix,rows,cols,path)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"66-机器人的运动范围\"><a href=\"#66-机器人的运动范围\" class=\"headerlink\" title=\"66.机器人的运动范围\"></a>66.机器人的运动范围</h3><p><strong>题目：</strong>地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？</p>\n<p><strong>思路：</strong>对未走过的路径进行遍历，搜索所有的路径值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.vis = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">movingCount</span><span class=\"params\">(self, threshold, rows, cols)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.moving(threshold, rows, cols, <span class=\"number\">0</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">moving</span><span class=\"params\">(self, threshold, rows, cols, row, col)</span>:</span></span><br><span class=\"line\">        rowans,colans=<span class=\"number\">0</span>,<span class=\"number\">0</span></span><br><span class=\"line\">        rowtemp,coltemp=row,col</span><br><span class=\"line\">        <span class=\"keyword\">while</span> rowtemp&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            rowans=rowans+rowtemp%<span class=\"number\">10</span></span><br><span class=\"line\">            rowtemp=rowtemp//<span class=\"number\">10</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> coltemp&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            colans=colans+coltemp%<span class=\"number\">10</span></span><br><span class=\"line\">            coltemp=coltemp//<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> rowans+colans&gt;threshold:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> row &gt;= rows <span class=\"keyword\">or</span> col &gt;= cols <span class=\"keyword\">or</span> row &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> col &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (row, col) <span class=\"keyword\">in</span> self.vis:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        self.vis[(row, col)] = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span> + self.moving(threshold, rows, cols, row - <span class=\"number\">1</span>, col) +\\</span><br><span class=\"line\">               self.moving(threshold, rows, cols, row + <span class=\"number\">1</span>,col) + \\</span><br><span class=\"line\">               self.moving(threshold, rows,cols, row,col - <span class=\"number\">1</span>) + \\</span><br><span class=\"line\">               self.moving(threshold, rows, cols, row, col + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    threshold=<span class=\"number\">10</span></span><br><span class=\"line\">    rows,cols=<span class=\"number\">1</span>,<span class=\"number\">100</span></span><br><span class=\"line\">    ans=solution.movingCount(threshold,rows,cols)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"67-推广\"><a href=\"#67-推广\" class=\"headerlink\" title=\"67.推广\"></a>67.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/07/29/《剑指Offer》Python版/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-二维数组中的查找\"><a href=\"#1-二维数组中的查找\" class=\"headerlink\" title=\"1.二维数组中的查找\"></a>1.二维数组中的查找</h3><p><strong>题目：</strong> 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p>\n<p><strong>思路：</strong>遍历每一行，查找该元素是否在该行之中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># array 二维列表</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Find</span><span class=\"params\">(self, target, array)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> array:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> target <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    target=<span class=\"number\">2</span></span><br><span class=\"line\">    array=[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>],[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],[<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>]]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Find(target,array)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-替换空格\"><a href=\"#2-替换空格\" class=\"headerlink\" title=\"2.替换空格\"></a>2.替换空格</h3><p><strong>题目：</strong> 请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。</p>\n<p><strong>思路：</strong>利用字符串中的replace直接替换即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># s 源字符串</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">replaceSpace</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        temp = s.replace(<span class=\"string\">\" \"</span>, <span class=\"string\">\"%20\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> temp</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=<span class=\"string\">'We Are Happy'</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.replaceSpace(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-从尾到头打印链表\"><a href=\"#3-从尾到头打印链表\" class=\"headerlink\" title=\"3.从尾到头打印链表\"></a>3.从尾到头打印链表</h3><p><strong>题目：</strong>输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。</p>\n<p><strong>思路：</strong>将链表中的值记录到list之中，然后进行翻转list。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回从尾部到头部的列表值序列，例如[1,2,3]</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printListFromTailToHead</span><span class=\"params\">(self, listNode)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        l=[]</span><br><span class=\"line\">        <span class=\"keyword\">while</span> listNode:</span><br><span class=\"line\">            l.append(listNode.val)</span><br><span class=\"line\">            listNode=listNode.next</span><br><span class=\"line\">        <span class=\"keyword\">return</span> l[::<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = ListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.next=A2</span><br><span class=\"line\">    A2.next=A3</span><br><span class=\"line\">    A3.next=A4</span><br><span class=\"line\">    A4.next=A5</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.printListFromTailToHead(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-重建二叉树\"><a href=\"#4-重建二叉树\" class=\"headerlink\" title=\"4.重建二叉树\"></a>4.重建二叉树</h3><p><strong>题目：</strong>输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。</p>\n<p><strong>题解：</strong>首先前序遍历的第一个元素为二叉树的根结点，那么便能够在中序遍历之中找到根节点，那么在根结点左侧则是左子树，假设长度为M.在根结点右侧，便是右子树,假设长度为N。然后在前序遍历根节点后面M长度的便是左子树的前序遍历序列，再后面的N个长度便是右子树的后序遍历的长度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回构造的TreeNode根节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reConstructBinaryTree</span><span class=\"params\">(self, pre, tin)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(pre)==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(pre)==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> TreeNode(pre[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            flag=TreeNode(pre[<span class=\"number\">0</span>])</span><br><span class=\"line\">            flag.left=self.reConstructBinaryTree(pre[<span class=\"number\">1</span>:tin.index(pre[<span class=\"number\">0</span>])+<span class=\"number\">1</span>],tin[:tin.index(pre[<span class=\"number\">0</span>])])</span><br><span class=\"line\">            flag.right=self.reConstructBinaryTree(pre[tin.index(pre[<span class=\"number\">0</span>])+<span class=\"number\">1</span>:],tin[tin.index(pre[<span class=\"number\">0</span>])+<span class=\"number\">1</span>:])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> flag</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    pre=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    tin=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    ans=solution.reConstructBinaryTree(pre,tin)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-用两个栈实现队列\"><a href=\"#5-用两个栈实现队列\" class=\"headerlink\" title=\"5.用两个栈实现队列\"></a>5.用两个栈实现队列</h3><p><strong>题目：</strong>用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。</p>\n<p><strong>题解：</strong>申请两个栈Stack1和Stack2，Stack1当作输入，Stack2当作pop。当Stack2空的时候，将Stack1进行反转，并且输入到Stack2。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.Stack1=[]</span><br><span class=\"line\">        self.Stack2=[]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">push</span><span class=\"params\">(self, node)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.Stack1.append(node)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pop</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># return xx</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.Stack2==[]:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> self.Stack1:</span><br><span class=\"line\">                self.Stack2.append(self.Stack1.pop())</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.Stack2.pop()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.Stack2.pop()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    solution.push(<span class=\"number\">1</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">2</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">3</span>)</span><br><span class=\"line\">    print(solution.pop())</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-旋转数组的最小数字\"><a href=\"#6-旋转数组的最小数字\" class=\"headerlink\" title=\"6.旋转数组的最小数字\"></a>6.旋转数组的最小数字</h3><p><strong>题目：</strong>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。</p>\n<p><strong>题解：</strong>遍历数组寻找数组最小值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">minNumberInRotateArray</span><span class=\"params\">(self, rotateArray)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        minnum=<span class=\"number\">999999</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(rotateArray)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> minnum&gt;rotateArray[i]:</span><br><span class=\"line\">                minnum=rotateArray[i]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> minnum:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> minnum</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    rotateArray=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    ans=solution.minNumberInRotateArray(rotateArray)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"7-斐波那契数列\"><a href=\"#7-斐波那契数列\" class=\"headerlink\" title=\"7.斐波那契数列\"></a>7.斐波那契数列</h3><p><strong>题目：</strong>大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39。</p>\n<p><strong>题解：</strong>递归和非递归方法。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Fibonacci</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        Fib=[<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,n+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        Fib[<span class=\"number\">0</span>],Fib[<span class=\"number\">1</span>]=<span class=\"number\">0</span>,<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>,n+<span class=\"number\">1</span>):</span><br><span class=\"line\">            Fib[i]=Fib[i<span class=\"number\">-1</span>]+Fib[i<span class=\"number\">-2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Fib[n]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Fibonacci1</span><span class=\"params\">(self,n)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">1</span> <span class=\"keyword\">or</span> n==<span class=\"number\">2</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.Fibonacci1(n<span class=\"number\">-1</span>)+self.Fibonacci1(n<span class=\"number\">-2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.Fibonacci1(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-跳台阶\"><a href=\"#8-跳台阶\" class=\"headerlink\" title=\"8.跳台阶\"></a>8.跳台阶</h3><p><strong>题目：</strong>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。</p>\n<p><strong>题解：</strong>ans[n]=ans[n-1]+ans[n-2]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">jumpFloor</span><span class=\"params\">(self, number)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">2</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span></span><br><span class=\"line\">        ans=[<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,number+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        ans[<span class=\"number\">1</span>],ans[<span class=\"number\">2</span>]=<span class=\"number\">1</span>,<span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>,number+<span class=\"number\">1</span>):</span><br><span class=\"line\">            ans[i]=ans[i<span class=\"number\">-1</span>]+ans[i<span class=\"number\">-2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans[number]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.jumpFloor(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"9-变态跳台阶\"><a href=\"#9-变态跳台阶\" class=\"headerlink\" title=\"9.变态跳台阶\"></a>9.变态跳台阶</h3><p><strong>题目：</strong>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p>\n<p><strong>题解：</strong>ans[n]=ans[n-1]+ans[n-2]+ans[n-3]+…+ans[n-n]，ans[n-1]=ans[n-2]+ans[n-3]+…+ans[n-n]，ans[n]=2*ans[n-1]。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">jumpFloorII</span><span class=\"params\">(self, number)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">2</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">2</span>*self.jumpFloorII(number<span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.jumpFloorII(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"10-矩形覆盖\"><a href=\"#10-矩形覆盖\" class=\"headerlink\" title=\"10.矩形覆盖\"></a>10.矩形覆盖</h3><p><strong>题目：</strong>我们可以用2<em>1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2</em>1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？</p>\n<p><strong>题解：</strong>新增加的小矩阵竖着放，则方法与n-1时相同，新增加的小矩阵横着放，则方法与n-2时相同，于是f(n)=f(n-1)+f(n-2)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rectCover</span><span class=\"params\">(self, number)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> number==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        Fib=[<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,number+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        Fib[<span class=\"number\">1</span>],Fib[<span class=\"number\">2</span>]=<span class=\"number\">1</span>,<span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>,number+<span class=\"number\">1</span>):</span><br><span class=\"line\">            Fib[i]=Fib[i<span class=\"number\">-1</span>]+Fib[i<span class=\"number\">-2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Fib[number]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.rectCover(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"11-二进制中1的个数\"><a href=\"#11-二进制中1的个数\" class=\"headerlink\" title=\"11.二进制中1的个数\"></a>11.二进制中1的个数</h3><p><strong>题目：</strong>输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。</p>\n<p><strong>题解：</strong>每次进行左移一位，然后与1进行相与，如果是1则进行加1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">NumberOf1</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        count = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">32</span>):</span><br><span class=\"line\">            count += (n &gt;&gt; i) &amp; <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> count</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    n=int(input())</span><br><span class=\"line\">    ans=solution.NumberOf1(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"12-数值的整次方\"><a href=\"#12-数值的整次方\" class=\"headerlink\" title=\"12.数值的整次方\"></a>12.数值的整次方</h3><p><strong>题目：</strong>给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Power</span><span class=\"params\">(self, base, exponent)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,abs(exponent)):</span><br><span class=\"line\">            ans=ans*base</span><br><span class=\"line\">        <span class=\"keyword\">if</span> exponent&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> ans</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span>/ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    base=float(input())</span><br><span class=\"line\">    exponent=int(input())</span><br><span class=\"line\">    ans=solution.Power(base,exponent)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"13-调整数组顺序使奇数位于偶数前面\"><a href=\"#13-调整数组顺序使奇数位于偶数前面\" class=\"headerlink\" title=\"13.调整数组顺序使奇数位于偶数前面\"></a>13.调整数组顺序使奇数位于偶数前面</h3><p><strong>题目：</strong>输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。</p>\n<p><strong>题解：</strong>申请奇数数组和偶数数组，分别存放奇数值和偶数值，数组相加便为结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reOrderArray</span><span class=\"params\">(self, array)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        array1=[]<span class=\"comment\">#奇数</span></span><br><span class=\"line\">        array2=[]<span class=\"comment\">#偶数</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(array)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> array[i]%<span class=\"number\">2</span>!=<span class=\"number\">0</span>:</span><br><span class=\"line\">                array1.append(array[i])</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                array2.append(array[i])</span><br><span class=\"line\">        ans=array1+array2</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    array=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    ans=solution.reOrderArray(array)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"14-链表中倒数第K个节点\"><a href=\"#14-链表中倒数第K个节点\" class=\"headerlink\" title=\"14.链表中倒数第K个节点\"></a>14.链表中倒数第K个节点</h3><p><strong>题目：</strong>输入一个链表，输出该链表中倒数第k个结点。</p>\n<p><strong>题解：</strong>反转链表，寻找第K个节点。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindKthToTail</span><span class=\"params\">(self, head, k)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"comment\">#反转链表</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> head <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">or</span> head.next <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> head</span><br><span class=\"line\">        pre=<span class=\"keyword\">None</span> <span class=\"comment\">#指向上一个节点</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> head:</span><br><span class=\"line\">            <span class=\"comment\">#先用temp保存当前节点的下一个节点信息</span></span><br><span class=\"line\">            temp=head.next</span><br><span class=\"line\">            <span class=\"comment\">#保存好next之后，便可以指向上一个节点</span></span><br><span class=\"line\">            head.next=pre</span><br><span class=\"line\">            <span class=\"comment\">#让pre,head指向下一个移动的节点</span></span><br><span class=\"line\">            pre=head</span><br><span class=\"line\">            head=temp</span><br><span class=\"line\">        <span class=\"comment\"># 寻找第K个元素的位置</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,k):</span><br><span class=\"line\">            pre=pre.next</span><br><span class=\"line\">        temp=pre</span><br><span class=\"line\">        <span class=\"keyword\">return</span> temp</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    k=<span class=\"number\">3</span></span><br><span class=\"line\">    p1=ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    p2=ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    p3=ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    p4=ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    p1.next=p2</span><br><span class=\"line\">    p2.next=p3</span><br><span class=\"line\">    p3.next=p4</span><br><span class=\"line\"></span><br><span class=\"line\">    ans=solution.FindKthToTail(p1,k)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"15-反转链表\"><a href=\"#15-反转链表\" class=\"headerlink\" title=\"15.反转链表\"></a>15.反转链表</h3><p><strong>题目：</strong>输入一个链表，反转链表后，输出新链表的表头。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回ListNode</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ReverseList</span><span class=\"params\">(self, pHead)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pHead <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">or</span> pHead.next <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pHead</span><br><span class=\"line\">        pre=<span class=\"keyword\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead:</span><br><span class=\"line\">            <span class=\"comment\">#暂存当前节点的下一个节点信息</span></span><br><span class=\"line\">            temp=pHead.next</span><br><span class=\"line\">            <span class=\"comment\">#反转节点</span></span><br><span class=\"line\">            pHead.next=pre</span><br><span class=\"line\">            <span class=\"comment\">#进行下一个节点</span></span><br><span class=\"line\">            pre = pHead</span><br><span class=\"line\">            pHead=temp</span><br><span class=\"line\">        <span class=\"keyword\">return</span> pre</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    p1=ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    p2=ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    p3=ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    p1.next=p2</span><br><span class=\"line\">    p2.next=p3</span><br><span class=\"line\">    ans=solution.ReverseList(p1)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"16-合并两个排序的列表\"><a href=\"#16-合并两个排序的列表\" class=\"headerlink\" title=\"16.合并两个排序的列表\"></a>16.合并两个排序的列表</h3><p><strong>题目：</strong>输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。</p>\n<p><strong>题解：</strong>将两个链表之中的数值转换到列表之中，并进行排序，将排序后的列表构造成链表。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回合并后列表</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Merge</span><span class=\"params\">(self,pHead1,pHead2)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pHead1 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> pHead2 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        num1,num2=[],[]</span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead1:</span><br><span class=\"line\">            num1.append(pHead1.val)</span><br><span class=\"line\">            pHead1=pHead1.next</span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead2:</span><br><span class=\"line\">            num2.append(pHead2.val)</span><br><span class=\"line\">            pHead2=pHead2.next</span><br><span class=\"line\">        ans=num1+num2</span><br><span class=\"line\">        ans.sort()</span><br><span class=\"line\">        head=ListNode(ans[<span class=\"number\">0</span>])</span><br><span class=\"line\">        pre=head</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(ans)):</span><br><span class=\"line\">            node=ListNode(ans[i])</span><br><span class=\"line\">            pre.next=node</span><br><span class=\"line\">            pre=pre.next</span><br><span class=\"line\">        <span class=\"keyword\">return</span> head</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    pHead1_1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead1_2 = ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    pHead1_3 = ListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    pHead1_1.next=pHead1_2</span><br><span class=\"line\">    pHead1_2.next=pHead1_3</span><br><span class=\"line\"></span><br><span class=\"line\">    pHead2_1 = ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    pHead2_2 = ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    pHead2_3 = ListNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    pHead2_1.next=pHead2_2</span><br><span class=\"line\">    pHead2_2.next=pHead2_3</span><br><span class=\"line\">    ans=solution.Merge(pHead1_1,pHead2_1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"17-树的子结构\"><a href=\"#17-树的子结构\" class=\"headerlink\" title=\"17.树的子结构\"></a>17.树的子结构</h3><p><strong>题目：</strong>输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）。</p>\n<p><strong>题解：</strong>将树转变为中序序列，然后转变为str类型，最后判断是否包含。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">HasSubtree</span><span class=\"params\">(self, pRoot1, pRoot2)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRoot1 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">or</span> pRoot2 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        pRoot1_result,pRoot2_result=[],[]</span><br><span class=\"line\">        self.order_traversal(pRoot1,pRoot1_result)</span><br><span class=\"line\">        self.order_traversal(pRoot2,pRoot2_result)</span><br><span class=\"line\">        str1=<span class=\"string\">''</span>.join(str(i) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> pRoot1_result)</span><br><span class=\"line\">        str2=<span class=\"string\">''</span>.join(str(i) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> pRoot2_result)</span><br><span class=\"line\">        print(str1,str2)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> str2 <span class=\"keyword\">in</span> str1:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">order_traversal</span><span class=\"params\">(self,root,result)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        self.order_traversal(root.left,result)</span><br><span class=\"line\">        result.append(root.val)</span><br><span class=\"line\">        self.order_traversal(root.right,result)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    pRootA1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pRootA2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    pRootA3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    pRootA4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    pRootA5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    pRootA1.left=pRootA2</span><br><span class=\"line\">    pRootA1.right=pRootA3</span><br><span class=\"line\">    pRootA2.left=pRootA4</span><br><span class=\"line\">    pRootA2.right=pRootA5</span><br><span class=\"line\"></span><br><span class=\"line\">    pRootB2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    pRootB4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    pRootB5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    pRootB2.left=pRootB4</span><br><span class=\"line\">    pRootB2.right = pRootB5</span><br><span class=\"line\">    ans=solution.HasSubtree(pRootA1,pRootB2)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"18-二叉树的镜像\"><a href=\"#18-二叉树的镜像\" class=\"headerlink\" title=\"18.二叉树的镜像\"></a>18.二叉树的镜像</h3><p><strong>题目：</strong> 操作给定的二叉树，将其变换为源二叉树的镜像。</p>\n<p><strong>输入描述：</strong></p>\n<p>​    源二叉树<br>          8<br>         /  \\<br>        6   10<br>       / \\  / \\<br>      5  7 9 11<br>      镜像二叉树<br>          8<br>         /  \\<br>        10   6<br>       / \\  / \\<br>      11 9 7  5</p>\n<p><strong>思路：</strong>递归实现反转每个子节点</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回镜像树的根节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Mirror</span><span class=\"params\">(self, root)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"comment\"># A1_order_result=[]</span></span><br><span class=\"line\">        <span class=\"comment\"># self.order_traversal(A1,A1_order_result)</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> root.right <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        temp=root.left</span><br><span class=\"line\">        root.left=root.right</span><br><span class=\"line\">        root.right=temp</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            self.Mirror(root.left)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            self.Mirror(root.right)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">order_traversal</span><span class=\"params\">(self,root,result)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        self.order_traversal(root.left,result)</span><br><span class=\"line\">        result.append(root.val)</span><br><span class=\"line\">        self.order_traversal(root.right,result)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">8</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">10</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">9</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">11</span>)</span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    temp1=[]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    solution.order_traversal(A1,temp1)</span><br><span class=\"line\">    print(temp1)</span><br><span class=\"line\">    solution.Mirror(A1)</span><br><span class=\"line\">    solution.order_traversal(A1,temp1)</span><br><span class=\"line\">    print(temp1)</span><br></pre></td></tr></table></figure>\n<h3 id=\"19-顺时针打印矩阵\"><a href=\"#19-顺时针打印矩阵\" class=\"headerlink\" title=\"19.顺时针打印矩阵\"></a>19.顺时针打印矩阵</h3><p><strong>题目：</strong></p>\n<blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，</span><br><span class=\"line\">&gt; 例如，如果输入如下矩阵：</span><br><span class=\"line\">&gt;  <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span></span><br><span class=\"line\">&gt;  <span class=\"number\">5</span> <span class=\"number\">6</span> <span class=\"number\">7</span> <span class=\"number\">8</span></span><br><span class=\"line\">&gt;  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span> <span class=\"number\">12</span></span><br><span class=\"line\">&gt;  <span class=\"number\">13</span> <span class=\"number\">14</span> <span class=\"number\">15</span> <span class=\"number\">16</span></span><br><span class=\"line\">&gt; 则依次打印出数字</span><br><span class=\"line\">&gt; <span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">12</span>,<span class=\"number\">16</span>,<span class=\"number\">15</span>,<span class=\"number\">14</span>,<span class=\"number\">13</span>,<span class=\"number\">9</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">11</span>,<span class=\"number\">10.</span></span><br><span class=\"line\">&gt; </span><br><span class=\"line\">&gt; </span><br><span class=\"line\">&gt;</span><br></pre></td></tr></table></figure>\n</blockquote>\n<p><strong>思路：</strong>每次打印圈，但要判断最后一次是打印横还是竖，另外判断数据是否已存在。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># matrix类型为二维列表，需要返回列表</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printMatrix</span><span class=\"params\">(self, matrix)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        m,n=len(matrix),len(matrix[<span class=\"number\">0</span>])</span><br><span class=\"line\">        res = []</span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">1</span> <span class=\"keyword\">and</span> m==<span class=\"number\">1</span>:</span><br><span class=\"line\">            res.append(matrix[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\">            <span class=\"keyword\">return</span> res</span><br><span class=\"line\">        <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,(min(m,n)+<span class=\"number\">1</span>)//<span class=\"number\">2</span>):</span><br><span class=\"line\">            [res.append(matrix[k][i]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(k, n - k)]</span><br><span class=\"line\">            [res.append(matrix[j][n-k<span class=\"number\">-1</span>]) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(k,m-k) <span class=\"keyword\">if</span> matrix[j][n-k<span class=\"number\">-1</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> res]</span><br><span class=\"line\">            [res.append(matrix[m-k<span class=\"number\">-1</span>][j]) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(n-k<span class=\"number\">-1</span>,k<span class=\"number\">-1</span>,<span class=\"number\">-1</span>) <span class=\"keyword\">if</span> matrix[m-k<span class=\"number\">-1</span>][j] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> res]</span><br><span class=\"line\">            [res.append(matrix[j][k]) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(m<span class=\"number\">-1</span>-k,k<span class=\"number\">-1</span>,<span class=\"number\">-1</span>) <span class=\"keyword\">if</span> matrix[j][k] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> res]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    m,n=<span class=\"number\">1</span>,<span class=\"number\">5</span></span><br><span class=\"line\">    matrix=[]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,m):</span><br><span class=\"line\">        matrix.append(list(map(int,input().split(<span class=\"string\">' '</span>))))</span><br><span class=\"line\">    print(matrix)</span><br><span class=\"line\">    ans=solution.printMatrix(matrix)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"20-包含Min函数的栈\"><a href=\"#20-包含Min函数的栈\" class=\"headerlink\" title=\"20.包含Min函数的栈\"></a>20.包含Min函数的栈</h3><p><strong>题目：</strong>定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.num=[]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">push</span><span class=\"params\">(self, node)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.num.append(node)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pop</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.num.pop()</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">top</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        numlen = len(self.num)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.num[numlen<span class=\"number\">-1</span>]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">min</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> min(self.num)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    solution.push(<span class=\"number\">1</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">2</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">3</span>)</span><br><span class=\"line\">    solution.push(<span class=\"number\">4</span>)</span><br><span class=\"line\">    solution.pop()</span><br><span class=\"line\">    print(solution.top())</span><br><span class=\"line\">    print(solution.min())</span><br></pre></td></tr></table></figure>\n<h3 id=\"21-栈的压入弹出序列\"><a href=\"#21-栈的压入弹出序列\" class=\"headerlink\" title=\"21.栈的压入弹出序列\"></a>21.栈的压入弹出序列</h3><p><strong>题目：</strong>输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）。</p>\n<p><strong>题解：</strong>新构建一个中间栈，来模拟栈的输入和栈的输出，比对输入结果和输出结果是否相等。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">IsPopOrder</span><span class=\"params\">(self, pushV, popV)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(pushV)==<span class=\"number\">1</span> <span class=\"keyword\">and</span> len(popV)==<span class=\"number\">1</span> <span class=\"keyword\">and</span> pushV[<span class=\"number\">0</span>]!=popV[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">        helpV=[]</span><br><span class=\"line\">        pushV.reverse()</span><br><span class=\"line\">        popV.reverse()</span><br><span class=\"line\">        <span class=\"comment\">#模拟给定栈的压入和压出</span></span><br><span class=\"line\">        helpV.append(pushV[len(pushV)<span class=\"number\">-1</span>])</span><br><span class=\"line\">        pushV.pop()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> helpV[len(helpV)<span class=\"number\">-1</span>]!=popV[len(popV)<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                helpV.append(pushV[len(pushV)<span class=\"number\">-1</span>])</span><br><span class=\"line\">                pushV.pop()</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> helpV[len(helpV)<span class=\"number\">-1</span>]==popV[len(popV)<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                helpV.pop()</span><br><span class=\"line\">                popV.pop()</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> pushV==[] <span class=\"keyword\">and</span> popV==[] <span class=\"keyword\">and</span> helpV==[]:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> pushV==[] <span class=\"keyword\">and</span> popV[len(popV)<span class=\"number\">-1</span>]!=helpV[len(helpV)<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    push=list(map(int,input().split(<span class=\"string\">' '</span>)))</span><br><span class=\"line\">    pop=list(map(int,input().split(<span class=\"string\">' '</span>)))</span><br><span class=\"line\">    ans=solution.IsPopOrder(push,pop)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"22-从上往下打印二叉树\"><a href=\"#22-从上往下打印二叉树\" class=\"headerlink\" title=\"22.从上往下打印二叉树\"></a>22.从上往下打印二叉树</h3><p><strong>题目：</strong>从上往下打印出二叉树的每个节点，同层节点从左至右打印。</p>\n<p><strong>思路：</strong>递归，每次将左子树结果和右子树结果存到结果集之中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回从上到下每个节点值列表，例：[1,2,3]</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">PrintFromTopToBottom</span><span class=\"params\">(self, root)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        ans.append(root.val)</span><br><span class=\"line\">        self.orderans(root,ans)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">orderans</span><span class=\"params\">(self,root,ans)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left:</span><br><span class=\"line\">            ans.append(root.left.val)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.right:</span><br><span class=\"line\">            ans.append(root.right.val)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.orderans(root.left, ans)</span><br><span class=\"line\">        self.orderans(root.right,ans)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    ans=solution.PrintFromTopToBottom(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"23-二叉树的后续遍历序列\"><a href=\"#23-二叉树的后续遍历序列\" class=\"headerlink\" title=\"23.二叉树的后续遍历序列\"></a>23.二叉树的后续遍历序列</h3><p><strong>题目：</strong>输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。</p>\n<p><strong>思路：</strong>二叉搜索树的特性是所有左子树值都小于中节点，所有右子树的值都大于中节点，递归遍历左子树和右子树的值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">VerifySquenceOfBST</span><span class=\"params\">(self, sequence)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> sequence:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(sequence)==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        i=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> sequence[i]&lt;sequence[<span class=\"number\">-1</span>]:</span><br><span class=\"line\">            i=i+<span class=\"number\">1</span></span><br><span class=\"line\">        k=i</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i,len(sequence)<span class=\"number\">-1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> sequence[j]&lt;sequence[<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">            </span><br><span class=\"line\">        leftsequence=sequence[:k]</span><br><span class=\"line\">        rightsequence=sequence[k:len(sequence)<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        leftans=<span class=\"keyword\">True</span></span><br><span class=\"line\">        rightans=<span class=\"keyword\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(leftsequence)&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            self.VerifySquenceOfBST(leftsequence)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(rightsequence)&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            self.VerifySquenceOfBST(rightsequence)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> leftans <span class=\"keyword\">and</span> rightans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    num=list(map(int,input().split(<span class=\"string\">' '</span>)))</span><br><span class=\"line\">    ans=solution.VerifySquenceOfBST(num)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"24-二叉树中和为某一值的路径\"><a href=\"#24-二叉树中和为某一值的路径\" class=\"headerlink\" title=\"24.二叉树中和为某一值的路径\"></a>24.二叉树中和为某一值的路径</h3><p><strong>题目：</strong>输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前)。</p>\n<p><strong>思路：</strong>利用递归的方法，计算加左子树和右子树之后的值，当参数较多是，可以将结果添加到函数变量之中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回二维列表，内部每个列表表示找到的路径</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindPath</span><span class=\"params\">(self, root, expectNumber)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        path=[]</span><br><span class=\"line\">        self.dfs(root,expectNumber,ans,path)</span><br><span class=\"line\">        ans.sort()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dfs</span><span class=\"params\">(self,root,target,ans,path)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\">        path.append(root.val)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> root.right <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> target==root.val:</span><br><span class=\"line\">            ans.append(path[:])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left:</span><br><span class=\"line\">            self.dfs(root.left,target-root.val,ans,path)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.right:</span><br><span class=\"line\">            self.dfs(root.right,target-root.val,ans,path)</span><br><span class=\"line\"></span><br><span class=\"line\">        path.pop()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1=TreeNode(<span class=\"number\">10</span>)</span><br><span class=\"line\">    A2=TreeNode(<span class=\"number\">8</span>)</span><br><span class=\"line\">    A3=TreeNode(<span class=\"number\">12</span>)</span><br><span class=\"line\">    A4=TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5=TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A6=TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A5.left=A6</span><br><span class=\"line\"></span><br><span class=\"line\">    expectNumber=<span class=\"number\">22</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindPath(A1,expectNumber)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"25-复杂链表的复制\"><a href=\"#25-复杂链表的复制\" class=\"headerlink\" title=\"25.复杂链表的复制\"></a>25.复杂链表的复制</h3><p><strong>题目：</strong>输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）。</p>\n<p><strong>思路：</strong>将大问题转变为小问题，每次都进行复制头部节点，然后进行递归，每次同样处理头部节点。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.label = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.random = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回 RandomListNode</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Clone</span><span class=\"params\">(self, pHead)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"comment\"># 复制头部节点</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pHead <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        newHead=RandomListNode(pHead.label)</span><br><span class=\"line\">        newHead.next=pHead.next</span><br><span class=\"line\">        newHead.random=pHead.random</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 递归其他节点</span></span><br><span class=\"line\">        newHead.next=self.Clone(pHead.next)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> newHead</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1=RandomListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A2=RandomListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A3=RandomListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A4=RandomListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A5=RandomListNode(<span class=\"number\">6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.next=A2</span><br><span class=\"line\">    A1.random=A3</span><br><span class=\"line\"></span><br><span class=\"line\">    A2.next=A3</span><br><span class=\"line\">    A2.random=A4</span><br><span class=\"line\"></span><br><span class=\"line\">    A3.next=A4</span><br><span class=\"line\">    A3.random=A5</span><br><span class=\"line\"></span><br><span class=\"line\">    A4.next=A5</span><br><span class=\"line\">    A4.random=A3</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Clone(A1)</span><br></pre></td></tr></table></figure>\n<h3 id=\"26-二叉搜索树与双向列表\"><a href=\"#26-二叉搜索树与双向列表\" class=\"headerlink\" title=\"26.二叉搜索树与双向列表\"></a>26.二叉搜索树与双向列表</h3><p><strong>题目：</strong>输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。</p>\n<p><strong>思路：</strong>递归将根结点和左子树的最右节点和右子树的最左节点进行连接起来。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Convert</span><span class=\"params\">(self, pRootOfTree)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRootOfTree <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pRootOfTree</span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRootOfTree.left <span class=\"keyword\">is</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> pRootOfTree.right <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pRootOfTree</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#处理左子树</span></span><br><span class=\"line\">        self.Convert(pRootOfTree.left)</span><br><span class=\"line\">        left=pRootOfTree.left</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> left:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> left.right:</span><br><span class=\"line\">                left=left.right</span><br><span class=\"line\">            pRootOfTree.left,left.right=left,pRootOfTree</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">#处理右子树</span></span><br><span class=\"line\">        self.Convert(pRootOfTree.right)</span><br><span class=\"line\">        right=pRootOfTree.right</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> right:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> right.left:</span><br><span class=\"line\">                right=right.left</span><br><span class=\"line\">            pRootOfTree.right,right.left=right,pRootOfTree</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pRootOfTree.left:</span><br><span class=\"line\">            pRootOfTree=pRootOfTree.left</span><br><span class=\"line\">        <span class=\"keyword\">return</span> pRootOfTree</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">15</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">8</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">19</span>)</span><br><span class=\"line\">    A8 = TreeNode(<span class=\"number\">24</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\">    A7.right=A8</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    solution.Convert(A1)</span><br></pre></td></tr></table></figure>\n<h3 id=\"27-字符串的排列\"><a href=\"#27-字符串的排列\" class=\"headerlink\" title=\"27.字符串的排列\"></a>27.字符串的排列</h3><p><strong>题目：</strong>输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。</p>\n<p><strong>输入：</strong>输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。</p>\n<p><strong>思路：</strong>通过将第k位的字符提取到最前面，然后进行和后面的每个字符进行交换，得到所有结果集。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Permutation</span><span class=\"params\">(self, ss)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> ss:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        res=[]</span><br><span class=\"line\">        self.helper(ss,res,<span class=\"string\">''</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sorted(list(set(res)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">helper</span><span class=\"params\">(self,ss,res,path)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> ss:</span><br><span class=\"line\">            res.append(path)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(ss)):</span><br><span class=\"line\">                self.helper(ss[:i]+ss[i+<span class=\"number\">1</span>:],res,path+ss[i])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    str=<span class=\"string\">'abbcDeefg'</span></span><br><span class=\"line\">    str1=<span class=\"string\">'abbc'</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Permutation(str1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"28-数组中出现次数超过一般的数字\"><a href=\"#28-数组中出现次数超过一般的数字\" class=\"headerlink\" title=\"28.数组中出现次数超过一般的数字\"></a>28.数组中出现次数超过一般的数字</h3><p><strong>题目：</strong>数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0</p>\n<p><strong>题解：</strong>利用list列表来存放每个数出现的次数ans[numbers[i]]=ans[numbers[i]]+1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">MoreThanHalfNum_Solution</span><span class=\"params\">(self, numbers)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        numlen=len(numbers)</span><br><span class=\"line\">        halflen=numlen//<span class=\"number\">2</span></span><br><span class=\"line\">        maxans=<span class=\"number\">0</span></span><br><span class=\"line\">        ans=[<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,<span class=\"number\">1000</span>)]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            ans[numbers[i]]=ans[numbers[i]]+<span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> ans[numbers[i]]&gt;maxans:</span><br><span class=\"line\">                maxans=numbers[i]</span><br><span class=\"line\">        ans.sort()</span><br><span class=\"line\">        ans.reverse()</span><br><span class=\"line\">        res=ans[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> res&gt;halflen:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> maxans</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    num=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.MoreThanHalfNum_Solution(num)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"29-最小的K个数\"><a href=\"#29-最小的K个数\" class=\"headerlink\" title=\"29.最小的K个数\"></a>29.最小的K个数</h3><p><strong>题目：</strong>输入n个整数，找出其中最小的K个数，例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetLeastNumbers_Solution</span><span class=\"params\">(self, tinput, k)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> k&gt;len(tinput):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        tinput.sort()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tinput[:k]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    num=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    k=int(input())</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.GetLeastNumbers_Solution(num,k)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"30-连续子数组的最大和\"><a href=\"#30-连续子数组的最大和\" class=\"headerlink\" title=\"30.连续子数组的最大和\"></a>30.连续子数组的最大和</h3><p><strong>题目：</strong>HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。你会不会被他忽悠住？(子向量的长度至少是1)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindGreatestSumOfSubArray</span><span class=\"params\">(self, array)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        maxsum,tempsum=array[<span class=\"number\">0</span>],array[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(array)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> tempsum&lt;<span class=\"number\">0</span>:</span><br><span class=\"line\">                tempsum=array[i]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                tempsum = tempsum + array[i]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> tempsum&gt;maxsum:</span><br><span class=\"line\">                maxsum=tempsum</span><br><span class=\"line\">        <span class=\"keyword\">return</span> maxsum</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    array=list(map(int,input().split(<span class=\"string\">','</span>)))</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindGreatestSumOfSubArray(array)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"31-整数中1出现的次数\"><a href=\"#31-整数中1出现的次数\" class=\"headerlink\" title=\"31.整数中1出现的次数\"></a>31.整数中1出现的次数</h3><p><strong>题目：</strong>求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。</p>\n<p><strong>思路：</strong>对每个数字的每位进行分解，含有1则结果加1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">NumberOf1Between1AndN_Solution</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,n+<span class=\"number\">1</span>):</span><br><span class=\"line\">            tempans=<span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> i!=<span class=\"number\">0</span>:</span><br><span class=\"line\">                eachnum=i%<span class=\"number\">10</span></span><br><span class=\"line\">                i=i//<span class=\"number\">10</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> eachnum==<span class=\"number\">1</span>:</span><br><span class=\"line\">                    tempans=tempans+<span class=\"number\">1</span></span><br><span class=\"line\">            ans=ans+tempans</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    n=<span class=\"number\">130</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.NumberOf1Between1AndN_Solution(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"32-把数组排成最小的数\"><a href=\"#32-把数组排成最小的数\" class=\"headerlink\" title=\"32.把数组排成最小的数\"></a>32.把数组排成最小的数</h3><p><strong>题目：</strong>输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。</p>\n<p><strong>思路：</strong>将数组转换成字符串之后，进行两两比较字符串的大小，比如3,32的大小由332和323确定，即3+32和32+3确定。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">PrintMinNumber</span><span class=\"params\">(self, numbers)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> numbers:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">\"\"</span></span><br><span class=\"line\">        num = map(str, numbers)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i,len(numbers)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> int(str(numbers[i])+str(numbers[j]))&gt;int(str(numbers[j])+str(numbers[i])):</span><br><span class=\"line\">                    numbers[i],numbers[j]=numbers[j],numbers[i]</span><br><span class=\"line\">        ans=<span class=\"string\">''</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            ans=ans+str(numbers[i])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    numbers=[<span class=\"number\">3</span>,<span class=\"number\">32</span>,<span class=\"number\">321</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.PrintMinNumber(numbers)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"33-丑数\"><a href=\"#33-丑数\" class=\"headerlink\" title=\"33.丑数\"></a>33.丑数</h3><p><strong>题目：</strong>把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。</p>\n<p><strong>思路：</strong>每一个丑数必然是由之前的某个丑数与2，3或5的乘积得到的，这样下一个丑数就用之前的丑数分别乘以2，3，5，找出这三这种最小的并且大于当前最大丑数的值，即为下一个要求的丑数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetUglyNumber_Solution</span><span class=\"params\">(self, index)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (index &lt;= <span class=\"number\">0</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        uglyList = [<span class=\"number\">1</span>]</span><br><span class=\"line\">        indexTwo = <span class=\"number\">0</span></span><br><span class=\"line\">        indexThree = <span class=\"number\">0</span></span><br><span class=\"line\">        indexFive = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(index<span class=\"number\">-1</span>):</span><br><span class=\"line\">            newUgly = min(uglyList[indexTwo]*<span class=\"number\">2</span>, uglyList[indexThree]*<span class=\"number\">3</span>, uglyList[indexFive]*<span class=\"number\">5</span>)</span><br><span class=\"line\">            uglyList.append(newUgly)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (newUgly % <span class=\"number\">2</span> == <span class=\"number\">0</span>):</span><br><span class=\"line\">                indexTwo += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (newUgly % <span class=\"number\">3</span> == <span class=\"number\">0</span>):</span><br><span class=\"line\">                indexThree += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (newUgly % <span class=\"number\">5</span> == <span class=\"number\">0</span>):</span><br><span class=\"line\">                indexFive += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> uglyList[<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    index=<span class=\"number\">200</span></span><br><span class=\"line\">    ans=solution.GetUglyNumber_Solution(index)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"34-第一个只出现一次的字符\"><a href=\"#34-第一个只出现一次的字符\" class=\"headerlink\" title=\"34.第一个只出现一次的字符\"></a>34.第一个只出现一次的字符</h3><p><strong>题目：</strong>在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1。</p>\n<p><strong>思路：</strong>找出所有出现一次的字符，然后进行遍历找到第一次出现字符的位置。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FirstNotRepeatingChar</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> s:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\">        sset=set(s)</span><br><span class=\"line\">        dict=&#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> sset:</span><br><span class=\"line\">            dict[c]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">            dict[s[i]]=dict[s[i]]+<span class=\"number\">1</span></span><br><span class=\"line\">        onetime=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> dict:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> dict[c]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                onetime.append(c)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> onetime <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            index=<span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> s[i] <span class=\"keyword\">in</span> onetime:</span><br><span class=\"line\">                    index=i</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> index</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=<span class=\"string\">'abbddebbac'</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FirstNotRepeatingChar(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"35-数组中的逆序对\"><a href=\"#35-数组中的逆序对\" class=\"headerlink\" title=\"35.数组中的逆序对\"></a>35.数组中的逆序对</h3><p><strong>题目描述：</strong>在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007。</p>\n<p><strong>输入描述：</strong>题目保证输入的数组中没有的相同的数字。</p>\n<p><strong>数据范围：</strong><br>   对于%50的数据,size&lt;=10^4<br>   对于%75的数据,size&lt;=10^5<br>   对于%100的数据,size&lt;=2*10^5</p>\n<blockquote>\n<p>示例1</p>\n<p>输入 1,2,3,4,5,6,7,0</p>\n<p>输出 7</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">InversePairs</span><span class=\"params\">(self, data)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">global</span> count</span><br><span class=\"line\">        count = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">A</span><span class=\"params\">(array)</span>:</span></span><br><span class=\"line\">            <span class=\"keyword\">global</span> count</span><br><span class=\"line\">            <span class=\"keyword\">if</span> len(array) &lt;= <span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> array</span><br><span class=\"line\">            k = int(len(array) / <span class=\"number\">2</span>)</span><br><span class=\"line\">            left = A(array[:k])</span><br><span class=\"line\">            right = A(array[k:])</span><br><span class=\"line\">            l = <span class=\"number\">0</span></span><br><span class=\"line\">            r = <span class=\"number\">0</span></span><br><span class=\"line\">            result = []</span><br><span class=\"line\">            <span class=\"keyword\">while</span> l &lt; len(left) <span class=\"keyword\">and</span> r &lt; len(right):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> left[l] &lt; right[r]:</span><br><span class=\"line\">                    result.append(left[l])</span><br><span class=\"line\">                    l += <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    result.append(right[r])</span><br><span class=\"line\">                    r += <span class=\"number\">1</span></span><br><span class=\"line\">                    count += len(left) - l</span><br><span class=\"line\">            result += left[l:]</span><br><span class=\"line\">            result += right[r:]</span><br><span class=\"line\">            <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\">        A(data)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> count % <span class=\"number\">1000000007</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    data=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.InversePairs(data)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"36-两个链表的第一个公共节点\"><a href=\"#36-两个链表的第一个公共节点\" class=\"headerlink\" title=\"36.两个链表的第一个公共节点\"></a>36.两个链表的第一个公共节点</h3><p><strong>题目：</strong>输入两个链表，找出它们的第一个公共结点。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindFirstCommonNode</span><span class=\"params\">(self, pHead1, pHead2)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        list1 = []</span><br><span class=\"line\">        list2 = []</span><br><span class=\"line\">        node1 = pHead1</span><br><span class=\"line\">        node2 = pHead2</span><br><span class=\"line\">        <span class=\"keyword\">while</span> node1:</span><br><span class=\"line\">            list1.append(node1.val)</span><br><span class=\"line\">            node1 = node1.next</span><br><span class=\"line\">        <span class=\"keyword\">while</span> node2:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> node2.val <span class=\"keyword\">in</span> list1:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> node2</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                node2 = node2.next</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A1.next=A2</span><br><span class=\"line\">    A2.next=A3</span><br><span class=\"line\"></span><br><span class=\"line\">    B4 = ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    B5 = ListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    B4.next=B5</span><br><span class=\"line\"></span><br><span class=\"line\">    C6=ListNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    C7=ListNode(<span class=\"number\">7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A3.next=C6</span><br><span class=\"line\">    B5.next=C6</span><br><span class=\"line\">    C6.next=C7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindFirstCommonNode(A1,B4)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"37-数字在排序数组中出现的次数\"><a href=\"#37-数字在排序数组中出现的次数\" class=\"headerlink\" title=\"37.数字在排序数组中出现的次数\"></a>37.数字在排序数组中出现的次数</h3><p><strong>题目：</strong>统计一个数字在排序数组中出现的次数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetNumberOfK</span><span class=\"params\">(self, data, k)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(data)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> data[i]==k:</span><br><span class=\"line\">                ans=ans+<span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> data[i]&gt;k:</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    data=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">    k=<span class=\"number\">3</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.GetNumberOfK(data,k)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"38-二叉树的深度\"><a href=\"#38-二叉树的深度\" class=\"headerlink\" title=\"38.二叉树的深度\"></a>38.二叉树的深度</h3><p><strong>题目：</strong>输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">TreeDepth</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRoot <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        left=self.TreeDepth(pRoot.left)</span><br><span class=\"line\">        right=self.TreeDepth(pRoot.right)</span><br><span class=\"line\">        print(left,right)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> max(left,right)+<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A4.left=A6</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.TreeDepth(A1)</span><br><span class=\"line\">    print(<span class=\"string\">'ans='</span>,ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"39-平衡二叉树\"><a href=\"#39-平衡二叉树\" class=\"headerlink\" title=\"39.平衡二叉树\"></a>39.平衡二叉树</h3><p><strong>题目：</strong>输入一棵二叉树，判断该二叉树是否是平衡二叉树。</p>\n<p><strong>题解：</strong>平衡二叉树是左右子数的距离不能大于1，因此递归左右子树，判断子树距离是否大于1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">IsBalanced_Solution</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pRoot <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> abs(self.TreeDepth(pRoot.left)-self.TreeDepth(pRoot.right))&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.IsBalanced_Solution(pRoot.left) <span class=\"keyword\">and</span> self.IsBalanced_Solution(pRoot.right)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">TreeDepth</span><span class=\"params\">(self,root)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        left=self.TreeDepth(root.left)</span><br><span class=\"line\">        right=self.TreeDepth(root.right)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> max(left+<span class=\"number\">1</span>,right+<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    <span class=\"comment\">#A4.left=A6</span></span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.IsBalanced_Solution(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"40-数组中只出现一次的数字\"><a href=\"#40-数组中只出现一次的数字\" class=\"headerlink\" title=\"40.数组中只出现一次的数字\"></a>40.数组中只出现一次的数字</h3><p><strong>题目：</strong>一个整型数组里除了两个数字之外，其他的数字都出现了偶数次。请写程序找出这两个只出现一次的数字。</p>\n<p><strong>题解：</strong>将数组中数转到set之中，然后利用dict存储每个数字出现的次数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回[a,b] 其中ab是出现一次的两个数字</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindNumsAppearOnce</span><span class=\"params\">(self, array)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        arrayset=set(array)</span><br><span class=\"line\">        dict=&#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> arrayset:</span><br><span class=\"line\">            dict[num]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(array)):</span><br><span class=\"line\">            dict[array[i]]=dict[array[i]]+<span class=\"number\">1</span></span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> arrayset:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> dict[num]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                ans.append(num)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    array=[<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">7</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindNumsAppearOnce(array)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"41-和为S的连续正整数序列\"><a href=\"#41-和为S的连续正整数序列\" class=\"headerlink\" title=\"41.和为S的连续正整数序列\"></a>41.和为S的连续正整数序列</h3><p><strong>题目：</strong>小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck!</p>\n<p><strong>输出描述：</strong>输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序。</p>\n<p><strong>思路：</strong>首项加尾项*2等于和，那么只要遍历项的开始和长度即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindContinuousSequence</span><span class=\"params\">(self, tsum)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,tsum//<span class=\"number\">2</span>+<span class=\"number\">1</span>):</span><br><span class=\"line\">            oneans=[]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,tsum):</span><br><span class=\"line\">                tempsum=((i+i+k<span class=\"number\">-1</span>)*k)//<span class=\"number\">2</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> tempsum==tsum:</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i,i+k):</span><br><span class=\"line\">                        oneans.append(j)</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> oneans !=[]:</span><br><span class=\"line\">                ans.append(oneans)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    tsum=<span class=\"number\">15</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindContinuousSequence(tsum)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"42-和为S的两个数字\"><a href=\"#42-和为S的两个数字\" class=\"headerlink\" title=\"42.和为S的两个数字\"></a>42.和为S的两个数字</h3><p><strong>题目：</strong>输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。</p>\n<p><strong>输出描述：</strong>对应每个测试案例，输出两个数，小的先输出。</p>\n<p><strong>思路：</strong>利用i和j从后面进行扫描结果，选取最小的乘积放入到结果集之中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FindNumbersWithSum</span><span class=\"params\">(self, array, tsum)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans=[]</span><br><span class=\"line\">        i,j,minres=<span class=\"number\">0</span>,len(array)<span class=\"number\">-1</span>,<span class=\"number\">1000000</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(array)<span class=\"number\">-1</span>):</span><br><span class=\"line\">            j=len(array)<span class=\"number\">-1</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">                tempsum = array[i] + array[j]</span><br><span class=\"line\">                <span class=\"keyword\">if</span> tempsum == tsum:</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> array[i]*array[j]&lt;minres:</span><br><span class=\"line\">                        ans=[]</span><br><span class=\"line\">                        ans.append(array[i])</span><br><span class=\"line\">                        ans.append(array[j])</span><br><span class=\"line\">                        minres=array[i]*array[j]</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    j = j - <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> tempsum&lt;tsum:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> j&lt;=i:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    array=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>,<span class=\"number\">7</span>,<span class=\"number\">11</span>,<span class=\"number\">15</span>]</span><br><span class=\"line\">    tsum=<span class=\"number\">15</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.FindNumbersWithSum(array,tsum)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"43-左旋字符子串\"><a href=\"#43-左旋字符子串\" class=\"headerlink\" title=\"43.左旋字符子串\"></a>43.左旋字符子串</h3><p><strong>题目：</strong>汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">LeftRotateString</span><span class=\"params\">(self, s, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> s==<span class=\"string\">''</span> <span class=\"keyword\">and</span> n==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">''</span></span><br><span class=\"line\">        ans=<span class=\"string\">''</span></span><br><span class=\"line\">        ans=s[n:]+s[<span class=\"number\">0</span>:n]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=<span class=\"string\">'abcdefg'</span></span><br><span class=\"line\">    n=<span class=\"number\">2</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.LeftRotateString(s,n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"44-反转单词顺序\"><a href=\"#44-反转单词顺序\" class=\"headerlink\" title=\"44.反转单词顺序\"></a>44.反转单词顺序</h3><p><strong>题目：</strong>牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ReverseSentence</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        ans,word=[],<span class=\"string\">''</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">            word = word + s[i]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> s[i]==<span class=\"string\">' '</span>:</span><br><span class=\"line\">                ans.append(word)</span><br><span class=\"line\">                word=<span class=\"string\">''</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> i==len(s)<span class=\"number\">-1</span>:</span><br><span class=\"line\">                word=word+<span class=\"string\">' '</span></span><br><span class=\"line\">                ans.append(word)</span><br><span class=\"line\">        ans.reverse()</span><br><span class=\"line\">        res=<span class=\"string\">''</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> ans:</span><br><span class=\"line\">            res=res+c</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res[:len(res)<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    s=<span class=\"string\">'I am a student.'</span></span><br><span class=\"line\">    ans=solution.ReverseSentence(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"45-扑克牌顺序\"><a href=\"#45-扑克牌顺序\" class=\"headerlink\" title=\"45.扑克牌顺序\"></a>45.扑克牌顺序</h3><p><strong>题目：</strong>LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\\小王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">IsContinuous</span><span class=\"params\">(self, numbers)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> numbers==[]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        numbers.sort()</span><br><span class=\"line\">        zero=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> numbers[i]==<span class=\"number\">0</span>:</span><br><span class=\"line\">                zero=zero+<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(zero+<span class=\"number\">1</span>,len(numbers)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> numbers[i]==numbers[i<span class=\"number\">-1</span>]:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> numbers[i]-numbers[i<span class=\"number\">-1</span>]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                diff=numbers[i]-numbers[i<span class=\"number\">-1</span>]<span class=\"number\">-1</span></span><br><span class=\"line\">                zero=zero-diff</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> zero&lt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    numbers=[<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.IsContinuous(numbers)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"46-孩子们的圈圈-圈圈中最后剩下的数\"><a href=\"#46-孩子们的圈圈-圈圈中最后剩下的数\" class=\"headerlink\" title=\"46.孩子们的圈圈(圈圈中最后剩下的数)\"></a>46.孩子们的圈圈(圈圈中最后剩下的数)</h3><p><strong>题目：</strong>每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)。</p>\n<p><strong>思路：</strong>约瑟夫环问题。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 题目</span></span><br><span class=\"line\"><span class=\"comment\"># 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。</span></span><br><span class=\"line\"><span class=\"comment\"># 其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。</span></span><br><span class=\"line\"><span class=\"comment\"># 每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,</span></span><br><span class=\"line\"><span class=\"comment\"># 从他的下一个小朋友开始,继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,</span></span><br><span class=\"line\"><span class=\"comment\"># 并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 思路</span></span><br><span class=\"line\"><span class=\"comment\"># 约瑟夫环问题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">LastRemaining_Solution</span><span class=\"params\">(self, n, m)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n&lt;<span class=\"number\">1</span> <span class=\"keyword\">or</span> m&lt;<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\">        last=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>,n+<span class=\"number\">1</span>):</span><br><span class=\"line\">            last=(last+m)%i</span><br><span class=\"line\">        <span class=\"keyword\">return</span> last</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    n,m=<span class=\"number\">8</span>,<span class=\"number\">4</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.LastRemaining_Solution(n,m)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"47-求1-2-3-…-n\"><a href=\"#47-求1-2-3-…-n\" class=\"headerlink\" title=\"47.求1+2+3+…+n\"></a>47.求1+2+3+…+n</h3><p><strong>题目：</strong>求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。</p>\n<p><strong>思路：</strong>利用递归当作计算结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Sum_Solution</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> n==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.Sum_Solution(n<span class=\"number\">-1</span>)+n</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    n=<span class=\"number\">6</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Sum_Solution(n)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"48-不用加减乘除做加法\"><a href=\"#48-不用加减乘除做加法\" class=\"headerlink\" title=\"48.不用加减乘除做加法\"></a>48.不用加减乘除做加法</h3><p><strong>题目：</strong>写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。</p>\n<p><strong>思路：</strong>二进制异或进位。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Add</span><span class=\"params\">(self, num1, num2)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> num2!=<span class=\"number\">0</span>:</span><br><span class=\"line\">            sum=num1^num2</span><br><span class=\"line\">            carry=(num1&amp;num2)&lt;&lt;<span class=\"number\">1</span></span><br><span class=\"line\">            num1=sum</span><br><span class=\"line\">            num2=carry</span><br><span class=\"line\">        <span class=\"keyword\">return</span> num1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    num1,num2=<span class=\"number\">10</span>,<span class=\"number\">500000</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.Add(num1,num2)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"49-把字符串转换成整数\"><a href=\"#49-把字符串转换成整数\" class=\"headerlink\" title=\"49.把字符串转换成整数\"></a>49.把字符串转换成整数</h3><p><strong>题目：</strong>将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。</p>\n<p><strong>输入描述：</strong>输入一个字符串,包括数字字母符号,可以为空输出描述:如果是合法的数值表达则返回该数字，否则返回0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">示例</span><br><span class=\"line\">+<span class=\"number\">2147483647</span></span><br><span class=\"line\">    <span class=\"number\">1</span>a33</span><br><span class=\"line\"><span class=\"number\">2147483647</span></span><br><span class=\"line\">    <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">StrToInt</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(s) == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> s[<span class=\"number\">0</span>] &gt; <span class=\"string\">'9'</span> <span class=\"keyword\">or</span> s[<span class=\"number\">0</span>] &lt; <span class=\"string\">'0'</span>:</span><br><span class=\"line\">                a = <span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                a = int(s[<span class=\"number\">0</span>]) * <span class=\"number\">10</span> ** (len(s) - <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> len(s) &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, len(s)):</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> s[i] &gt;= <span class=\"string\">'0'</span> <span class=\"keyword\">and</span> s[i] &lt;= <span class=\"string\">'9'</span>:</span><br><span class=\"line\">                        a = a + int(s[i]) * <span class=\"number\">10</span> ** (len(s) - <span class=\"number\">1</span> - i)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> s[<span class=\"number\">0</span>] == <span class=\"string\">'+'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> a</span><br><span class=\"line\">        <span class=\"keyword\">if</span> s[<span class=\"number\">0</span>] == <span class=\"string\">'-'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> -a</span><br><span class=\"line\">        <span class=\"keyword\">return</span> a</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    s=<span class=\"string\">'115'</span></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.StrToInt(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"50-数组中重复的数字\"><a href=\"#50-数组中重复的数字\" class=\"headerlink\" title=\"50.数组中重复的数字\"></a>50.数组中重复的数字</h3><p><strong>题目：</strong>在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。</p>\n<p><strong>思路：</strong>利用dict计算重复数字。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 这里要特别注意~找到任意重复的一个值并赋值到duplication[0]</span></span><br><span class=\"line\">    <span class=\"comment\"># 函数返回True/False</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">duplicate</span><span class=\"params\">(self, numbers, duplication)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        numset=set(numbers)</span><br><span class=\"line\">        dict=&#123;&#125;</span><br><span class=\"line\">        duplication.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> val <span class=\"keyword\">in</span> numbers:</span><br><span class=\"line\">            dict[val]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(numbers)):</span><br><span class=\"line\">            dict[numbers[i]]=dict[numbers[i]]+<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> val <span class=\"keyword\">in</span> numset:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> dict[val]&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">                duplication[<span class=\"number\">0</span>]=val</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    numbers=[<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>,<span class=\"number\">4</span>]</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    duplication=[]</span><br><span class=\"line\">    ans=solution.duplicate(numbers,duplication)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"51-构建乘积数组\"><a href=\"#51-构建乘积数组\" class=\"headerlink\" title=\"51.构建乘积数组\"></a>51.构建乘积数组</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 题目</span></span><br><span class=\"line\"><span class=\"comment\"># 给定一个数组A[0,1,...,n-1],请构建一个数组B[0,1,...,n-1],</span></span><br><span class=\"line\"><span class=\"comment\"># 其中B中的元素B[i]=A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]。不能使用除法。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 思路</span></span><br><span class=\"line\"><span class=\"comment\"># 审题仔细 没有A[i]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">multiply</span><span class=\"params\">(self, A)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        B=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(A)):</span><br><span class=\"line\">            temp=<span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(A)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> j==i:</span><br><span class=\"line\">                    <span class=\"keyword\">continue</span></span><br><span class=\"line\">                temp=temp*A[j]</span><br><span class=\"line\">            B.append(temp)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> B</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    A=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\">    ans=solution.multiply(A)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"52-正则表达式匹配\"><a href=\"#52-正则表达式匹配\" class=\"headerlink\" title=\"52.正则表达式匹配\"></a>52.正则表达式匹配</h3><p><strong>题目：</strong>请实现一个函数用来匹配包括’.’和’*‘的正则表达式。模式中的字符’.’表示任意一个字符，而’*‘表示它前面的字符可以出现任意次（包含0次）。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”ab*ac*a”匹配，但是与”aa.a”和”ab*a”均不匹配。</p>\n<p><strong>思路：</strong></p>\n<blockquote>\n<p>当模式中的第二个字符不是<code>*</code>时： </p>\n<ul>\n<li>如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 </li>\n<li>如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。</li>\n</ul>\n<p>当模式中的第二个字符是<code>*</code>时：</p>\n<ul>\n<li>如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。</li>\n<li>如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式。<ul>\n<li>模式后移2字符，相当于<code>x*</code>被忽略。即模式串中*与他前面的字符和字符串匹配0次。 </li>\n<li>字符串后移1字符，模式后移2字符。即模式串中*与他前面的字符和字符串匹配1次。</li>\n<li>字符串后移1字符，模式不变，即继续匹配字符下一位，因为<code>*</code>可以匹配多位。即模式串中*与他前面的字符和字符串匹配多次。</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># s, pattern都是字符串</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">match</span><span class=\"params\">(self, s, pattern)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> s == pattern:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> pattern:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(pattern) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> pattern[<span class=\"number\">1</span>] == <span class=\"string\">'*'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (s <span class=\"keyword\">and</span> s[<span class=\"number\">0</span>] == pattern[<span class=\"number\">0</span>]) <span class=\"keyword\">or</span> (s <span class=\"keyword\">and</span> pattern[<span class=\"number\">0</span>] == <span class=\"string\">'.'</span>):</span><br><span class=\"line\">                <span class=\"keyword\">return</span> self.match(s, pattern[<span class=\"number\">2</span>:]) \\</span><br><span class=\"line\">                       <span class=\"keyword\">or</span> self.match(s[<span class=\"number\">1</span>:], pattern) \\</span><br><span class=\"line\">                       <span class=\"keyword\">or</span> self.match(s[<span class=\"number\">1</span>:], pattern[<span class=\"number\">2</span>:])</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> self.match(s, pattern[<span class=\"number\">2</span>:])</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> s <span class=\"keyword\">and</span> (s[<span class=\"number\">0</span>] == pattern[<span class=\"number\">0</span>] <span class=\"keyword\">or</span> pattern[<span class=\"number\">0</span>] == <span class=\"string\">'.'</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.match(s[<span class=\"number\">1</span>:], pattern[<span class=\"number\">1</span>:])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    s=<span class=\"string\">'aaa'</span></span><br><span class=\"line\">    pattern=<span class=\"string\">'a*a.a'</span></span><br><span class=\"line\">    ans=solution.match(s,pattern)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"53-表示数值的字符串\"><a href=\"#53-表示数值的字符串\" class=\"headerlink\" title=\"53.表示数值的字符串\"></a>53.表示数值的字符串</h3><p><strong>题目：</strong>请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># s字符串</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isNumeric</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"comment\"># 标记符号、小数点、e是否出现过</span></span><br><span class=\"line\">        sign,decimal,hasE=<span class=\"keyword\">False</span>,<span class=\"keyword\">False</span>,<span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> s[i]==<span class=\"string\">'e'</span> <span class=\"keyword\">or</span> s[i]==<span class=\"string\">'E'</span>:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i==len(s)<span class=\"number\">-1</span>:<span class=\"comment\"># e后面一定要接数字</span></span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> hasE==<span class=\"keyword\">True</span>:<span class=\"comment\"># 不能出现两次e</span></span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                hasE=<span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> s[i]==<span class=\"string\">'+'</span> <span class=\"keyword\">or</span> s[i]==<span class=\"string\">'-'</span>:</span><br><span class=\"line\">                <span class=\"comment\">#第二次出现+或-一定要在e之后</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> sign <span class=\"keyword\">and</span> s[i<span class=\"number\">-1</span>]!=<span class=\"string\">'e'</span> <span class=\"keyword\">and</span> s[i<span class=\"number\">-1</span>]!=<span class=\"string\">'E'</span>:</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                <span class=\"comment\"># 第一次出现+或-，如果不是出现在字符最前面，那么就要出现在e或者E后面</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> sign==<span class=\"keyword\">False</span> <span class=\"keyword\">and</span> i&gt;<span class=\"number\">0</span> <span class=\"keyword\">and</span> s[i<span class=\"number\">-1</span>]!=<span class=\"string\">'e'</span> <span class=\"keyword\">and</span> s[i<span class=\"number\">-1</span>]!=<span class=\"string\">'E'</span>:</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                sign=<span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> s[i]==<span class=\"string\">'.'</span>:</span><br><span class=\"line\">                <span class=\"comment\"># e后面不能出现小数点，小数点不能出现两次</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> decimal <span class=\"keyword\">or</span> hasE:</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">                decimal=<span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> s[i]&gt;<span class=\"string\">'9'</span> <span class=\"keyword\">or</span> s[i]&lt;<span class=\"string\">'0'</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    s=<span class=\"string\">'123e.1416'</span></span><br><span class=\"line\">    ans=solution.isNumeric(s)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"54-字符流中第一个不重复的字符\"><a href=\"#54-字符流中第一个不重复的字符\" class=\"headerlink\" title=\"54.字符流中第一个不重复的字符\"></a>54.字符流中第一个不重复的字符</h3><p><strong>题目：</strong>请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。</p>\n<p><strong>输出描述：</strong>如果当前字符流没有存在出现一次的字符，返回#字符。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回对应char</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.all=&#123;&#125;</span><br><span class=\"line\">        self.ch=[]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">FirstAppearingOnce</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.all <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"string\">'#'</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> self.ch:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.all[c]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> c</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">'#'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Insert</span><span class=\"params\">(self, char)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.ch.append(char)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> char <span class=\"keyword\">in</span> self.all:</span><br><span class=\"line\">            self.all[char]=self.all[char]+<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.all[char]=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'g'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'o'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'o'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'g'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'l'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"string\">'e'</span>)</span><br><span class=\"line\">    ans = solution.FirstAppearingOnce()</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"55-链表中环的入口节点\"><a href=\"#55-链表中环的入口节点\" class=\"headerlink\" title=\"55.链表中环的入口节点\"></a>55.链表中环的入口节点</h3><p><strong>题目：</strong>给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。</p>\n<p><strong>思路：</strong>把链表中节点值放到dict数组中，并记录出现的次数，如果出现次数超过一次，则为环的入口节点。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">EntryNodeOfLoop</span><span class=\"params\">(self, pHead)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pHead <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        num,dict,flag=[],&#123;&#125;,<span class=\"keyword\">True</span></span><br><span class=\"line\">        tempans=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead <span class=\"keyword\">and</span> flag==<span class=\"keyword\">True</span>:</span><br><span class=\"line\">            num.append(pHead.val)</span><br><span class=\"line\">            numset=set(num)</span><br><span class=\"line\">            <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> numset:</span><br><span class=\"line\">                dict[c]=<span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">                dict[c]=dict[c]+<span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> dict[c]&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">                    flag=<span class=\"keyword\">False</span></span><br><span class=\"line\">                    tempans=c</span><br><span class=\"line\">            pHead=pHead.next</span><br><span class=\"line\">        <span class=\"keyword\">while</span> pHead:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> pHead.val==tempans:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> pHead</span><br><span class=\"line\">            pHead=pHead.next</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    pHead1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead2 = ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    pHead3 = ListNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    pHead4 = ListNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    pHead5 = ListNode(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    pHead1.next=pHead2</span><br><span class=\"line\">    pHead2.next=pHead3</span><br><span class=\"line\">    pHead3.next=pHead4</span><br><span class=\"line\">    pHead4.next=pHead5</span><br><span class=\"line\">    pHead5.next=pHead1</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.EntryNodeOfLoop(pHead1)</span><br><span class=\"line\">    print(ans.val)</span><br></pre></td></tr></table></figure>\n<h3 id=\"56-删除链表中重复的节点\"><a href=\"#56-删除链表中重复的节点\" class=\"headerlink\" title=\"56.删除链表中重复的节点\"></a>56.删除链表中重复的节点</h3><p><strong>题目：</strong>在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5。</p>\n<p><strong>思路：</strong>记录链表中出现的数字，然后构建新链表。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deleteDuplication</span><span class=\"params\">(self, pHead)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        num=[]</span><br><span class=\"line\">        tempnum1=pHead</span><br><span class=\"line\">        <span class=\"keyword\">while</span> tempnum1:</span><br><span class=\"line\">            num.append(tempnum1.val)</span><br><span class=\"line\">            tempnum1=tempnum1.next</span><br><span class=\"line\">        dict=&#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">            dict[c]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">            dict[c]=dict[c]+<span class=\"number\">1</span></span><br><span class=\"line\">        newnum=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> num:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> dict[c]==<span class=\"number\">1</span>:</span><br><span class=\"line\">                newnum.append(c)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> newnum==[]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        head=ListNode(newnum[<span class=\"number\">0</span>])</span><br><span class=\"line\">        temphead=head</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(newnum)):</span><br><span class=\"line\">            tempnode=ListNode(newnum[i])</span><br><span class=\"line\">            temphead.next=tempnode</span><br><span class=\"line\">            temphead=tempnode</span><br><span class=\"line\">        <span class=\"comment\"># while head:</span></span><br><span class=\"line\">        <span class=\"comment\">#     print(head.val)</span></span><br><span class=\"line\">        <span class=\"comment\">#     head=head.next</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> head</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    pHead1 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead2 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead3 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead4 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead5 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead6 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    pHead7 = ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    pHead1.next=pHead2</span><br><span class=\"line\">    pHead2.next=pHead3</span><br><span class=\"line\">    pHead3.next=pHead4</span><br><span class=\"line\">    pHead4.next=pHead5</span><br><span class=\"line\">    pHead5.next=pHead6</span><br><span class=\"line\">    pHead6.next=pHead7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    ans=solution.deleteDuplication(pHead1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"57-二叉树中的下一个节点\"><a href=\"#57-二叉树中的下一个节点\" class=\"headerlink\" title=\"57. 二叉树中的下一个节点\"></a>57. 二叉树中的下一个节点</h3><p><strong>题目：</strong>给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。</p>\n<p><strong>思路：</strong>分析二叉树的下一个节点，一共有以下情况：1.二叉树为空，则返回空；2.节点右孩子存在，则设置一个指针从该节点的右孩子出发，一直沿着指向左子结点的指针找到的叶子节点即为下一个节点；3.节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点；否则继续向上遍历其父节点的父节点，重复之前的判断，返回结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeLinkNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.next = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetNext</span><span class=\"params\">(self, pNode)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> pNode:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pNode</span><br><span class=\"line\">        <span class=\"keyword\">if</span> pNode.right:</span><br><span class=\"line\">            left1=pNode.right</span><br><span class=\"line\">            <span class=\"keyword\">while</span> left1.left:</span><br><span class=\"line\">                   left1=left1.left</span><br><span class=\"line\">            <span class=\"keyword\">return</span> left1</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pNode.next:</span><br><span class=\"line\">            tmp=pNode.next</span><br><span class=\"line\">            <span class=\"keyword\">if</span> tmp.left==pNode:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> tmp</span><br><span class=\"line\">            pNode=tmp</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br></pre></td></tr></table></figure>\n<h3 id=\"58-对称的二叉树\"><a href=\"#58-对称的二叉树\" class=\"headerlink\" title=\"58.对称的二叉树\"></a>58.对称的二叉树</h3><p><strong>题目：</strong>请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。</p>\n<p><strong>思路：</strong>采用递归的方法来判断两数是否相同。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isSymmetrical</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> pRoot:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        result=self.same(pRoot,pRoot)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">same</span><span class=\"params\">(self,root1,root2)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root1 <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> root2:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root1 <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> root2:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root1 <span class=\"keyword\">and</span> root2:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> root1.val!= root2.val:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">        left=self.same(root1.left,root2.right)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> left:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        right=self.same(root1.right,root2.left)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> right:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.isSymmetrical(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"59-按之字形顺序打印二叉树\"><a href=\"#59-按之字形顺序打印二叉树\" class=\"headerlink\" title=\"59.按之字形顺序打印二叉树\"></a>59.按之字形顺序打印二叉树</h3><p><strong>题目：</strong>请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。</p>\n<p><strong>思路：</strong> 把当前列结果存放到list之中，设置翻转变量，依次从左到右打印和从右到左打印。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Print</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        root=pRoot</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        level=[root]</span><br><span class=\"line\">        result=[]</span><br><span class=\"line\">        righttoleft=<span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> level:</span><br><span class=\"line\">            curvalues=[]</span><br><span class=\"line\">            nextlevel=[]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> level:</span><br><span class=\"line\">                curvalues.append(i.val)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i.left:</span><br><span class=\"line\">                    nextlevel.append(i.left)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i.right:</span><br><span class=\"line\">                    nextlevel.append(i.right)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> righttoleft:</span><br><span class=\"line\">                    curvalues.reverse()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> curvalues:</span><br><span class=\"line\">                    result.append(curvalues)</span><br><span class=\"line\">            level = nextlevel</span><br><span class=\"line\">            righttoleft = <span class=\"keyword\">not</span> righttoleft</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.Print(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"60-把二叉树打印成多行\"><a href=\"#60-把二叉树打印成多行\" class=\"headerlink\" title=\"60.把二叉树打印成多行\"></a>60.把二叉树打印成多行</h3><p><strong>题目：</strong>从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回二维列表[[1,2],[4,5]]</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Print</span><span class=\"params\">(self, pRoot)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        root=pRoot</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        level=[root]</span><br><span class=\"line\">        result=[]</span><br><span class=\"line\">        <span class=\"keyword\">while</span> level:</span><br><span class=\"line\">            curvalues=[]</span><br><span class=\"line\">            nextlevel=[]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> level:</span><br><span class=\"line\">                curvalues.append(i.val)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i.left:</span><br><span class=\"line\">                    nextlevel.append(i.left)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> i.right:</span><br><span class=\"line\">                    nextlevel.append(i.right)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> curvalues:</span><br><span class=\"line\">                    result.append(curvalues)</span><br><span class=\"line\">            level = nextlevel</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.Print(A1)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"61-序列化二叉树\"><a href=\"#61-序列化二叉树\" class=\"headerlink\" title=\"61.序列化二叉树\"></a>61.序列化二叉树</h3><p><strong>题目：</strong>请实现两个函数，分别用来序列化和反序列化二叉树。</p>\n<p><strong>思路：</strong>转变成前序遍历，空元素利用”#”代替，然后进行解序列。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> collections</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Serialize</span><span class=\"params\">(self, root)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        res=[]</span><br><span class=\"line\">        self.pre(root,res)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pre</span><span class=\"params\">(self,root,res)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        res.append(root.val)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.left:</span><br><span class=\"line\">            self.pre(root.left, res)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            res.append(<span class=\"string\">'#'</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> root.right:</span><br><span class=\"line\">            self.pre(root.right,res)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            res.append(<span class=\"string\">'#'</span>)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Deserialize</span><span class=\"params\">(self, s)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> s==<span class=\"string\">''</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        vals=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(s)):</span><br><span class=\"line\">            vals.append(s[i])</span><br><span class=\"line\">        vals=collections.deque(vals)</span><br><span class=\"line\">        ans=self.build(vals)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ans</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">build</span><span class=\"params\">(self,vals)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> vals:</span><br><span class=\"line\">            val = vals.popleft()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> val == <span class=\"string\">'#'</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">            root = TreeNode(int(val))</span><br><span class=\"line\">            root.left = self.build(vals)</span><br><span class=\"line\">            root.right = self.build(vals)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> root</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.build(vals)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># [1, ',', 2, ',', 4, ',', ',', ',', 5, ',', ',', ',', 3, ',', 6, ',', ',', ',', 7, ',', ',']</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.Serialize(A1)</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    root=solution.Deserialize(ans)</span><br><span class=\"line\">    res=solution.Serialize(root)</span><br><span class=\"line\">    print(res)</span><br></pre></td></tr></table></figure>\n<h3 id=\"62-二叉搜索树中的第K个节点\"><a href=\"#62-二叉搜索树中的第K个节点\" class=\"headerlink\" title=\"62.二叉搜索树中的第K个节点\"></a>62.二叉搜索树中的第K个节点</h3><p><strong>题目：</strong>给定一棵二叉搜索树，请找出其中的第k小的结点。例如（5，3，7，2，4，6，8）中，按结点数值大小顺序第三小结点的值为4。</p>\n<p><strong>思路：</strong>中序遍历后，返回第K个节点值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        self.val = x</span><br><span class=\"line\">        self.left = <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.right = <span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回对应节点TreeNode</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">KthNode</span><span class=\"params\">(self, pRoot, k)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        res=[]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> pRoot:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        self.order(pRoot,res)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(res)&lt;k <span class=\"keyword\">or</span> k&lt;=<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> res[k<span class=\"number\">-1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">order</span><span class=\"params\">(self,root,res)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        self.order(root.left,res)</span><br><span class=\"line\">        res.append(root)</span><br><span class=\"line\">        self.order(root.right,res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    A1 = TreeNode(<span class=\"number\">5</span>)</span><br><span class=\"line\">    A2 = TreeNode(<span class=\"number\">3</span>)</span><br><span class=\"line\">    A3 = TreeNode(<span class=\"number\">7</span>)</span><br><span class=\"line\">    A4 = TreeNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">    A5 = TreeNode(<span class=\"number\">4</span>)</span><br><span class=\"line\">    A6 = TreeNode(<span class=\"number\">6</span>)</span><br><span class=\"line\">    A7 = TreeNode(<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    A1.left=A2</span><br><span class=\"line\">    A1.right=A3</span><br><span class=\"line\">    A2.left=A4</span><br><span class=\"line\">    A2.right=A5</span><br><span class=\"line\">    A3.left=A6</span><br><span class=\"line\">    A3.right=A7</span><br><span class=\"line\"></span><br><span class=\"line\">    k=<span class=\"number\">3</span></span><br><span class=\"line\">    solution = Solution()</span><br><span class=\"line\">    ans=solution.KthNode(A1,k)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"63-数据流中的中位数\"><a href=\"#63-数据流中的中位数\" class=\"headerlink\" title=\"63.数据流中的中位数\"></a>63.数据流中的中位数</h3><p><strong>题目：</strong>如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.data=[]</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Insert</span><span class=\"params\">(self, num)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        self.data.append(num)</span><br><span class=\"line\">        self.data.sort()</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">GetMedian</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        length=len(self.data)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> length%<span class=\"number\">2</span>==<span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> (self.data[length//<span class=\"number\">2</span>]+self.data[length//<span class=\"number\">2</span><span class=\"number\">-1</span>])/<span class=\"number\">2.0</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.data[int(length//<span class=\"number\">2</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">5</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">2</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">3</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">4</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br><span class=\"line\">    solution.Insert(<span class=\"number\">1</span>)</span><br><span class=\"line\">    ans = solution.GetMedian()</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"64-滑动窗口的最大值\"><a href=\"#64-滑动窗口的最大值\" class=\"headerlink\" title=\"64.滑动窗口的最大值\"></a>64.滑动窗口的最大值</h3><p><strong>题目：</strong>给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}，{2,3,4,2,6,[2,5,1]}。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maxInWindows</span><span class=\"params\">(self, num, size)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> size==<span class=\"number\">0</span> <span class=\"keyword\">or</span> num==[]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> []</span><br><span class=\"line\">        res=[]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,len(num)-size+<span class=\"number\">1</span>):</span><br><span class=\"line\">            tempnum=[]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i,i+size):</span><br><span class=\"line\">                tempnum.append(num[j])</span><br><span class=\"line\">            res.append(max(tempnum))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    num=[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">6</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">    size=<span class=\"number\">3</span></span><br><span class=\"line\">    ans=solution.maxInWindows(num,size)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"66-矩阵中的路径\"><a href=\"#66-矩阵中的路径\" class=\"headerlink\" title=\"66.矩阵中的路径\"></a>66.矩阵中的路径</h3><p><strong>题目：</strong>请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。</p>\n<p><strong>思路：</strong>当起点第一个字符相同时，开始进行递归搜索，设计搜索函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasPath</span><span class=\"params\">(self, matrix, rows, cols, path)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,rows):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>,cols):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> matrix[i*rows+j]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> self.find_path(list(matrix),rows,cols,path[<span class=\"number\">1</span>:],i,j):</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">find_path</span><span class=\"params\">(self,matrix,rows,cols,path,i,j)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> path:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br><span class=\"line\">        matrix[i*cols+j]=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> j+<span class=\"number\">1</span>&lt;cols <span class=\"keyword\">and</span> matrix[i*cols+j+<span class=\"number\">1</span>]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.find_path(matrix,rows,cols,path[<span class=\"number\">1</span>:],i,j+<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> j<span class=\"number\">-1</span>&gt;=<span class=\"number\">0</span> <span class=\"keyword\">and</span> matrix[i*cols+j<span class=\"number\">-1</span>]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.find_path(matrix, rows, cols, path[<span class=\"number\">1</span>:], i, j - <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> i+<span class=\"number\">1</span>&lt;rows <span class=\"keyword\">and</span> matrix[(i+<span class=\"number\">1</span>)*cols+j]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.find_path(matrix, rows, cols, path[<span class=\"number\">1</span>:], i+<span class=\"number\">1</span>, j)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> i<span class=\"number\">-1</span>&gt;=<span class=\"number\">0</span> <span class=\"keyword\">and</span> matrix[(i<span class=\"number\">-1</span>)*cols+j]==path[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.find_path(matrix, rows, cols, path[<span class=\"number\">1</span>:], i<span class=\"number\">-1</span>, j)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    matrix=<span class=\"string\">'ABCEHJIGSFCSLOPQADEEMNOEADIDEJFMVCEIFGGS'</span></span><br><span class=\"line\">    rows=<span class=\"number\">5</span></span><br><span class=\"line\">    cols=<span class=\"number\">8</span></span><br><span class=\"line\">    path=<span class=\"string\">'SGGFIECVAASABCEHJIGQEMS'</span></span><br><span class=\"line\">    ans=solution.hasPath(matrix,rows,cols,path)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"66-机器人的运动范围\"><a href=\"#66-机器人的运动范围\" class=\"headerlink\" title=\"66.机器人的运动范围\"></a>66.机器人的运动范围</h3><p><strong>题目：</strong>地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？</p>\n<p><strong>思路：</strong>对未走过的路径进行遍历，搜索所有的路径值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.vis = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">movingCount</span><span class=\"params\">(self, threshold, rows, cols)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># write code here</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.moving(threshold, rows, cols, <span class=\"number\">0</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">moving</span><span class=\"params\">(self, threshold, rows, cols, row, col)</span>:</span></span><br><span class=\"line\">        rowans,colans=<span class=\"number\">0</span>,<span class=\"number\">0</span></span><br><span class=\"line\">        rowtemp,coltemp=row,col</span><br><span class=\"line\">        <span class=\"keyword\">while</span> rowtemp&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            rowans=rowans+rowtemp%<span class=\"number\">10</span></span><br><span class=\"line\">            rowtemp=rowtemp//<span class=\"number\">10</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> coltemp&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">            colans=colans+coltemp%<span class=\"number\">10</span></span><br><span class=\"line\">            coltemp=coltemp//<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> rowans+colans&gt;threshold:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> row &gt;= rows <span class=\"keyword\">or</span> col &gt;= cols <span class=\"keyword\">or</span> row &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> col &lt; <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (row, col) <span class=\"keyword\">in</span> self.vis:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">        self.vis[(row, col)] = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span> + self.moving(threshold, rows, cols, row - <span class=\"number\">1</span>, col) +\\</span><br><span class=\"line\">               self.moving(threshold, rows, cols, row + <span class=\"number\">1</span>,col) + \\</span><br><span class=\"line\">               self.moving(threshold, rows,cols, row,col - <span class=\"number\">1</span>) + \\</span><br><span class=\"line\">               self.moving(threshold, rows, cols, row, col + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    solution=Solution()</span><br><span class=\"line\">    threshold=<span class=\"number\">10</span></span><br><span class=\"line\">    rows,cols=<span class=\"number\">1</span>,<span class=\"number\">100</span></span><br><span class=\"line\">    ans=solution.movingCount(threshold,rows,cols)</span><br><span class=\"line\">    print(ans)</span><br></pre></td></tr></table></figure>\n<h3 id=\"67-推广\"><a href=\"#67-推广\" class=\"headerlink\" title=\"67.推广\"></a>67.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"/2018/07/29/《剑指Offer》Python版/推广.png\" alt=\"推广\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/Markdown写作教程/图片03.png","slug":"图片03.png","post":"cjktv5d380007jiz5unucdms0","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/12.png","slug":"12.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png","slug":"Python之Sklearn使用教程图片02.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png","slug":"机器学习之SVM支持向量机（一）图片05.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png","slug":"机器学习之SVM支持向量机（一）图片03.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png","slug":"机器学习之SVM支持向量机（二）图像06.png","post":"cjktv5d460017jiz5gdv74el2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/公式01.png","slug":"公式01.png","post":"cjktv5d4k001pjiz5tiyre64o","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/公式03.png","slug":"公式03.png","post":"cjktv5d4k001pjiz5tiyre64o","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/图片03.png","slug":"图片03.png","post":"cjktv5d4k001pjiz5tiyre64o","modified":0,"renderable":0},{"_id":"source/_posts/网店工商信息图片文字提取/网店工商信息图片文字提取02.png","slug":"网店工商信息图片文字提取02.png","post":"cjktv5d57002ijiz5hufi4ixt","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.3.png","slug":"图片6.3.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程05.png","slug":"Python之Sklearn使用教程05.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之Apriori算法/机器学习之Apriori算法图片01.png","slug":"机器学习之Apriori算法图片01.png","post":"cjktv5d3r000sjiz5hujxudi7","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png","slug":"机器学习之SVM支持向量机（一）图片04.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png","slug":"机器学习之SVM支持向量机（一）图片11.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png","slug":"机器学习之SVM支持向量机（一）图片14.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png","slug":"机器学习之SVM支持向量机（一）图片17.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/公式02.png","slug":"公式02.png","post":"cjktv5d4k001pjiz5tiyre64o","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png","slug":"机器学习之自适应增强Adaboost图片02.png","post":"cjktv5d4m001rjiz5j3eg73t7","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png","slug":"机器学习之自适应增强Adaboost图片03.png","post":"cjktv5d4m001rjiz5j3eg73t7","modified":0,"renderable":0},{"_id":"source/_posts/机器学习知识体系/图片03.png","slug":"图片03.png","post":"cjktv5d4x0025jiz502px4h2s","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片01.png","slug":"深度神经网络之正则化图片01.png","post":"cjktv5d54002fjiz5q8uygzb6","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片7.2.png","slug":"图片7.2.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/15.png","slug":"15.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记02.PNG","slug":"软件推荐印象笔记02.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归02.png","slug":"机器学习之Logistic回归02.png","post":"cjktv5d3x000xjiz561kzqk0s","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png","slug":"机器学习之SVM支持向量机（一）图片02.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png","slug":"机器学习之最大期望算法图片01.png","post":"cjktv5d4e001fjiz5jjkjhk8p","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/图片02.png","slug":"图片02.png","post":"cjktv5d4k001pjiz5tiyre64o","modified":0,"renderable":0},{"_id":"source/_posts/Linux常用命令/推广.png","slug":"推广.png","post":"cjktv5d2x0001jiz5qil4o1sm","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png","slug":"机器学习之分类与回归树CART图片01.png","post":"cjktv5d4c001djiz5boejbr79","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之随机森林/机器学习之随机森林图片01.png","slug":"机器学习之随机森林图片01.png","post":"cjktv5d4o001wjiz5eshooije","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之反向传播算法/推广.png","slug":"推广.png","post":"cjktv5d4u0021jiz5k5sp9ij6","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之决策树-C4-5算法/机器学习之决策树图片01.png","slug":"机器学习之决策树图片01.png","post":"cjktv5d49001bjiz5rdq9nnwm","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之决策树-C4-5算法/机器学习之决策树图片02.png","slug":"机器学习之决策树图片02.png","post":"cjktv5d49001bjiz5rdq9nnwm","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png","slug":"机器学习之梯度提升决策树图片01.png","post":"cjktv5d4h001kjiz5uby8906t","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png","slug":"机器学习之梯度提升决策树图片02.png","post":"cjktv5d4h001kjiz5uby8906t","modified":0,"renderable":0},{"_id":"source/_posts/网店工商信息图片文字提取/网店工商信息图片文字提取01.png","slug":"网店工商信息图片文字提取01.png","post":"cjktv5d57002ijiz5hufi4ixt","modified":0,"renderable":0},{"_id":"source/_posts/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片01.png","slug":"基于google-protobuf的gRPC实现-python版图片01.png","post":"cjktv5d3n000njiz5j4jt0gka","modified":1,"renderable":0},{"_id":"source/_posts/基于google-protobuf的gRPC实现-python版/基于google-protobuf的gRPC实现-python版图片02.png","slug":"基于google-protobuf的gRPC实现-python版图片02.png","post":"cjktv5d3n000njiz5j4jt0gka","modified":1,"renderable":0},{"_id":"source/_posts/基于google-protobuf的gRPC实现-python版/推广.png","slug":"推广.png","post":"cjktv5d3n000njiz5j4jt0gka","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png","slug":"机器学习值K均值算法图片01.png","post":"cjktv5d3z0010jiz5r86vcdsl","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png","slug":"机器学习值K均值算法图片02.png","post":"cjktv5d3z0010jiz5r86vcdsl","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png","slug":"机器学习值K均值算法图片03.png","post":"cjktv5d3z0010jiz5r86vcdsl","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png","slug":"机器学习之自适应增强Adaboost图片01.png","post":"cjktv5d4m001rjiz5j3eg73t7","modified":0,"renderable":0},{"_id":"source/_posts/Markdown写作教程/图片01.png","slug":"图片01.png","post":"cjktv5d380007jiz5unucdms0","modified":0,"renderable":0},{"_id":"source/_posts/Markdown写作教程/图片02.png","slug":"图片02.png","post":"cjktv5d380007jiz5unucdms0","modified":0,"renderable":0},{"_id":"source/_posts/Markdown写作教程/推广.png","slug":"推广.png","post":"cjktv5d380007jiz5unucdms0","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之Logistic回归/推广.png","slug":"推广.png","post":"cjktv5d3x000xjiz561kzqk0s","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归01.png","slug":"机器学习之Logistic回归01.png","post":"cjktv5d3x000xjiz561kzqk0s","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归03.png","slug":"机器学习之Logistic回归03.png","post":"cjktv5d3x000xjiz561kzqk0s","modified":0,"renderable":0},{"_id":"source/_posts/机器学习知识体系/图片01.png","slug":"图片01.png","post":"cjktv5d4x0025jiz502px4h2s","modified":0,"renderable":0},{"_id":"source/_posts/机器学习知识体系/图片02.png","slug":"图片02.png","post":"cjktv5d4x0025jiz502px4h2s","modified":0,"renderable":0},{"_id":"source/_posts/机器学习知识体系/图片04.png","slug":"图片04.png","post":"cjktv5d4x0025jiz502px4h2s","modified":0,"renderable":0},{"_id":"source/_posts/机器学习知识体系/推广.png","slug":"推广.png","post":"cjktv5d4x0025jiz502px4h2s","modified":0,"renderable":0},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/001.png","slug":"001.png","post":"cjktv5d4z0028jiz5cr4m2yg9","modified":0,"renderable":0},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/002.png","slug":"002.png","post":"cjktv5d4z0028jiz5cr4m2yg9","modified":0,"renderable":0},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/003.png","slug":"003.png","post":"cjktv5d4z0028jiz5cr4m2yg9","modified":0,"renderable":0},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/004.png","slug":"004.png","post":"cjktv5d4z0028jiz5cr4m2yg9","modified":0,"renderable":0},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文/005.png","slug":"005.png","post":"cjktv5d4z0028jiz5cr4m2yg9","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png","slug":"机器学习之K近邻算法图片01.png","post":"cjktv5d3t000ujiz5667327i2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png","slug":"机器学习之K近邻算法图片02.png","post":"cjktv5d3t000ujiz5667327i2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png","slug":"机器学习之K近邻算法图片03.png","post":"cjktv5d3t000ujiz5667327i2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png","slug":"机器学习之K近邻算法图片04.png","post":"cjktv5d3t000ujiz5667327i2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png","slug":"机器学习之K近邻算法图片05.png","post":"cjktv5d3t000ujiz5667327i2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png","slug":"机器学习之K近邻算法图片06.png","post":"cjktv5d3t000ujiz5667327i2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像01.png","slug":"机器学习之SVM支持向量机（二）图像01.png","post":"cjktv5d460017jiz5gdv74el2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像02.png","slug":"机器学习之SVM支持向量机（二）图像02.png","post":"cjktv5d460017jiz5gdv74el2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像03.png","slug":"机器学习之SVM支持向量机（二）图像03.png","post":"cjktv5d460017jiz5gdv74el2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png","slug":"机器学习之SVM支持向量机（二）图像05.png","post":"cjktv5d460017jiz5gdv74el2","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png","slug":"机器学习之SVM支持向量机（二）图片推广.png","post":"cjktv5d460017jiz5gdv74el2","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之正则化/推广.png","slug":"推广.png","post":"cjktv5d54002fjiz5q8uygzb6","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片02.png","slug":"深度神经网络之正则化图片02.png","post":"cjktv5d54002fjiz5q8uygzb6","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片03.png","slug":"深度神经网络之正则化图片03.png","post":"cjktv5d54002fjiz5q8uygzb6","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片04.png","slug":"深度神经网络之正则化图片04.png","post":"cjktv5d54002fjiz5q8uygzb6","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之正则化/深度神经网络之正则化图片05.png","slug":"深度神经网络之正则化图片05.png","post":"cjktv5d54002fjiz5q8uygzb6","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/图片01.png","slug":"图片01.png","post":"cjktv5d4k001pjiz5tiyre64o","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/推广.png","slug":"推广.png","post":"cjktv5d4k001pjiz5tiyre64o","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之前向传播算法/推广.png","slug":"推广.png","post":"cjktv5d4r001yjiz5xk6kmmyt","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络01.png","slug":"深度神经网络01.png","post":"cjktv5d4r001yjiz5xk6kmmyt","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络02.png","slug":"深度神经网络02.png","post":"cjktv5d4r001yjiz5xk6kmmyt","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络03.png","slug":"深度神经网络03.png","post":"cjktv5d4r001yjiz5xk6kmmyt","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络04.png","slug":"深度神经网络04.png","post":"cjktv5d4r001yjiz5xk6kmmyt","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络05.png","slug":"深度神经网络05.png","post":"cjktv5d4r001yjiz5xk6kmmyt","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络06.png","slug":"深度神经网络06.png","post":"cjktv5d4r001yjiz5xk6kmmyt","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之前向传播算法/深度神经网络07.png","slug":"深度神经网络07.png","post":"cjktv5d4r001yjiz5xk6kmmyt","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/推广.png","slug":"推广.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片01.png","slug":"深度神经网络之损失函数和激活函数图片01.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片02.png","slug":"深度神经网络之损失函数和激活函数图片02.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片03.png","slug":"深度神经网络之损失函数和激活函数图片03.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片04.png","slug":"深度神经网络之损失函数和激活函数图片04.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片05.png","slug":"深度神经网络之损失函数和激活函数图片05.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片06.png","slug":"深度神经网络之损失函数和激活函数图片06.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片07.png","slug":"深度神经网络之损失函数和激活函数图片07.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/深度神经网络之损失函数和激活函数/深度神经网络之损失函数和激活函数图片08.png","slug":"深度神经网络之损失函数和激活函数图片08.png","post":"cjktv5d51002cjiz5lfvsw44c","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记01.PNG","slug":"软件推荐印象笔记01.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记03.PNG","slug":"软件推荐印象笔记03.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记04.PNG","slug":"软件推荐印象笔记04.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐印象笔记05.PNG","slug":"软件推荐印象笔记05.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐夸克浏览器01.PNG","slug":"软件推荐夸克浏览器01.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐夸克浏览器02.PNG","slug":"软件推荐夸克浏览器02.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐夸克浏览器03.PNG","slug":"软件推荐夸克浏览器03.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐滴答清单01.PNG","slug":"软件推荐滴答清单01.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/效率软件推荐（一）/软件推荐滴答清单02.PNG","slug":"软件推荐滴答清单02.PNG","post":"cjktv5d3o000ojiz52klutbv4","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片3.3.png","slug":"图片3.3.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片4.1.png","slug":"图片4.1.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.1.png","slug":"图片6.1.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.2.png","slug":"图片6.2.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.4.png","slug":"图片6.4.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.5.png","slug":"图片6.5.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.6.png","slug":"图片6.6.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片7.1.png","slug":"图片7.1.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/推广.png","slug":"推广.png","post":"cjktv5d310003jiz5jrlbe751","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程03.png","slug":"Python之Sklearn使用教程03.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程04.png","slug":"Python之Sklearn使用教程04.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程06.png","slug":"Python之Sklearn使用教程06.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程07.png","slug":"Python之Sklearn使用教程07.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程08.png","slug":"Python之Sklearn使用教程08.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程09.png","slug":"Python之Sklearn使用教程09.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程10.png","slug":"Python之Sklearn使用教程10.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程11.png","slug":"Python之Sklearn使用教程11.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png","slug":"Python之Sklearn使用教程图片01.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程推广.png","slug":"Python之Sklearn使用教程推广.png","post":"cjktv5d3i000ijiz58kkvlylp","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0003.jpg","slug":"0003.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0004.jpg","slug":"0004.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0005.jpg","slug":"0005.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0006.jpg","slug":"0006.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0007.jpg","slug":"0007.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0008.jpg","slug":"0008.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0009.jpg","slug":"0009.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0010.jpg","slug":"0010.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0011.jpg","slug":"0011.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0012.jpg","slug":"0012.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0013.jpg","slug":"0013.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0014.jpg","slug":"0014.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0015.jpg","slug":"0015.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/智慧考古探测/0016.jpg","slug":"0016.jpg","post":"cjktv5d3k000jjiz57cp6io50","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png","slug":"机器学习之SVM支持向量机（一）图片01.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png","slug":"机器学习之SVM支持向量机（一）图片06.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png","slug":"机器学习之SVM支持向量机（一）图片07.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png","slug":"机器学习之SVM支持向量机（一）图片08.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png","slug":"机器学习之SVM支持向量机（一）图片09.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png","slug":"机器学习之SVM支持向量机（一）图片10.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png","slug":"机器学习之SVM支持向量机（一）图片12.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png","slug":"机器学习之SVM支持向量机（一）图片13.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png","slug":"机器学习之SVM支持向量机（一）图片15.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png","slug":"机器学习之SVM支持向量机（一）图片16.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png","slug":"机器学习之SVM支持向量机（一）图片推广.png.png","post":"cjktv5d430014jiz52d2qvolh","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0001.jpg","slug":"0001.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0002.jpg","slug":"0002.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0003.jpg","slug":"0003.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0004.jpg","slug":"0004.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0005.jpg","slug":"0005.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0006.jpg","slug":"0006.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0007.jpg","slug":"0007.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0008.jpg","slug":"0008.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0009.jpg","slug":"0009.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0010.jpg","slug":"0010.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0011.jpg","slug":"0011.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0012.jpg","slug":"0012.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0013.jpg","slug":"0013.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0014.jpg","slug":"0014.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0015.jpg","slug":"0015.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0016.jpg","slug":"0016.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0017.jpg","slug":"0017.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0018.jpg","slug":"0018.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0019.jpg","slug":"0019.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0020.jpg","slug":"0020.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0021.jpg","slug":"0021.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0022.jpg","slug":"0022.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Out-of-Gas-and-Driving-on-E/0023.jpg","slug":"0023.jpg","post":"cjktv5d3e000djiz5pfkh6p5j","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/01.png","slug":"01.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/02.png","slug":"02.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/03.png","slug":"03.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/04.png","slug":"04.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/05.png","slug":"05.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/06.png","slug":"06.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/07.png","slug":"07.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/08.png","slug":"08.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/09.png","slug":"09.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/10.png","slug":"10.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/11.png","slug":"11.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/13.png","slug":"13.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/14.png","slug":"14.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像18.png","slug":"图像18.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像19.png","slug":"图像19.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像20.png","slug":"图像20.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像21.png","slug":"图像21.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像22.png","slug":"图像22.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像23.png","slug":"图像23.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图片16.png","slug":"图片16.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图片17.png","slug":"图片17.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/推广.png","slug":"推广.png","post":"cjktv5d3a0008jiz5akpjqz46","modified":0,"renderable":0},{"_id":"source/_posts/《剑指Offer》Python版/推广.png","slug":"推广.png","post":"cjktv5ddx005vjiz5pjqvwuqh","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cjktv5d2x0001jiz5qil4o1sm","category_id":"cjktv5d340004jiz5pebey8p1","_id":"cjktv5d3h000fjiz5tnr6bs0g"},{"post_id":"cjktv5d310003jiz5jrlbe751","category_id":"cjktv5d3d000ajiz5q16j4bwx","_id":"cjktv5d3l000kjiz50xc3878g"},{"post_id":"cjktv5d3i000ijiz58kkvlylp","category_id":"cjktv5d3i000gjiz5rmoxy1sw","_id":"cjktv5d3p000pjiz5y7r9ajvx"},{"post_id":"cjktv5d3a0008jiz5akpjqz46","category_id":"cjktv5d3i000gjiz5rmoxy1sw","_id":"cjktv5d3t000tjiz53yx0vhnj"},{"post_id":"cjktv5d3c0009jiz59ivzbrai","category_id":"cjktv5d3i000gjiz5rmoxy1sw","_id":"cjktv5d3w000vjiz51s48dlus"},{"post_id":"cjktv5d3e000djiz5pfkh6p5j","category_id":"cjktv5d3q000qjiz531g44954","_id":"cjktv5d420012jiz5qyek47ay"},{"post_id":"cjktv5d3g000ejiz5uve4wie1","category_id":"cjktv5d3i000gjiz5rmoxy1sw","_id":"cjktv5d490019jiz5l4bpi45m"},{"post_id":"cjktv5d3k000jjiz57cp6io50","category_id":"cjktv5d3q000qjiz531g44954","_id":"cjktv5d4e001ejiz56m104l47"},{"post_id":"cjktv5d3n000njiz5j4jt0gka","category_id":"cjktv5d480018jiz58qcrxvhg","_id":"cjktv5d4j001ljiz5l5vq4q41"},{"post_id":"cjktv5d3o000ojiz52klutbv4","category_id":"cjktv5d4f001gjiz5vupefi1h","_id":"cjktv5d4n001sjiz5q0zfcjb7"},{"post_id":"cjktv5d4k001pjiz5tiyre64o","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d4t001zjiz5vrbmfqst"},{"post_id":"cjktv5d3r000sjiz5hujxudi7","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d4v0022jiz503im9lz6"},{"post_id":"cjktv5d4m001rjiz5j3eg73t7","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d4y0026jiz57p7vubde"},{"post_id":"cjktv5d4o001wjiz5eshooije","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d500029jiz5ls9185q0"},{"post_id":"cjktv5d3t000ujiz5667327i2","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d53002djiz5wobrwnhl"},{"post_id":"cjktv5d3x000xjiz561kzqk0s","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d56002gjiz56bw51g5l"},{"post_id":"cjktv5d4x0025jiz502px4h2s","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d58002jjiz5j9svvm5j"},{"post_id":"cjktv5d3z0010jiz5r86vcdsl","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d5a002ljiz5yhj26i0w"},{"post_id":"cjktv5d430014jiz52d2qvolh","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d5c002pjiz57pn6bayv"},{"post_id":"cjktv5d460017jiz5gdv74el2","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d5e002ujiz5jwmmxhxy"},{"post_id":"cjktv5d49001bjiz5rdq9nnwm","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d5f002xjiz5vappjshm"},{"post_id":"cjktv5d4c001djiz5boejbr79","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d5g0030jiz5y97wj73v"},{"post_id":"cjktv5d4e001fjiz5jjkjhk8p","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d5h0033jiz5bkanunt0"},{"post_id":"cjktv5d4g001ijiz5yg8px6rg","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d5i0037jiz5brx2i25d"},{"post_id":"cjktv5d4h001kjiz5uby8906t","category_id":"cjktv5d4j001mjiz5p6oc9nbl","_id":"cjktv5d5j003bjiz5fcbjz5yc"},{"post_id":"cjktv5d4r001yjiz5xk6kmmyt","category_id":"cjktv5d5i0036jiz5ycqzam5h","_id":"cjktv5d5k003fjiz59u720y2x"},{"post_id":"cjktv5d4u0021jiz5k5sp9ij6","category_id":"cjktv5d5i0036jiz5ycqzam5h","_id":"cjktv5d5l003jjiz5yo08ddco"},{"post_id":"cjktv5d4z0028jiz5cr4m2yg9","category_id":"cjktv5d5k003ejiz5b4wvc5co","_id":"cjktv5d5n003ojiz5s930zbbi"},{"post_id":"cjktv5d51002cjiz5lfvsw44c","category_id":"cjktv5d5i0036jiz5ycqzam5h","_id":"cjktv5d5o003tjiz5zw2yxsza"},{"post_id":"cjktv5d54002fjiz5q8uygzb6","category_id":"cjktv5d5i0036jiz5ycqzam5h","_id":"cjktv5d5p003vjiz5uucf1z9n"},{"post_id":"cjktv5d57002ijiz5hufi4ixt","category_id":"cjktv5d5o003sjiz5gyvxfjwu","_id":"cjktv5d5q003zjiz5x2t0nfzw"},{"post_id":"cjktv5ddx005vjiz5pjqvwuqh","category_id":"cjktv5de0005wjiz58ksp725c","_id":"cjktv5de1005yjiz510nert5d"}],"PostTag":[{"post_id":"cjktv5d2x0001jiz5qil4o1sm","tag_id":"cjktv5d370005jiz5e70fbqta","_id":"cjktv5d3e000cjiz58mldhr0h"},{"post_id":"cjktv5d310003jiz5jrlbe751","tag_id":"cjktv5d3d000bjiz5psu0jq5f","_id":"cjktv5d3z000zjiz5rx24y779"},{"post_id":"cjktv5d310003jiz5jrlbe751","tag_id":"cjktv5d3i000hjiz54214e30u","_id":"cjktv5d420013jiz5fubhmz6d"},{"post_id":"cjktv5d310003jiz5jrlbe751","tag_id":"cjktv5d3l000mjiz50ttjrosb","_id":"cjktv5d460016jiz5dkhjj8qa"},{"post_id":"cjktv5d310003jiz5jrlbe751","tag_id":"cjktv5d3r000rjiz5ymcpn8lr","_id":"cjktv5d49001ajiz50b8v3bky"},{"post_id":"cjktv5d380007jiz5unucdms0","tag_id":"cjktv5d3z000yjiz5j2pl8net","_id":"cjktv5d4h001jjiz5odzfk4uv"},{"post_id":"cjktv5d380007jiz5unucdms0","tag_id":"cjktv5d3r000rjiz5ymcpn8lr","_id":"cjktv5d4k001njiz552p4hgpf"},{"post_id":"cjktv5d380007jiz5unucdms0","tag_id":"cjktv5d4b001cjiz54gfla3rb","_id":"cjktv5d4m001qjiz5ngmubm4m"},{"post_id":"cjktv5d3a0008jiz5akpjqz46","tag_id":"cjktv5d4f001hjiz5x1xrp67t","_id":"cjktv5d4n001tjiz5mqb4sprv"},{"post_id":"cjktv5d3c0009jiz59ivzbrai","tag_id":"cjktv5d4f001hjiz5x1xrp67t","_id":"cjktv5d4r001xjiz539lj1low"},{"post_id":"cjktv5d3e000djiz5pfkh6p5j","tag_id":"cjktv5d4o001vjiz51u4ioznh","_id":"cjktv5d4w0024jiz52wbyyhol"},{"post_id":"cjktv5d3g000ejiz5uve4wie1","tag_id":"cjktv5d4f001hjiz5x1xrp67t","_id":"cjktv5d51002bjiz5xwydbrqw"},{"post_id":"cjktv5d3i000ijiz58kkvlylp","tag_id":"cjktv5d50002ajiz5tuoisl5x","_id":"cjktv5d5c002njiz5i3dala13"},{"post_id":"cjktv5d3i000ijiz58kkvlylp","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5d002qjiz5yr363jey"},{"post_id":"cjktv5d3k000jjiz57cp6io50","tag_id":"cjktv5d4o001vjiz51u4ioznh","_id":"cjktv5d5e002sjiz56v0f7f48"},{"post_id":"cjktv5d3n000njiz5j4jt0gka","tag_id":"cjktv5d5d002rjiz58zvkejxi","_id":"cjktv5d5i0034jiz5gde5lxpc"},{"post_id":"cjktv5d3n000njiz5j4jt0gka","tag_id":"cjktv5d5f002vjiz5lu8p77uc","_id":"cjktv5d5i0035jiz5zw4pjs7a"},{"post_id":"cjktv5d3n000njiz5j4jt0gka","tag_id":"cjktv5d5g002yjiz540m5pid0","_id":"cjktv5d5j0039jiz5pqn7d4so"},{"post_id":"cjktv5d3o000ojiz52klutbv4","tag_id":"cjktv5d5h0031jiz5u0a1nr00","_id":"cjktv5d5j003ajiz5c17rj8uo"},{"post_id":"cjktv5d3r000sjiz5hujxudi7","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5l003hjiz5cxl2ai50"},{"post_id":"cjktv5d3r000sjiz5hujxudi7","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5l003ijiz5y5kdqkgv"},{"post_id":"cjktv5d3t000ujiz5667327i2","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5n003pjiz5lgapufip"},{"post_id":"cjktv5d3t000ujiz5667327i2","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5o003qjiz59nfh0at7"},{"post_id":"cjktv5d3x000xjiz561kzqk0s","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5p003wjiz53hl25cgq"},{"post_id":"cjktv5d3x000xjiz561kzqk0s","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5p003xjiz5j3m8kc8o"},{"post_id":"cjktv5d3z0010jiz5r86vcdsl","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5r0041jiz5ox85n5w4"},{"post_id":"cjktv5d3z0010jiz5r86vcdsl","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5r0042jiz5t4z65czj"},{"post_id":"cjktv5d430014jiz52d2qvolh","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5s0045jiz5wa71m5c5"},{"post_id":"cjktv5d430014jiz52d2qvolh","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5s0046jiz5q20jkjzb"},{"post_id":"cjktv5d460017jiz5gdv74el2","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5t0049jiz5hfd2vueh"},{"post_id":"cjktv5d460017jiz5gdv74el2","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5u004ajiz521mr4itz"},{"post_id":"cjktv5d49001bjiz5rdq9nnwm","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5w004djiz5tivo2sue"},{"post_id":"cjktv5d49001bjiz5rdq9nnwm","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5w004ejiz5qm8ax10c"},{"post_id":"cjktv5d4c001djiz5boejbr79","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5y004hjiz50r6x4wpu"},{"post_id":"cjktv5d4c001djiz5boejbr79","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5y004ijiz57l1d4u85"},{"post_id":"cjktv5d4e001fjiz5jjkjhk8p","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d5z004ljiz5hs8olvq4"},{"post_id":"cjktv5d4e001fjiz5jjkjhk8p","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d5z004mjiz5udazo8gv"},{"post_id":"cjktv5d4g001ijiz5yg8px6rg","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d60004pjiz53nkm96ld"},{"post_id":"cjktv5d4g001ijiz5yg8px6rg","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d60004qjiz5q2k3my7x"},{"post_id":"cjktv5d4h001kjiz5uby8906t","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d61004tjiz5bf0h86ot"},{"post_id":"cjktv5d4h001kjiz5uby8906t","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d62004ujiz50rdr3hl4"},{"post_id":"cjktv5d4k001pjiz5tiyre64o","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d63004xjiz5u3dd8que"},{"post_id":"cjktv5d4k001pjiz5tiyre64o","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d63004yjiz5kq6udepf"},{"post_id":"cjktv5d4m001rjiz5j3eg73t7","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d650051jiz5x5shnonj"},{"post_id":"cjktv5d4m001rjiz5j3eg73t7","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d660052jiz5q9y0ne4y"},{"post_id":"cjktv5d4o001wjiz5eshooije","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d670055jiz5ot5v8jm8"},{"post_id":"cjktv5d4o001wjiz5eshooije","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d6d0056jiz56jd2flx1"},{"post_id":"cjktv5d4r001yjiz5xk6kmmyt","tag_id":"cjktv5d660054jiz5zn02za08","_id":"cjktv5d6y0059jiz50ywygd3y"},{"post_id":"cjktv5d4r001yjiz5xk6kmmyt","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d74005ajiz5d0ib28ki"},{"post_id":"cjktv5d4u0021jiz5k5sp9ij6","tag_id":"cjktv5d660054jiz5zn02za08","_id":"cjktv5d7s005djiz5b449ejkt"},{"post_id":"cjktv5d4u0021jiz5k5sp9ij6","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d7x005ejiz57x1a1vbk"},{"post_id":"cjktv5d4x0025jiz502px4h2s","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d82005gjiz5ndgkr3yn"},{"post_id":"cjktv5d4z0028jiz5cr4m2yg9","tag_id":"cjktv5d7x005fjiz50zxqozlk","_id":"cjktv5d83005ijiz5ik06ocml"},{"post_id":"cjktv5d51002cjiz5lfvsw44c","tag_id":"cjktv5d660054jiz5zn02za08","_id":"cjktv5d85005ljiz5j8bczem0"},{"post_id":"cjktv5d51002cjiz5lfvsw44c","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d85005mjiz586grwr3y"},{"post_id":"cjktv5d54002fjiz5q8uygzb6","tag_id":"cjktv5d56002hjiz51uoczrm7","_id":"cjktv5d86005qjiz59lfo8qin"},{"post_id":"cjktv5d54002fjiz5q8uygzb6","tag_id":"cjktv5d660054jiz5zn02za08","_id":"cjktv5d86005rjiz5vpxqp2of"},{"post_id":"cjktv5d54002fjiz5q8uygzb6","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5d86005sjiz5e6wmd426"},{"post_id":"cjktv5d57002ijiz5hufi4ixt","tag_id":"cjktv5d86005pjiz5deu7d6xt","_id":"cjktv5d86005tjiz5rk3q6qmk"},{"post_id":"cjktv5ddx005vjiz5pjqvwuqh","tag_id":"cjktv5d5j003cjiz5ogvlxk4z","_id":"cjktv5de0005xjiz5ejxfmwoo"}],"Tag":[{"name":"Linux","_id":"cjktv5d370005jiz5e70fbqta"},{"name":"Mac","_id":"cjktv5d3d000bjiz5psu0jq5f"},{"name":"Hexo","_id":"cjktv5d3i000hjiz54214e30u"},{"name":"GitHub","_id":"cjktv5d3l000mjiz50ttjrosb"},{"name":"博客","_id":"cjktv5d3r000rjiz5ymcpn8lr"},{"name":"Markdown","_id":"cjktv5d3z000yjiz5j2pl8net"},{"name":"教程","_id":"cjktv5d4b001cjiz54gfla3rb"},{"name":"python","_id":"cjktv5d4f001hjiz5x1xrp67t"},{"name":"数学建模","_id":"cjktv5d4o001vjiz51u4ioznh"},{"name":"Python","_id":"cjktv5d50002ajiz5tuoisl5x"},{"name":"机器学习","_id":"cjktv5d56002hjiz51uoczrm7"},{"name":"protobuf","_id":"cjktv5d5d002rjiz58zvkejxi"},{"name":"RPC","_id":"cjktv5d5f002vjiz5lu8p77uc"},{"name":"gRPC","_id":"cjktv5d5g002yjiz540m5pid0"},{"name":"效率软件","_id":"cjktv5d5h0031jiz5u0a1nr00"},{"name":"算法","_id":"cjktv5d5j003cjiz5ogvlxk4z"},{"name":"深度学习","_id":"cjktv5d660054jiz5zn02za08"},{"name":"推荐系统","_id":"cjktv5d7x005fjiz50zxqozlk"},{"name":"文字识别","_id":"cjktv5d86005pjiz5deu7d6xt"}]}}