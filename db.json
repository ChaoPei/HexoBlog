{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/robots.txt","path":"robots.txt","modified":1,"renderable":0},{"_id":"themes/next/source/CNAME","path":"CNAME","modified":1,"renderable":1},{"_id":"source/about/index/公众号.jpg","path":"about/index/公众号.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.png","path":"images/avatar.png","modified":1,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/LICENSE","path":"lib/canvas-nest/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/README.md","path":"lib/canvas-nest/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/LICENSE","path":"lib/canvas-ribbon/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/README.md","path":"lib/canvas-ribbon/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/README.md","path":"lib/fancybox/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/LICENSE","path":"lib/fancybox/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/LICENSE","path":"lib/three/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/README.md","path":"lib/three/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.min.css","path":"lib/fancybox/source/jquery.fancybox.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.min.js","path":"lib/fancybox/source/jquery.fancybox.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/.md_configs.data","hash":"8842f205e55d4dee88e990327ceeebfc55470fe5","modified":1521978664289},{"_id":"source/.me_configs.data","hash":"be0d1ba93f31dadd375f266a8a131bab230e5522","modified":1521978664283},{"_id":"source/.z_sync_configs.data","hash":"d35bb00a1634c739a93585c06ebea3d272c9a023","modified":1522057383406},{"_id":"source/.DS_Store","hash":"27faed8473737e96e7eb5c611f4f8d7ab4d667cb","modified":1528388511703},{"_id":"source/robots.txt","hash":"2be28bdb6f77c8f7627632bea984e2b3d735b411","modified":1521046018000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1526730959857},{"_id":"themes/next/.DS_Store","hash":"6e88389330d8364a26fac0d43d7ad7528cb635cd","modified":1526806693239},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1526730959857},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1526730959858},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1526730959858},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1526730959861},{"_id":"themes/next/.gitignore","hash":"a18c2e83bb20991b899b58e6aeadcb87dd8aa16e","modified":1526730959860},{"_id":"themes/next/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1526730959861},{"_id":"themes/next/.stickler.yml","hash":"b7939095038cbdc4883fc10950e163a60a643b43","modified":1526730959860},{"_id":"themes/next/_config.yml","hash":"ac486797f2f4d7f6435b382e0bbb7633a061ca81","modified":1526809332184},{"_id":"themes/next/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1526730959861},{"_id":"themes/next/README.md","hash":"807c28ad6473b221101251d244aa08e2a61b0d60","modified":1526730959861},{"_id":"themes/next/bower.json","hash":"a8c832da6aad5245052aed7ff26c246f85d68c6c","modified":1526730959862},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1526730959862},{"_id":"themes/next/gulpfile.coffee","hash":"48d2f9fa88a4210308fc41cc7d3f6d53989f71b7","modified":1526730959870},{"_id":"themes/next/package.json","hash":"11a0b27f92da8abf1efbea6e7a0af4271d7bff9e","modified":1526730959893},{"_id":"source/about/.DS_Store","hash":"07fcad405c7168836b16dd5fce0ddf8bc8522fd7","modified":1526820544363},{"_id":"source/about/index.md","hash":"f0235fe64c62806661655a6d7c856309f515edcf","modified":1526820510543},{"_id":"source/_posts/.DS_Store","hash":"898b2ccb69ea96a2eb4e5261b1680d452acd5559","modified":1526819742682},{"_id":"source/_posts/.md_configs.data","hash":"8842f205e55d4dee88e990327ceeebfc55470fe5","modified":1521978792542},{"_id":"source/_posts/.me_configs.data","hash":"65f09b99475d152a6e9f0bb6fe935aa0013c06b7","modified":1521978792538},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程.md","hash":"7d29cba206b63a40203d3340875f57115224cf9b","modified":1521282184618},{"_id":"source/_posts/Markdown写作教程.md","hash":"6c8017412a5aa44a943d5c7f39222c8f35a53397","modified":1522082235932},{"_id":"source/_posts/Python之MatPlotLib使用教程.md","hash":"ed5285c90b220de16a24ace0a9ec6b3167c03fee","modified":1521979165077},{"_id":"source/_posts/Python之NumPy使用教程.md","hash":"b9f70712ccacbb533082563ebe4af95f550198de","modified":1520997413604},{"_id":"source/_posts/Python之Pandas使用教程.md","hash":"3b4633ac2ac2ea2396d134bad5d6930879462ad6","modified":1521079444778},{"_id":"source/_posts/Python之Sklearn使用教程.md","hash":"38fe9c817675e9421c07c1bc00bd87d4b6548253","modified":1524101186055},{"_id":"source/_posts/机器学习之Apriori算法.md","hash":"55f7ae126ac06dc917b83caaea11e06f17108a4c","modified":1528346714805},{"_id":"source/_posts/机器学习之K均值-K-Means-算法.md","hash":"1a64dbedc51e91b9e88ebd882d70997d88578113","modified":1526788941696},{"_id":"source/_posts/机器学习之K近邻-KNN-算法.md","hash":"944624aa9ecd239025ead832f43993b8e23b7557","modified":1526788934778},{"_id":"source/_posts/机器学习之Logistic回归.md","hash":"c4237876b04f71831a90d84e2a87fd95c558dac6","modified":1522255415044},{"_id":"source/_posts/机器学习之SVM支持向量机（一）.md","hash":"a3619c763746e5362f1b52835e49ac5be990dcd6","modified":1522723962498},{"_id":"source/_posts/机器学习之SVM支持向量机（二）.md","hash":"c1a1e2a1d1b25ef7569fbe74f032997b1ae33261","modified":1523020283223},{"_id":"source/_posts/机器学习之决策树-C4-5算法.md","hash":"df8a167449c12d6e80596004e6f3515ed6a40f10","modified":1524219003597},{"_id":"source/_posts/机器学习之分类与回归树-CART.md","hash":"74944aa24e13cd967e5d92b63ac636265731864e","modified":1525072337076},{"_id":"source/_posts/机器学习之最大期望-EM-算法.md","hash":"a9c3d551ca1ed6b1e666acd30be759b3a521b9be","modified":1526788949484},{"_id":"source/_posts/机器学习之朴素贝叶斯算法.md","hash":"7c87f1ab9442c6b08ce8b20ac729a7b7f1fc8f07","modified":1526788927918},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT.md","hash":"2c2f8b3ea056b8819eae246a0632d23d2e71e2d0","modified":1525248656595},{"_id":"source/_posts/机器学习之线性回归.md","hash":"21d5f4943139f0e6e0b9a565d160c60f2cf475a1","modified":1526883968793},{"_id":"source/_posts/机器学习之自适应增强-Adaboost.md","hash":"8a3bcbb8b7c36c9c4600ad111af8c44f6df53026","modified":1525657533255},{"_id":"source/_posts/机器学习之随机森林.md","hash":"80024add151a44cd23df1061e40f89594153b5da","modified":1525079509847},{"_id":"source/_posts/机器学习知识体系.md","hash":"0e24a6d9ec95380259b42e344e4600adf35dc5ae","modified":1522308274332},{"_id":"source/_posts/面向知乎的个性化推荐模型研究论文.md","hash":"fc8f9cb053c02f0ab9e8818d5e3e322c3d4ab27d","modified":1520997428367},{"_id":"source/categories/index.md","hash":"953b432e5293289c926e7eff1b5594652d0bab40","modified":1520961992190},{"_id":"source/favorite/index.md","hash":"830a6a126a5b6238741ed67cc13904464adcaa4d","modified":1528377059780},{"_id":"source/tags/index.md","hash":"04610e91300b8e0df44c872460f285b6a6595f14","modified":1520962000814},{"_id":"source/favorite/.DS_Store","hash":"5592ccb1e426a16b5f264774610bee71cd327b2a","modified":1528388537869},{"_id":"themes/next/.git/config","hash":"e2ca9fa6f115d4406d24bf0df53fc26ce13e0c9b","modified":1526730959853},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1526730959851},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1526730951888},{"_id":"themes/next/.git/index","hash":"e8e609c30332b96f2fdeace56bb957f42f2b5e57","modified":1526730959950},{"_id":"themes/next/.git/packed-refs","hash":"e277289607b97219cf350f6a9de7e6cbb9c9e510","modified":1526730959849},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"b63696d41f022525e40d7e7870c3785b6bc7536b","modified":1526730959858},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1526730959859},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"f846118d7fc68c053df47b24e1f661241645373f","modified":1526730959858},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"7abbb4c8a29b2c14e576a00f53dbc0b4f5669c13","modified":1526730959859},{"_id":"themes/next/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1526730959860},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1526730959860},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1526730959863},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"141e989844d0b5ae2e09fb162a280715afb39b0d","modified":1526730959863},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1526730959864},{"_id":"themes/next/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1526730959864},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"120750c03ec30ccaa470b113bbe39f3d423c67f0","modified":1526730959865},{"_id":"themes/next/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1526730959864},{"_id":"themes/next/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1526730959865},{"_id":"themes/next/docs/MATH.md","hash":"0ae4258950de01a457ea8123a8d13ec6db496e53","modified":1526730959866},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1526730959866},{"_id":"themes/next/languages/de.yml","hash":"fb478c5040a4e58a4c1ad5fb52a91e5983d65a3a","modified":1526730959870},{"_id":"themes/next/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1526730959871},{"_id":"themes/next/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1526730959871},{"_id":"themes/next/languages/fr.yml","hash":"0162a85ae4175e66882a9ead1249fedb89200467","modified":1526730959872},{"_id":"themes/next/languages/id.yml","hash":"e7fb582e117a0785036dcdbb853a6551263d6aa6","modified":1526730959872},{"_id":"themes/next/languages/it.yml","hash":"62ef41d0a9a3816939cb4d93a524e6930ab9c517","modified":1526730959873},{"_id":"themes/next/languages/ja.yml","hash":"5f8e54c666393d1ca2e257f6b1e3b4116f6657d8","modified":1526730959873},{"_id":"themes/next/languages/ko.yml","hash":"fae155018ae0efdf68669b2c7dd3f959c2e45cc9","modified":1526730959873},{"_id":"themes/next/languages/nl.yml","hash":"bb9ce8adfa5ee94bc6b5fac6ad24ba4605d180d3","modified":1526730959874},{"_id":"themes/next/languages/pt-BR.yml","hash":"bfc80c8a363fa2e8dde38ea2bc85cd19e15ab653","modified":1526730959874},{"_id":"themes/next/languages/pt.yml","hash":"3cb51937d13ff12fcce747f972ccb664840a9ef3","modified":1526730959874},{"_id":"themes/next/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1526730959874},{"_id":"themes/next/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1526730959874},{"_id":"themes/next/languages/vi.yml","hash":"8da921dd8335dd676efce31bf75fdd4af7ce6448","modified":1526730959875},{"_id":"themes/next/languages/zh-CN.yml","hash":"041fd4769f133952e093afd2a9782513cae0b2bb","modified":1526737769289},{"_id":"themes/next/languages/zh-HK.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1526730959875},{"_id":"themes/next/languages/zh-TW.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1526730959875},{"_id":"themes/next/layout/.DS_Store","hash":"4703bddd4f40ed30847bbbd84cef149da7e74435","modified":1526734462166},{"_id":"themes/next/layout/_layout.swig","hash":"09e8a6bfe5aa901c66d314601c872e57f05509e8","modified":1526730959876},{"_id":"themes/next/layout/archive.swig","hash":"2b6450c6b6d2bcbcd123ad9f59922a5e323d77a5","modified":1526730959892},{"_id":"themes/next/layout/category.swig","hash":"5d955284a42f802a48560b4452c80906a5d1da02","modified":1526730959892},{"_id":"themes/next/layout/index.swig","hash":"53300ca42c00cba050bc98b0a3f2d888d71829b1","modified":1526730959892},{"_id":"themes/next/layout/page.swig","hash":"79040bae5ec14291441b33eea341a24a7c0e9f93","modified":1526730959893},{"_id":"themes/next/layout/post.swig","hash":"e7458f896ac33086d9427979f0f963475b43338e","modified":1526730959893},{"_id":"themes/next/layout/schedule.swig","hash":"3e9cba5313bf3b98a38ccb6ef78b56ffa11d66ee","modified":1526730959893},{"_id":"themes/next/layout/tag.swig","hash":"ba402ce8fd55e80b240e019e8d8c48949b194373","modified":1526730959893},{"_id":"themes/next/scripts/helpers.js","hash":"392cda207757d4c055b53492a98f81386379fc4f","modified":1526730959894},{"_id":"themes/next/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1526730959894},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1526730959895},{"_id":"themes/next/scripts/.DS_Store","hash":"504874f8f368b85046d5b7439014f6c0d989ff81","modified":1526806736812},{"_id":"themes/next/source/.DS_Store","hash":"b690d14a4306765bff3479984371117a0761c345","modified":1526801221485},{"_id":"themes/next/source/CNAME","hash":"5455120583e7a62be755f95d1b53d5efbbec5bf4","modified":1521078314026},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1526730959947},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1526730959948},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1526730959949},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526730959921},{"_id":"source/about/index/公众号.jpg","hash":"df033d0e4e9de400ba207e9b6b7f28817d5996d8","modified":1520581948988},{"_id":"source/about/index/.DS_Store","hash":"6f6d730c6b9445730cf530e7ce6e24ff2474bbcb","modified":1522310371259},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/.DS_Store","hash":"0c3c383472616d584b5d541505642243bfefef75","modified":1521280981924},{"_id":"source/_posts/Markdown写作教程/.DS_Store","hash":"0e67b693bce4e8dbc97f39025f629fc6cb30ae35","modified":1521446575930},{"_id":"source/_posts/Python之MatPlotLib使用教程/03.png","hash":"613cb5e4cfc842bd987ed3a3da446ff779605ff4","modified":1520752253049},{"_id":"source/_posts/Python之MatPlotLib使用教程/02.png","hash":"e68d02686e078ee777ecb9c62586843dc0fa4d95","modified":1520751248093},{"_id":"source/_posts/Python之MatPlotLib使用教程/04.png","hash":"29b910fc5dd61d9dc017a43ebe5fa7835a7cd33c","modified":1520752577517},{"_id":"source/_posts/Python之MatPlotLib使用教程/05.png","hash":"abe67138ba5186ca80125b35f68a24ada5da397a","modified":1520753289453},{"_id":"source/_posts/Python之MatPlotLib使用教程/.DS_Store","hash":"17e17dcee88fab62efa4a8d27c70c43ec11b5c07","modified":1521280973916},{"_id":"source/_posts/Python之MatPlotLib使用教程/06.png","hash":"07003112ee77f408788693ccdb71fa6578d635f2","modified":1520753426282},{"_id":"source/_posts/Python之MatPlotLib使用教程/07.png","hash":"7f0e85e07ca1ae562025c8031cfc87eb9a99b51a","modified":1520757420269},{"_id":"source/_posts/Python之MatPlotLib使用教程/08.png","hash":"4e3061fc73c47c5040c2920567185f906298fab7","modified":1520758048771},{"_id":"source/_posts/Python之MatPlotLib使用教程/09.png","hash":"352b9d0ab5678717465660ca2f7d243ac605166a","modified":1520760208929},{"_id":"source/_posts/Python之MatPlotLib使用教程/11.png","hash":"44dfb34c533a659f62e0213a9b6320329503857c","modified":1520763424602},{"_id":"source/_posts/Python之MatPlotLib使用教程/13.png","hash":"e10a1daaaac28c0d04bf4a3183e08ae9b5e0b985","modified":1520767651002},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像20.png","hash":"414c76614aa0e1f9b0832aced52ed8e0702fcb1d","modified":1521018437593},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程03.png","hash":"ef9781638e5f177bb192245d906da889af3f6aa3","modified":1523772589009},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程09.png","hash":"56374280cab86a969f853c967201d3d3729cf920","modified":1523780341443},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程10.png","hash":"b89f99c644b86aec73b4f6b0a93566ddd765ad15","modified":1523780543763},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程11.png","hash":"bddde196beb8e1f8b8fe1ec4b9ddb82682953f38","modified":1523780896578},{"_id":"source/_posts/Python之Sklearn使用教程/.DS_Store","hash":"eff22146e8828ad9dd12c68c3007d69338ed803e","modified":1523788258093},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png","hash":"ad73ccab95e08bb762f941a4630dd3c6a9388a2e","modified":1526106299434},{"_id":"source/_posts/机器学习之Apriori算法/.DS_Store","hash":"8cdf4a50c7f83e3ec03b277020cbcd0249be87ed","modified":1526635239354},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/.DS_Store","hash":"12ff89e05b2d4b8e201bc3f9b25cdae95e306243","modified":1526113925530},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png","hash":"a21052a33e97fc36bd411a2b42eb7abec2ae6736","modified":1526197637195},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png","hash":"a1324b9077b0916ac0936bf0440e907d4d70e137","modified":1526189929003},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png","hash":"c577553d106ea0027b569274eb357e96cda02ed3","modified":1526194825368},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png","hash":"4cccd6b50cdd9d4933ea11bb74c175d27ec3d57f","modified":1526199779629},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png","hash":"8ae7791d2a6604a0ceda324af19292ab54e092f3","modified":1526203003055},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png","hash":"fe51a131f9553b96995b546ec9515dc9a3b59c27","modified":1526203857852},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/.DS_Store","hash":"5da2599e15af005a3ecbeec466fc87452b1b373c","modified":1526206542284},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归01.png","hash":"1817f152628ed2d0e63b71a9071cbcb7765edd41","modified":1522221930283},{"_id":"source/_posts/机器学习之Logistic回归/.DS_Store","hash":"16ae78eba755b9bc559e864077fef80bd911fd16","modified":1522245286031},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png","hash":"2c766de1fa96945fbadff336ad36ddb470026c58","modified":1522719717371},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/.DS_Store","hash":"298e9892310d3c33cc36aed29eb22b7ee50c6f65","modified":1523008752797},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像01.png","hash":"f5b42a7d64bcf4e42dbddea25f9d91dacd47ae8e","modified":1519290534407},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像02.png","hash":"911aacc5c8458e2a9789dabcf11bee3b8fb08254","modified":1519290529866},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像03.png","hash":"db3af154b414462add94cafcf711c95dc8fce7d2","modified":1519290530309},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/.DS_Store","hash":"08974a08332afa53d94a33f0a6d9d44761e68a2e","modified":1523016650880},{"_id":"source/_posts/机器学习之决策树-C4-5算法/机器学习之决策树图片01.png","hash":"94ede414f10507f9d1a5433b3db69e819d36b605","modified":1524212210853},{"_id":"source/_posts/机器学习之分类与回归树-CART/.DS_Store","hash":"0f5ea30b8c60a7357f4a78e9f75a0df111ff9fbf","modified":1524710704374},{"_id":"source/_posts/机器学习之决策树-C4-5算法/.DS_Store","hash":"68e4aef0339f605f365952692815a0cf05dcbc0c","modified":1524217176457},{"_id":"source/_posts/机器学习之最大期望-EM-算法/.DS_Store","hash":"be07df2f6c2887f0cbc62f2908ed97e56ab3aa5c","modified":1525945987780},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png","hash":"b99b737fc9473f0ffda6ba8a3c3f225a9bf04908","modified":1525188304227},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/.DS_Store","hash":"b0f6461d66c6a01e7db9bb898dee6a994bf1fed0","modified":1525616178905},{"_id":"source/_posts/机器学习之线性回归/.DS_Store","hash":"cc59d5c34db63e8223e7bee9d0ae80c905291e9b","modified":1522075010225},{"_id":"source/_posts/机器学习之线性回归/图片01.png","hash":"2b715466ee24cacfb4596159b9e5d548c7508a98","modified":1521907108234},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/.DS_Store","hash":"fdd79c71f11996193d4bd9724a70c01da6169e01","modified":1525657549567},{"_id":"source/_posts/机器学习知识体系/图片01.png","hash":"2242e0694775cb96fac26c3d88feabbc965c0c59","modified":1521874547062},{"_id":"source/_posts/机器学习知识体系/图片02.png","hash":"0a2bb3b98d4750f7b7f7830aefb1347ea1e6414c","modified":1521874559462},{"_id":"source/_posts/机器学习之随机森林/.DS_Store","hash":"6bcd0e89dca238693f11da2c1937eb230f45ee2d","modified":1525079642074},{"_id":"source/_posts/机器学习知识体系/.DS_Store","hash":"0cf9f687c90ec4689d06e3b9b90988365ace4a2b","modified":1521904877534},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1526730951889},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1526730951889},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1526730951890},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1526730951890},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1526730951889},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1526730951890},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1526730951889},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1526730951890},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1526730951890},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1526730951891},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1526730951888},{"_id":"themes/next/.git/logs/HEAD","hash":"e9d796fd8f288c02936ca4972545016df7845ba2","modified":1526730959852},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1526730959866},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1526730959867},{"_id":"themes/next/docs/ru/README.md","hash":"712d9a9a557c54dd6638adfb0e1d2bb345b60756","modified":1526730959867},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1526730959867},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"6855402e2ef59aae307e8bd2a990647d3a605eb8","modified":1526730959867},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"a45a791b49954331390d548ac34169d573ea5922","modified":1526730959868},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"44e4fb7ce2eca20dfa98cdd1700b50d6def4086f","modified":1526730959868},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1526730959868},{"_id":"themes/next/docs/zh-CN/README.md","hash":"84d349fda6b9973c81a9ad4677db9d9ee1828506","modified":1526730959869},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"24cf2618d164440b047bb9396263de83bee5b993","modified":1526730959869},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1526730959868},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"e03607b608db4aa7d46f6726827c51ac16623339","modified":1526730959869},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"c1ba919f70efe87a39e6217883e1625af0b2c23c","modified":1526730959870},{"_id":"themes/next/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1526730959876},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1526730959876},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1526730959876},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1526730959877},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"05e67c50a4f3a20fad879ed61b890de8ca6ba4ea","modified":1526730959877},{"_id":"themes/next/layout/_macro/post-related.swig","hash":"08fe30ce8909b920540231e36c97e28cfbce62b6","modified":1526730959877},{"_id":"themes/next/layout/_macro/post.swig","hash":"686e60ede86547bdd7bc34c3629e4c9dbd134a21","modified":1526730959878},{"_id":"themes/next/layout/_macro/reward.swig","hash":"bd5778d509c51f4b1d8da3a2bc35462929f08c75","modified":1526730959878},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"1f3121ef66a4698fd78f34bf2594ef79a407c92c","modified":1526730959878},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"a9e1346b83cf99e06bed59a53fc069279751e52a","modified":1526730959878},{"_id":"themes/next/layout/_partials/breadcrumb.swig","hash":"6994d891e064f10607bce23f6e2997db7994010e","modified":1526730959879},{"_id":"themes/next/layout/_partials/.DS_Store","hash":"911169c11ffdbb7b8ea9e062c859ae734b8e7dac","modified":1526734542044},{"_id":"themes/next/layout/_partials/comments.swig","hash":"5df32b286a8265ba82a4ef5e1439ff34751545ad","modified":1526730959879},{"_id":"themes/next/layout/_partials/footer.swig","hash":"1ae77b6a369f83c9986408f2ab448090e37cd2dc","modified":1526730959879},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1aaf32bed57b976c4c1913fd801be34d4838cc72","modified":1526730959881},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1526730959881},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"0a0129e926c27fffc6e7ef87fe370016bc7a4564","modified":1526730959883},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"6fc63d5da49cb6157b8792f39c7305b55a0d1593","modified":1526730959883},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"ac3ad2c0eccdf16edaa48816d111aaf51200a54b","modified":1526730959883},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1526730959884},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"60001c8e08b21bf3a7afaf029839e1455340e95d","modified":1526730959887},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"a8ab2035654dd06d94faf11a35750529e922d719","modified":1526730959888},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"f532ce257fca6108e84b8f35329c53f272c2ce84","modified":1526730959889},{"_id":"themes/next/layout/_third-party/github-banner.swig","hash":"cabd9640dc3027a0b3ac06f5ebce777e50754065","modified":1526730959889},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"927f19160ae14e7030df306fc7114ba777476282","modified":1526730959890},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"6b75c5fd76ae7cf0a7b04024510bd5221607eab3","modified":1526730959890},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1526730959890},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1526730959890},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"b0ca46e0d1ff4c08cb0a3a8c1994f20d0260cef9","modified":1526730959890},{"_id":"themes/next/scripts/tags/button.js","hash":"5a61c2da25970a4981fbd65f4a57c5e85db4dcda","modified":1526730959895},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"db70a841e7c1708f95ca97b44413b526b267fa9b","modified":1526730959895},{"_id":"themes/next/scripts/tags/full-image.js","hash":"a98fc19a90924f2368e1982f8c449cbc09df8439","modified":1526730959895},{"_id":"themes/next/scripts/tags/exturl.js","hash":"2b3a4dc15dea33972c0b6d46a1483dabbf06fb5b","modified":1526730959895},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"1b97b1b5364945b8ab3e50813bef84273055234f","modified":1526730959896},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"b7600f6b868d8f4f7032126242d9738cd1e6ad71","modified":1526730959896},{"_id":"themes/next/scripts/tags/label.js","hash":"621004f2836040b12c4e8fef77e62cf22c561297","modified":1526730959896},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"460e5e1f305847dcd4bcab9da2038a85f0a1c273","modified":1526730959896},{"_id":"themes/next/scripts/tags/note.js","hash":"4975d4433e11161b2e9a5744b7287c2d667b3c76","modified":1526730959896},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1526730959897},{"_id":"themes/next/source/css/.DS_Store","hash":"e151ac3bcfcce274304d96d080d6e94e0dc2159c","modified":1526801221484},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1526730959921},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1526730959921},{"_id":"themes/next/source/images/avatar.png","hash":"d985c17188c7c966df863e797976d98ce8b40771","modified":1520578140450},{"_id":"themes/next/source/css/main.styl","hash":"c26ca6e7b5bd910b9046d6722c8e00be672890e0","modified":1526730959920},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1526730959922},{"_id":"themes/next/source/images/.DS_Store","hash":"8e224858919d46acc9cfc60ab124e4b5b2ebbf8f","modified":1526732774241},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1526730959923},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1526730959923},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1526730959923},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1526730959924},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1526730959924},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1526730959925},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"78aec0fd86b154f1483a3fddf8bacb4632404bb5","modified":1520934309231},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"78aec0fd86b154f1483a3fddf8bacb4632404bb5","modified":1520934309231},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1526730959925},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1526730959926},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1526730959926},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1526730959926},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1526730959927},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1526730959927},{"_id":"source/_posts/Markdown写作教程/图片01.png","hash":"37bf22348a323e3708cfe540ae74e6d8e89bd4d6","modified":1521443192789},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"source/_posts/Markdown写作教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"source/_posts/Python之MatPlotLib使用教程/01.png","hash":"33e230638dfea5aecb04dfbdd3a6099c248ef2c3","modified":1520750712546},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像18.png","hash":"3eb66a4726829f310e9317433813bafcae824183","modified":1521017855982},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像19.png","hash":"a9ce5acc9a9bd616faf50581b802aa69d4ec5e5d","modified":1521018063252},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像21.png","hash":"6c844ab24a7b2fa744f14a70bac46b613fc4cca3","modified":1521019203108},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像22.png","hash":"e9cf3a459468a6696855da633138be48e6ee3d7b","modified":1521019668154},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像23.png","hash":"84f4972fd31cc00510f9cd5b5b5c23bce1661072","modified":1521022889615},{"_id":"source/_posts/Python之MatPlotLib使用教程/图片16.png","hash":"f61b216e3015ff8461953b80a41c404ab9c05f7a","modified":1521014672819},{"_id":"source/_posts/Python之MatPlotLib使用教程/图片17.png","hash":"05d26db61bbd1b7ad73219630d6423e1c712d91c","modified":1521014682865},{"_id":"source/_posts/Python之MatPlotLib使用教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程04.png","hash":"54e7eb64aeff4e62baa1f465b9ecd8b0163c298d","modified":1523775847000},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程06.png","hash":"397f01c175a32a327bd0f5e25737bebfb30a429a","modified":1523778871147},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程07.png","hash":"f73d270ea01afc9494a85cb642779e25ceb87aa7","modified":1523779122431},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png","hash":"305df6ad6e8549a280eda7178c796df59c3efc7a","modified":1526113620281},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png","hash":"8be383416d0e78739d78ee2332876bbc366a75c9","modified":1526113740927},{"_id":"source/_posts/机器学习之Logistic回归/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归03.png","hash":"414ee5813fc11417114610e2e6af70a89650f185","modified":1522244498598},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png","hash":"b44c4687d5db6136f7ad2d8c07ce5f8810315df5","modified":1523007080985},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png","hash":"7efc5e982bb03590dadca9b1d7bcff20937026a4","modified":1525186292278},{"_id":"source/_posts/机器学习之线性回归/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png","hash":"7efc5e982bb03590dadca9b1d7bcff20937026a4","modified":1525186292278},{"_id":"source/_posts/机器学习知识体系/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1521024709018},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526730959884},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526730959884},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526730959914},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526730959914},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526730959914},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526730959920},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526730959920},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 20-19-07.md","hash":"4b912fca9b44721a93d346f24902fcc1d9518a4b","modified":1521980347112},{"_id":"source/_posts/.Archive/Python之MatPlotLib使用教程.md/2018-03-25 19-59-25.md","hash":"d4f91fd5d44d6beddd58eec9f426f0887583e1a6","modified":1521979165076},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 20-10-45.md","hash":"049fb26ee773eaf41946ef49b09daf6bb243fe2f","modified":1521979845135},{"_id":"source/_posts/.Archive/Python之MatPlotLib使用教程.md/2018-03-25 19-55-55.md","hash":"eb5a4de8344a6843371745efa3cbccee02fef7a3","modified":1521978955153},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 20-40-57.md","hash":"d0cfca39104d291dee12c76bcfcce956d20b39d7","modified":1521981657156},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 21-52-20.md","hash":"f46cc95f707d6ef281191a30f6184020cff607fd","modified":1521985940888},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 21-57-50.md","hash":"3fbf2ab4576c8e8e5142583adcfbef684a5ede9b","modified":1521986270896},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 21-55-10.md","hash":"32350f1cd9bc33d88c34ad2627911fd2c896b220","modified":1521986110893},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 21-56-10.md","hash":"c168af8ad8b522e4e48e7cf3b148376e70396ade","modified":1521986170894},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-01-20.md","hash":"c39ea2086a8b044afe3e18c065fe8b10f04843c1","modified":1521986480927},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-04-40.md","hash":"e9c2d0a19630ffb04a3eae8540d8841ed8c71d56","modified":1521986680909},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-02-50.md","hash":"e452abb9fca7c44053cd2e998f9eb1d82ac273de","modified":1521986570905},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-07-10.md","hash":"bf01038be7871345ae5e13d67fcf2bf40dd3a116","modified":1521986830916},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-08-30.md","hash":"f9b76652cee53c59c03f4c0d83b8db0ad616b8e4","modified":1521986910913},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-10-40.md","hash":"ad568931dda35f76d84d7b8403c47a2ebe36c7ea","modified":1521987040916},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-12-00.md","hash":"22cc0ff5ac4cc3818c442182f02bbd4a7bce1982","modified":1521987120919},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-15-30.md","hash":"74864eb152380179f7e65e77fc154c234a03779e","modified":1521987330923},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-20-30.md","hash":"d9579c4616c8cb15464eafe998c00bc98f8fc033","modified":1521987630931},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-16-30.md","hash":"a0dd34efea9fe196efe06452b3b2d7b8f87774a7","modified":1521987390926},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-22-00.md","hash":"7af87c891b584f41c215ee7d1ce56ef05d39cb27","modified":1521987720935},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-23-40.md","hash":"19cfae3d212a34d0c0de41ccc2534d96f1376e2a","modified":1521987820939},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 22-40-21.md","hash":"483109e9481cff47d6780cde0909c38b45a254ef","modified":1521988821126},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 23-17-48.md","hash":"d7378f3302b3427c4ef0aa51d87bbbcdab7f2e42","modified":1521991068510},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 23-25-31.md","hash":"be501dc660ce5477c996116d45d4c509a013a836","modified":1521991531474},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-25 23-32-38.md","hash":"54af3335021febd12841463741bac435ce78aed3","modified":1521991958983},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 00-40-11.md","hash":"3eef9d1b2d90e4cbd9ada9e6828956a699fd7c2e","modified":1521996011594},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 00-41-36.md","hash":"a16b36716e6bd24fd078622801c7abe58e97d8ac","modified":1521996096036},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-31-16.md","hash":"df215088593c0372eabcaa8f86814f48bb2fce68","modified":1522035076748},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-22-26.md","hash":"69e3b51eab2d30816d8b8b1326dfa738e34242e8","modified":1522034546704},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-21-06.md","hash":"6db531e0dffd53d94d09af155f768568699f5259","modified":1522034466685},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-23-56.md","hash":"c66db085f5534adccc3f2a29958b2f6ca908954a","modified":1522034636691},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-33-06.md","hash":"aac252ce042db8a06086a02d4f45cd094fe0b4b0","modified":1522035186706},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-34-26.md","hash":"ef03e21c450b5c01291525905bf3d6e47f556dcb","modified":1522035266725},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-35-36.md","hash":"bd1dafa423779089e951fed3b68fffda1b649af7","modified":1522035336708},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-36-46.md","hash":"fbdbeff0a51aeacd2ebd0a07e3bfd45f34345390","modified":1522035406710},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-37-46.md","hash":"58c78bf8b049e9b29b10c2979bed346583ee424f","modified":1522035466715},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-41-46.md","hash":"b80095e406003ec53a4d6c7a26fdbd0cc0cd08c6","modified":1522035706717},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-39-26.md","hash":"bac24c55eedc782459b12c8b26c2b02c61ad8e79","modified":1522035566714},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-40-46.md","hash":"20651a1855db4f2389033635e5d20ee7758f113d","modified":1522035646716},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-44-06.md","hash":"e827a734db603b0b4a48b63f9b71250833268059","modified":1522035846723},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-46-44.md","hash":"7a7096c08001cb5c21eb1de2011a0d00d1d7fa32","modified":1522036004961},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-42-46.md","hash":"33a3d5d5e4fb23a23ec5fc5c127dfc4baf7ce0b4","modified":1522035766724},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 11-45-06.md","hash":"6a78156cf700bf4d80bd4ac9bb0ca799d44a6646","modified":1522035906724},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-23-48.md","hash":"da3d957091a84bf19391bcc51cb9b840eef82026","modified":1522038228952},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-21-28.md","hash":"a1f09efb8dfbcd09c5acd39c66feae094721a6ff","modified":1522038088951},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-22-38.md","hash":"56f392a82fafaa724043fcc173b0ed60aba88559","modified":1522038158950},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-25-38.md","hash":"87a8eb8d631ecba93341319704ba8e27d74707fa","modified":1522038338949},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-31-58.md","hash":"34b3ef1e8a026e9079f33a9463729e13a1f827a3","modified":1522038718949},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-33-29.md","hash":"6ee8200a93119f6221cfd4384d68753e8d18792d","modified":1522038809022},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-34-29.md","hash":"ab40e56cdf66dc96cf2e616d6b262b8a83f0029d","modified":1522038869416},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-38-49.md","hash":"151224915158d949e45a855f869a5fdbcaf32230","modified":1522039129424},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-42-49.md","hash":"33a29aad02da9bd2d15b8a4e39dd6d3072734b57","modified":1522039369554},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-43-59.md","hash":"6ead39fb81ce29b51a3dc45a5e7b60f29313d666","modified":1522039439411},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-45-29.md","hash":"88cd551cb991c96982436b921d72abbda9812656","modified":1522039529449},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-47-09.md","hash":"a78752c4a66a15abe83e8e17012a2b6bf7c99bf4","modified":1522039629411},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-50-29.md","hash":"cc5d24a1e98f874dc399c5c524c51c3e269ef87b","modified":1522039829436},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-48-09.md","hash":"40998f1150b08de3784b3e1b217fe49830d9517a","modified":1522039689466},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-52-39.md","hash":"f7b85011363e9533f202e843b254fa312c2fc464","modified":1522039959570},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-51-39.md","hash":"218a24d8cbd0f93077d86faf8bef3185160d4802","modified":1522039899443},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-54-59.md","hash":"ab08bc472e72d309786c16e77f03caf6746f387e","modified":1522040099440},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-53-49.md","hash":"cec8c17a758b931b28659172e7eff49a6b7f14e2","modified":1522040029551},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-57-59.md","hash":"6d9f2de0e3cd46f82c351422777655371ca87ad9","modified":1522040279443},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-55-59.md","hash":"890b192408dc5f4e27a9101151082331a156fadb","modified":1522040159442},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 12-59-29.md","hash":"33d5e6ec3b7c9c89ce5ae8ceb676e166176d06be","modified":1522040369443},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-00-39.md","hash":"373df51de30881e6d6943be5f9a4768db52dbb1d","modified":1522040439446},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-07-09.md","hash":"b9039a1b1b0a792611fbe471bc23fa4ade6077d7","modified":1522040829468},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-08-09.md","hash":"ab3a5085c37fc851054050d5477ca9606fbf6d2b","modified":1522040889470},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-10-19.md","hash":"21d1eed734d5425872fb57e15049efb8a19eebde","modified":1522041019477},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-11-29.md","hash":"b30fe788bb1acff82e9bae45017f59f64958f2fc","modified":1522041089476},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-17-49.md","hash":"57f684edb79da0ca486029db3379deb06f76b579","modified":1522041469488},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-19-09.md","hash":"187db145fa18ea582f965dbf849ffd66fb09de95","modified":1522041549488},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-22-19.md","hash":"9925fe82ebd2b81377e12e85fc024583e5dc4caf","modified":1522041739495},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-21-09.md","hash":"fa41b1fda333357495ff41d7fd2470417d94f90e","modified":1522041669616},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 13-24-29.md","hash":"a233a10461172c85b8c0f77aa403df112a2358c3","modified":1522041869583},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-04-32.md","hash":"68fd4dd7c304738f8ed74cea3bfc7c7578324b4a","modified":1522055072770},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-05-42.md","hash":"c0abdf7d187ef4ef2779c9c308c40dfd3024987e","modified":1522055142716},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-06-42.md","hash":"2439b5a290bdf02d0b90ecc99a8870ab060a055f","modified":1522055202723},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-08-22.md","hash":"0f9dd1f345389999e3abcca0118eb7d1abce791b","modified":1522055302717},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-09-22.md","hash":"ee47ba3255a6528bd3887a557b6312334c8367d4","modified":1522055362808},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-10-32.md","hash":"22ca7f872bf5bdcd8bfe56cec7fe58289c38476d","modified":1522055432813},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-13-22.md","hash":"46f1e288b3ae476af2ba9a6d1516e8d031091e18","modified":1522055602746},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-14-22.md","hash":"94b4a9eb03304f851dc4ee069584aed9d81810ed","modified":1522055662770},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-11-42.md","hash":"7e9c35e90f602ee22419a4bd851505f55b6cef8c","modified":1522055502726},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-15-22.md","hash":"75b349c8f2df5af30e3418b42a508f248b420653","modified":1522055722808},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-16-42.md","hash":"b92eef8a9f13713d6aeba4ed75eea9199560a293","modified":1522055802746},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-19-52.md","hash":"237f34cb4e7b927abd307445dd7bff3bfa454e9c","modified":1522055992746},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-18-22.md","hash":"ca5df8507ac02d0c686037e50888b9fbf8929df8","modified":1522055902791},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-22-22.md","hash":"219c7feb17f955025807a13fa83ea12c0842e0d6","modified":1522056142751},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-24-13.md","hash":"17eb8fd7a29cf7b1b77725e05d659d2c6a840feb","modified":1522056253756},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-50-13.md","hash":"def480dd8ca7d7e7863b09112d7df1a1e307e167","modified":1522057813809},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-46-23.md","hash":"2f31e399a623554c0c26a915151d3ab25f85d4ae","modified":1522057583799},{"_id":"source/_posts/.Archive/机器学习之线性回归.md/2018-03-26 17-52-32.md","hash":"0ab39dbd96b12445c3df9139e996bcd67117309c","modified":1522057952914},{"_id":"source/_posts/Markdown写作教程/图片03.png","hash":"244052c1fc5344f9d755cb942517d9d5da14afe3","modified":1521444578570},{"_id":"source/_posts/Python之MatPlotLib使用教程/12.png","hash":"4812b5371a7cb9d823327f8e95dffa5aaf97934b","modified":1520763832783},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png","hash":"2be19ce3efa76780e9a18a93a1dac729b1b7a3f5","modified":1523772055010},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png","hash":"2f7a3455e1fd13aaa582f7274d4c7d50e8fae5d9","modified":1522677611570},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png","hash":"ed89cf8ee5f651f2e1769321716565748b043bc3","modified":1522656180663},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png","hash":"ed008a6f9a39ee597bf965da9cf3349181e88980","modified":1523007598843},{"_id":"source/_posts/机器学习之线性回归/公式01.png","hash":"e3afd52d851957f06f23168c198ccd1814e9de25","modified":1522074544547},{"_id":"source/_posts/机器学习之线性回归/公式03.png","hash":"3ce320dce755145da770585a40c55c22da3d671e","modified":1522074870692},{"_id":"source/_posts/机器学习之线性回归/图片03.png","hash":"e322d56b0e286a2ca13cbbb470ce25010f8b1c5b","modified":1522054969819},{"_id":"themes/next/.git/refs/heads/master","hash":"35ef56b48346d32c3e02bdaa3f15f06ef9af7ce6","modified":1526730959852},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1526730959877},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"d1b73c926109145e52605929b75914cc8b60fb89","modified":1526730959877},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1526730959879},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"a7e376b087ae77f2e2a61ba6af81cde5af693174","modified":1526730959879},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"1dd6cc34d041705e960831615f22f66deb1026ff","modified":1526734638460},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"fd780171713aada5eb4f4ffed8e714617c8ae6be","modified":1526730959880},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1526730959880},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"3db735d0cd2d449edf2674310ac1e7c0043cb357","modified":1526730959880},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"88b4b6051592d26bff59788acb76346ce4e398c2","modified":1526730959880},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a33b29ccbdc2248aedff23b04e0627f435824406","modified":1526730959881},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1526730959881},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1526730959881},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1526730959882},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1526730959882},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1526730959882},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1526730959882},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1526730959882},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"cc865af4a3cb6d25a0be171b7fc919ade306bb50","modified":1526730959883},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1526730959884},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1526730959884},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1526730959884},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1526730959884},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1526730959885},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"67f0cb55e6702c492e99a9f697827629da036a0c","modified":1526730959885},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1526730959885},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1526730959885},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"94b26dfbcd1cf2eb87dd9752d58213338926af27","modified":1526730959885},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"beb53371c035b62e1a2c7bb76c63afbb595fe6e5","modified":1526730959886},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1526730959886},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"cee047575ae324398025423696b760db64d04e6f","modified":1526730959886},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1526730959886},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1526730959886},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1526730959887},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1526730959887},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"8878241797f8494a70968756c57cacdfc77b61c7","modified":1526730959887},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1526730959887},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"17a54796f6e03fc834880a58efca45c286e40e40","modified":1526730959888},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"40e3cacbd5fa5f2948d0179eff6dd88053e8648e","modified":1526730959888},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"6f340d122a9816ccdf4b45b662880a4b2d087671","modified":1526730959888},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"c0eb6123464d745ac5324ce6deac8ded601f432f","modified":1526730959888},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"42f62695029834d45934705c619035733762309e","modified":1526730959888},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"a6fc00ec7f5642aabd66aa1cf51c6acc5b10e012","modified":1526730959889},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"97dbc2035bcb5aa7eafb80a4202dc827cce34983","modified":1526730959889},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"9b9ff4cc6d5474ab03f09835a2be80e0dba9fe89","modified":1526730959889},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1526730959891},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"b15e10abe85b4270860a56c970b559baa258b2a8","modified":1526730959891},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1526730959892},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1526730959892},{"_id":"themes/next/source/css/_common/.DS_Store","hash":"17473d69ad911d3a69aa8b285f7877ac7f644115","modified":1526801227221},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1526730959913},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1526730959913},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"81ca13d6d0beff8b1a4b542a51e3b0fb68f08efd","modified":1526730959914},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"2640a54fa63bdd4c547eab7ce2fc1192cf0ccec8","modified":1526730959914},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"7a2706304465b9e673d5561b715e7c72a238437c","modified":1526730959919},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"32392d213f5d05bc26b2dc452f2fc6fea9d44f6d","modified":1526730959920},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1526730959919},{"_id":"themes/next/source/css/_variables/base.styl","hash":"cfb03ec629f13883509eac66e561e9dba562333f","modified":1526730959920},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1526730959927},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1526730959928},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1526730959928},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1526730959928},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1526730959929},{"_id":"themes/next/source/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1526730959929},{"_id":"themes/next/source/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1526730959929},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1526730959930},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1526730959930},{"_id":"themes/next/source/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1526730959931},{"_id":"themes/next/source/lib/canvas-nest/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1526794407740},{"_id":"themes/next/source/lib/canvas-nest/README.md","hash":"bf7819cbb879bb82ec1097513d8f799df8835e0f","modified":1526794407740},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1526794407741},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"d8bf9cb15d9d91c7ad022ba2954b5b4d326f17f7","modified":1526797279821},{"_id":"themes/next/source/lib/canvas-ribbon/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1526794790182},{"_id":"themes/next/source/lib/canvas-ribbon/README.md","hash":"07e86d67c508a3f974c45ff61cd0760d79bcebc5","modified":1526794790182},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1526794790183},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1526797279821},{"_id":"themes/next/source/lib/fancybox/README.md","hash":"a40db80eb6386b085ff810fd3e302f12e76b8df7","modified":1526797279822},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1526730959932},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1526730959933},{"_id":"themes/next/source/lib/fancybox/LICENSE","hash":"8624bcdae55baeef00cd11d5dfcfa60f68710a02","modified":1526797279822},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1526730959933},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1526730959934},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1526730959934},{"_id":"themes/next/source/lib/three/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1526794611265},{"_id":"themes/next/source/lib/three/README.md","hash":"5096005b6bfa0e554b245c6c1ffe10e7edc8130d","modified":1526794611265},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1526794611266},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1526794611266},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1526794611267},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1526730959946},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1526730959946},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1526730959947},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.3.png","hash":"9c5d6ffa38f3ff94979444e7ac5e38943c2404f8","modified":1521171912741},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程05.png","hash":"256d9aadf22876b4fe5beea1c6fb126d245df262","modified":1523777912548},{"_id":"source/_posts/机器学习之Apriori算法/机器学习之Apriori算法图片01.png","hash":"f8ae81474b95fe79f07ebc5473e45574bec40829","modified":1526628611362},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png","hash":"c6c24fbb1b1f06aee99d1c7ed558d54edf48e1ed","modified":1522655857701},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png","hash":"f6aea31b36824cf4958541fd47067965a5057d81","modified":1522717780858},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png","hash":"8e40abc5b044df8a0b256edbbca383df20e1a485","modified":1522717929705},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png","hash":"28a4092973b644020d1b7b8f4d7ce80619006334","modified":1522670595184},{"_id":"source/_posts/机器学习之线性回归/公式02.png","hash":"09937e4c48b4bfaf597eb85131c7db518c73fce9","modified":1522074559149},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png","hash":"578f8b3f574ea4f6969c28ac881582e550b6df8e","modified":1525614373950},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png","hash":"07ef180e8842ac7765b0d07f3f15a4ffb9841dc7","modified":1525614601600},{"_id":"source/_posts/机器学习知识体系/图片03.png","hash":"8d75e5be4fbaa92f0ab0e411159cbfbb201673df","modified":1521872892514},{"_id":"themes/next/.git/objects/pack/pack-9829b0de980dc403efcab0fab027fd96c783804d.idx","hash":"118683259e5b358ac99d3441dedb260235a098dc","modified":1526730959839},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1526730959941},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片7.2.png","hash":"02359d570ab5830d815f61d8fe16ab25179c9c89","modified":1521278826350},{"_id":"source/_posts/Python之MatPlotLib使用教程/15.png","hash":"9adb17a88751a53d34588418786ee941c412ee1f","modified":1520768401624},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归02.png","hash":"bfa20ad1c15607d24bd562d56286006dad364d9f","modified":1522244886133},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png","hash":"6cb18abfc66ca9a0570bcd2dd2149a9568714ef2","modified":1522718276755},{"_id":"source/_posts/机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png","hash":"492fd9787fc36e529f216bf8f8453b4b9e4835d1","modified":1525932603367},{"_id":"source/_posts/机器学习之线性回归/图片02.png","hash":"bfa20ad1c15607d24bd562d56286006dad364d9f","modified":1522055595336},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"e9d796fd8f288c02936ca4972545016df7845ba2","modified":1526730959852},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1526730959851},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"6958a97fde63e03983ec2394a4f8e408860fb42b","modified":1526730959891},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1526730959891},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1526730959897},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1526730959897},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1526730959897},{"_id":"themes/next/source/css/_common/components/.DS_Store","hash":"23df40c75faacd0197e7056ebc479d30a6848786","modified":1526802062537},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1526730959898},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1526730959898},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1526730959902},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1526730959908},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"18309b68ff33163a6f76a39437e618bb6ed411f8","modified":1526730959912},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1526730959912},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1526730959912},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1526730959912},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"0810e7c43d6c8adc8434a8fa66eabe0436ab8178","modified":1526730959913},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1526730959913},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1526730959913},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1526730959915},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1526730959915},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1526730959915},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"f362fbc791dafb378807cabbc58abf03e097af6d","modified":1526730959914},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"f43c821ea272f80703862260b140932fe4aa0e1f","modified":1526730959915},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"2212511ae14258d93bec57993c0385e5ffbb382b","modified":1526730959915},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1526730959915},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"5e12572b18846250e016a872a738026478ceef37","modified":1526730959916},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1526730959916},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1526730959917},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"35f093fe4c1861661ac1542d6e8ea5a9bbfeb659","modified":1526730959917},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1526730959917},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"d5e8ea6336bc2e237d501ed0d5bbcbbfe296c832","modified":1526730959917},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1526730959918},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"ba1842dbeb97e46c6c4d2ae0e7a2ca6d610ada67","modified":1526730959918},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"05a5abf02e84ba8f639b6f9533418359f0ae4ecb","modified":1526730959918},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1526730959918},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"41f9cdafa00e256561c50ae0b97ab7fcd7c1d6a2","modified":1526730959918},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"ffa870c3fa37a48b01dc6f967e66f5df508d02bf","modified":1526730959918},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"5779cc8086b1cfde9bc4f1afdd85223bdc45f0a0","modified":1526730959919},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1526730959930},{"_id":"themes/next/source/lib/canvas-nest/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1526794407729},{"_id":"themes/next/source/lib/canvas-nest/.git/config","hash":"6aef6bbfcb8ccf9d042bf26defe7d08167b6d81e","modified":1526794407732},{"_id":"themes/next/source/lib/canvas-nest/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1526794405039},{"_id":"themes/next/source/lib/canvas-nest/.git/index","hash":"63550b126e8b32e613431f2a194d3f3663a232f0","modified":1526794407741},{"_id":"themes/next/source/lib/canvas-nest/.git/packed-refs","hash":"60cebf9a6bcd88fd6b2132480cdef3ebf7a7dcf7","modified":1526794407720},{"_id":"themes/next/source/lib/canvas-ribbon/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1526794790176},{"_id":"themes/next/source/lib/canvas-ribbon/.git/config","hash":"f79e33176f8d3e893553102df6eb347a74fd078f","modified":1526794790178},{"_id":"themes/next/source/lib/canvas-ribbon/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1526794780940},{"_id":"themes/next/source/lib/canvas-ribbon/.git/index","hash":"3b9dd48c1ae81b09dda2ee0e198a30904412bc6e","modified":1526794790183},{"_id":"themes/next/source/lib/canvas-ribbon/.git/packed-refs","hash":"35076774be8e5f297f34b1efbd310b067d4601b7","modified":1526794790174},{"_id":"themes/next/source/lib/fancybox/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1526797279815},{"_id":"themes/next/source/lib/fancybox/.git/config","hash":"0bd9061daa1ae14bc8c15ccb2616123858620669","modified":1526797279817},{"_id":"themes/next/source/lib/fancybox/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1526797276747},{"_id":"themes/next/source/lib/fancybox/.git/index","hash":"ad24e38b3d7439feef84b985bd2e655d0f5dd053","modified":1526797279825},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"a5913612c237bb7443c6006a386edd775201d423","modified":1526797279822},{"_id":"themes/next/source/lib/fancybox/.git/packed-refs","hash":"a1338c173c56e17c1fb2c7a539ccd3a7eb0da164","modified":1526797279813},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.min.css","hash":"84adea69673c392c1d34a5a316e8e5960aa348b5","modified":1526797279823},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.min.js","hash":"b85b75426ff7569d54a1f69689895315e80ed85d","modified":1526797279824},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1526730959935},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1526730959936},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1526730959935},{"_id":"themes/next/source/lib/three/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1526794611259},{"_id":"themes/next/source/lib/three/.git/config","hash":"65f90ae2568e8c72066580065d3e1df0d03f415c","modified":1526794611261},{"_id":"themes/next/source/lib/three/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1526794607368},{"_id":"themes/next/source/lib/three/.git/index","hash":"4a981e81abdcc4f33ceecb1a8fceff3759488ae6","modified":1526794611272},{"_id":"themes/next/source/lib/three/.git/packed-refs","hash":"3e9cbba9f328f4378f17624962e553d1a6020e3f","modified":1526794611256},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1526730959942},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1526730959943},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.1.png","hash":"6e71a79873cf1b4ce889c6bfd9bfdf7be1ac9009","modified":1521171871303},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片4.1.png","hash":"faa30796bf710258a987c3277c77de1e5af0e709","modified":1521185354700},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.4.png","hash":"36ce82cebff199c501a9cba78fef0169aae62225","modified":1521206164160},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png","hash":"4d8692aec2e5ac5ead5da05c1833bdef07aa1d6e","modified":1523707430660},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png","hash":"6def4e9f93f518f2210e44d9c7d23aad339ffa56","modified":1522717592439},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png","hash":"29d5e6087b241a8a4eae416a0997028f28b066e2","modified":1522717387984},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png","hash":"11f3b12ef3ea65d97c6a92fffc71d73c094807b4","modified":1522717742465},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png","hash":"840d9444a423bb1e43200f77bf6b2b6b4ef5f200","modified":1522717855374},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png","hash":"4cf2f1a3596e6381019ac52c02a6fb7d78b02ac2","modified":1522717997105},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1526730959939},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1526730959939},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1526730959945},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png","hash":"7f105dfa87769b6894feb53226af206130ae2a0a","modified":1522717496601},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png","hash":"c4aa4e10466b8af8ee430875266251cfd8669822","modified":1522717674777},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png","hash":"562f9f4ef0d0b6fe7b1dbdd274ea417385b2e8cb","modified":1522718108545},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"e9d796fd8f288c02936ca4972545016df7845ba2","modified":1526730959851},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"39dee82d481dd9d44e33658960ec63e47cd0a715","modified":1526730959898},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"7cc3f36222494c9a1325c5347d7eb9ae53755a32","modified":1526730959898},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"ee37e6c465b9b2a7e39175fccfcbed14f2db039b","modified":1526730959898},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1526730959899},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1526730959899},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1526730959899},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1526730959899},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1526730959899},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"17b95828f9db7f131ec0361a8c0e89b0b5c9bff5","modified":1526730959900},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1526730959900},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1526730959901},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"fb451dc4cc0355b57849c27d3eb110c73562f794","modified":1526730959901},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"7dd9a0378ccff3e4a2003f486b1a34e74c20dac6","modified":1526730959900},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1526730959900},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1526730959901},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1526730959901},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1526730959902},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1526730959902},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1526730959903},{"_id":"themes/next/source/css/_common/components/post/.DS_Store","hash":"a605681d409186d46ed28de44224d6948b084a4b","modified":1526801992453},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1526801388046},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"417f05ff12a2aaca6ceeac8b7e7eb26e9440c4c3","modified":1526730959904},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1526730959904},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1526730959904},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1526730959904},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ca89b167d368eac50a4f808fa53ba67e69cbef94","modified":1526730959903},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1526730959905},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1526730959905},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1526730959905},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"c0ac49fadd33ca4a9a0a04d5ff2ac6560d0ecd9e","modified":1526730959905},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1526730959905},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1526730959906},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8bf095377d28881f63a30bd7db97526829103bf2","modified":1526730959906},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"35c0350096921dd8e2222ec41b6c17a4ea6b44f2","modified":1526730959906},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"bbe0d111f6451fc04e52719fd538bd0753ec17f9","modified":1526730959906},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1526730959906},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1526730959906},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1526730959907},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1526730959907},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"4427ed3250483ed5b7baad74fa93474bd1eda729","modified":1526730959907},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1526730959907},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"43bc58daa8d35d5d515dc787ceb21dd77633fe49","modified":1526730959908},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1526730959908},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1526730959908},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1526730959908},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"5d15cc8bbefe44c77a9b9f96bf04a6033a4b35b8","modified":1526730959909},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1526730959909},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1526730959909},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1526730959909},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1526730959909},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1526730959910},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1526730959909},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"10e9bb3392826a5a8f4cabfc14c6d81645f33fe6","modified":1526730959910},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1526730959910},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1526730959910},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1526730959910},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1526730959911},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1526730959911},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1526730959911},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1526730959911},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"76937db9702053d772f6758d9cea4088c2a6e2a3","modified":1526730959911},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1c06be422bc41fd35e5c7948cdea2c09961207f6","modified":1526730959912},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1526730959916},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1526730959916},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1526730959917},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1526794405041},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1526794405040},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1526794405042},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1526794405042},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1526794405040},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1526794405042},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1526794405040},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1526794405041},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1526794405041},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1526794405043},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/HEAD","hash":"582d677034c993733017d2e2b06c5ffa51c383e5","modified":1526794407731},{"_id":"themes/next/source/lib/canvas-nest/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1526794405039},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1526794780942},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1526794780941},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1526794780943},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1526794780942},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1526794780944},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1526794780944},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1526794780941},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1526794780943},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1526794780943},{"_id":"themes/next/source/lib/canvas-ribbon/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1526794780945},{"_id":"themes/next/source/lib/canvas-ribbon/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1526794780940},{"_id":"themes/next/source/lib/canvas-ribbon/.git/logs/HEAD","hash":"f14c0473b69657054f76b820d9a1113741da99d8","modified":1526794790177},{"_id":"themes/next/source/lib/fancybox/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1526797276750},{"_id":"themes/next/source/lib/fancybox/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1526797276747},{"_id":"themes/next/source/lib/fancybox/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1526797276754},{"_id":"themes/next/source/lib/fancybox/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1526797276755},{"_id":"themes/next/source/lib/fancybox/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1526797276749},{"_id":"themes/next/source/lib/fancybox/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1526797276756},{"_id":"themes/next/source/lib/fancybox/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1526797276748},{"_id":"themes/next/source/lib/fancybox/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1526797276750},{"_id":"themes/next/source/lib/fancybox/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1526797276753},{"_id":"themes/next/source/lib/fancybox/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1526797276757},{"_id":"themes/next/source/lib/fancybox/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1526797276746},{"_id":"themes/next/source/lib/fancybox/.git/logs/HEAD","hash":"c60adcfb0f754ffabf091ef04b3951f6ef42ea4e","modified":1526797279816},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1526730959938},{"_id":"themes/next/source/lib/three/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1526794607372},{"_id":"themes/next/source/lib/three/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1526794607368},{"_id":"themes/next/source/lib/three/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1526794607369},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"fec45d19179030b8f43ebd0eabc5467892c14136","modified":1526797279823},{"_id":"themes/next/source/lib/three/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1526794607371},{"_id":"themes/next/source/lib/three/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1526794607370},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"fec45d19179030b8f43ebd0eabc5467892c14136","modified":1526797279825},{"_id":"themes/next/source/lib/three/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1526794607370},{"_id":"themes/next/source/lib/three/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1526794607369},{"_id":"themes/next/source/lib/three/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1526794607373},{"_id":"themes/next/source/lib/three/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1526794607370},{"_id":"themes/next/source/lib/three/.git/logs/HEAD","hash":"cd9f8e74f97a7766bcb3c9ac2bf270de7f1e8f8a","modified":1526794611260},{"_id":"themes/next/source/lib/three/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1526794607374},{"_id":"themes/next/source/lib/three/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1526794607367},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.5.png","hash":"bec5459c79cfa6fdabea06979fe34404781e106b","modified":1521207413223},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片7.1.png","hash":"f70709d5d8b754a9fc2869781dc69a935d711de0","modified":1521274188922},{"_id":"source/_posts/Python之MatPlotLib使用教程/10.png","hash":"edb7d8ae73e7c28e9dc0af865e00422e93ae9fc4","modified":1520761621071},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png","hash":"d8ed9a84f2cb143da24a448a746eb27a6b948110","modified":1522718172862},{"_id":"source/_posts/机器学习知识体系/图片04.png","hash":"078a6395cfe259faaaafc9509152721c309ae446","modified":1521872847278},{"_id":"source/_posts/机器学习之决策树-C4-5算法/机器学习之决策树图片02.png","hash":"a0bd7baea72ce6eb49b8e443fffa61cb4ce8fafc","modified":1524194884897},{"_id":"source/_posts/机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png","hash":"c0dda60bc9bb970d061a4e1abb9c12a301b0978d","modified":1524638765110},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/69/39233ece53c9bdb9a1faf3271ed5768b034aad","hash":"5a770d418c1bb7b0f031f4d5416530002032fcf3","modified":1526794407693},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/2a/f622a4d7df40a2708946e91d6d7a0df1dc468c","hash":"3da7207fb18d361b83c56f4e35f67e9e945abd82","modified":1526794407695},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/5a/69ce9c2e4a1a34f6063ae9a121af1555669c69","hash":"dad25cc0f450e2827b5676975f4a70636e3fd2c8","modified":1526794407692},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/45/9262fe92f0115707bf8d8764f1886bc5e7c9e0","hash":"36040483f8af76775b7e4b6d87cec53729625399","modified":1526794407697},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/86/1c9f4241fe0eb6af02ad770d5ce04c1f68972b","hash":"7005c3e36015a4af30d4b91bd5a849a7861a073e","modified":1526794407694},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/69/a20d65d83035fdb01734a8eabe3340f740a4cb","hash":"9e95b02d8e43ec92e06bee3f60dffb74e8e7b9fa","modified":1526794407691},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/75/de2b8fa62d52690de32c351c63ab6446104ed5","hash":"52d10122d633ce4895a0690c5955e1b356f5a391","modified":1526794407696},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/90/f6477118d05f5f96ce0a63c6f18b7b2baea200","hash":"385f58e92981f27fa54eb52bf60424e87c70a9d8","modified":1526794407689},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/d4/95d28a8fab74d23908f6ccef9e4db2625fbacb","hash":"59e6067b0a806deee7bda6460b36c0f63e2e1db5","modified":1526794407690},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/98/67d1132e0e50bbb7df754a63358d70741df6d5","hash":"3cb710a1faee73c08036f5e2df7df3a7ce29e9dd","modified":1526794407687},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/aa/da83ad9aa55faa2b34ede31b1d41e16966f80b","hash":"b304541ab95b7969a63ba2ec4f60f5391bd8bb44","modified":1526794407688},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/02/5cf882fc75a324b1d71b0921eb52ea427387fd","hash":"4a0f15d00f50c629f01c2274e26ef05b3779b6c6","modified":1526794790159},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/15/9cfb8a3050111dfb3e6635e3f37fe68de2ab30","hash":"f4e1752896135edd6c9a36164b45893a59839548","modified":1526794790162},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/20/47d8728f7950fcc2276c339ee199302805838d","hash":"813e7f6c3b8954c40b250e6ba592d6bac7874397","modified":1526794790166},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/56/4913335d11e112ac80e8d550c8347016007f6d","hash":"50e9db730ceac6b2d0c8e8d5a3fec450290fdd8e","modified":1526794790163},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/57/6d268825c6e7b3892828b61b9b408600faa1f2","hash":"2ca9617d2459b9e010e71b3b221fec42d76cae32","modified":1526794790165},{"_id":"themes/next/source/lib/canvas-nest/.git/refs/heads/master","hash":"5442226ab36d787824e89f50241336839b376133","modified":1526794407730},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/69/a20d65d83035fdb01734a8eabe3340f740a4cb","hash":"9e95b02d8e43ec92e06bee3f60dffb74e8e7b9fa","modified":1526794790163},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/6c/34e310dd41c6bfa8db842190388ab83e6d4fa2","hash":"b327c0ce763240a6a8bd2f3fb55ace6b9e3df671","modified":1526794790161},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/7e/871dba93cad35ffcec76a936259ae9a3641bc7","hash":"957e9e7db09ee72fc49ca852d3cc67242567fa45","modified":1526794790167},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/ab/2197a6f5baffdd1d2895c169f4777921cc4027","hash":"81a75ca40bcfd278d6a9d67dedc8d61d959e2baa","modified":1526794790166},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/c6/d0381e6cd08086561110f76b5f921bf81a6f87","hash":"be98eb9329c91132e90d942a44d05a41c78b4470","modified":1526794790160},{"_id":"themes/next/source/lib/canvas-ribbon/.git/objects/b9/07949515b208cc605b513ff9d5b8153cbe2fe1","hash":"8fb6e8f2e6b4df8787239335ef9777a6c888d3c0","modified":1526794790164},{"_id":"themes/next/source/lib/canvas-ribbon/.git/refs/heads/master","hash":"6c198187e9ce9589dc0c44efb29b1bdcc9afa0c0","modified":1526794790177},{"_id":"themes/next/source/lib/fancybox/.git/objects/00/c03f6be011e8878608eec12f68caf42b73f38e","hash":"8516bd35bc8f9223e13de5877778c8d14d49d6db","modified":1526797279597},{"_id":"themes/next/source/lib/fancybox/.git/objects/19/3567a3107003507fafacd255c349857e417926","hash":"ef5eab75e8c6998cb223edb4eb8a26c4cfc9415c","modified":1526797279794},{"_id":"themes/next/source/lib/fancybox/.git/objects/36/9bab09306448a2970d378b59bb21c059edad63","hash":"877e0c15623d0d6ff8f09bb627fc60a489f3c105","modified":1526797279601},{"_id":"themes/next/source/lib/fancybox/.git/objects/54/0a7b36ee26decfc3f0f34bf73bc85c48899128","hash":"2e5447a9dd879d71368e9dddd34d93849d00b934","modified":1526797279804},{"_id":"themes/next/source/lib/fancybox/.git/objects/7b/15d3cb03fda86241f8b2b335f04e9b9de0e1c4","hash":"d1fe3bd82c90f7d93874798a8ee8ebf1391d7207","modified":1526797279604},{"_id":"themes/next/source/lib/fancybox/.git/objects/89/9d7a75b543fbed2a785f67d995bc77e06eb2e9","hash":"5651e2b80703225f642625c0fb2646543096d2cf","modified":1526797279593},{"_id":"themes/next/source/lib/fancybox/.git/objects/78/068b93f813cecbbd50c8247de547035009d512","hash":"4bc2bee779bd7e3cca13ee34801cf1e12585e5ae","modified":1526797279603},{"_id":"themes/next/source/lib/fancybox/.git/objects/92/4369c371444afb18fb86309229f5b4c24c6cf4","hash":"9eb6fa8ff9081e6650f6bee350d21567df105737","modified":1526797279605},{"_id":"themes/next/source/lib/fancybox/.git/objects/bd/e1f741357b44b49290d43fdd193125202fef81","hash":"9beefc75cc0c37d04e98dd13b51ad85df40e77b9","modified":1526797279598},{"_id":"themes/next/source/lib/fancybox/.git/objects/ac/97c2cc9f61c52753abe4174a4a74b2064e5af0","hash":"272e74036b0612de83d9d0aa9604d3edf888b249","modified":1526797279793},{"_id":"themes/next/source/lib/fancybox/.git/objects/94/a9ed024d3859793618152ea559a168bbcbb5e2","hash":"1c2d080a86f03eb960e112a94910a5115addf57a","modified":1526797279601},{"_id":"themes/next/source/lib/fancybox/.git/objects/aa/654e17af8c354994f706c4e33bba6b5b70caeb","hash":"22b1bdf0b0974bf5e9022953ac26066056c235ff","modified":1526797279803},{"_id":"themes/next/source/lib/fancybox/.git/refs/heads/master","hash":"01fe0b1cf911f9e2e30415be1c967b1eaae2ee38","modified":1526797279816},{"_id":"themes/next/source/lib/fancybox/.git/objects/c2/fc5def1b6c38369e5e8b849adb956bd79b549e","hash":"27f03b9616e615b2724bac0fa4507d152697f3f1","modified":1526797279595},{"_id":"themes/next/source/lib/fancybox/.git/objects/f6/bb280a0b2c68256a8e906b35c6976c80c1b3be","hash":"5daaee11fc384fbe0f02c7123036c954ee9a73fd","modified":1526797279598},{"_id":"themes/next/source/lib/three/.git/objects/1b/3d1cd16f3e6ce99ea92c7e9c0b78ee30a8c346","hash":"16569e1c4912fd762f32b13de839d05b9c319e54","modified":1526794610162},{"_id":"themes/next/source/lib/three/.git/objects/21/77c1416c8e91ed918b6d3cf7f333d99d659a24","hash":"7330016805a652c2153bfcfbdd4c3c50e5b5a642","modified":1526794611249},{"_id":"themes/next/source/lib/three/.git/objects/3c/6fef4314903036253335d4550346f6b927c721","hash":"1ee4e7d110363dc5e2676b675c7f7030731cf3a5","modified":1526794610392},{"_id":"themes/next/source/lib/three/.git/objects/67/6b9f6189356d0201fd72dd6f98db19628a2ff5","hash":"250795ea0af79524945e66b28f233eb8bd9ee7dd","modified":1526794610391},{"_id":"themes/next/source/lib/three/.git/objects/69/a20d65d83035fdb01734a8eabe3340f740a4cb","hash":"9e95b02d8e43ec92e06bee3f60dffb74e8e7b9fa","modified":1526794610165},{"_id":"themes/next/source/lib/three/.git/objects/7f/6a7ddb8a04644dc980d83c9878b4621acf82f5","hash":"c517caf5d47c9f1eba2ced6800dab03e2e29a130","modified":1526794610163},{"_id":"themes/next/source/lib/three/.git/objects/82/69b6fd29a514f9bd746497b298176228bd4061","hash":"cf527c08b5d88d72939a61446d128eba2fb94be2","modified":1526794610387},{"_id":"themes/next/source/lib/three/.git/objects/79/ae5f64ea4be7e1a108b2f1fc8afbca7bfa1fba","hash":"4e28517d80599b1a6051b1d3f657654aca6333b9","modified":1526794610166},{"_id":"themes/next/source/lib/three/.git/objects/89/b7ae569312ae577534d7b58647232fb48bfb08","hash":"a265c0a7e24267bd82d11a48f091d2241edc0080","modified":1526794610164},{"_id":"themes/next/source/lib/three/.git/objects/d0/32ce3fe05be57b72cc925f86149497229cda13","hash":"59e5036e95b496852ad3b918eac2e4b28a5951f2","modified":1526794611247},{"_id":"themes/next/source/lib/three/.git/objects/e5/6e96bd99ebe3f0dc8f02c979703c3666f6dd9e","hash":"b971fcaae8424ea1b507379afdcdff74359a1525","modified":1526794611248},{"_id":"themes/next/source/lib/three/.git/objects/e0/52c7542703df4969a95779bd4dc58b568bcb5e","hash":"b1ccf1650011f74939f07233834bdff21baf4952","modified":1526794610163},{"_id":"themes/next/source/lib/three/.git/refs/heads/master","hash":"f17437f700f7d1a2476f16b375d982f5cd36ddc2","modified":1526794611260},{"_id":"themes/next/source/lib/three/.git/objects/09/3d290f60c3b467d503633531d7ce7cdd42dd75","hash":"ab09bc8bfc9079e408dc6facc1bfd9aa5d6bdef9","modified":1526794611246},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1526794611271},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/refs/heads/master","hash":"582d677034c993733017d2e2b06c5ffa51c383e5","modified":1526794407731},{"_id":"themes/next/source/lib/canvas-nest/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1526794407727},{"_id":"themes/next/source/lib/canvas-ribbon/.git/logs/refs/heads/master","hash":"f14c0473b69657054f76b820d9a1113741da99d8","modified":1526794790177},{"_id":"themes/next/source/lib/canvas-ribbon/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1526794790176},{"_id":"themes/next/source/lib/fancybox/.git/logs/refs/heads/master","hash":"c60adcfb0f754ffabf091ef04b3951f6ef42ea4e","modified":1526797279816},{"_id":"themes/next/source/lib/fancybox/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1526797279815},{"_id":"themes/next/source/lib/three/.git/logs/refs/heads/master","hash":"cd9f8e74f97a7766bcb3c9ac2bf270de7f1e8f8a","modified":1526794611260},{"_id":"themes/next/source/lib/three/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1526794611258},{"_id":"themes/next/source/lib/three/.git/objects/76/310b5a7e93e66e7d98aaab2960d6354b2288c5","hash":"84de52193b47c83d01eb8800b063ccd83ad42cf9","modified":1526794611246},{"_id":"source/_posts/Python之MatPlotLib使用教程/14.png","hash":"43f3564c467b1029cf25e46c2749b93e408be3cf","modified":1520768005391},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/refs/remotes/origin/HEAD","hash":"582d677034c993733017d2e2b06c5ffa51c383e5","modified":1526794407727},{"_id":"themes/next/source/lib/canvas-ribbon/.git/logs/refs/remotes/origin/HEAD","hash":"f14c0473b69657054f76b820d9a1113741da99d8","modified":1526794790176},{"_id":"themes/next/source/lib/fancybox/.git/logs/refs/remotes/origin/HEAD","hash":"c60adcfb0f754ffabf091ef04b3951f6ef42ea4e","modified":1526797279814},{"_id":"themes/next/source/lib/three/.git/logs/refs/remotes/origin/HEAD","hash":"cd9f8e74f97a7766bcb3c9ac2bf270de7f1e8f8a","modified":1526794611258},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程08.png","hash":"12a6e73e9a4c6a21addd5542ebc96addfd889498","modified":1523779527345},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.6.png","hash":"fdfdc025a22244646018a4105b4ffaa6b3ecdef6","modified":1521208606331},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片3.3.png","hash":"4e466a4caae62c4d477208251f8f9ad259faeebe","modified":1521186691269},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.2.png","hash":"34877fc9266906c05387350a9c9d13b0c8411f92","modified":1521171886540},{"_id":"themes/next/.git/objects/pack/pack-9829b0de980dc403efcab0fab027fd96c783804d.pack","hash":"9e1759de63e19531772b171ae72c0b4bb3787630","modified":1526730959837},{"_id":"source/_posts/机器学习之随机森林/机器学习之随机森林图片01.png","hash":"b17a64c76af269ced0330cc80c64bb4f67f637c1","modified":1525069322854},{"_id":"source/_posts/Markdown写作教程/图片02.png","hash":"4becc8fdd53dbe2c6e7117270d2297a27abb3c2d","modified":1521430634072},{"_id":"public/about/index.html","hash":"e8edd34137d673706eb1347ebaaa1bc4e89b7a0c","modified":1528389076265},{"_id":"public/categories/index.html","hash":"f7c7c47e9015cb3d0d6a91591332d06090c8f1ae","modified":1528389076266},{"_id":"public/tags/index.html","hash":"d63dbfa3645cbe06800ab2d6a0e9b239710effb3","modified":1528389076266},{"_id":"public/favorite/index.html","hash":"2276e4ae2facc6327de628efcaf36368a667bc95","modified":1528389076266},{"_id":"public/2018/05/18/机器学习之Apriori算法/index.html","hash":"8abc1f8d22baa6063538ad5295d6741917ba4125","modified":1528389076266},{"_id":"public/2018/05/14/机器学习之朴素贝叶斯算法/index.html","hash":"07c270923289a541c531750ea1fccd6e4646b7a4","modified":1528389076266},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/index.html","hash":"a08414a30796522f91a3da72f888119ad46332f1","modified":1528389076267},{"_id":"public/2018/05/12/机器学习之K均值-K-Means-算法/index.html","hash":"b844a0577bb92963da6bd6fb41e153652fe1f52f","modified":1528389076267},{"_id":"public/2018/05/09/机器学习之最大期望-EM-算法/index.html","hash":"ed76586ecdcbeedabf06006cd9238adbabb8fd84","modified":1528389076267},{"_id":"public/2018/05/03/机器学习之自适应增强-Adaboost/index.html","hash":"48da7b37f1e6c2dcb94db43dfb8eed39ead58945","modified":1528389076267},{"_id":"public/2018/04/30/机器学习之梯度提升决策树-GBDT/index.html","hash":"84633f877a49a481b3d28871fb8b9cfcd435413a","modified":1528389076267},{"_id":"public/2018/04/30/机器学习之随机森林/index.html","hash":"9c7dad39c0f25a9476630208bbc9a4dfa8944989","modified":1528389076268},{"_id":"public/2018/04/22/机器学习之分类与回归树-CART/index.html","hash":"b1aae19dc3bf2faaba07a27a00fa2e4fec00af75","modified":1528389076268},{"_id":"public/2018/04/19/机器学习之决策树-C4-5算法/index.html","hash":"7ea443ebfdc7d7a3801d40e151079598291673e6","modified":1528389076268},{"_id":"public/2018/04/15/Python之Sklearn使用教程/index.html","hash":"83ca2ca9e9bfc7ca454b10873c1f27a5c4c23c60","modified":1528389076268},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/index.html","hash":"18f0d0a8ac28ebfdf001e8fe9f31c2e28c7a6d26","modified":1528389076268},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/index.html","hash":"634d6bfb0188e2c379a36390ac62825ed77c67db","modified":1528389076268},{"_id":"public/2018/03/27/机器学习之Logistic回归/index.html","hash":"983e2cf615adc0f2a167d9f9650f3cf59f68590b","modified":1528389076269},{"_id":"public/2018/03/24/机器学习之线性回归/index.html","hash":"16d23097a42e0f5dff13d87d64b66f78dec7d799","modified":1528389076269},{"_id":"public/2018/03/24/机器学习知识体系/index.html","hash":"2608b5eff9f3da2182c4ab388a7b50dbc9a4368b","modified":1528389076269},{"_id":"public/2018/03/18/Markdown写作教程/index.html","hash":"7eaa1d34a328f486e51e205dde43a85cbd0de011","modified":1528389076269},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/index.html","hash":"783d39e89e4c9d1312b01121eeef72a6c01bc234","modified":1528389076269},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/index.html","hash":"b833bf3e887dd216f7ab608ce2fd43267d47232b","modified":1528389076269},{"_id":"public/2018/03/13/Python之NumPy使用教程/index.html","hash":"5650318803da885c06f5f70133a422446a71be85","modified":1528389076269},{"_id":"public/2018/03/12/Python之Pandas使用教程/index.html","hash":"b0e470f836d1273cca96f4b10161bc2ccd609d28","modified":1528389076270},{"_id":"public/2018/03/12/面向知乎的个性化推荐模型研究论文/index.html","hash":"7f5a07437693e3da629ce76c2e6a3889ebbd5c35","modified":1528389076270},{"_id":"public/categories/教程/index.html","hash":"c386a8ea752dec97611c4dd5156cdc6b21f57be6","modified":1528389076270},{"_id":"public/categories/Python库/index.html","hash":"b9af452ae977878f7a4b8e625a9aa069b2c87ab5","modified":1528389076270},{"_id":"public/categories/机器学习/index.html","hash":"b117dfb82c90bf049a463c05cc21e6b1443ee5e3","modified":1528389076270},{"_id":"public/categories/机器学习/page/2/index.html","hash":"2669143db2e749374d1b28adaabf1f85095afcad","modified":1528389076270},{"_id":"public/categories/推荐系统/index.html","hash":"cf53c7abeb693be8a41970db6293740b7c83682b","modified":1528389076271},{"_id":"public/archives/index.html","hash":"4035c93a9d3e5352eedfd64a1112468f2afe5f49","modified":1528389076271},{"_id":"public/archives/page/2/index.html","hash":"c0f61595b1a093347c81f753c9547e35a263963c","modified":1528389076271},{"_id":"public/archives/page/3/index.html","hash":"3bb7c1c0a54aec9a1b5f282bcb8ae3fc82f9a91e","modified":1528389076271},{"_id":"public/archives/2018/index.html","hash":"c333b53e8b2097a4395d50ba9623d6379cc81b08","modified":1528389076271},{"_id":"public/archives/2018/page/2/index.html","hash":"0417a178b0e53cd6444ea15a2aeb77317a0a539c","modified":1528389076271},{"_id":"public/archives/2018/page/3/index.html","hash":"ebafacd9b0f5d2a401ee6f6e075e6b1bccb91c1e","modified":1528389076271},{"_id":"public/archives/2018/03/index.html","hash":"5d2fe4a7c28e6bbe0da15129d6dad19f9d42abf2","modified":1528389076271},{"_id":"public/archives/2018/04/index.html","hash":"623475cfa4710d0e8d448690634ecc6ee0c6d5ff","modified":1528389076272},{"_id":"public/archives/2018/05/index.html","hash":"dd355c60d21c6ae5b8f76df8c785ba44414f8e37","modified":1528389076272},{"_id":"public/index.html","hash":"d91c2a4efa739ad06543de828e48ac3c2b046c29","modified":1528389076272},{"_id":"public/page/2/index.html","hash":"53edd1c3b192a170116c4d0bc8d183bbcfc7bd3c","modified":1528389076272},{"_id":"public/page/3/index.html","hash":"f93a7371101614c7ffb459c9eea0b63a062fa039","modified":1528389076272},{"_id":"public/tags/Mac/index.html","hash":"1e7f76fa828fd0090036f91aed851862ae102ffb","modified":1528389076272},{"_id":"public/tags/Hexo/index.html","hash":"3371b890bb834e45a2242ae86a0b96beb6178f6a","modified":1528389076272},{"_id":"public/tags/GitHub/index.html","hash":"387aabd965897aebf1e1bbfa6613fa54687f3c93","modified":1528389076272},{"_id":"public/tags/博客/index.html","hash":"45c65f69b699f123a9a902801386b716a4bb29fe","modified":1528389076273},{"_id":"public/tags/Markdown/index.html","hash":"cc41280fcf5257502a54d55bfed47b266b3a6848","modified":1528389076273},{"_id":"public/tags/教程/index.html","hash":"06182f2088e76e8f3f27bceaa0246549d073cc33","modified":1528389076273},{"_id":"public/tags/python/index.html","hash":"f48c4ce424a6a53869274c95ba4cb01808bf9e2d","modified":1528389076273},{"_id":"public/tags/机器学习/index.html","hash":"4b2861f863aecc8a294dc6284f21e0c720d0b600","modified":1528389076273},{"_id":"public/tags/机器学习/page/2/index.html","hash":"ac685aee3a30824ac738b79723ef1be633772c35","modified":1528389076273},{"_id":"public/tags/算法/index.html","hash":"5377b282908d24008467bc02924ec2fc313b60bc","modified":1528389076273},{"_id":"public/tags/算法/page/2/index.html","hash":"d884b0980e2126f3cc10579c7cb9c3022fa68649","modified":1528389076274},{"_id":"public/tags/Python/index.html","hash":"ba77061e62ba948cf2fd37c6b1da7371393f1040","modified":1528389076274},{"_id":"public/tags/推荐系统/index.html","hash":"38d83d0a593543943d69a370c3c3ebfa9c4f8e8a","modified":1528389076274},{"_id":"public/about/index/公众号.jpg","hash":"df033d0e4e9de400ba207e9b6b7f28817d5996d8","modified":1528389076301},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1528389076302},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1528389076302},{"_id":"public/images/avatar.png","hash":"d985c17188c7c966df863e797976d98ce8b40771","modified":1528389076302},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1528389076302},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1528389076302},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1528389076302},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1528389076302},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1528389076302},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1528389076302},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1528389076303},{"_id":"public/images/favicon-16x16-next.png","hash":"78aec0fd86b154f1483a3fddf8bacb4632404bb5","modified":1528389076303},{"_id":"public/images/favicon-32x32-next.png","hash":"78aec0fd86b154f1483a3fddf8bacb4632404bb5","modified":1528389076303},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1528389076303},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1528389076303},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1528389076303},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1528389076303},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1528389076303},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1528389076303},{"_id":"public/CNAME","hash":"5455120583e7a62be755f95d1b53d5efbbec5bf4","modified":1528389076303},{"_id":"public/lib/canvas-nest/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1528389076303},{"_id":"public/lib/canvas-ribbon/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1528389076303},{"_id":"public/robots.txt","hash":"2be28bdb6f77c8f7627632bea984e2b3d735b411","modified":1528389076303},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1528389076303},{"_id":"public/lib/three/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1528389076304},{"_id":"public/lib/fancybox/LICENSE","hash":"8624bcdae55baeef00cd11d5dfcfa60f68710a02","modified":1528389076304},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1528389076304},{"_id":"public/2018/04/19/机器学习之决策树-C4-5算法/机器学习之决策树图片01.png","hash":"94ede414f10507f9d1a5433b3db69e819d36b605","modified":1528389076304},{"_id":"public/2018/04/30/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png","hash":"b99b737fc9473f0ffda6ba8a3c3f225a9bf04908","modified":1528389076304},{"_id":"public/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png","hash":"ad73ccab95e08bb762f941a4630dd3c6a9388a2e","modified":1528389076304},{"_id":"public/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归01.png","hash":"1817f152628ed2d0e63b71a9071cbcb7765edd41","modified":1528389076304},{"_id":"public/2018/03/24/机器学习知识体系/图片01.png","hash":"2242e0694775cb96fac26c3d88feabbc965c0c59","modified":1528389076304},{"_id":"public/2018/03/24/机器学习知识体系/图片02.png","hash":"0a2bb3b98d4750f7b7f7830aefb1347ea1e6414c","modified":1528389076304},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png","hash":"a1324b9077b0916ac0936bf0440e907d4d70e137","modified":1528389076305},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png","hash":"a21052a33e97fc36bd411a2b42eb7abec2ae6736","modified":1528389076305},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png","hash":"c577553d106ea0027b569274eb357e96cda02ed3","modified":1528389076305},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png","hash":"4cccd6b50cdd9d4933ea11bb74c175d27ec3d57f","modified":1528389076305},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png","hash":"8ae7791d2a6604a0ceda324af19292ab54e092f3","modified":1528389076305},{"_id":"public/2018/05/13/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png","hash":"fe51a131f9553b96995b546ec9515dc9a3b59c27","modified":1528389076305},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像01.png","hash":"f5b42a7d64bcf4e42dbddea25f9d91dacd47ae8e","modified":1528389076305},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像02.png","hash":"911aacc5c8458e2a9789dabcf11bee3b8fb08254","modified":1528389076305},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像03.png","hash":"db3af154b414462add94cafcf711c95dc8fce7d2","modified":1528389076305},{"_id":"public/2018/03/24/机器学习之线性回归/图片01.png","hash":"2b715466ee24cacfb4596159b9e5d548c7508a98","modified":1528389076306},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程03.png","hash":"ef9781638e5f177bb192245d906da889af3f6aa3","modified":1528389076306},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程09.png","hash":"56374280cab86a969f853c967201d3d3729cf920","modified":1528389076306},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程10.png","hash":"b89f99c644b86aec73b4f6b0a93566ddd765ad15","modified":1528389076306},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程11.png","hash":"bddde196beb8e1f8b8fe1ec4b9ddb82682953f38","modified":1528389076306},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png","hash":"2c766de1fa96945fbadff336ad36ddb470026c58","modified":1528389076306},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/02.png","hash":"e68d02686e078ee777ecb9c62586843dc0fa4d95","modified":1528389076306},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/03.png","hash":"613cb5e4cfc842bd987ed3a3da446ff779605ff4","modified":1528389076306},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/04.png","hash":"29b910fc5dd61d9dc017a43ebe5fa7835a7cd33c","modified":1528389076307},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/05.png","hash":"abe67138ba5186ca80125b35f68a24ada5da397a","modified":1528389076307},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/06.png","hash":"07003112ee77f408788693ccdb71fa6578d635f2","modified":1528389076307},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/07.png","hash":"7f0e85e07ca1ae562025c8031cfc87eb9a99b51a","modified":1528389076307},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/08.png","hash":"4e3061fc73c47c5040c2920567185f906298fab7","modified":1528389076307},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/09.png","hash":"352b9d0ab5678717465660ca2f7d243ac605166a","modified":1528389076307},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/11.png","hash":"44dfb34c533a659f62e0213a9b6320329503857c","modified":1528389076307},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/13.png","hash":"e10a1daaaac28c0d04bf4a3183e08ae9b5e0b985","modified":1528389076308},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像20.png","hash":"414c76614aa0e1f9b0832aced52ed8e0702fcb1d","modified":1528389076308},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1528389077104},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1528389077110},{"_id":"public/2018/04/30/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png","hash":"7efc5e982bb03590dadca9b1d7bcff20937026a4","modified":1528389077116},{"_id":"public/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png","hash":"305df6ad6e8549a280eda7178c796df59c3efc7a","modified":1528389077116},{"_id":"public/2018/05/12/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png","hash":"8be383416d0e78739d78ee2332876bbc366a75c9","modified":1528389077118},{"_id":"public/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png","hash":"7efc5e982bb03590dadca9b1d7bcff20937026a4","modified":1528389077118},{"_id":"public/2018/03/18/Markdown写作教程/图片01.png","hash":"37bf22348a323e3708cfe540ae74e6d8e89bd4d6","modified":1528389077118},{"_id":"public/2018/03/18/Markdown写作教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077118},{"_id":"public/2018/03/27/机器学习之Logistic回归/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077119},{"_id":"public/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归03.png","hash":"414ee5813fc11417114610e2e6af70a89650f185","modified":1528389077119},{"_id":"public/2018/03/24/机器学习知识体系/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077119},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png","hash":"b44c4687d5db6136f7ad2d8c07ce5f8810315df5","modified":1528389077119},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077119},{"_id":"public/2018/03/24/机器学习之线性回归/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077119},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077120},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程04.png","hash":"54e7eb64aeff4e62baa1f465b9ecd8b0163c298d","modified":1528389077120},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程06.png","hash":"397f01c175a32a327bd0f5e25737bebfb30a429a","modified":1528389077120},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程07.png","hash":"f73d270ea01afc9494a85cb642779e25ceb87aa7","modified":1528389077120},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077120},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077120},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/01.png","hash":"33e230638dfea5aecb04dfbdd3a6099c248ef2c3","modified":1528389077121},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像19.png","hash":"a9ce5acc9a9bd616faf50581b802aa69d4ec5e5d","modified":1528389077121},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像18.png","hash":"3eb66a4726829f310e9317433813bafcae824183","modified":1528389077121},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像21.png","hash":"6c844ab24a7b2fa744f14a70bac46b613fc4cca3","modified":1528389077121},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像22.png","hash":"e9cf3a459468a6696855da633138be48e6ee3d7b","modified":1528389077121},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图像23.png","hash":"84f4972fd31cc00510f9cd5b5b5c23bce1661072","modified":1528389077121},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图片16.png","hash":"f61b216e3015ff8461953b80a41c404ab9c05f7a","modified":1528389077121},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/图片17.png","hash":"05d26db61bbd1b7ad73219630d6423e1c712d91c","modified":1528389077121},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/推广.png","hash":"fdc0ae32fe7c6bcfaf9d72ee6dbe92aeec6b63ea","modified":1528389077122},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1528389077142},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1528389077143},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1528389077143},{"_id":"public/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1528389077143},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1528389077143},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1528389077143},{"_id":"public/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1528389077143},{"_id":"public/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1528389077143},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1528389077143},{"_id":"public/lib/canvas-nest/README.html","hash":"9bb29b415d0859e097fdfcc6689b235c0e699224","modified":1528389077143},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1528389077143},{"_id":"public/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1528389077143},{"_id":"public/lib/canvas-ribbon/README.html","hash":"712f095b72b24b2bafa3548404bb4062c7859304","modified":1528389077144},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1528389077144},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1528389077144},{"_id":"public/lib/fancybox/README.html","hash":"acfa25ca9b6457a4a70a4b35cf72c9dafc33584e","modified":1528389077144},{"_id":"public/lib/three/README.html","hash":"b9ead5e5ec2847bc3ae33c34e5d1a2c39dd747e9","modified":1528389077144},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1528389077144},{"_id":"public/lib/fancybox/source/jquery.fancybox.min.css","hash":"84adea69673c392c1d34a5a316e8e5960aa348b5","modified":1528389077144},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1528389077144},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1528389077144},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1528389077144},{"_id":"public/css/main.css","hash":"42d25c4d0e9a83eb66d4791e10b9f609467b83bf","modified":1528389077144},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1528389077144},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1528389077145},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1528389077145},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1528389077145},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"a5913612c237bb7443c6006a386edd775201d423","modified":1528389077145},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1528389077145},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1528389077145},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1528389077145},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1528389077146},{"_id":"public/lib/fancybox/source/jquery.fancybox.min.js","hash":"b85b75426ff7569d54a1f69689895315e80ed85d","modified":1528389077146},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1528389077146},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"fec45d19179030b8f43ebd0eabc5467892c14136","modified":1528389077146},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"fec45d19179030b8f43ebd0eabc5467892c14136","modified":1528389077146},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1528389077146},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/12.png","hash":"4812b5371a7cb9d823327f8e95dffa5aaf97934b","modified":1528389077146},{"_id":"public/2018/03/18/Markdown写作教程/图片03.png","hash":"244052c1fc5344f9d755cb942517d9d5da14afe3","modified":1528389077146},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png","hash":"2be19ce3efa76780e9a18a93a1dac729b1b7a3f5","modified":1528389077147},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png","hash":"2f7a3455e1fd13aaa582f7274d4c7d50e8fae5d9","modified":1528389077147},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png","hash":"ed89cf8ee5f651f2e1769321716565748b043bc3","modified":1528389077147},{"_id":"public/2018/03/24/机器学习之线性回归/公式01.png","hash":"e3afd52d851957f06f23168c198ccd1814e9de25","modified":1528389077147},{"_id":"public/2018/04/04/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png","hash":"ed008a6f9a39ee597bf965da9cf3349181e88980","modified":1528389077148},{"_id":"public/2018/03/24/机器学习之线性回归/公式03.png","hash":"3ce320dce755145da770585a40c55c22da3d671e","modified":1528389077148},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1528389077148},{"_id":"public/2018/03/24/机器学习之线性回归/图片03.png","hash":"e322d56b0e286a2ca13cbbb470ce25010f8b1c5b","modified":1528389077149},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.3.png","hash":"9c5d6ffa38f3ff94979444e7ac5e38943c2404f8","modified":1528389077169},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程05.png","hash":"256d9aadf22876b4fe5beea1c6fb126d245df262","modified":1528389077169},{"_id":"public/2018/05/18/机器学习之Apriori算法/机器学习之Apriori算法图片01.png","hash":"f8ae81474b95fe79f07ebc5473e45574bec40829","modified":1528389077170},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png","hash":"c6c24fbb1b1f06aee99d1c7ed558d54edf48e1ed","modified":1528389077170},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png","hash":"f6aea31b36824cf4958541fd47067965a5057d81","modified":1528389077170},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png","hash":"8e40abc5b044df8a0b256edbbca383df20e1a485","modified":1528389077171},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png","hash":"28a4092973b644020d1b7b8f4d7ce80619006334","modified":1528389077171},{"_id":"public/2018/03/24/机器学习之线性回归/公式02.png","hash":"09937e4c48b4bfaf597eb85131c7db518c73fce9","modified":1528389077172},{"_id":"public/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png","hash":"578f8b3f574ea4f6969c28ac881582e550b6df8e","modified":1528389077172},{"_id":"public/2018/05/03/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png","hash":"07ef180e8842ac7765b0d07f3f15a4ffb9841dc7","modified":1528389077172},{"_id":"public/2018/03/24/机器学习知识体系/图片03.png","hash":"8d75e5be4fbaa92f0ab0e411159cbfbb201673df","modified":1528389077173},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片7.2.png","hash":"02359d570ab5830d815f61d8fe16ab25179c9c89","modified":1528389077186},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/15.png","hash":"9adb17a88751a53d34588418786ee941c412ee1f","modified":1528389077186},{"_id":"public/2018/03/27/机器学习之Logistic回归/机器学习之Logistic回归02.png","hash":"bfa20ad1c15607d24bd562d56286006dad364d9f","modified":1528389077186},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png","hash":"6cb18abfc66ca9a0570bcd2dd2149a9568714ef2","modified":1528389077187},{"_id":"public/2018/05/09/机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png","hash":"492fd9787fc36e529f216bf8f8453b4b9e4835d1","modified":1528389077187},{"_id":"public/2018/03/24/机器学习之线性回归/图片02.png","hash":"bfa20ad1c15607d24bd562d56286006dad364d9f","modified":1528389077188},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.1.png","hash":"6e71a79873cf1b4ce889c6bfd9bfdf7be1ac9009","modified":1528389077205},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片4.1.png","hash":"faa30796bf710258a987c3277c77de1e5af0e709","modified":1528389077205},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.4.png","hash":"36ce82cebff199c501a9cba78fef0169aae62225","modified":1528389077206},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png","hash":"4d8692aec2e5ac5ead5da05c1833bdef07aa1d6e","modified":1528389077211},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png","hash":"29d5e6087b241a8a4eae416a0997028f28b066e2","modified":1528389077211},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png","hash":"6def4e9f93f518f2210e44d9c7d23aad339ffa56","modified":1528389077212},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png","hash":"11f3b12ef3ea65d97c6a92fffc71d73c094807b4","modified":1528389077213},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png","hash":"840d9444a423bb1e43200f77bf6b2b6b4ef5f200","modified":1528389077213},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png","hash":"4cf2f1a3596e6381019ac52c02a6fb7d78b02ac2","modified":1528389077214},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png","hash":"7f105dfa87769b6894feb53226af206130ae2a0a","modified":1528389077228},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png","hash":"c4aa4e10466b8af8ee430875266251cfd8669822","modified":1528389077229},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png","hash":"562f9f4ef0d0b6fe7b1dbdd274ea417385b2e8cb","modified":1528389077229},{"_id":"public/2018/03/24/机器学习知识体系/图片04.png","hash":"078a6395cfe259faaaafc9509152721c309ae446","modified":1528389077244},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.5.png","hash":"bec5459c79cfa6fdabea06979fe34404781e106b","modified":1528389077244},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片7.1.png","hash":"f70709d5d8b754a9fc2869781dc69a935d711de0","modified":1528389077244},{"_id":"public/2018/03/29/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png","hash":"d8ed9a84f2cb143da24a448a746eb27a6b948110","modified":1528389077247},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/10.png","hash":"edb7d8ae73e7c28e9dc0af865e00422e93ae9fc4","modified":1528389077248},{"_id":"public/2018/04/22/机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png","hash":"c0dda60bc9bb970d061a4e1abb9c12a301b0978d","modified":1528389077263},{"_id":"public/2018/04/19/机器学习之决策树-C4-5算法/机器学习之决策树图片02.png","hash":"a0bd7baea72ce6eb49b8e443fffa61cb4ce8fafc","modified":1528389077263},{"_id":"public/2018/03/14/Python之MatPlotLib使用教程/14.png","hash":"43f3564c467b1029cf25e46c2749b93e408be3cf","modified":1528389077328},{"_id":"public/2018/04/15/Python之Sklearn使用教程/Python之Sklearn使用教程08.png","hash":"12a6e73e9a4c6a21addd5542ebc96addfd889498","modified":1528389077372},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.6.png","hash":"fdfdc025a22244646018a4105b4ffaa6b3ecdef6","modified":1528389077380},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片6.2.png","hash":"34877fc9266906c05387350a9c9d13b0c8411f92","modified":1528389077380},{"_id":"public/2018/03/16/Mac+Hexo+GitHub博客搭建教程/图片3.3.png","hash":"4e466a4caae62c4d477208251f8f9ad259faeebe","modified":1528389077384},{"_id":"public/2018/04/30/机器学习之随机森林/机器学习之随机森林图片01.png","hash":"b17a64c76af269ced0330cc80c64bb4f67f637c1","modified":1528389077395},{"_id":"public/2018/03/18/Markdown写作教程/图片02.png","hash":"4becc8fdd53dbe2c6e7117270d2297a27abb3c2d","modified":1528389077412}],"Category":[{"name":"教程","_id":"cji4rdzd600042e01yy8d7xwr"},{"name":"Python库","_id":"cji4rdzdf000b2e01uio21srh"},{"name":"机器学习","_id":"cji4rdzdq000l2e011q91w2jr"},{"name":"推荐系统","_id":"cji4rdzeb001t2e014nhmc42r"}],"Data":[],"Page":[{"title":"关于","date":"2018-03-13T13:34:44.000Z","comments":1,"_content":"「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...欢迎关注公众号阅读更多内容。\n\n  ![公众号](index/公众号.jpg)","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2018-03-13 21:34:44\ncomments: true\n---\n「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...欢迎关注公众号阅读更多内容。\n\n  ![公众号](index/公众号.jpg)","updated":"2018-05-20T12:48:30.543Z","path":"about/index.html","layout":"page","_id":"cji4rdzcx00002e017gnn5ovo","content":"<p>「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…欢迎关注公众号阅读更多内容。</p>\n<p>  <img src=\"index/公众号.jpg\" alt=\"公众号\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…欢迎关注公众号阅读更多内容。</p>\n<p>  <img src=\"index/公众号.jpg\" alt=\"公众号\"></p>\n"},{"title":"分类","type":"categories","comments":0,"date":"2018-03-13T15:29:19.000Z","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ntype: \"categories\"\ncomments: false\ndate: 2018-03-13 23:29:19\n---\n","updated":"2018-03-13T17:26:32.190Z","path":"categories/index.html","layout":"page","_id":"cji4rdzd200022e01fqzv08xz","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"标签","type":"tags","comments":0,"date":"2018-03-13T15:29:35.000Z","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ntype: \"tags\"\ncomments: false\ndate: 2018-03-13 23:29:35\n---\n","updated":"2018-03-13T17:26:40.814Z","path":"tags/index.html","layout":"page","_id":"cji4rdzd800062e01issg6gyj","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"喜欢","date":"2018-05-20T10:00:03.000Z","_content":"\n## 1.书籍\n\n{% stream %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png 《明朝那些事儿》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png  《沉默的大多数》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%B6%E9%97%B4%E7%AE%80%E5%8F%B2.png  《时间简史》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png  《看见》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png  《瓦尔登湖》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png  《小王子》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png  《围城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png  《挪威的森林》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png  《呼兰河传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png  《边城》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png  《白鹿原》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png  《平凡的世界》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png  《老人与海》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png  《雪国》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png  《千只鹤》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png  [《羊脂球》 ](https://book.douban.com/subject/3144827/) %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png  《解密》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png  《骄傲的印度》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png  《斯里兰卡 》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png  《越禁忌越美丽 》%}\n\n{% figure https://img3.doubanio.com/lpic/s28357056.jpg  《三体 》 %}\n\n{% figure https://img3.doubanio.com/lpic/s1168991.jpg 《阿甘正传》%}\n\n{% figure https://img3.doubanio.com/lpic/s6384944.jpg 《百年孤独》%}\n\n{% figure https://img3.doubanio.com/lpic/s3278363.jpg 《穆斯林的葬礼》%}\n\n{% figure https://img3.doubanio.com/lpic/s3735710.jpg 《岛》%}\n\n{% figure https://img3.doubanio.com/lpic/s11284102.jpg 《霍乱时期的爱情》%}\n\n{% figure https://img3.doubanio.com/lpic/s6509536.jpg 《悟空传》%}\n\n{% figure https://img3.doubanio.com/lpic/s27988606.jpg [《硅谷之火》](https://book.douban.com/subject/26306584/)%}\n\n{% figure https://img3.doubanio.com/lpic/s9034256.jpg [《千年一叹》](https://book.douban.com/subject/6808159/)%}\n\n{% figure https://img1.doubanio.com/lpic/s27226968.jpg 《文化苦旅》%}\n\n{% figure https://img3.doubanio.com/lpic/s1466042.jpg 《狼图腾》%}\n\n{% figure https://img3.doubanio.com/lpic/s27314106.jpg 《生活的艺术》%}\n\n{% figure https://img1.doubanio.com/lpic/s27595957.jpg 《荆棘鸟》%}\n\n{% endstream %}\n\n## 2.电影\n\n{% stream %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png 《凸变英雄Leaf》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png 《妖猫传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png 《暴雪将至》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png 《钢之炼金术师》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png 《十月围城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png 《无间道3》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png 《无间道2》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png 《西西里的美丽传说》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/K.png 《K》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png 《我的英雄学院》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\n《暴裂无声》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\n《南极之恋》%}\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\n《未闻花名》%}\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png 《刻刻》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\n《万物理论》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\n《来自风平浪静的明天》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png 《唐人街探案（二）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png 《捉妖记（二）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg 《疯狂动物城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png 《机器人总动员》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\n《盗梦空间》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\n《摔跤吧！爸爸》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\n《寻梦环游记》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\n《星际穿越》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\n《霸王别姬（1993）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png 《少年派的奇幻漂流》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg 《飞屋环游记》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png 《二十二》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png 《V字仇杀队》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\n《驯龙高手》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\n《让子弹飞》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\n《血战钢锯岭》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\n《超能陆战队》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png 《无敌破坏王》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\n《你的名字》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\n《神偷奶爸》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\n《啪嗒啪嗒》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\n《小羊肖恩》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png 《麦兜当当伴我心》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png 《麦兜我和我妈妈》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png 《哆啦A梦：伴我同行》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png 《西游记之大圣归来》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png 《昆虫总动员》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png  《湄公河行动》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png 《冰雪奇缘》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\n《百鸟朝凤》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png 《魁拔》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\n《老炮儿》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\n《大护法》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\n《喊山》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\n《功夫熊猫》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\n《金蝉脱壳》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\n《绣春刀》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png 《寒战》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\n《人在囧途》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png 《北京遇上西雅图》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png 《诺亚方舟漂流记》%}\n\n{% endstream %}\n\n## 3.音乐\n\n<!-- 胭脂雪-->\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=28660030&auto=1&height=66\"></iframe>\n\n<!-- 红昭愿-->\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=525317068&auto=1&height=66\"></iframe>\n\n","source":"favorite/index.md","raw":"---\ntitle: 喜欢\ndate: 2018-05-20 18:00:03\n---\n\n## 1.书籍\n\n{% stream %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png 《明朝那些事儿》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png  《沉默的大多数》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%B6%E9%97%B4%E7%AE%80%E5%8F%B2.png  《时间简史》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png  《看见》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png  《瓦尔登湖》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png  《小王子》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png  《围城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png  《挪威的森林》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png  《呼兰河传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png  《边城》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png  《白鹿原》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png  《平凡的世界》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png  《老人与海》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png  《雪国》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png  《千只鹤》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png  [《羊脂球》 ](https://book.douban.com/subject/3144827/) %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png  《解密》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png  《骄傲的印度》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png  《斯里兰卡 》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png  《越禁忌越美丽 》%}\n\n{% figure https://img3.doubanio.com/lpic/s28357056.jpg  《三体 》 %}\n\n{% figure https://img3.doubanio.com/lpic/s1168991.jpg 《阿甘正传》%}\n\n{% figure https://img3.doubanio.com/lpic/s6384944.jpg 《百年孤独》%}\n\n{% figure https://img3.doubanio.com/lpic/s3278363.jpg 《穆斯林的葬礼》%}\n\n{% figure https://img3.doubanio.com/lpic/s3735710.jpg 《岛》%}\n\n{% figure https://img3.doubanio.com/lpic/s11284102.jpg 《霍乱时期的爱情》%}\n\n{% figure https://img3.doubanio.com/lpic/s6509536.jpg 《悟空传》%}\n\n{% figure https://img3.doubanio.com/lpic/s27988606.jpg [《硅谷之火》](https://book.douban.com/subject/26306584/)%}\n\n{% figure https://img3.doubanio.com/lpic/s9034256.jpg [《千年一叹》](https://book.douban.com/subject/6808159/)%}\n\n{% figure https://img1.doubanio.com/lpic/s27226968.jpg 《文化苦旅》%}\n\n{% figure https://img3.doubanio.com/lpic/s1466042.jpg 《狼图腾》%}\n\n{% figure https://img3.doubanio.com/lpic/s27314106.jpg 《生活的艺术》%}\n\n{% figure https://img1.doubanio.com/lpic/s27595957.jpg 《荆棘鸟》%}\n\n{% endstream %}\n\n## 2.电影\n\n{% stream %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png 《凸变英雄Leaf》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png 《妖猫传》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png 《暴雪将至》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png 《钢之炼金术师》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png 《十月围城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png 《无间道3》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png 《无间道2》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png 《西西里的美丽传说》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/K.png 《K》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png 《我的英雄学院》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\n《暴裂无声》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\n《南极之恋》%}\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\n《未闻花名》%}\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png 《刻刻》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\n《万物理论》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\n《来自风平浪静的明天》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png 《唐人街探案（二）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png 《捉妖记（二）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg 《疯狂动物城》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png 《机器人总动员》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\n《盗梦空间》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\n《摔跤吧！爸爸》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\n《寻梦环游记》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\n《星际穿越》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\n《霸王别姬（1993）》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png 《少年派的奇幻漂流》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg 《飞屋环游记》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png 《二十二》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png 《V字仇杀队》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\n《驯龙高手》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\n《让子弹飞》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\n《血战钢锯岭》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\n《超能陆战队》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png 《无敌破坏王》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\n《你的名字》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\n《神偷奶爸》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\n《啪嗒啪嗒》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\n《小羊肖恩》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png 《麦兜当当伴我心》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png 《麦兜我和我妈妈》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png 《哆啦A梦：伴我同行》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png 《西游记之大圣归来》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png 《昆虫总动员》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png  《湄公河行动》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png 《冰雪奇缘》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\n《百鸟朝凤》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png 《魁拔》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\n《老炮儿》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\n《大护法》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\n《喊山》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\n《功夫熊猫》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\n《金蝉脱壳》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\n《绣春刀》%}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png 《寒战》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\n《人在囧途》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png 《北京遇上西雅图》 %}\n\n{% figure http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png 《诺亚方舟漂流记》%}\n\n{% endstream %}\n\n## 3.音乐\n\n<!-- 胭脂雪-->\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=28660030&auto=1&height=66\"></iframe>\n\n<!-- 红昭愿-->\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=525317068&auto=1&height=66\"></iframe>\n\n","updated":"2018-06-07T13:10:59.780Z","path":"favorite/index.html","comments":1,"layout":"page","_id":"cji4rdzdb00082e01xsl57rt7","content":"<h2 id=\"1-书籍\"><a href=\"#1-书籍\" class=\"headerlink\" title=\"1.书籍\"></a>1.书籍</h2><script src=\"//cdn.bootcss.com/jquery/2.1.0/jquery.min.js\"></script><script src=\"//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js\"></script><div class=\"hexo-img-stream\"><style type=\"text/css\">.hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}</style>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png\"></noscript><figcaption>《明朝那些事儿》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png\"></noscript><figcaption>《沉默的大多数》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%B6%E9%97%B4%E7%AE%80%E5%8F%B2.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%B6%E9%97%B4%E7%AE%80%E5%8F%B2.png\"></noscript><figcaption>《时间简史》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png\"></noscript><figcaption>《看见》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png\"></noscript><figcaption>《瓦尔登湖》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png\"></noscript><figcaption>《小王子》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png\"></noscript><figcaption>《围城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png\"></noscript><figcaption>《挪威的森林》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png\"></noscript><figcaption>《呼兰河传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png\"></noscript><figcaption>《边城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png\"></noscript><figcaption>《白鹿原》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png\"></noscript><figcaption>《平凡的世界》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png\"></noscript><figcaption>《老人与海》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png\"></noscript><figcaption>《雪国》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png\"></noscript><figcaption>《千只鹤》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png\"></noscript><figcaption><a href=\"https://book.douban.com/subject/3144827/\" target=\"_blank\" rel=\"noopener\">《羊脂球》 </a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png\"></noscript><figcaption>《解密》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png\"></noscript><figcaption>《骄傲的印度》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png\"></noscript><figcaption>《斯里兰卡 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png\"></noscript><figcaption>《越禁忌越美丽 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s28357056.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s28357056.jpg\"></noscript><figcaption>《三体 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s1168991.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s1168991.jpg\"></noscript><figcaption>《阿甘正传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s6384944.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s6384944.jpg\"></noscript><figcaption>《百年孤独》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s3278363.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s3278363.jpg\"></noscript><figcaption>《穆斯林的葬礼》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s3735710.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s3735710.jpg\"></noscript><figcaption>《岛》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s11284102.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s11284102.jpg\"></noscript><figcaption>《霍乱时期的爱情》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s6509536.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s6509536.jpg\"></noscript><figcaption>《悟空传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s27988606.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s27988606.jpg\"></noscript><figcaption><a href=\"https://book.douban.com/subject/26306584/\" target=\"_blank\" rel=\"noopener\">《硅谷之火》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s9034256.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s9034256.jpg\"></noscript><figcaption><a href=\"https://book.douban.com/subject/6808159/\" target=\"_blank\" rel=\"noopener\">《千年一叹》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img1.doubanio.com/lpic/s27226968.jpg\"><noscript><img src=\"https://img1.doubanio.com/lpic/s27226968.jpg\"></noscript><figcaption>《文化苦旅》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s1466042.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s1466042.jpg\"></noscript><figcaption>《狼图腾》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s27314106.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s27314106.jpg\"></noscript><figcaption>《生活的艺术》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img1.doubanio.com/lpic/s27595957.jpg\"><noscript><img src=\"https://img1.doubanio.com/lpic/s27595957.jpg\"></noscript><figcaption>《荆棘鸟》\n</figcaption></figure>\n</div><script type=\"text/javascript\">$('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });</script>\n<h2 id=\"2-电影\"><a href=\"#2-电影\" class=\"headerlink\" title=\"2.电影\"></a>2.电影</h2><script src=\"//cdn.bootcss.com/jquery/2.1.0/jquery.min.js\"></script><script src=\"//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js\"></script><div class=\"hexo-img-stream\"><style type=\"text/css\">.hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}</style>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png\"></noscript><figcaption>《凸变英雄Leaf》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png\"></noscript><figcaption>《妖猫传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png\"></noscript><figcaption>《暴雪将至》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png\"></noscript><figcaption>《钢之炼金术师》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png\"></noscript><figcaption>《十月围城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png\"></noscript><figcaption>《无间道3》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png\"></noscript><figcaption>《无间道2》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png\"></noscript><figcaption>《西西里的美丽传说》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/K.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/K.png\"></noscript><figcaption>《K》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png\"></noscript><figcaption>《我的英雄学院》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\"></noscript><figcaption>《暴裂无声》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\"></noscript><figcaption>《南极之恋》\n</figcaption></figure>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\"></noscript><figcaption>《未闻花名》\n</figcaption></figure>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png\"></noscript><figcaption>《刻刻》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\"></noscript><figcaption>《万物理论》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\"></noscript><figcaption>《来自风平浪静的明天》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png\"></noscript><figcaption>《唐人街探案（二）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png\"></noscript><figcaption>《捉妖记（二）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg\"></noscript><figcaption>《疯狂动物城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png\"></noscript><figcaption>《机器人总动员》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\"></noscript><figcaption>《盗梦空间》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\"></noscript><figcaption>《摔跤吧！爸爸》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\"></noscript><figcaption>《寻梦环游记》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\"></noscript><figcaption>《星际穿越》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\"></noscript><figcaption>《霸王别姬（1993）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png\"></noscript><figcaption>《少年派的奇幻漂流》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg\"></noscript><figcaption>《飞屋环游记》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png\"></noscript><figcaption>《二十二》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png\"></noscript><figcaption>《V字仇杀队》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\"></noscript><figcaption>《驯龙高手》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\"></noscript><figcaption>《让子弹飞》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\"></noscript><figcaption>《血战钢锯岭》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\"></noscript><figcaption>《超能陆战队》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png\"></noscript><figcaption>《无敌破坏王》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\"></noscript><figcaption>《你的名字》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\"></noscript><figcaption>《神偷奶爸》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\"></noscript><figcaption>《啪嗒啪嗒》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\"></noscript><figcaption>《小羊肖恩》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png\"></noscript><figcaption>《麦兜当当伴我心》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png\"></noscript><figcaption>《麦兜我和我妈妈》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png\"></noscript><figcaption>《哆啦A梦：伴我同行》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png\"></noscript><figcaption>《西游记之大圣归来》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png\"></noscript><figcaption>《昆虫总动员》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png\"></noscript><figcaption>《湄公河行动》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png\"></noscript><figcaption>《冰雪奇缘》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\"></noscript><figcaption>《百鸟朝凤》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png\"></noscript><figcaption>《魁拔》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\"></noscript><figcaption>《老炮儿》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\"></noscript><figcaption>《大护法》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\"></noscript><figcaption>《喊山》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\"></noscript><figcaption>《功夫熊猫》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\"></noscript><figcaption>《金蝉脱壳》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\"></noscript><figcaption>《绣春刀》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png\"></noscript><figcaption>《寒战》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\"></noscript><figcaption>《人在囧途》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png\"></noscript><figcaption>《北京遇上西雅图》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png\"></noscript><figcaption>《诺亚方舟漂流记》\n</figcaption></figure>\n</div><script type=\"text/javascript\">$('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });</script>\n<h2 id=\"3-音乐\"><a href=\"#3-音乐\" class=\"headerlink\" title=\"3.音乐\"></a>3.音乐</h2><!-- 胭脂雪-->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=28660030&auto=1&height=66\"></iframe>\n\n<!-- 红昭愿-->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=525317068&auto=1&height=66\"></iframe>\n\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-书籍\"><a href=\"#1-书籍\" class=\"headerlink\" title=\"1.书籍\"></a>1.书籍</h2><script src=\"//cdn.bootcss.com/jquery/2.1.0/jquery.min.js\"></script><script src=\"//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js\"></script><div class=\"hexo-img-stream\"><style type=\"text/css\">.hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}</style>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF.png\"></noscript><figcaption>《明朝那些事儿》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0.png\"></noscript><figcaption>《沉默的大多数》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%B6%E9%97%B4%E7%AE%80%E5%8F%B2.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%B6%E9%97%B4%E7%AE%80%E5%8F%B2.png\"></noscript><figcaption>《时间简史》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9C%8B%E8%A7%81.png\"></noscript><figcaption>《看见》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%93%A6%E5%B0%94%E7%99%BB%E6%B9%96.png\"></noscript><figcaption>《瓦尔登湖》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%8E%8B%E5%AD%90.png\"></noscript><figcaption>《小王子》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%9B%B4%E5%9F%8E.png\"></noscript><figcaption>《围城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97.png\"></noscript><figcaption>《挪威的森林》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%91%BC%E5%85%B0%E6%B2%B3%E4%BC%A0.png\"></noscript><figcaption>《呼兰河传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%BE%B9%E5%9F%8E.png\"></noscript><figcaption>《边城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BD%E9%B9%BF%E5%8E%9F.png\"></noscript><figcaption>《白鹿原》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C.png\"></noscript><figcaption>《平凡的世界》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7.png\"></noscript><figcaption>《老人与海》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9B%AA%E5%9B%BD.png\"></noscript><figcaption>《雪国》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%83%E5%8F%AA%E9%B9%A4.png\"></noscript><figcaption>《千只鹤》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BE%8A%E8%84%82%E7%90%83.png\"></noscript><figcaption><a href=\"https://book.douban.com/subject/3144827/\" target=\"_blank\" rel=\"noopener\">《羊脂球》 </a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A7%A3%E5%AF%86.png\"></noscript><figcaption>《解密》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AA%84%E5%82%B2%E7%9A%84%E5%8D%B0%E5%BA%A6.png\"></noscript><figcaption>《骄傲的印度》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%96%AF%E9%87%8C%E5%85%B0%E5%8D%A1.png\"></noscript><figcaption>《斯里兰卡 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%8A%E7%A6%81%E5%BF%8C%E8%B6%8A%E7%BE%8E%E4%B8%BD.png\"></noscript><figcaption>《越禁忌越美丽 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s28357056.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s28357056.jpg\"></noscript><figcaption>《三体 》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s1168991.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s1168991.jpg\"></noscript><figcaption>《阿甘正传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s6384944.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s6384944.jpg\"></noscript><figcaption>《百年孤独》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s3278363.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s3278363.jpg\"></noscript><figcaption>《穆斯林的葬礼》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s3735710.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s3735710.jpg\"></noscript><figcaption>《岛》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s11284102.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s11284102.jpg\"></noscript><figcaption>《霍乱时期的爱情》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s6509536.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s6509536.jpg\"></noscript><figcaption>《悟空传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s27988606.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s27988606.jpg\"></noscript><figcaption><a href=\"https://book.douban.com/subject/26306584/\" target=\"_blank\" rel=\"noopener\">《硅谷之火》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s9034256.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s9034256.jpg\"></noscript><figcaption><a href=\"https://book.douban.com/subject/6808159/\" target=\"_blank\" rel=\"noopener\">《千年一叹》</a>\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img1.doubanio.com/lpic/s27226968.jpg\"><noscript><img src=\"https://img1.doubanio.com/lpic/s27226968.jpg\"></noscript><figcaption>《文化苦旅》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s1466042.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s1466042.jpg\"></noscript><figcaption>《狼图腾》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img3.doubanio.com/lpic/s27314106.jpg\"><noscript><img src=\"https://img3.doubanio.com/lpic/s27314106.jpg\"></noscript><figcaption>《生活的艺术》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"https://img1.doubanio.com/lpic/s27595957.jpg\"><noscript><img src=\"https://img1.doubanio.com/lpic/s27595957.jpg\"></noscript><figcaption>《荆棘鸟》\n</figcaption></figure>\n</div><script type=\"text/javascript\">$('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });</script>\n<h2 id=\"2-电影\"><a href=\"#2-电影\" class=\"headerlink\" title=\"2.电影\"></a>2.电影</h2><script src=\"//cdn.bootcss.com/jquery/2.1.0/jquery.min.js\"></script><script src=\"//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js\"></script><div class=\"hexo-img-stream\"><style type=\"text/css\">.hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}</style>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%87%B8%E5%8F%98%E8%8B%B1%E9%9B%84Leaf.png\"></noscript><figcaption>《凸变英雄Leaf》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A6%96%E7%8C%AB%E4%BC%A0.png\"></noscript><figcaption>《妖猫传》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E9%9B%AA%E5%B0%86%E8%87%B3.png\"></noscript><figcaption>《暴雪将至》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%92%A2%E4%B9%8B%E7%82%BC%E9%87%91%E6%9C%AF%E5%B8%88.png\"></noscript><figcaption>《钢之炼金术师》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%81%E6%9C%88%E5%9B%B4%E5%9F%8E.png\"></noscript><figcaption>《十月围城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%933.png\"></noscript><figcaption>《无间道3》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E9%97%B4%E9%81%932.png\"></noscript><figcaption>《无间道2》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E8%A5%BF%E9%87%8C%E7%9A%84%E7%BE%8E%E4%B8%BD%E4%BC%A0%E8%AF%B4.png\"></noscript><figcaption>《西西里的美丽传说》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/K.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/K.png\"></noscript><figcaption>《K》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2.png\"></noscript><figcaption>《我的英雄学院》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9A%B4%E8%A3%82%E6%97%A0%E5%A3%B0.png\"></noscript><figcaption>《暴裂无声》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8D%97%E6%9E%81%E4%B9%8B%E6%81%8B.png\"></noscript><figcaption>《南极之恋》\n</figcaption></figure>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.png\"></noscript><figcaption>《未闻花名》\n</figcaption></figure>\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%88%BB%E5%88%BB.png\"></noscript><figcaption>《刻刻》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA.png\"></noscript><figcaption>《万物理论》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9D%A5%E8%87%AA%E9%A3%8E%E5%B9%B3%E6%B5%AA%E9%9D%99%E7%9A%84%E6%98%8E%E5%A4%A9.png\"></noscript><figcaption>《来自风平浪静的明天》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%94%90%E4%BA%BA%E8%A1%97%E6%8E%A2%E6%A1%882.png\"></noscript><figcaption>《唐人街探案（二）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8D%89%E5%A6%96%E8%AE%B02.png\"></noscript><figcaption>《捉妖记（二）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%96%AF%E7%8B%82%E5%8A%A8%E7%89%A9%E5%9F%8E.jpg\"></noscript><figcaption>《疯狂动物城》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%80%BB%E5%8A%A8%E5%91%98.png\"></noscript><figcaption>《机器人总动员》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%9B%97%E6%A2%A6%E7%A9%BA%E9%97%B4.png\"></noscript><figcaption>《盗梦空间》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%91%94%E8%B7%A4%E5%90%A7%EF%BC%81%E7%88%B8%E7%88%B8.png\"></noscript><figcaption>《摔跤吧！爸爸》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0.png\"></noscript><figcaption>《寻梦环游记》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A.png\"></noscript><figcaption>《星际穿越》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%9C%B8%E7%8E%8B%E5%88%AB%E5%A7%AC.png\"></noscript><figcaption>《霸王别姬（1993）》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%91%E5%B9%B4%E6%B4%BE%E7%9A%84%E5%A5%87%E5%B9%BB%E6%BC%82%E6%B5%81.png\"></noscript><figcaption>《少年派的奇幻漂流》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A3%9E%E5%B1%8B%E7%8E%AF%E6%B8%B8%E8%AE%B0.jpg\"></noscript><figcaption>《飞屋环游记》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%8C%E5%8D%81%E4%BA%8C.png\"></noscript><figcaption>《二十二》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/V%E5%AD%97%E4%BB%87%E6%9D%80%E9%98%9F.png\"></noscript><figcaption>《V字仇杀队》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%A9%AF%E9%BE%99%E9%AB%98%E6%89%8B.png\"></noscript><figcaption>《驯龙高手》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AE%A9%E5%AD%90%E5%BC%B9%E9%A3%9E.png\"></noscript><figcaption>《让子弹飞》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A1%80%E6%88%98%E9%92%A2%E9%94%AF%E5%B2%AD.png\"></noscript><figcaption>《血战钢锯岭》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%B6%85%E8%83%BD%E9%99%86%E6%88%98%E9%98%9F.png\"></noscript><figcaption>《超能陆战队》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%97%A0%E6%95%8C%E7%A0%B4%E5%9D%8F%E7%8E%8B.png\"></noscript><figcaption>《无敌破坏王》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BD%A0%E5%BE%97%E5%90%8D%E5%AD%97.png\"></noscript><figcaption>《你的名字》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%A5%9E%E5%81%B7%E5%A5%B6%E7%88%B8.png\"></noscript><figcaption>《神偷奶爸》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%95%AA%E5%97%92%E5%95%AA%E5%97%92.png\"></noscript><figcaption>《啪嗒啪嗒》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%B0%8F%E7%BE%8A%E8%82%96%E6%81%A9.png\"></noscript><figcaption>《小羊肖恩》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E5%BD%93%E5%BD%93%E4%BC%B4%E6%88%91%E5%BF%83.png\"></noscript><figcaption>《麦兜当当伴我心》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%BA%A6%E5%85%9C%E6%88%91%E5%92%8C%E6%88%91%E5%A6%88%E5%A6%88.png\"></noscript><figcaption>《麦兜我和我妈妈》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%93%86%E5%95%A6A%E6%A2%A6%EF%BC%9A%E4%BC%B4%E6%88%91%E5%90%8C%E8%A1%8C.png\"></noscript><figcaption>《哆啦A梦：伴我同行》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%A5%BF%E6%B8%B8%E8%AE%B0%E4%B9%8B%E5%A4%A7%E5%9C%A3%E5%BD%92%E6%9D%A5.png\"></noscript><figcaption>《西游记之大圣归来》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%98%86%E8%99%AB%E6%80%BB%E5%8A%A8%E5%91%98.png\"></noscript><figcaption>《昆虫总动员》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%B9%84%E5%85%AC%E6%B2%B3%E8%A1%8C%E5%8A%A8.png\"></noscript><figcaption>《湄公河行动》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%86%B0%E9%9B%AA%E5%A5%87%E7%BC%98.png\"></noscript><figcaption>《冰雪奇缘》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%99%BE%E9%B8%9F%E6%9C%9D%E5%87%A4.png\"></noscript><figcaption>《百鸟朝凤》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%AD%81%E6%8B%94.png\"></noscript><figcaption>《魁拔》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%80%81%E7%82%AE%E5%84%BF.png\"></noscript><figcaption>《老炮儿》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%A4%A7%E6%8A%A4%E6%B3%95.png\"></noscript><figcaption>《大护法》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%96%8A%E5%B1%B1.png\"></noscript><figcaption>《喊山》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8A%9F%E5%A4%AB%E7%86%8A%E7%8C%AB.png\"></noscript><figcaption>《功夫熊猫》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E9%87%91%E8%9D%89%E8%84%B1%E5%A3%B3.png\"></noscript><figcaption>《金蝉脱壳》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E7%BB%A3%E6%98%A5%E5%88%80.png\"></noscript><figcaption>《绣春刀》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%AF%92%E6%88%98.png\"></noscript><figcaption>《寒战》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E4%BA%BA%E5%9C%A8%E5%9B%A7%E9%80%94.png\"></noscript><figcaption>《人在囧途》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E5%8C%97%E4%BA%AC%E9%81%87%E4%B8%8A%E8%A5%BF%E9%9B%85%E5%9B%BE.png\"></noscript><figcaption>《北京遇上西雅图》\n</figcaption></figure>\n\n<figure><img class=\"hexo-image-steam-lazy nofancy\" src=\"https://ws4.sinaimg.cn/large/e724cbefgw1etyppy7bgwg2001001017.gif\" data-original=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png\"><noscript><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E6%BC%82%E6%B5%81%E8%AE%B0.png\"></noscript><figcaption>《诺亚方舟漂流记》\n</figcaption></figure>\n</div><script type=\"text/javascript\">$('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });</script>\n<h2 id=\"3-音乐\"><a href=\"#3-音乐\" class=\"headerlink\" title=\"3.音乐\"></a>3.音乐</h2><!-- 胭脂雪-->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=28660030&auto=1&height=66\"></iframe>\n\n<!-- 红昭愿-->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=525317068&auto=1&height=66\"></iframe>\n\n"}],"Post":[{"title":"Mac+Hexo+GitHub博客搭建教程","date":"2018-03-16T03:41:35.000Z","toc":true,"comments":1,"_content":"\n### 1.为什么写博客\n\n以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。\n\n### 2.Mac+Hexo+GitHub博客\n\n现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。\n\n+ Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。\n+ Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。\n\n选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是*百度爬虫无法爬去博客内容*，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。\n\n### 3.博客本地环境搭建\n\n#### 3.1安装Node.js和Git\n\nMac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录*/usr/local/bin*目录下。测试Node.js和npm，出现下述信息则安装成功。\n\n```\nnode -v\nv8.10.0\n```\n\n```\nnpm -v\n5.6.0\n```\n\nGit官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。\n\n> Git --version \n>\n> git version 2.15.0\n\n#### 3.2安装Hexo\n\nNode.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。\n\n```mac\nsudo npm install -g hexo\n```\n\n#### 3.3博客初始化\n\n创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。\n\n```\ncd myblog\n```\n\n执行下述命令初始化本地博客，下载一些列文件。\n\n```\nhexo init\n```\n\n执行下述命令安装npm。\n\n```\nsudo npm install\n```\n\n执行下述命令生成本地html文件并开启服务器，然后通过http://localhost:4000查看本地博客。\n\n```\nhexo g\nhexo s\n```\n\n![图片3.3](Mac+Hexo+GitHub博客搭建教程/图片3.3.png)\n\n### 4.本地博客关联GitHub\n\n#### 4.1本地博客代码上传GitHub\n\n注册并登陆GitHub账号后，新建仓库，名称必须为`user.github.io`，如`weizhixiaoyi.github.io`。\n\n![图片01](Mac+Hexo+GitHub博客搭建教程/图片4.1.png)\n\n终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。\n\n```Vim\nvim _config.yml\n```\n\n打开后至文档最后部分，将deploy配置如下。\n\n```Python\ndeploy:\n  type: git\n  repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git\n  branch: master\n```\n\n其中将repository中`weizhixiaoyi`改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。\n\n```\nhexo g\nhexo d\n```\n\n若执行`hexo g`出错则执行`npm install hexo --save`，若执行`hexo d`出错则执行`npm install hexo-deployer-git --save `。错误修正后再次执行`hexo g`和`hexo d`。\n\n若未关联GitHub，执行`hexo d`时会提示输入GitHub账号用户名和密码，即:\n\n```\nusername for 'https://github.com':\npassword for 'https://github.com':\n```\n\n`hexo d`执行成功后便可通过https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。\n\n#### 4.2添加ssh keys到GitHub\n\n添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。\n\n执行下述命令生成新的ssh key，将`your_email@example.com`改成自己以注册的GitHub邮箱地址。默认会在`~/.ssh/id_rsa.pub`中生成`id_rsa`和`id_rsa.pub`文件。\n\n```\nssh-keygen -t rsa -C \"your_email@exampl\"\t\t\n```\n\nMac下利用`open ~/.ssh  `打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key`路径GitHub->Setting->SSH keys->add SSH key`界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。\n\n此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过`hexo g`和`hexo d`便可更新到GitHub之中，通过https://weizhixiaoyi.github.io访问便可看到更新内容。\n\n### 5.更换Hexo主题\n\n可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。\n\n终端cd到myblog目录下执行如下所示命令。\n\n```\ngit clone https://github.com/iissnan/hexo-theme-next themes/next\n```\n\n将blog目录下_config.yml里的theme的名称`landscape`更改为`next`。\n\n执行如下命令（每次部署文章的步骤）\n\n```\nhexo g  //生成缓存和静态文件\nhexo d  //重新部署到服务器\n```\n\n当本地博客部署到服务器后，网页端无变化时可以采用下述命令。\n\n```\nhexo clean  //清楚缓存文件(db.json)和已生成的静态文件(public)\n```\n\n### 6.配置Hexo-theme-next主题\n\nHexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问[next官网](http://theme-next.iissnan.com/)查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.1.png)\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.2.png)\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.3.png)\n\n#### 6.1增加标签、分类、归档页\n\n首先将next/config.yml文件中将`menu`中`tags` ` catagories` `archive`前面的`#`。例如增加标签页，通过`hexo new page 'tags'`增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为`tags`。利用`hexo g`和`hexo d`将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。\n\n#### 6.2增加喜欢界面\n\n喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。\n\n从GitHub上https://github.com/weizhixiaoyi 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。\n\n```Query\nimage_stream:\n\tjquery: false\n```\n\n在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。\n\n```\nmenu:\n  home: / || home\n  about: /about/ || user\n  favorite: /favorite/ || heart\n  tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive\n```\n\n然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。\n\n```\n{% stream %}\n\n{% figure https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg\n[《万物理论》]（https://movie.douban.com/subject/24815950/）%}\n\n{% endstream %}\n```\n#### 6.3文章阅读统计\n\n文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。\n\n注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。\n\n```\nleancloud_visitors:\n  enable: true \n  app_id: Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz #<app_id>\n  app_key: qJejurdHKM06N75OQedX4SDK #<app_key>\n```\n\n![图片6.4](Mac+Hexo+GitHub博客搭建教程/图片6.4.png)\n\n#### 6.4增加百度统计\n\n百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到`代码获取`。\n\n```javascript\n<script>\nvar _hmt = _hmt || [];\n(function() {\n  var hm = document.createElement(\"script\");\n  hm.src = \"https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645\";\n  var s = document.getElementsByTagName(\"script\")[0]; \n  s.parentNode.insertBefore(hm, s);\n})();\n</script>\n```\n\n将代码中`b54e835b3551fd0696954b3aedf5d645`复制到next主题_config.yml的`baidu_analytics`中。接下来通过`代码安装检查`来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。\n\n![图片6.5](Mac+Hexo+GitHub博客搭建教程/图片6.5.png)\n\n#### 6.4增加评论功能\n\n多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。\n\n![图片6.6](Mac+Hexo+GitHub博客搭建教程/图片6.6.png)进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。`编写文章时应在头部添加comments: true`\n\n### 7.绑定个人域名\n\n现在使用的域名`weizhixiaoyi.github.io`是github提供的二级域名，也可绑定自己的个性域名`weizhixiaoyi.com`。域名是在阿里云购买，年费为55元，也可以在狗爹`https://sg.godaddy.com`购买，购买好域名之后便可以直接解析。\n\n#### 7.1GitHub端\n\n在next主题中source文件夹中创建`CNAME`文件，没有后缀名，然后将个人域名`weizhixiaoyi.com`添加进`CNAME`文件即可，然后通过`hexo g` `hexo d`重新部署网站。\n\n#### 7.2域名解析\n\n如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。\n\n+ 记录类型：CNAME\n+ 主机记录：@\n+ 解析线路：默认\n+ 记录值：weizhixiaoyi.github.io\n\n解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。\n\n### 7.博客SEO优化\n\nSEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。\n\n#### 7.1确认收录情况\n\n在谷歌上搜索`site:weizhixiaoyi.com`，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。\n\n![图片7.1](Mac+Hexo+GitHub博客搭建教程/图片7.1.png)\n\n#### 7.1网站身份验证\n\n验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过`shadowsock`+`搬瓦工`自行搭建。\n\n进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。\n\n```\n{% if theme.google_site_verification %}\n  <meta name=\"google-site-verification\" content=\"E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo\" />\n{% endif %}\n```\n\n然后回到`myblog`文件夹下将_config.yml中google_site_vertification设置为`true`。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用`hexo g`和`hexo d`更新博客内容，至此网站身份验证结束。\n\n#### 7.2添加Sitemap\n\nsitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。\n\n首先安装针对谷歌的插件`npm install hexo-generator-sitemap --save`，然后进入`myblog`文件夹下将`sitemap`设置如下。\n\n```\n# sitemap\nsitemap:\n  path: sitemap.xml\n```\n\n#### 7.3谷歌收录博客\n\n谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过`site:weizhixiaoyi.com`能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。\n\n![图片7.2](Mac+Hexo+GitHub博客搭建教程/图片7.2.png)\n\n另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。\n\n### 8.ToDoList\n\n+ 寻找更好的方法解决百度爬虫无法爬取博客内容的问题\n+ 博客增加转发功能\n\n------\n\n### 9.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Mac+Hexo+GitHub博客搭建教程/推广.png)","source":"_posts/Mac+Hexo+GitHub博客搭建教程.md","raw":"---\ntitle: Mac+Hexo+GitHub博客搭建教程\ndate: 2018-03-16 11:41:35\ntags: [Mac,Hexo,GitHub,博客]\ncategories: 教程\ntoc: true\ncomments: true\n---\n\n### 1.为什么写博客\n\n以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。\n\n### 2.Mac+Hexo+GitHub博客\n\n现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。\n\n+ Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。\n+ Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。\n\n选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是*百度爬虫无法爬去博客内容*，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。\n\n### 3.博客本地环境搭建\n\n#### 3.1安装Node.js和Git\n\nMac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录*/usr/local/bin*目录下。测试Node.js和npm，出现下述信息则安装成功。\n\n```\nnode -v\nv8.10.0\n```\n\n```\nnpm -v\n5.6.0\n```\n\nGit官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。\n\n> Git --version \n>\n> git version 2.15.0\n\n#### 3.2安装Hexo\n\nNode.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。\n\n```mac\nsudo npm install -g hexo\n```\n\n#### 3.3博客初始化\n\n创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。\n\n```\ncd myblog\n```\n\n执行下述命令初始化本地博客，下载一些列文件。\n\n```\nhexo init\n```\n\n执行下述命令安装npm。\n\n```\nsudo npm install\n```\n\n执行下述命令生成本地html文件并开启服务器，然后通过http://localhost:4000查看本地博客。\n\n```\nhexo g\nhexo s\n```\n\n![图片3.3](Mac+Hexo+GitHub博客搭建教程/图片3.3.png)\n\n### 4.本地博客关联GitHub\n\n#### 4.1本地博客代码上传GitHub\n\n注册并登陆GitHub账号后，新建仓库，名称必须为`user.github.io`，如`weizhixiaoyi.github.io`。\n\n![图片01](Mac+Hexo+GitHub博客搭建教程/图片4.1.png)\n\n终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。\n\n```Vim\nvim _config.yml\n```\n\n打开后至文档最后部分，将deploy配置如下。\n\n```Python\ndeploy:\n  type: git\n  repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git\n  branch: master\n```\n\n其中将repository中`weizhixiaoyi`改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。\n\n```\nhexo g\nhexo d\n```\n\n若执行`hexo g`出错则执行`npm install hexo --save`，若执行`hexo d`出错则执行`npm install hexo-deployer-git --save `。错误修正后再次执行`hexo g`和`hexo d`。\n\n若未关联GitHub，执行`hexo d`时会提示输入GitHub账号用户名和密码，即:\n\n```\nusername for 'https://github.com':\npassword for 'https://github.com':\n```\n\n`hexo d`执行成功后便可通过https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。\n\n#### 4.2添加ssh keys到GitHub\n\n添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。\n\n执行下述命令生成新的ssh key，将`your_email@example.com`改成自己以注册的GitHub邮箱地址。默认会在`~/.ssh/id_rsa.pub`中生成`id_rsa`和`id_rsa.pub`文件。\n\n```\nssh-keygen -t rsa -C \"your_email@exampl\"\t\t\n```\n\nMac下利用`open ~/.ssh  `打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key`路径GitHub->Setting->SSH keys->add SSH key`界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。\n\n此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过`hexo g`和`hexo d`便可更新到GitHub之中，通过https://weizhixiaoyi.github.io访问便可看到更新内容。\n\n### 5.更换Hexo主题\n\n可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。\n\n终端cd到myblog目录下执行如下所示命令。\n\n```\ngit clone https://github.com/iissnan/hexo-theme-next themes/next\n```\n\n将blog目录下_config.yml里的theme的名称`landscape`更改为`next`。\n\n执行如下命令（每次部署文章的步骤）\n\n```\nhexo g  //生成缓存和静态文件\nhexo d  //重新部署到服务器\n```\n\n当本地博客部署到服务器后，网页端无变化时可以采用下述命令。\n\n```\nhexo clean  //清楚缓存文件(db.json)和已生成的静态文件(public)\n```\n\n### 6.配置Hexo-theme-next主题\n\nHexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问[next官网](http://theme-next.iissnan.com/)查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.1.png)\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.2.png)\n\n![图片6.1](Mac+Hexo+GitHub博客搭建教程/图片6.3.png)\n\n#### 6.1增加标签、分类、归档页\n\n首先将next/config.yml文件中将`menu`中`tags` ` catagories` `archive`前面的`#`。例如增加标签页，通过`hexo new page 'tags'`增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为`tags`。利用`hexo g`和`hexo d`将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。\n\n#### 6.2增加喜欢界面\n\n喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。\n\n从GitHub上https://github.com/weizhixiaoyi 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。\n\n```Query\nimage_stream:\n\tjquery: false\n```\n\n在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。\n\n```\nmenu:\n  home: / || home\n  about: /about/ || user\n  favorite: /favorite/ || heart\n  tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive\n```\n\n然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。\n\n```\n{% stream %}\n\n{% figure https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg\n[《万物理论》]（https://movie.douban.com/subject/24815950/）%}\n\n{% endstream %}\n```\n#### 6.3文章阅读统计\n\n文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。\n\n注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。\n\n```\nleancloud_visitors:\n  enable: true \n  app_id: Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz #<app_id>\n  app_key: qJejurdHKM06N75OQedX4SDK #<app_key>\n```\n\n![图片6.4](Mac+Hexo+GitHub博客搭建教程/图片6.4.png)\n\n#### 6.4增加百度统计\n\n百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到`代码获取`。\n\n```javascript\n<script>\nvar _hmt = _hmt || [];\n(function() {\n  var hm = document.createElement(\"script\");\n  hm.src = \"https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645\";\n  var s = document.getElementsByTagName(\"script\")[0]; \n  s.parentNode.insertBefore(hm, s);\n})();\n</script>\n```\n\n将代码中`b54e835b3551fd0696954b3aedf5d645`复制到next主题_config.yml的`baidu_analytics`中。接下来通过`代码安装检查`来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。\n\n![图片6.5](Mac+Hexo+GitHub博客搭建教程/图片6.5.png)\n\n#### 6.4增加评论功能\n\n多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。\n\n![图片6.6](Mac+Hexo+GitHub博客搭建教程/图片6.6.png)进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。`编写文章时应在头部添加comments: true`\n\n### 7.绑定个人域名\n\n现在使用的域名`weizhixiaoyi.github.io`是github提供的二级域名，也可绑定自己的个性域名`weizhixiaoyi.com`。域名是在阿里云购买，年费为55元，也可以在狗爹`https://sg.godaddy.com`购买，购买好域名之后便可以直接解析。\n\n#### 7.1GitHub端\n\n在next主题中source文件夹中创建`CNAME`文件，没有后缀名，然后将个人域名`weizhixiaoyi.com`添加进`CNAME`文件即可，然后通过`hexo g` `hexo d`重新部署网站。\n\n#### 7.2域名解析\n\n如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。\n\n+ 记录类型：CNAME\n+ 主机记录：@\n+ 解析线路：默认\n+ 记录值：weizhixiaoyi.github.io\n\n解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。\n\n### 7.博客SEO优化\n\nSEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。\n\n#### 7.1确认收录情况\n\n在谷歌上搜索`site:weizhixiaoyi.com`，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。\n\n![图片7.1](Mac+Hexo+GitHub博客搭建教程/图片7.1.png)\n\n#### 7.1网站身份验证\n\n验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过`shadowsock`+`搬瓦工`自行搭建。\n\n进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。\n\n```\n{% if theme.google_site_verification %}\n  <meta name=\"google-site-verification\" content=\"E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo\" />\n{% endif %}\n```\n\n然后回到`myblog`文件夹下将_config.yml中google_site_vertification设置为`true`。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用`hexo g`和`hexo d`更新博客内容，至此网站身份验证结束。\n\n#### 7.2添加Sitemap\n\nsitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。\n\n首先安装针对谷歌的插件`npm install hexo-generator-sitemap --save`，然后进入`myblog`文件夹下将`sitemap`设置如下。\n\n```\n# sitemap\nsitemap:\n  path: sitemap.xml\n```\n\n#### 7.3谷歌收录博客\n\n谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过`site:weizhixiaoyi.com`能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。\n\n![图片7.2](Mac+Hexo+GitHub博客搭建教程/图片7.2.png)\n\n另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。\n\n### 8.ToDoList\n\n+ 寻找更好的方法解决百度爬虫无法爬取博客内容的问题\n+ 博客增加转发功能\n\n------\n\n### 9.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Mac+Hexo+GitHub博客搭建教程/推广.png)","slug":"Mac+Hexo+GitHub博客搭建教程","published":1,"updated":"2018-03-17T10:23:04.618Z","layout":"post","photos":[],"link":"","_id":"cji4rdzcz00012e01ln70yhxw","content":"<h3 id=\"1-为什么写博客\"><a href=\"#1-为什么写博客\" class=\"headerlink\" title=\"1.为什么写博客\"></a>1.为什么写博客</h3><p>以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。</p>\n<h3 id=\"2-Mac-Hexo-GitHub博客\"><a href=\"#2-Mac-Hexo-GitHub博客\" class=\"headerlink\" title=\"2.Mac+Hexo+GitHub博客\"></a>2.Mac+Hexo+GitHub博客</h3><p>现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。</p>\n<ul>\n<li>Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。</li>\n<li>Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。</li>\n</ul>\n<p>选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是<em>百度爬虫无法爬去博客内容</em>，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。</p>\n<h3 id=\"3-博客本地环境搭建\"><a href=\"#3-博客本地环境搭建\" class=\"headerlink\" title=\"3.博客本地环境搭建\"></a>3.博客本地环境搭建</h3><h4 id=\"3-1安装Node-js和Git\"><a href=\"#3-1安装Node-js和Git\" class=\"headerlink\" title=\"3.1安装Node.js和Git\"></a>3.1安装Node.js和Git</h4><p>Mac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录<em>/usr/local/bin</em>目录下。测试Node.js和npm，出现下述信息则安装成功。</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">node</span> <span class=\"title\">-v</span></span><br><span class=\"line\">v8.<span class=\"number\">10.0</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-tag\">npm</span> <span class=\"selector-tag\">-v</span></span><br><span class=\"line\">5<span class=\"selector-class\">.6</span><span class=\"selector-class\">.0</span></span><br></pre></td></tr></table></figure>\n<p>Git官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。</p>\n<blockquote>\n<p>Git –version </p>\n<p>git version 2.15.0</p>\n</blockquote>\n<h4 id=\"3-2安装Hexo\"><a href=\"#3-2安装Hexo\" class=\"headerlink\" title=\"3.2安装Hexo\"></a>3.2安装Hexo</h4><p>Node.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm install -g hexo</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-3博客初始化\"><a href=\"#3-3博客初始化\" class=\"headerlink\" title=\"3.3博客初始化\"></a>3.3博客初始化</h4><p>创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> myblog</span><br></pre></td></tr></table></figure>\n<p>执行下述命令初始化本地博客，下载一些列文件。</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo init</span></span><br></pre></td></tr></table></figure>\n<p>执行下述命令安装npm。</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm <span class=\"keyword\">install</span></span><br></pre></td></tr></table></figure>\n<p>执行下述命令生成本地html文件并开启服务器，然后通过<a href=\"http://localhost:4000查看本地博客。\" target=\"_blank\" rel=\"noopener\">http://localhost:4000查看本地博客。</a></p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo g</span></span><br><span class=\"line\"><span class=\"attribute\">hexo s</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片3.3.png\" alt=\"图片3.3\"></p>\n<h3 id=\"4-本地博客关联GitHub\"><a href=\"#4-本地博客关联GitHub\" class=\"headerlink\" title=\"4.本地博客关联GitHub\"></a>4.本地博客关联GitHub</h3><h4 id=\"4-1本地博客代码上传GitHub\"><a href=\"#4-1本地博客代码上传GitHub\" class=\"headerlink\" title=\"4.1本地博客代码上传GitHub\"></a>4.1本地博客代码上传GitHub</h4><p>注册并登陆GitHub账号后，新建仓库，名称必须为<code>user.github.io</code>，如<code>weizhixiaoyi.github.io</code>。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片4.1.png\" alt=\"图片01\"></p>\n<p>终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">vim</span> _config.yml</span><br></pre></td></tr></table></figure>\n<p>打开后至文档最后部分，将deploy配置如下。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure>\n<p>其中将repository中<code>weizhixiaoyi</code>改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo g</span></span><br><span class=\"line\"><span class=\"attribute\">hexo d</span></span><br></pre></td></tr></table></figure>\n<p>若执行<code>hexo g</code>出错则执行<code>npm install hexo --save</code>，若执行<code>hexo d</code>出错则执行<code>npm install hexo-deployer-git --save</code>。错误修正后再次执行<code>hexo g</code>和<code>hexo d</code>。</p>\n<p>若未关联GitHub，执行<code>hexo d</code>时会提示输入GitHub账号用户名和密码，即:</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">username <span class=\"keyword\">for</span> <span class=\"symbol\">'https</span>:<span class=\"comment\">//github.com':</span></span><br><span class=\"line\">password <span class=\"keyword\">for</span> <span class=\"symbol\">'https</span>:<span class=\"comment\">//github.com':</span></span><br></pre></td></tr></table></figure>\n<p><code>hexo d</code>执行成功后便可通过<a href=\"https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。\" target=\"_blank\" rel=\"noopener\">https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。</a></p>\n<h4 id=\"4-2添加ssh-keys到GitHub\"><a href=\"#4-2添加ssh-keys到GitHub\" class=\"headerlink\" title=\"4.2添加ssh keys到GitHub\"></a>4.2添加ssh keys到GitHub</h4><p>添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。</p>\n<p>执行下述命令生成新的ssh key，将<a href=\"mailto:`your_email@example.com\" target=\"_blank\" rel=\"noopener\">`your_email@example.com</a><code>改成自己以注册的GitHub邮箱地址。默认会在</code>~/.ssh/id_rsa.pub<code>中生成</code>id_rsa<code>和</code>id_rsa.pub`文件。</p>\n<figure class=\"highlight excel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -<span class=\"built_in\">t</span> rsa -C <span class=\"string\">\"your_email@exampl\"</span></span><br></pre></td></tr></table></figure>\n<p>Mac下利用<code>open ~/.ssh</code>打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key<code>路径GitHub-&gt;Setting-&gt;SSH keys-&gt;add SSH key</code>界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。</p>\n<p>此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过<code>hexo g</code>和<code>hexo d</code>便可更新到GitHub之中，通过<a href=\"https://weizhixiaoyi.github.io访问便可看到更新内容。\" target=\"_blank\" rel=\"noopener\">https://weizhixiaoyi.github.io访问便可看到更新内容。</a></p>\n<h3 id=\"5-更换Hexo主题\"><a href=\"#5-更换Hexo主题\" class=\"headerlink\" title=\"5.更换Hexo主题\"></a>5.更换Hexo主题</h3><p>可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。</p>\n<p>终端cd到myblog目录下执行如下所示命令。</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone http<span class=\"variable\">s:</span>//github.<span class=\"keyword\">com</span>/iissnan/hexo-theme-<span class=\"keyword\">next</span> themes/<span class=\"keyword\">next</span></span><br></pre></td></tr></table></figure>\n<p>将blog目录下_config.yml里的theme的名称<code>landscape</code>更改为<code>next</code>。</p>\n<p>执行如下命令（每次部署文章的步骤）</p>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g  <span class=\"comment\">//生成缓存和静态文件</span></span><br><span class=\"line\">hexo d  <span class=\"comment\">//重新部署到服务器</span></span><br></pre></td></tr></table></figure>\n<p>当本地博客部署到服务器后，网页端无变化时可以采用下述命令。</p>\n<figure class=\"highlight x86asm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean  //清楚缓存文件(<span class=\"built_in\">db</span>.json)和已生成的静态文件(<span class=\"meta\">public</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-配置Hexo-theme-next主题\"><a href=\"#6-配置Hexo-theme-next主题\" class=\"headerlink\" title=\"6.配置Hexo-theme-next主题\"></a>6.配置Hexo-theme-next主题</h3><p>Hexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问<a href=\"http://theme-next.iissnan.com/\" target=\"_blank\" rel=\"noopener\">next官网</a>查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.1.png\" alt=\"图片6.1\"></p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.2.png\" alt=\"图片6.1\"></p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.3.png\" alt=\"图片6.1\"></p>\n<h4 id=\"6-1增加标签、分类、归档页\"><a href=\"#6-1增加标签、分类、归档页\" class=\"headerlink\" title=\"6.1增加标签、分类、归档页\"></a>6.1增加标签、分类、归档页</h4><p>首先将next/config.yml文件中将<code>menu</code>中<code>tags</code> <code>catagories</code> <code>archive</code>前面的<code>#</code>。例如增加标签页，通过<code>hexo new page &#39;tags&#39;</code>增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为<code>tags</code>。利用<code>hexo g</code>和<code>hexo d</code>将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。</p>\n<h4 id=\"6-2增加喜欢界面\"><a href=\"#6-2增加喜欢界面\" class=\"headerlink\" title=\"6.2增加喜欢界面\"></a>6.2增加喜欢界面</h4><p>喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。</p>\n<p>从GitHub上<a href=\"https://github.com/weizhixiaoyi\" target=\"_blank\" rel=\"noopener\">https://github.com/weizhixiaoyi</a> 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">image_stream:</span><br><span class=\"line\">\tjquery: false</span><br></pre></td></tr></table></figure>\n<p>在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。</p>\n<figure class=\"highlight dts\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">menu:</span></span><br><span class=\"line\"><span class=\"symbol\">  home:</span> / || home</span><br><span class=\"line\"><span class=\"symbol\">  about:</span> <span class=\"meta-keyword\">/about/</span> || user</span><br><span class=\"line\"><span class=\"symbol\">  favorite:</span> <span class=\"meta-keyword\">/favorite/</span> || heart</span><br><span class=\"line\"><span class=\"symbol\">  tags:</span> <span class=\"meta-keyword\">/tags/</span> || tags</span><br><span class=\"line\"><span class=\"symbol\">  categories:</span> <span class=\"meta-keyword\">/categories/</span> || th</span><br><span class=\"line\"><span class=\"symbol\">  archives:</span> <span class=\"meta-keyword\">/archives/</span> || archive</span><br></pre></td></tr></table></figure>\n<p>然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">stream</span> %&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">figure</span> https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg</span></span><br><span class=\"line\"><span class=\"template-tag\">[《万物理论》]（https://movie.douban.com/subject/24815950/）%&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">endstream</span> %&#125;</span><span class=\"xml\"></span></span><br></pre></td></tr></table></figure>\n<h4 id=\"6-3文章阅读统计\"><a href=\"#6-3文章阅读统计\" class=\"headerlink\" title=\"6.3文章阅读统计\"></a>6.3文章阅读统计</h4><p>文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。</p>\n<p>注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">leancloud_visitors:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span> </span><br><span class=\"line\"><span class=\"attr\">  app_id:</span> <span class=\"string\">Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz</span> <span class=\"comment\">#&lt;app_id&gt;</span></span><br><span class=\"line\"><span class=\"attr\">  app_key:</span> <span class=\"string\">qJejurdHKM06N75OQedX4SDK</span> <span class=\"comment\">#&lt;app_key&gt;</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.4.png\" alt=\"图片6.4\"></p>\n<h4 id=\"6-4增加百度统计\"><a href=\"#6-4增加百度统计\" class=\"headerlink\" title=\"6.4增加百度统计\"></a>6.4增加百度统计</h4><p>百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到<code>代码获取</code>。</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;script&gt;</span><br><span class=\"line\"><span class=\"keyword\">var</span> _hmt = _hmt || [];</span><br><span class=\"line\">(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> hm = <span class=\"built_in\">document</span>.createElement(<span class=\"string\">\"script\"</span>);</span><br><span class=\"line\">  hm.src = <span class=\"string\">\"https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645\"</span>;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> s = <span class=\"built_in\">document</span>.getElementsByTagName(<span class=\"string\">\"script\"</span>)[<span class=\"number\">0</span>]; </span><br><span class=\"line\">  s.parentNode.insertBefore(hm, s);</span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\">&lt;<span class=\"regexp\">/script&gt;</span></span><br></pre></td></tr></table></figure>\n<p>将代码中<code>b54e835b3551fd0696954b3aedf5d645</code>复制到next主题_config.yml的<code>baidu_analytics</code>中。接下来通过<code>代码安装检查</code>来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.5.png\" alt=\"图片6.5\"></p>\n<h4 id=\"6-4增加评论功能\"><a href=\"#6-4增加评论功能\" class=\"headerlink\" title=\"6.4增加评论功能\"></a>6.4增加评论功能</h4><p>多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.6.png\" alt=\"图片6.6\">进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。<code>编写文章时应在头部添加comments: true</code></p>\n<h3 id=\"7-绑定个人域名\"><a href=\"#7-绑定个人域名\" class=\"headerlink\" title=\"7.绑定个人域名\"></a>7.绑定个人域名</h3><p>现在使用的域名<code>weizhixiaoyi.github.io</code>是github提供的二级域名，也可绑定自己的个性域名<code>weizhixiaoyi.com</code>。域名是在阿里云购买，年费为55元，也可以在狗爹<code>https://sg.godaddy.com</code>购买，购买好域名之后便可以直接解析。</p>\n<h4 id=\"7-1GitHub端\"><a href=\"#7-1GitHub端\" class=\"headerlink\" title=\"7.1GitHub端\"></a>7.1GitHub端</h4><p>在next主题中source文件夹中创建<code>CNAME</code>文件，没有后缀名，然后将个人域名<code>weizhixiaoyi.com</code>添加进<code>CNAME</code>文件即可，然后通过<code>hexo g</code> <code>hexo d</code>重新部署网站。</p>\n<h4 id=\"7-2域名解析\"><a href=\"#7-2域名解析\" class=\"headerlink\" title=\"7.2域名解析\"></a>7.2域名解析</h4><p>如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。</p>\n<ul>\n<li>记录类型：CNAME</li>\n<li>主机记录：@</li>\n<li>解析线路：默认</li>\n<li>记录值：weizhixiaoyi.github.io</li>\n</ul>\n<p>解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。</p>\n<h3 id=\"7-博客SEO优化\"><a href=\"#7-博客SEO优化\" class=\"headerlink\" title=\"7.博客SEO优化\"></a>7.博客SEO优化</h3><p>SEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。</p>\n<h4 id=\"7-1确认收录情况\"><a href=\"#7-1确认收录情况\" class=\"headerlink\" title=\"7.1确认收录情况\"></a>7.1确认收录情况</h4><p>在谷歌上搜索<code>site:weizhixiaoyi.com</code>，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片7.1.png\" alt=\"图片7.1\"></p>\n<h4 id=\"7-1网站身份验证\"><a href=\"#7-1网站身份验证\" class=\"headerlink\" title=\"7.1网站身份验证\"></a>7.1网站身份验证</h4><p>验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过<code>shadowsock</code>+<code>搬瓦工</code>自行搭建。</p>\n<p>进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">if</span></span> theme.google_site_verification %&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\">  <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">\"google-site-verification\"</span> <span class=\"attr\">content</span>=<span class=\"string\">\"E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo\"</span> /&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">endif</span></span> %&#125;</span><span class=\"xml\"></span></span><br></pre></td></tr></table></figure>\n<p>然后回到<code>myblog</code>文件夹下将_config.yml中google_site_vertification设置为<code>true</code>。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用<code>hexo g</code>和<code>hexo d</code>更新博客内容，至此网站身份验证结束。</p>\n<h4 id=\"7-2添加Sitemap\"><a href=\"#7-2添加Sitemap\" class=\"headerlink\" title=\"7.2添加Sitemap\"></a>7.2添加Sitemap</h4><p>sitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。</p>\n<p>首先安装针对谷歌的插件<code>npm install hexo-generator-sitemap --save</code>，然后进入<code>myblog</code>文件夹下将<code>sitemap</code>设置如下。</p>\n<figure class=\"highlight dts\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"># sitemap</span></span><br><span class=\"line\"><span class=\"symbol\">sitemap:</span></span><br><span class=\"line\"><span class=\"symbol\">  path:</span> sitemap.xml</span><br></pre></td></tr></table></figure>\n<h4 id=\"7-3谷歌收录博客\"><a href=\"#7-3谷歌收录博客\" class=\"headerlink\" title=\"7.3谷歌收录博客\"></a>7.3谷歌收录博客</h4><p>谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过<code>site:weizhixiaoyi.com</code>能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片7.2.png\" alt=\"图片7.2\"></p>\n<p>另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。</p>\n<h3 id=\"8-ToDoList\"><a href=\"#8-ToDoList\" class=\"headerlink\" title=\"8.ToDoList\"></a>8.ToDoList</h3><ul>\n<li>寻找更好的方法解决百度爬虫无法爬取博客内容的问题</li>\n<li>博客增加转发功能</li>\n</ul>\n<hr>\n<h3 id=\"9-推广\"><a href=\"#9-推广\" class=\"headerlink\" title=\"9.推广\"></a>9.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-为什么写博客\"><a href=\"#1-为什么写博客\" class=\"headerlink\" title=\"1.为什么写博客\"></a>1.为什么写博客</h3><p>以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。</p>\n<h3 id=\"2-Mac-Hexo-GitHub博客\"><a href=\"#2-Mac-Hexo-GitHub博客\" class=\"headerlink\" title=\"2.Mac+Hexo+GitHub博客\"></a>2.Mac+Hexo+GitHub博客</h3><p>现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。</p>\n<ul>\n<li>Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。</li>\n<li>Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。</li>\n</ul>\n<p>选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是<em>百度爬虫无法爬去博客内容</em>，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。</p>\n<h3 id=\"3-博客本地环境搭建\"><a href=\"#3-博客本地环境搭建\" class=\"headerlink\" title=\"3.博客本地环境搭建\"></a>3.博客本地环境搭建</h3><h4 id=\"3-1安装Node-js和Git\"><a href=\"#3-1安装Node-js和Git\" class=\"headerlink\" title=\"3.1安装Node.js和Git\"></a>3.1安装Node.js和Git</h4><p>Mac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录<em>/usr/local/bin</em>目录下。测试Node.js和npm，出现下述信息则安装成功。</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">node</span> <span class=\"title\">-v</span></span><br><span class=\"line\">v8.<span class=\"number\">10.0</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-tag\">npm</span> <span class=\"selector-tag\">-v</span></span><br><span class=\"line\">5<span class=\"selector-class\">.6</span><span class=\"selector-class\">.0</span></span><br></pre></td></tr></table></figure>\n<p>Git官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。</p>\n<blockquote>\n<p>Git –version </p>\n<p>git version 2.15.0</p>\n</blockquote>\n<h4 id=\"3-2安装Hexo\"><a href=\"#3-2安装Hexo\" class=\"headerlink\" title=\"3.2安装Hexo\"></a>3.2安装Hexo</h4><p>Node.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm install -g hexo</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-3博客初始化\"><a href=\"#3-3博客初始化\" class=\"headerlink\" title=\"3.3博客初始化\"></a>3.3博客初始化</h4><p>创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> myblog</span><br></pre></td></tr></table></figure>\n<p>执行下述命令初始化本地博客，下载一些列文件。</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo init</span></span><br></pre></td></tr></table></figure>\n<p>执行下述命令安装npm。</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm <span class=\"keyword\">install</span></span><br></pre></td></tr></table></figure>\n<p>执行下述命令生成本地html文件并开启服务器，然后通过<a href=\"http://localhost:4000查看本地博客。\" target=\"_blank\" rel=\"noopener\">http://localhost:4000查看本地博客。</a></p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo g</span></span><br><span class=\"line\"><span class=\"attribute\">hexo s</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片3.3.png\" alt=\"图片3.3\"></p>\n<h3 id=\"4-本地博客关联GitHub\"><a href=\"#4-本地博客关联GitHub\" class=\"headerlink\" title=\"4.本地博客关联GitHub\"></a>4.本地博客关联GitHub</h3><h4 id=\"4-1本地博客代码上传GitHub\"><a href=\"#4-1本地博客代码上传GitHub\" class=\"headerlink\" title=\"4.1本地博客代码上传GitHub\"></a>4.1本地博客代码上传GitHub</h4><p>注册并登陆GitHub账号后，新建仓库，名称必须为<code>user.github.io</code>，如<code>weizhixiaoyi.github.io</code>。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片4.1.png\" alt=\"图片01\"></p>\n<p>终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">vim</span> _config.yml</span><br></pre></td></tr></table></figure>\n<p>打开后至文档最后部分，将deploy配置如下。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure>\n<p>其中将repository中<code>weizhixiaoyi</code>改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">hexo g</span></span><br><span class=\"line\"><span class=\"attribute\">hexo d</span></span><br></pre></td></tr></table></figure>\n<p>若执行<code>hexo g</code>出错则执行<code>npm install hexo --save</code>，若执行<code>hexo d</code>出错则执行<code>npm install hexo-deployer-git --save</code>。错误修正后再次执行<code>hexo g</code>和<code>hexo d</code>。</p>\n<p>若未关联GitHub，执行<code>hexo d</code>时会提示输入GitHub账号用户名和密码，即:</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">username <span class=\"keyword\">for</span> <span class=\"symbol\">'https</span>:<span class=\"comment\">//github.com':</span></span><br><span class=\"line\">password <span class=\"keyword\">for</span> <span class=\"symbol\">'https</span>:<span class=\"comment\">//github.com':</span></span><br></pre></td></tr></table></figure>\n<p><code>hexo d</code>执行成功后便可通过<a href=\"https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。\" target=\"_blank\" rel=\"noopener\">https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。</a></p>\n<h4 id=\"4-2添加ssh-keys到GitHub\"><a href=\"#4-2添加ssh-keys到GitHub\" class=\"headerlink\" title=\"4.2添加ssh keys到GitHub\"></a>4.2添加ssh keys到GitHub</h4><p>添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。</p>\n<p>执行下述命令生成新的ssh key，将<a href=\"mailto:`your_email@example.com\" target=\"_blank\" rel=\"noopener\">`your_email@example.com</a><code>改成自己以注册的GitHub邮箱地址。默认会在</code>~/.ssh/id_rsa.pub<code>中生成</code>id_rsa<code>和</code>id_rsa.pub`文件。</p>\n<figure class=\"highlight excel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -<span class=\"built_in\">t</span> rsa -C <span class=\"string\">\"your_email@exampl\"</span></span><br></pre></td></tr></table></figure>\n<p>Mac下利用<code>open ~/.ssh</code>打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key<code>路径GitHub-&gt;Setting-&gt;SSH keys-&gt;add SSH key</code>界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。</p>\n<p>此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过<code>hexo g</code>和<code>hexo d</code>便可更新到GitHub之中，通过<a href=\"https://weizhixiaoyi.github.io访问便可看到更新内容。\" target=\"_blank\" rel=\"noopener\">https://weizhixiaoyi.github.io访问便可看到更新内容。</a></p>\n<h3 id=\"5-更换Hexo主题\"><a href=\"#5-更换Hexo主题\" class=\"headerlink\" title=\"5.更换Hexo主题\"></a>5.更换Hexo主题</h3><p>可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。</p>\n<p>终端cd到myblog目录下执行如下所示命令。</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone http<span class=\"variable\">s:</span>//github.<span class=\"keyword\">com</span>/iissnan/hexo-theme-<span class=\"keyword\">next</span> themes/<span class=\"keyword\">next</span></span><br></pre></td></tr></table></figure>\n<p>将blog目录下_config.yml里的theme的名称<code>landscape</code>更改为<code>next</code>。</p>\n<p>执行如下命令（每次部署文章的步骤）</p>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g  <span class=\"comment\">//生成缓存和静态文件</span></span><br><span class=\"line\">hexo d  <span class=\"comment\">//重新部署到服务器</span></span><br></pre></td></tr></table></figure>\n<p>当本地博客部署到服务器后，网页端无变化时可以采用下述命令。</p>\n<figure class=\"highlight x86asm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean  //清楚缓存文件(<span class=\"built_in\">db</span>.json)和已生成的静态文件(<span class=\"meta\">public</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-配置Hexo-theme-next主题\"><a href=\"#6-配置Hexo-theme-next主题\" class=\"headerlink\" title=\"6.配置Hexo-theme-next主题\"></a>6.配置Hexo-theme-next主题</h3><p>Hexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问<a href=\"http://theme-next.iissnan.com/\" target=\"_blank\" rel=\"noopener\">next官网</a>查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.1.png\" alt=\"图片6.1\"></p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.2.png\" alt=\"图片6.1\"></p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.3.png\" alt=\"图片6.1\"></p>\n<h4 id=\"6-1增加标签、分类、归档页\"><a href=\"#6-1增加标签、分类、归档页\" class=\"headerlink\" title=\"6.1增加标签、分类、归档页\"></a>6.1增加标签、分类、归档页</h4><p>首先将next/config.yml文件中将<code>menu</code>中<code>tags</code> <code>catagories</code> <code>archive</code>前面的<code>#</code>。例如增加标签页，通过<code>hexo new page &#39;tags&#39;</code>增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为<code>tags</code>。利用<code>hexo g</code>和<code>hexo d</code>将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。</p>\n<h4 id=\"6-2增加喜欢界面\"><a href=\"#6-2增加喜欢界面\" class=\"headerlink\" title=\"6.2增加喜欢界面\"></a>6.2增加喜欢界面</h4><p>喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。</p>\n<p>从GitHub上<a href=\"https://github.com/weizhixiaoyi\" target=\"_blank\" rel=\"noopener\">https://github.com/weizhixiaoyi</a> 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">image_stream:</span><br><span class=\"line\">\tjquery: false</span><br></pre></td></tr></table></figure>\n<p>在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。</p>\n<figure class=\"highlight dts\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">menu:</span></span><br><span class=\"line\"><span class=\"symbol\">  home:</span> / || home</span><br><span class=\"line\"><span class=\"symbol\">  about:</span> <span class=\"meta-keyword\">/about/</span> || user</span><br><span class=\"line\"><span class=\"symbol\">  favorite:</span> <span class=\"meta-keyword\">/favorite/</span> || heart</span><br><span class=\"line\"><span class=\"symbol\">  tags:</span> <span class=\"meta-keyword\">/tags/</span> || tags</span><br><span class=\"line\"><span class=\"symbol\">  categories:</span> <span class=\"meta-keyword\">/categories/</span> || th</span><br><span class=\"line\"><span class=\"symbol\">  archives:</span> <span class=\"meta-keyword\">/archives/</span> || archive</span><br></pre></td></tr></table></figure>\n<p>然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">stream</span> %&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">figure</span> https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg</span></span><br><span class=\"line\"><span class=\"template-tag\">[《万物理论》]（https://movie.douban.com/subject/24815950/）%&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\">endstream</span> %&#125;</span><span class=\"xml\"></span></span><br></pre></td></tr></table></figure>\n<h4 id=\"6-3文章阅读统计\"><a href=\"#6-3文章阅读统计\" class=\"headerlink\" title=\"6.3文章阅读统计\"></a>6.3文章阅读统计</h4><p>文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。</p>\n<p>注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">leancloud_visitors:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span> </span><br><span class=\"line\"><span class=\"attr\">  app_id:</span> <span class=\"string\">Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz</span> <span class=\"comment\">#&lt;app_id&gt;</span></span><br><span class=\"line\"><span class=\"attr\">  app_key:</span> <span class=\"string\">qJejurdHKM06N75OQedX4SDK</span> <span class=\"comment\">#&lt;app_key&gt;</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.4.png\" alt=\"图片6.4\"></p>\n<h4 id=\"6-4增加百度统计\"><a href=\"#6-4增加百度统计\" class=\"headerlink\" title=\"6.4增加百度统计\"></a>6.4增加百度统计</h4><p>百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到<code>代码获取</code>。</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;script&gt;</span><br><span class=\"line\"><span class=\"keyword\">var</span> _hmt = _hmt || [];</span><br><span class=\"line\">(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> hm = <span class=\"built_in\">document</span>.createElement(<span class=\"string\">\"script\"</span>);</span><br><span class=\"line\">  hm.src = <span class=\"string\">\"https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645\"</span>;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> s = <span class=\"built_in\">document</span>.getElementsByTagName(<span class=\"string\">\"script\"</span>)[<span class=\"number\">0</span>]; </span><br><span class=\"line\">  s.parentNode.insertBefore(hm, s);</span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\">&lt;<span class=\"regexp\">/script&gt;</span></span><br></pre></td></tr></table></figure>\n<p>将代码中<code>b54e835b3551fd0696954b3aedf5d645</code>复制到next主题_config.yml的<code>baidu_analytics</code>中。接下来通过<code>代码安装检查</code>来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.5.png\" alt=\"图片6.5\"></p>\n<h4 id=\"6-4增加评论功能\"><a href=\"#6-4增加评论功能\" class=\"headerlink\" title=\"6.4增加评论功能\"></a>6.4增加评论功能</h4><p>多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片6.6.png\" alt=\"图片6.6\">进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。<code>编写文章时应在头部添加comments: true</code></p>\n<h3 id=\"7-绑定个人域名\"><a href=\"#7-绑定个人域名\" class=\"headerlink\" title=\"7.绑定个人域名\"></a>7.绑定个人域名</h3><p>现在使用的域名<code>weizhixiaoyi.github.io</code>是github提供的二级域名，也可绑定自己的个性域名<code>weizhixiaoyi.com</code>。域名是在阿里云购买，年费为55元，也可以在狗爹<code>https://sg.godaddy.com</code>购买，购买好域名之后便可以直接解析。</p>\n<h4 id=\"7-1GitHub端\"><a href=\"#7-1GitHub端\" class=\"headerlink\" title=\"7.1GitHub端\"></a>7.1GitHub端</h4><p>在next主题中source文件夹中创建<code>CNAME</code>文件，没有后缀名，然后将个人域名<code>weizhixiaoyi.com</code>添加进<code>CNAME</code>文件即可，然后通过<code>hexo g</code> <code>hexo d</code>重新部署网站。</p>\n<h4 id=\"7-2域名解析\"><a href=\"#7-2域名解析\" class=\"headerlink\" title=\"7.2域名解析\"></a>7.2域名解析</h4><p>如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。</p>\n<ul>\n<li>记录类型：CNAME</li>\n<li>主机记录：@</li>\n<li>解析线路：默认</li>\n<li>记录值：weizhixiaoyi.github.io</li>\n</ul>\n<p>解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。</p>\n<h3 id=\"7-博客SEO优化\"><a href=\"#7-博客SEO优化\" class=\"headerlink\" title=\"7.博客SEO优化\"></a>7.博客SEO优化</h3><p>SEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。</p>\n<h4 id=\"7-1确认收录情况\"><a href=\"#7-1确认收录情况\" class=\"headerlink\" title=\"7.1确认收录情况\"></a>7.1确认收录情况</h4><p>在谷歌上搜索<code>site:weizhixiaoyi.com</code>，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片7.1.png\" alt=\"图片7.1\"></p>\n<h4 id=\"7-1网站身份验证\"><a href=\"#7-1网站身份验证\" class=\"headerlink\" title=\"7.1网站身份验证\"></a>7.1网站身份验证</h4><p>验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过<code>shadowsock</code>+<code>搬瓦工</code>自行搭建。</p>\n<p>进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">if</span></span> theme.google_site_verification %&#125;</span><span class=\"xml\"></span></span><br><span class=\"line\"><span class=\"xml\">  <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">\"google-site-verification\"</span> <span class=\"attr\">content</span>=<span class=\"string\">\"E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo\"</span> /&gt;</span></span></span><br><span class=\"line\"><span class=\"xml\"></span><span class=\"template-tag\">&#123;% <span class=\"name\"><span class=\"name\">endif</span></span> %&#125;</span><span class=\"xml\"></span></span><br></pre></td></tr></table></figure>\n<p>然后回到<code>myblog</code>文件夹下将_config.yml中google_site_vertification设置为<code>true</code>。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用<code>hexo g</code>和<code>hexo d</code>更新博客内容，至此网站身份验证结束。</p>\n<h4 id=\"7-2添加Sitemap\"><a href=\"#7-2添加Sitemap\" class=\"headerlink\" title=\"7.2添加Sitemap\"></a>7.2添加Sitemap</h4><p>sitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。</p>\n<p>首先安装针对谷歌的插件<code>npm install hexo-generator-sitemap --save</code>，然后进入<code>myblog</code>文件夹下将<code>sitemap</code>设置如下。</p>\n<figure class=\"highlight dts\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"># sitemap</span></span><br><span class=\"line\"><span class=\"symbol\">sitemap:</span></span><br><span class=\"line\"><span class=\"symbol\">  path:</span> sitemap.xml</span><br></pre></td></tr></table></figure>\n<h4 id=\"7-3谷歌收录博客\"><a href=\"#7-3谷歌收录博客\" class=\"headerlink\" title=\"7.3谷歌收录博客\"></a>7.3谷歌收录博客</h4><p>谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过<code>site:weizhixiaoyi.com</code>能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/图片7.2.png\" alt=\"图片7.2\"></p>\n<p>另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。</p>\n<h3 id=\"8-ToDoList\"><a href=\"#8-ToDoList\" class=\"headerlink\" title=\"8.ToDoList\"></a>8.ToDoList</h3><ul>\n<li>寻找更好的方法解决百度爬虫无法爬取博客内容的问题</li>\n<li>博客增加转发功能</li>\n</ul>\n<hr>\n<h3 id=\"9-推广\"><a href=\"#9-推广\" class=\"headerlink\" title=\"9.推广\"></a>9.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"Mac+Hexo+GitHub博客搭建教程/推广.png\" alt=\"推广\"></p>\n"},{"title":"Markdown写作教程","date":"2018-03-18T15:29:14.000Z","toc":true,"comments":1,"_content":"\n[博客搭建教程](https://weizhixiaoyi.com/2018/03/16/Mac+Hexo+GitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/)写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。\n\n### 1.为什么选择Markdown\n\n首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用**印象笔记**至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。\n\n### 2.标题\n\n标题通过`#`的个数进行区分，Markdown共支持6级标题。\n\n![标题](Markdown写作教程/图片01.png)\n\n### 3.字体设置\n\n#### 3.1粗体\n\n文字前后加`**`来表示粗体。\n\n```markdown\n**粗体**\n```\n\n**粗体**\n\n#### 3.2斜体\n\n文字前后加`*`来表示斜体。\n\n```markdown\n*斜体*\n```\n\n*斜体*\n\n#### 3.3粗斜体\n\n文字前后加`***`来表示粗斜体。\n\n```markdown\n***粗斜体***\n```\n\n***粗斜体***\n\n#### 3.4下划线\n\n文字前后加`<u>` `</u>`来表示下划线。\n\n```markdown\n<u>下滑线</u>\n```\n\n<u>下划线</u>\n\n#### 3.5删除线\n\n文字前后加`~~`来表示删除线。\n\n```markdown\n~~删除线~~\n```\n\n~~删除线~~\n\n#### 3.6标记\n\n文字前后加`` `来表示标记，该符号位于Esc键下面。\n\n```markdown\n`标记`\n```\n\n`标记`\n\n#### 3.7Html标签\n\n```Markdown\n<font face=\"微软雅黑\" color=\"red\" size=\"6\">字体及字体颜色和大小</font>\n```\n\n<font face=\"微软雅黑\" color=\"red\" size=\"3\">字体及字体颜色和大小</font>\n\n### 4.列表\n\n#### 4.1有序列表\n\n采用`1.  ` 后加空格形式表示有序列表。\n\n```markdown\n1. 有序列表1\n2. 有序列表2\n3. 有序列表3\n```\n\n1. 有序列表1\n2. 有序列表2\n3. 有序列表3\n\n#### 4.2无序列表\n\n采用`+` `-` `* ` `=`符号表示无序列表，支持多级嵌套。\n\n```markdown\n+ 有序列表1\n+ + 有序列表1.1\n+ + 有序列表1.2\n+ 有序列表2\n+ 有序列表3\n```\n\n- 无序列表1\n  - 无序列表1.1\n  - 无序列表1.2\n- 无序列表2\n- 无序列表3\n\n#### 4.3未完成列表\n\n采用`- []`表示未完成任务，各符号间均有空格。\n\n```markdown\n- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3\n```\n\n- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3\n\n#### 4.4已完成任务\n\n采用`- [x] `表示已完成任务，各符号间均有空格。同时可直接在未完成任务间`打勾`来转换成已完成任务。\n\n```markdown\n- [x] 已完成任务1\n- [x] 已完成任务2\n- [x] 已完成任务3\n```\n\n- [ ] 已完成任务1\n- [ ] 已完成任务2\n- [ ] 已完成任务3\n\n### 5.表格\n\n表格对齐方式\n\n- 居左：:----\n- 居中：:----:或-----\n- 居由：----:\n\n```markdown\n| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |\n```\n\n| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |\n\n### 6.段落和换行\n\n#### 6.1首行缩进方式\n\n+ `&emsp;`中文空格\n+ `&ensp;`半中文空格\n+ `&nbsp;`英文空格\n+ ` `输入法切换到全角双击空格\n\n#### 6.2换行\n\n+ ` ` ` `换行处连续打两个空格\n+ 换行处使用`<br>`进行换行\n\n#### 6.3空行\n\n+ ` ` ` ` 空行处连续打两个空格\n+ 换行处使用`<br>`进行空行\n\n### 6.引用和代码块\n\n#### 6.1引用\n\n若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。\n\n```Markdown\n> 引用1\n> > 引用1.1\n> > 引用1.2\n> 引用2\n```\n\n> 引用1\n>\n> > 引用1.1\n> >\n> > 引用1.2\n>\n> 引用2\n\n#### 6.2代码块\n\n代码前后添加```` `表示代码块。\n\n```markdown\n​```Python\nprint('代码块')\n​```\n```\n\n```python\nprint（'代码块'）\n```\n\n### 7.链接\n\n#### 7.1图片链接\n\n采用`![]()`来表示图片链接。\n\n```Markdown\n![图片名称](链接地址)\n```\n\n![图片名称](Markdown写作教程/图片02.png)\n\n#### 7.2文字链接\n\n采用`[]()`表示文字链接。\n\n```Markdown\n[链接名称](链接地址)\n```\n\n[文字链接](weizhixiaoyi.com)\n\n#### 7.3参考链接\n\n采用`[ ]: `表示参考链接，注意符号后有空格。\n\n```markdown\n[ ]: url title\n```\n\n[参考链接]: https://weizhixiaoyi.com\t\"谓之小一\"\n\n### 8.分割线\n\n上下文无关时可使用分割符进行分开。\n\n- 连续多个`-`  (>=3)\n- 连续多个`*` （>=3）\n- 连续多个下划线`_` （>=3）\n\n```\n---分割线\n***分割线\n___分割线\n```\n\n------\n\n------\n\n------\n\n### 9.脚注和注释\n\n#### 9.1脚注\n\n采用`[^]:表示脚注，注意空格。\n\n```markdown\n[^]: 脚注\n```\n\n[^]: 脚注\n\n#### 9.2注释\n\n采用`<!---->`表示注释.\n\n```markdown\n<!--注释-->\n```\n\n<!--注释-->\n\n### 11.转义\n\nMarkdown通过反斜杠`\\`来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。\n\n```markdown\n\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号\n```\n\n\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号\n\n### 12.目录\n\n采用`[TOC]`来生成文章目录。\n\n```Markdown\n[TOC]\n```\n\n![图片03](Markdown写作教程/图片03.png)\n\n-----\n\n### 13.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Markdown写作教程/推广.png)\n\n","source":"_posts/Markdown写作教程.md","raw":"---\ntitle: Markdown写作教程\ndate: 2018-03-18 23:29:14\ntags: [Markdown,博客,教程]\ntoc: true\ncomments: true\n---\n\n[博客搭建教程](https://weizhixiaoyi.com/2018/03/16/Mac+Hexo+GitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/)写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。\n\n### 1.为什么选择Markdown\n\n首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用**印象笔记**至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。\n\n### 2.标题\n\n标题通过`#`的个数进行区分，Markdown共支持6级标题。\n\n![标题](Markdown写作教程/图片01.png)\n\n### 3.字体设置\n\n#### 3.1粗体\n\n文字前后加`**`来表示粗体。\n\n```markdown\n**粗体**\n```\n\n**粗体**\n\n#### 3.2斜体\n\n文字前后加`*`来表示斜体。\n\n```markdown\n*斜体*\n```\n\n*斜体*\n\n#### 3.3粗斜体\n\n文字前后加`***`来表示粗斜体。\n\n```markdown\n***粗斜体***\n```\n\n***粗斜体***\n\n#### 3.4下划线\n\n文字前后加`<u>` `</u>`来表示下划线。\n\n```markdown\n<u>下滑线</u>\n```\n\n<u>下划线</u>\n\n#### 3.5删除线\n\n文字前后加`~~`来表示删除线。\n\n```markdown\n~~删除线~~\n```\n\n~~删除线~~\n\n#### 3.6标记\n\n文字前后加`` `来表示标记，该符号位于Esc键下面。\n\n```markdown\n`标记`\n```\n\n`标记`\n\n#### 3.7Html标签\n\n```Markdown\n<font face=\"微软雅黑\" color=\"red\" size=\"6\">字体及字体颜色和大小</font>\n```\n\n<font face=\"微软雅黑\" color=\"red\" size=\"3\">字体及字体颜色和大小</font>\n\n### 4.列表\n\n#### 4.1有序列表\n\n采用`1.  ` 后加空格形式表示有序列表。\n\n```markdown\n1. 有序列表1\n2. 有序列表2\n3. 有序列表3\n```\n\n1. 有序列表1\n2. 有序列表2\n3. 有序列表3\n\n#### 4.2无序列表\n\n采用`+` `-` `* ` `=`符号表示无序列表，支持多级嵌套。\n\n```markdown\n+ 有序列表1\n+ + 有序列表1.1\n+ + 有序列表1.2\n+ 有序列表2\n+ 有序列表3\n```\n\n- 无序列表1\n  - 无序列表1.1\n  - 无序列表1.2\n- 无序列表2\n- 无序列表3\n\n#### 4.3未完成列表\n\n采用`- []`表示未完成任务，各符号间均有空格。\n\n```markdown\n- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3\n```\n\n- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3\n\n#### 4.4已完成任务\n\n采用`- [x] `表示已完成任务，各符号间均有空格。同时可直接在未完成任务间`打勾`来转换成已完成任务。\n\n```markdown\n- [x] 已完成任务1\n- [x] 已完成任务2\n- [x] 已完成任务3\n```\n\n- [ ] 已完成任务1\n- [ ] 已完成任务2\n- [ ] 已完成任务3\n\n### 5.表格\n\n表格对齐方式\n\n- 居左：:----\n- 居中：:----:或-----\n- 居由：----:\n\n```markdown\n| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |\n```\n\n| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |\n\n### 6.段落和换行\n\n#### 6.1首行缩进方式\n\n+ `&emsp;`中文空格\n+ `&ensp;`半中文空格\n+ `&nbsp;`英文空格\n+ ` `输入法切换到全角双击空格\n\n#### 6.2换行\n\n+ ` ` ` `换行处连续打两个空格\n+ 换行处使用`<br>`进行换行\n\n#### 6.3空行\n\n+ ` ` ` ` 空行处连续打两个空格\n+ 换行处使用`<br>`进行空行\n\n### 6.引用和代码块\n\n#### 6.1引用\n\n若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。\n\n```Markdown\n> 引用1\n> > 引用1.1\n> > 引用1.2\n> 引用2\n```\n\n> 引用1\n>\n> > 引用1.1\n> >\n> > 引用1.2\n>\n> 引用2\n\n#### 6.2代码块\n\n代码前后添加```` `表示代码块。\n\n```markdown\n​```Python\nprint('代码块')\n​```\n```\n\n```python\nprint（'代码块'）\n```\n\n### 7.链接\n\n#### 7.1图片链接\n\n采用`![]()`来表示图片链接。\n\n```Markdown\n![图片名称](链接地址)\n```\n\n![图片名称](Markdown写作教程/图片02.png)\n\n#### 7.2文字链接\n\n采用`[]()`表示文字链接。\n\n```Markdown\n[链接名称](链接地址)\n```\n\n[文字链接](weizhixiaoyi.com)\n\n#### 7.3参考链接\n\n采用`[ ]: `表示参考链接，注意符号后有空格。\n\n```markdown\n[ ]: url title\n```\n\n[参考链接]: https://weizhixiaoyi.com\t\"谓之小一\"\n\n### 8.分割线\n\n上下文无关时可使用分割符进行分开。\n\n- 连续多个`-`  (>=3)\n- 连续多个`*` （>=3）\n- 连续多个下划线`_` （>=3）\n\n```\n---分割线\n***分割线\n___分割线\n```\n\n------\n\n------\n\n------\n\n### 9.脚注和注释\n\n#### 9.1脚注\n\n采用`[^]:表示脚注，注意空格。\n\n```markdown\n[^]: 脚注\n```\n\n[^]: 脚注\n\n#### 9.2注释\n\n采用`<!---->`表示注释.\n\n```markdown\n<!--注释-->\n```\n\n<!--注释-->\n\n### 11.转义\n\nMarkdown通过反斜杠`\\`来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。\n\n```markdown\n\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号\n```\n\n\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号\n\n### 12.目录\n\n采用`[TOC]`来生成文章目录。\n\n```Markdown\n[TOC]\n```\n\n![图片03](Markdown写作教程/图片03.png)\n\n-----\n\n### 13.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](Markdown写作教程/推广.png)\n\n","slug":"Markdown写作教程","published":1,"updated":"2018-03-26T16:37:15.932Z","layout":"post","photos":[],"link":"","_id":"cji4rdzd300032e01manfcwdl","content":"<p><a href=\"https://weizhixiaoyi.com/2018/03/16/Mac+Hexo+GitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/\">博客搭建教程</a>写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。</p>\n<h3 id=\"1-为什么选择Markdown\"><a href=\"#1-为什么选择Markdown\" class=\"headerlink\" title=\"1.为什么选择Markdown\"></a>1.为什么选择Markdown</h3><p>首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用<strong>印象笔记</strong>至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。</p>\n<h3 id=\"2-标题\"><a href=\"#2-标题\" class=\"headerlink\" title=\"2.标题\"></a>2.标题</h3><p>标题通过<code>#</code>的个数进行区分，Markdown共支持6级标题。</p>\n<p><img src=\"Markdown写作教程/图片01.png\" alt=\"标题\"></p>\n<h3 id=\"3-字体设置\"><a href=\"#3-字体设置\" class=\"headerlink\" title=\"3.字体设置\"></a>3.字体设置</h3><h4 id=\"3-1粗体\"><a href=\"#3-1粗体\" class=\"headerlink\" title=\"3.1粗体\"></a>3.1粗体</h4><p>文字前后加<code>**</code>来表示粗体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">**粗体**</span></span><br></pre></td></tr></table></figure>\n<p><strong>粗体</strong></p>\n<h4 id=\"3-2斜体\"><a href=\"#3-2斜体\" class=\"headerlink\" title=\"3.2斜体\"></a>3.2斜体</h4><p>文字前后加<code>*</code>来表示斜体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"emphasis\">*斜体*</span></span><br></pre></td></tr></table></figure>\n<p><em>斜体</em></p>\n<h4 id=\"3-3粗斜体\"><a href=\"#3-3粗斜体\" class=\"headerlink\" title=\"3.3粗斜体\"></a>3.3粗斜体</h4><p>文字前后加<code>***</code>来表示粗斜体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">***粗斜体**</span>*</span><br></pre></td></tr></table></figure>\n<p><strong><em>粗斜体</em></strong></p>\n<h4 id=\"3-4下划线\"><a href=\"#3-4下划线\" class=\"headerlink\" title=\"3.4下划线\"></a>3.4下划线</h4><p>文字前后加<code>&lt;u&gt;</code> <code>&lt;/u&gt;</code>来表示下划线。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">u</span>&gt;</span></span>下滑线<span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">u</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p><u>下划线</u></p>\n<h4 id=\"3-5删除线\"><a href=\"#3-5删除线\" class=\"headerlink\" title=\"3.5删除线\"></a>3.5删除线</h4><p>文字前后加<code>~~</code>来表示删除线。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~~删除线~~</span><br></pre></td></tr></table></figure>\n<p><del>删除线</del></p>\n<h4 id=\"3-6标记\"><a href=\"#3-6标记\" class=\"headerlink\" title=\"3.6标记\"></a>3.6标记</h4><p>文字前后加<code></code> `来表示标记，该符号位于Esc键下面。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"code\">`标记`</span></span><br></pre></td></tr></table></figure>\n<p><code>标记</code></p>\n<h4 id=\"3-7Html标签\"><a href=\"#3-7Html标签\" class=\"headerlink\" title=\"3.7Html标签\"></a>3.7Html标签</h4><figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">font</span> <span class=\"attr\">face</span>=<span class=\"string\">\"微软雅黑\"</span> <span class=\"attr\">color</span>=<span class=\"string\">\"red\"</span> <span class=\"attr\">size</span>=<span class=\"string\">\"6\"</span>&gt;</span></span>字体及字体颜色和大小<span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">font</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<font face=\"微软雅黑\" color=\"red\" size=\"3\">字体及字体颜色和大小</font>\n\n<h3 id=\"4-列表\"><a href=\"#4-列表\" class=\"headerlink\" title=\"4.列表\"></a>4.列表</h3><h4 id=\"4-1有序列表\"><a href=\"#4-1有序列表\" class=\"headerlink\" title=\"4.1有序列表\"></a>4.1有序列表</h4><p>采用<code>1.</code> 后加空格形式表示有序列表。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">1. </span>有序列表1</span><br><span class=\"line\"><span class=\"bullet\">2. </span>有序列表2</span><br><span class=\"line\"><span class=\"bullet\">3. </span>有序列表3</span><br></pre></td></tr></table></figure>\n<ol>\n<li>有序列表1</li>\n<li>有序列表2</li>\n<li>有序列表3</li>\n</ol>\n<h4 id=\"4-2无序列表\"><a href=\"#4-2无序列表\" class=\"headerlink\" title=\"4.2无序列表\"></a>4.2无序列表</h4><p>采用<code>+</code> <code>-</code> <code>*</code> <code>=</code>符号表示无序列表，支持多级嵌套。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">+ </span>有序列表1</span><br><span class=\"line\"><span class=\"bullet\">+ </span>+ 有序列表1.1</span><br><span class=\"line\"><span class=\"bullet\">+ </span>+ 有序列表1.2</span><br><span class=\"line\"><span class=\"bullet\">+ </span>有序列表2</span><br><span class=\"line\"><span class=\"bullet\">+ </span>有序列表3</span><br></pre></td></tr></table></figure>\n<ul>\n<li>无序列表1<ul>\n<li>无序列表1.1</li>\n<li>无序列表1.2</li>\n</ul>\n</li>\n<li>无序列表2</li>\n<li>无序列表3</li>\n</ul>\n<h4 id=\"4-3未完成列表\"><a href=\"#4-3未完成列表\" class=\"headerlink\" title=\"4.3未完成列表\"></a>4.3未完成列表</h4><p>采用<code>- []</code>表示未完成任务，各符号间均有空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务1</span><br><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务2</span><br><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务3</span><br></pre></td></tr></table></figure>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 未完成任务1</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 未完成任务2</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 未完成任务3</li>\n</ul>\n<h4 id=\"4-4已完成任务\"><a href=\"#4-4已完成任务\" class=\"headerlink\" title=\"4.4已完成任务\"></a>4.4已完成任务</h4><p>采用<code>- [x]</code>表示已完成任务，各符号间均有空格。同时可直接在未完成任务间<code>打勾</code>来转换成已完成任务。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务1</span><br><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务2</span><br><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务3</span><br></pre></td></tr></table></figure>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 已完成任务1</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 已完成任务2</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 已完成任务3</li>\n</ul>\n<h3 id=\"5-表格\"><a href=\"#5-表格\" class=\"headerlink\" title=\"5.表格\"></a>5.表格</h3><p>表格对齐方式</p>\n<ul>\n<li>居左：:—-</li>\n<li>居中：:—-:或—–</li>\n<li>居由：—-:</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">| 标题1           |      标题2      |           标题3 |</span><br><span class=\"line\">| :-------------- | :-------------: | --------------: |</span><br><span class=\"line\">| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |</span><br><span class=\"line\">| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">标题1</th>\n<th style=\"text-align:center\">标题2</th>\n<th style=\"text-align:right\">标题3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">居左测试文本1.1</td>\n<td style=\"text-align:center\">居中测试文本2.1</td>\n<td style=\"text-align:right\">居右测试文本3.1</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">居左测试文本1.2</td>\n<td style=\"text-align:center\">居中测试文本2.2</td>\n<td style=\"text-align:right\">居右测试文本3.2</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"6-段落和换行\"><a href=\"#6-段落和换行\" class=\"headerlink\" title=\"6.段落和换行\"></a>6.段落和换行</h3><h4 id=\"6-1首行缩进方式\"><a href=\"#6-1首行缩进方式\" class=\"headerlink\" title=\"6.1首行缩进方式\"></a>6.1首行缩进方式</h4><ul>\n<li><code>&amp;emsp;</code>中文空格</li>\n<li><code>&amp;ensp;</code>半中文空格</li>\n<li><code>&amp;nbsp;</code>英文空格</li>\n<li><code></code>输入法切换到全角双击空格</li>\n</ul>\n<h4 id=\"6-2换行\"><a href=\"#6-2换行\" class=\"headerlink\" title=\"6.2换行\"></a>6.2换行</h4><ul>\n<li><code></code> <code></code>换行处连续打两个空格</li>\n<li>换行处使用<code>&lt;br&gt;</code>进行换行</li>\n</ul>\n<h4 id=\"6-3空行\"><a href=\"#6-3空行\" class=\"headerlink\" title=\"6.3空行\"></a>6.3空行</h4><ul>\n<li><code></code> <code></code> 空行处连续打两个空格</li>\n<li>换行处使用<code>&lt;br&gt;</code>进行空行</li>\n</ul>\n<h3 id=\"6-引用和代码块\"><a href=\"#6-引用和代码块\" class=\"headerlink\" title=\"6.引用和代码块\"></a>6.引用和代码块</h3><h4 id=\"6-1引用\"><a href=\"#6-1引用\" class=\"headerlink\" title=\"6.1引用\"></a>6.1引用</h4><p>若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"quote\">&gt; 引用1</span></span><br><span class=\"line\"><span class=\"quote\">&gt; &gt; 引用1.1</span></span><br><span class=\"line\"><span class=\"quote\">&gt; &gt; 引用1.2</span></span><br><span class=\"line\"><span class=\"quote\">&gt; 引用2</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>引用1</p>\n<blockquote>\n<p>引用1.1</p>\n<p>引用1.2</p>\n</blockquote>\n<p>引用2</p>\n</blockquote>\n<h4 id=\"6-2代码块\"><a href=\"#6-2代码块\" class=\"headerlink\" title=\"6.2代码块\"></a>6.2代码块</h4><p>代码前后添加<figure class=\"highlight plain\"><figcaption><span>`表示代码块。</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```markdown</span><br><span class=\"line\">​```Python</span><br><span class=\"line\">print(&apos;代码块&apos;)</span><br><span class=\"line\">​</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight clean\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```python</span><br><span class=\"line\">print（<span class=\"string\">'代码块'</span>）</span><br></pre></td></tr></table></figure>\n<h3 id=\"7-链接\"><a href=\"#7-链接\" class=\"headerlink\" title=\"7.链接\"></a>7.链接</h3><h4 id=\"7-1图片链接\"><a href=\"#7-1图片链接\" class=\"headerlink\" title=\"7.1图片链接\"></a>7.1图片链接</h4><p>采用<code>![]()</code>来表示图片链接。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">![<span class=\"string\">图片名称</span>](<span class=\"link\">链接地址</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"Markdown写作教程/图片02.png\" alt=\"图片名称\"></p>\n<h4 id=\"7-2文字链接\"><a href=\"#7-2文字链接\" class=\"headerlink\" title=\"7.2文字链接\"></a>7.2文字链接</h4><p>采用<code>[]()</code>表示文字链接。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"string\">链接名称</span>](<span class=\"link\">链接地址</span>)</span><br></pre></td></tr></table></figure>\n<p><a href=\"weizhixiaoyi.com\">文字链接</a></p>\n<h4 id=\"7-3参考链接\"><a href=\"#7-3参考链接\" class=\"headerlink\" title=\"7.3参考链接\"></a>7.3参考链接</h4><p>采用<code>[ ]:</code>表示参考链接，注意符号后有空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"symbol\"> </span>]: <span class=\"link\">url title</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"8-分割线\"><a href=\"#8-分割线\" class=\"headerlink\" title=\"8.分割线\"></a>8.分割线</h3><p>上下文无关时可使用分割符进行分开。</p>\n<ul>\n<li>连续多个<code>-</code>  (&gt;=3)</li>\n<li>连续多个<code>*</code> （&gt;=3）</li>\n<li>连续多个下划线<code>_</code> （&gt;=3）</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---分割线</span><br><span class=\"line\"><span class=\"emphasis\">***</span>分割线</span><br><span class=\"line\"><span class=\"emphasis\">___</span>分割线</span><br></pre></td></tr></table></figure>\n<hr>\n<hr>\n<hr>\n<h3 id=\"9-脚注和注释\"><a href=\"#9-脚注和注释\" class=\"headerlink\" title=\"9.脚注和注释\"></a>9.脚注和注释</h3><h4 id=\"9-1脚注\"><a href=\"#9-1脚注\" class=\"headerlink\" title=\"9.1脚注\"></a>9.1脚注</h4><p>采用`<a href=\"脚注\">^</a>:表示脚注，注意空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"symbol\">^</span>]: <span class=\"link\">脚注</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"9-2注释\"><a href=\"#9-2注释\" class=\"headerlink\" title=\"9.2注释\"></a>9.2注释</h4><p>采用<code>&lt;!----&gt;</code>表示注释.</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"comment\">&lt;!--注释--&gt;</span></span></span><br></pre></td></tr></table></figure>\n<!--注释-->\n<h3 id=\"11-转义\"><a href=\"#11-转义\" class=\"headerlink\" title=\"11.转义\"></a>11.转义</h3><p>Markdown通过反斜杠<code>\\</code>来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\反斜线</span><br><span class=\"line\">\\`反引号</span><br><span class=\"line\">\\*星号</span><br><span class=\"line\">\\_下划线</span><br><span class=\"line\">\\&#123;&#125;花括号</span><br><span class=\"line\">\\[]方括号</span><br><span class=\"line\">\\()括弧</span><br><span class=\"line\">\\#井字号</span><br><span class=\"line\">\\+加号</span><br><span class=\"line\">\\-减号</span><br><span class=\"line\">\\.英文句点</span><br><span class=\"line\">\\!感叹号</span><br></pre></td></tr></table></figure>\n<p>\\反斜线<br>`反引号<br>*星号<br>_下划线<br>{}花括号<br>[]方括号<br>()括弧<br>#井字号<br>+加号<br>-减号<br>.英文句点<br>!感叹号</p>\n<h3 id=\"12-目录\"><a href=\"#12-目录\" class=\"headerlink\" title=\"12.目录\"></a>12.目录</h3><p>采用<code>[TOC]</code>来生成文章目录。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[TOC]</span><br></pre></td></tr></table></figure>\n<p><img src=\"Markdown写作教程/图片03.png\" alt=\"图片03\"></p>\n<hr>\n<h3 id=\"13-推广\"><a href=\"#13-推广\" class=\"headerlink\" title=\"13.推广\"></a>13.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"Markdown写作教程/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://weizhixiaoyi.com/2018/03/16/Mac+Hexo+GitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/\">博客搭建教程</a>写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。</p>\n<h3 id=\"1-为什么选择Markdown\"><a href=\"#1-为什么选择Markdown\" class=\"headerlink\" title=\"1.为什么选择Markdown\"></a>1.为什么选择Markdown</h3><p>首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用<strong>印象笔记</strong>至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。</p>\n<h3 id=\"2-标题\"><a href=\"#2-标题\" class=\"headerlink\" title=\"2.标题\"></a>2.标题</h3><p>标题通过<code>#</code>的个数进行区分，Markdown共支持6级标题。</p>\n<p><img src=\"Markdown写作教程/图片01.png\" alt=\"标题\"></p>\n<h3 id=\"3-字体设置\"><a href=\"#3-字体设置\" class=\"headerlink\" title=\"3.字体设置\"></a>3.字体设置</h3><h4 id=\"3-1粗体\"><a href=\"#3-1粗体\" class=\"headerlink\" title=\"3.1粗体\"></a>3.1粗体</h4><p>文字前后加<code>**</code>来表示粗体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">**粗体**</span></span><br></pre></td></tr></table></figure>\n<p><strong>粗体</strong></p>\n<h4 id=\"3-2斜体\"><a href=\"#3-2斜体\" class=\"headerlink\" title=\"3.2斜体\"></a>3.2斜体</h4><p>文字前后加<code>*</code>来表示斜体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"emphasis\">*斜体*</span></span><br></pre></td></tr></table></figure>\n<p><em>斜体</em></p>\n<h4 id=\"3-3粗斜体\"><a href=\"#3-3粗斜体\" class=\"headerlink\" title=\"3.3粗斜体\"></a>3.3粗斜体</h4><p>文字前后加<code>***</code>来表示粗斜体。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">***粗斜体**</span>*</span><br></pre></td></tr></table></figure>\n<p><strong><em>粗斜体</em></strong></p>\n<h4 id=\"3-4下划线\"><a href=\"#3-4下划线\" class=\"headerlink\" title=\"3.4下划线\"></a>3.4下划线</h4><p>文字前后加<code>&lt;u&gt;</code> <code>&lt;/u&gt;</code>来表示下划线。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">u</span>&gt;</span></span>下滑线<span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">u</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<p><u>下划线</u></p>\n<h4 id=\"3-5删除线\"><a href=\"#3-5删除线\" class=\"headerlink\" title=\"3.5删除线\"></a>3.5删除线</h4><p>文字前后加<code>~~</code>来表示删除线。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~~删除线~~</span><br></pre></td></tr></table></figure>\n<p><del>删除线</del></p>\n<h4 id=\"3-6标记\"><a href=\"#3-6标记\" class=\"headerlink\" title=\"3.6标记\"></a>3.6标记</h4><p>文字前后加<code></code> `来表示标记，该符号位于Esc键下面。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"code\">`标记`</span></span><br></pre></td></tr></table></figure>\n<p><code>标记</code></p>\n<h4 id=\"3-7Html标签\"><a href=\"#3-7Html标签\" class=\"headerlink\" title=\"3.7Html标签\"></a>3.7Html标签</h4><figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"tag\">&lt;<span class=\"name\">font</span> <span class=\"attr\">face</span>=<span class=\"string\">\"微软雅黑\"</span> <span class=\"attr\">color</span>=<span class=\"string\">\"red\"</span> <span class=\"attr\">size</span>=<span class=\"string\">\"6\"</span>&gt;</span></span>字体及字体颜色和大小<span class=\"xml\"><span class=\"tag\">&lt;/<span class=\"name\">font</span>&gt;</span></span></span><br></pre></td></tr></table></figure>\n<font face=\"微软雅黑\" color=\"red\" size=\"3\">字体及字体颜色和大小</font>\n\n<h3 id=\"4-列表\"><a href=\"#4-列表\" class=\"headerlink\" title=\"4.列表\"></a>4.列表</h3><h4 id=\"4-1有序列表\"><a href=\"#4-1有序列表\" class=\"headerlink\" title=\"4.1有序列表\"></a>4.1有序列表</h4><p>采用<code>1.</code> 后加空格形式表示有序列表。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">1. </span>有序列表1</span><br><span class=\"line\"><span class=\"bullet\">2. </span>有序列表2</span><br><span class=\"line\"><span class=\"bullet\">3. </span>有序列表3</span><br></pre></td></tr></table></figure>\n<ol>\n<li>有序列表1</li>\n<li>有序列表2</li>\n<li>有序列表3</li>\n</ol>\n<h4 id=\"4-2无序列表\"><a href=\"#4-2无序列表\" class=\"headerlink\" title=\"4.2无序列表\"></a>4.2无序列表</h4><p>采用<code>+</code> <code>-</code> <code>*</code> <code>=</code>符号表示无序列表，支持多级嵌套。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">+ </span>有序列表1</span><br><span class=\"line\"><span class=\"bullet\">+ </span>+ 有序列表1.1</span><br><span class=\"line\"><span class=\"bullet\">+ </span>+ 有序列表1.2</span><br><span class=\"line\"><span class=\"bullet\">+ </span>有序列表2</span><br><span class=\"line\"><span class=\"bullet\">+ </span>有序列表3</span><br></pre></td></tr></table></figure>\n<ul>\n<li>无序列表1<ul>\n<li>无序列表1.1</li>\n<li>无序列表1.2</li>\n</ul>\n</li>\n<li>无序列表2</li>\n<li>无序列表3</li>\n</ul>\n<h4 id=\"4-3未完成列表\"><a href=\"#4-3未完成列表\" class=\"headerlink\" title=\"4.3未完成列表\"></a>4.3未完成列表</h4><p>采用<code>- []</code>表示未完成任务，各符号间均有空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务1</span><br><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务2</span><br><span class=\"line\"><span class=\"bullet\">- </span>[ ] 未完成任务3</span><br></pre></td></tr></table></figure>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 未完成任务1</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 未完成任务2</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 未完成任务3</li>\n</ul>\n<h4 id=\"4-4已完成任务\"><a href=\"#4-4已完成任务\" class=\"headerlink\" title=\"4.4已完成任务\"></a>4.4已完成任务</h4><p>采用<code>- [x]</code>表示已完成任务，各符号间均有空格。同时可直接在未完成任务间<code>打勾</code>来转换成已完成任务。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务1</span><br><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务2</span><br><span class=\"line\"><span class=\"bullet\">- </span>[x] 已完成任务3</span><br></pre></td></tr></table></figure>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 已完成任务1</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 已完成任务2</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 已完成任务3</li>\n</ul>\n<h3 id=\"5-表格\"><a href=\"#5-表格\" class=\"headerlink\" title=\"5.表格\"></a>5.表格</h3><p>表格对齐方式</p>\n<ul>\n<li>居左：:—-</li>\n<li>居中：:—-:或—–</li>\n<li>居由：—-:</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">| 标题1           |      标题2      |           标题3 |</span><br><span class=\"line\">| :-------------- | :-------------: | --------------: |</span><br><span class=\"line\">| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |</span><br><span class=\"line\">| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">标题1</th>\n<th style=\"text-align:center\">标题2</th>\n<th style=\"text-align:right\">标题3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">居左测试文本1.1</td>\n<td style=\"text-align:center\">居中测试文本2.1</td>\n<td style=\"text-align:right\">居右测试文本3.1</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">居左测试文本1.2</td>\n<td style=\"text-align:center\">居中测试文本2.2</td>\n<td style=\"text-align:right\">居右测试文本3.2</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"6-段落和换行\"><a href=\"#6-段落和换行\" class=\"headerlink\" title=\"6.段落和换行\"></a>6.段落和换行</h3><h4 id=\"6-1首行缩进方式\"><a href=\"#6-1首行缩进方式\" class=\"headerlink\" title=\"6.1首行缩进方式\"></a>6.1首行缩进方式</h4><ul>\n<li><code>&amp;emsp;</code>中文空格</li>\n<li><code>&amp;ensp;</code>半中文空格</li>\n<li><code>&amp;nbsp;</code>英文空格</li>\n<li><code></code>输入法切换到全角双击空格</li>\n</ul>\n<h4 id=\"6-2换行\"><a href=\"#6-2换行\" class=\"headerlink\" title=\"6.2换行\"></a>6.2换行</h4><ul>\n<li><code></code> <code></code>换行处连续打两个空格</li>\n<li>换行处使用<code>&lt;br&gt;</code>进行换行</li>\n</ul>\n<h4 id=\"6-3空行\"><a href=\"#6-3空行\" class=\"headerlink\" title=\"6.3空行\"></a>6.3空行</h4><ul>\n<li><code></code> <code></code> 空行处连续打两个空格</li>\n<li>换行处使用<code>&lt;br&gt;</code>进行空行</li>\n</ul>\n<h3 id=\"6-引用和代码块\"><a href=\"#6-引用和代码块\" class=\"headerlink\" title=\"6.引用和代码块\"></a>6.引用和代码块</h3><h4 id=\"6-1引用\"><a href=\"#6-1引用\" class=\"headerlink\" title=\"6.1引用\"></a>6.1引用</h4><p>若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"quote\">&gt; 引用1</span></span><br><span class=\"line\"><span class=\"quote\">&gt; &gt; 引用1.1</span></span><br><span class=\"line\"><span class=\"quote\">&gt; &gt; 引用1.2</span></span><br><span class=\"line\"><span class=\"quote\">&gt; 引用2</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>引用1</p>\n<blockquote>\n<p>引用1.1</p>\n<p>引用1.2</p>\n</blockquote>\n<p>引用2</p>\n</blockquote>\n<h4 id=\"6-2代码块\"><a href=\"#6-2代码块\" class=\"headerlink\" title=\"6.2代码块\"></a>6.2代码块</h4><p>代码前后添加<figure class=\"highlight plain\"><figcaption><span>`表示代码块。</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```markdown</span><br><span class=\"line\">​```Python</span><br><span class=\"line\">print(&apos;代码块&apos;)</span><br><span class=\"line\">​</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight clean\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">```python</span><br><span class=\"line\">print（<span class=\"string\">'代码块'</span>）</span><br></pre></td></tr></table></figure>\n<h3 id=\"7-链接\"><a href=\"#7-链接\" class=\"headerlink\" title=\"7.链接\"></a>7.链接</h3><h4 id=\"7-1图片链接\"><a href=\"#7-1图片链接\" class=\"headerlink\" title=\"7.1图片链接\"></a>7.1图片链接</h4><p>采用<code>![]()</code>来表示图片链接。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">![<span class=\"string\">图片名称</span>](<span class=\"link\">链接地址</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"Markdown写作教程/图片02.png\" alt=\"图片名称\"></p>\n<h4 id=\"7-2文字链接\"><a href=\"#7-2文字链接\" class=\"headerlink\" title=\"7.2文字链接\"></a>7.2文字链接</h4><p>采用<code>[]()</code>表示文字链接。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"string\">链接名称</span>](<span class=\"link\">链接地址</span>)</span><br></pre></td></tr></table></figure>\n<p><a href=\"weizhixiaoyi.com\">文字链接</a></p>\n<h4 id=\"7-3参考链接\"><a href=\"#7-3参考链接\" class=\"headerlink\" title=\"7.3参考链接\"></a>7.3参考链接</h4><p>采用<code>[ ]:</code>表示参考链接，注意符号后有空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"symbol\"> </span>]: <span class=\"link\">url title</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"8-分割线\"><a href=\"#8-分割线\" class=\"headerlink\" title=\"8.分割线\"></a>8.分割线</h3><p>上下文无关时可使用分割符进行分开。</p>\n<ul>\n<li>连续多个<code>-</code>  (&gt;=3)</li>\n<li>连续多个<code>*</code> （&gt;=3）</li>\n<li>连续多个下划线<code>_</code> （&gt;=3）</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---分割线</span><br><span class=\"line\"><span class=\"emphasis\">***</span>分割线</span><br><span class=\"line\"><span class=\"emphasis\">___</span>分割线</span><br></pre></td></tr></table></figure>\n<hr>\n<hr>\n<hr>\n<h3 id=\"9-脚注和注释\"><a href=\"#9-脚注和注释\" class=\"headerlink\" title=\"9.脚注和注释\"></a>9.脚注和注释</h3><h4 id=\"9-1脚注\"><a href=\"#9-1脚注\" class=\"headerlink\" title=\"9.1脚注\"></a>9.1脚注</h4><p>采用`<a href=\"脚注\">^</a>:表示脚注，注意空格。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"symbol\">^</span>]: <span class=\"link\">脚注</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"9-2注释\"><a href=\"#9-2注释\" class=\"headerlink\" title=\"9.2注释\"></a>9.2注释</h4><p>采用<code>&lt;!----&gt;</code>表示注释.</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"xml\"><span class=\"comment\">&lt;!--注释--&gt;</span></span></span><br></pre></td></tr></table></figure>\n<!--注释-->\n<h3 id=\"11-转义\"><a href=\"#11-转义\" class=\"headerlink\" title=\"11.转义\"></a>11.转义</h3><p>Markdown通过反斜杠<code>\\</code>来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\反斜线</span><br><span class=\"line\">\\`反引号</span><br><span class=\"line\">\\*星号</span><br><span class=\"line\">\\_下划线</span><br><span class=\"line\">\\&#123;&#125;花括号</span><br><span class=\"line\">\\[]方括号</span><br><span class=\"line\">\\()括弧</span><br><span class=\"line\">\\#井字号</span><br><span class=\"line\">\\+加号</span><br><span class=\"line\">\\-减号</span><br><span class=\"line\">\\.英文句点</span><br><span class=\"line\">\\!感叹号</span><br></pre></td></tr></table></figure>\n<p>\\反斜线<br>`反引号<br>*星号<br>_下划线<br>{}花括号<br>[]方括号<br>()括弧<br>#井字号<br>+加号<br>-减号<br>.英文句点<br>!感叹号</p>\n<h3 id=\"12-目录\"><a href=\"#12-目录\" class=\"headerlink\" title=\"12.目录\"></a>12.目录</h3><p>采用<code>[TOC]</code>来生成文章目录。</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[TOC]</span><br></pre></td></tr></table></figure>\n<p><img src=\"Markdown写作教程/图片03.png\" alt=\"图片03\"></p>\n<hr>\n<h3 id=\"13-推广\"><a href=\"#13-推广\" class=\"headerlink\" title=\"13.推广\"></a>13.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"Markdown写作教程/推广.png\" alt=\"推广\"></p>\n"},{"title":"Python之MatPlotLib使用教程","date":"2018-03-14T03:43:07.000Z","toc":true,"comments":1,"_content":"### 1.Matplotlib简介\n\n 1. Matplotlib是非常强大的python画图工具\n 2. Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 \n\n### 2.Matplotlib安装\n```\npip3 install matplotlib#python3\n```\n### 3.Matplotlib引入\n```\nimport matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas\n```\n### 4.Matplotlib基本应用\n```\nx=np.linspace(-1,1,50)#定义x数据范围\ny1=2*x+1#定义y数据范围\ny2=x**2\nplt.figure()#定义一个图像窗口\nplt.plot(x,y)#plot()画出曲线\nplt.show()#显示图像\n```\n![图片01](http://img.blog.csdn.net/20180311144537863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 4.1figure图像\nmatplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。\n```\nx=np.linspace(-3,3,50)#50为生成的样本数\ny1=2*x+1\ny2=x**2\nplt.figure(num=1,figsize=(8,5))#定义编号为1 大小为(8,5)\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')#颜色为红色，线宽度为2，线风格为--\nplt.plot(x,y2)#进行画图\nplt.show()#显示图\n```\n![图片02](http://img.blog.csdn.net/20180311145427674?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 4.2设置坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='-')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(\"I'm x\")\nplt.ylabel(\"I'm y\")\nplt.show()\n```\n![图片03](http://img.blog.csdn.net/20180311151115803?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n自定义坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='-')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(\"I'm x\")\nplt.ylabel(\"I'm y\")\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nprint(new_ticks)\n#[-1.   -0.25  0.5   1.25  2.  ]\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nplt.show()\n```\n![图片04](http://img.blog.csdn.net/2018031115185255?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n设置边框属性\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nax=plt.gca()#gca=get current axis\nax.spines['right'].set_color('none')#边框属性设置为none 不显示\nax.spines['top'].set_color('none')\nplt.show()\n```\n![图片05](http://img.blog.csdn.net/20180311152822953?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n调整移动坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nax=plt.gca()#gca=get current axis\nax.spines['right'].set_color('none')#边框属性设置为none 不显示\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none\nax.spines['bottom'].set_position(('data', 0))#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))#坐标中心点在(0,0)位置\nplt.show()\n```\n![这里写图片描述](http://img.blog.csdn.net/20180311153109404?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 4.3添加图例\nmatplotlib中legend图例帮助我们展示数据对应的图像名称。\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\n\nl1,=plt.plot(x,y1,color='red',linewidth=2,linestyle='--',label='linear line')\nl2,=plt.plot(x,y2,label='square line')#进行画图\nplt.legend(loc='best')#显示在最好的位置\nplt.show()#显示图\n```\n![图片07](http://img.blog.csdn.net/20180311163819992?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数\n```\nplt.legend(handles=[l1, l2], labels=['up', 'down'],  loc='best')\n#loc有很多参数 其中best自分配最佳位置\n'''\n 'best' : 0,          \n 'upper right'  : 1,\n 'upper left'   : 2,\n 'lower left'   : 3,\n 'lower right'  : 4,\n 'right'        : 5,\n 'center left'  : 6,\n 'center right' : 7,\n 'lower center' : 8,\n 'upper center' : 9,\n 'center'       : 10,\n '''\n```\n#### 4.4标注\n\n```\nx=np.linspace(-3,3,50)\ny = 2*x + 1\nplt.figure(num=1, figsize=(8, 5))\nplt.plot(x, y,)\n\n#移动坐标轴\nax = plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data', 0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data', 0))\n\n#标注信息\nx0=1\ny0=2*x0+1\nplt.scatter(x0,y0,s=50,color='b')\nplt.plot([x0,x0],[y0,0],'k--',lw=2.5)#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细\n#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置\nplt.annotate(r'$2x0+1=%s$' % y0, xy=(x0, y0), xycoords='data', xytext=(+30, -30),\n             textcoords='offset points', fontsize=16,\n             arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=.2\"))\n#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             \nplt.text(-3.7, 3, r'$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$',\n         fontdict={'size': 16, 'color': 'r'})\nplt.show()\n```\n![图片08](http://img.blog.csdn.net/2018031116474811?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 4.5能见度调整\n```\nx=np.linspace(-3, 3, 50)\ny=0.1*x\nplt.figure()\nplt.plot(x, y, linewidth=10, zorder=1)\nplt.ylim(-2, 2)\n\n#移动坐标轴\nax = plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data', 0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data', 0))\n\n#label.set_fontsize(12)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序\nfor label in ax.get_xticklabels() + ax.get_yticklabels():\n    label.set_fontsize(12)\n    label.set_bbox(dict(facecolor='red', edgecolor='None', alpha=0.7, zorder=2))\nplt.show()\n```\n![图片09](http://img.blog.csdn.net/20180311172352949?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n### 5.画图种类\n#### 5.1Scatter散点图\n```\nn=1024\nX=np.random.normal(0,1,n)#每一个点的X值\nY=np.random.normal(0,1,n)#每一个点的Y值\nT=np.arctan2(Y,X)#arctan2返回给定的X和Y值的反正切值\n#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴\nplt.scatter(X,Y,s=75,c=T,alpha=0.5)\nplt.xlim(-1.5,1.5)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.5,1.5)\nplt.yticks(())#忽略yticks\nplt.show()\n```\n\n![图片10](http://img.blog.csdn.net/20180311174847167?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 5.2条形图\n```\n#基本图形\nn=12\nX=np.arange(n)\nY1=(1-X/float(n))*np.random.uniform(0.5,1,n)\nY2=(1-X/float(n))*np.random.uniform(0.5,1,n)\nplt.bar(X,+Y1,facecolor='#9999ff',edgecolor='white')\nplt.bar(X,-Y2,facecolor='#ff9999',edgecolor='white')\n\n#标记值\nfor x,y in zip(X,Y1):#zip表示可以传递两个值\n    plt.text(x+0.4,y+0.05,'%.2f'%y,ha='center',va='bottom')#ha表示横向对齐 bottom表示向下对齐\nfor x,y in zip(X,Y2):\n    plt.text(x+0.4,-y-0.05,'%.2f'%y,ha='center',va='top')\nplt.xlim(-0.5,n)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.25,1.25)\nplt.yticks(())#忽略yticks\nplt.show()\n```\n![图片11](http://img.blog.csdn.net/20180311181815729?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 5.3等高线图\n\n```\nn=256\nx=np.linspace(-3,3,n)\ny=np.linspace(-3,3,n)\nX,Y=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵\n#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中\ndef f(x,y):\n    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)\nplt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map\n#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5\nC=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)\n#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10\nplt.clabel(C,inline=True,fontsize=10)\nplt.xticks(())#隐藏坐标轴\nplt.yticks(())\nplt.show()\n```\n![图片12](http://img.blog.csdn.net/20180311182506284?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 5.4Image图片\n利用matplotlib打印出图像\n```\na = np.array([0.313660827978, 0.365348418405, 0.423733120134,\n              0.365348418405, 0.439599930621, 0.525083754405,\n              0.423733120134, 0.525083754405, 0.651536351379]).reshape(3,3)\n#origin='lower'代表的就是选择的原点位置\nplt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color map\nplt.colorbar(shrink=.92)#右边颜色说明 shrink参数是将图片长度变为原来的92%\nplt.xticks(())\nplt.yticks(())\nplt.show()              \n```\n![图片13](http://img.blog.csdn.net/20180311192757270?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n出图方式 此处采用内插法中的nearest-neighbor\n![图片14](http://img.blog.csdn.net/20180311193343476?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 5.53D图像\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D\nfig=plt.figure()#定义图像窗口\nax=Axes3D(fig)#在窗口上添加3D坐标轴\n#将X和Y值编织成栅格\nX=np.arange(-4,4,0.25)\nY=np.arange(-4,4,0.25)\nX,Y=np.meshgrid(X,Y)\nR=np.sqrt(X**2+Y**2)\nZ=np.sin(R)#高度值\n#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度\nax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图\n#添加XY平面等高线 投影到z平面\nax.contourf(X,Y,Z,zdir='z',offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置\nax.set_zlim(-2,2)\nplt.show()\n```\n![图片15](http://img.blog.csdn.net/20180311194735746?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n### 6.多图合并显示\n\n#### 6.1Subplot多合一显示\n\n均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。\n\n```python\nplt.figure()\nplt.subplot(2,1,1)#表示整个图像分割成2行2列，当前位置为1\nplt.plot([0,1],[0,1])#横坐标变化为[0,1] 竖坐标变化为[0,2]\n\nplt.subplot(2,3,4)\nplt.plot([0,1],[0,2])\n\nplt.subplot(2,3,5)\nplt.plot([0,1],[0,3])\n\nplt.subplot(2,3,6)\nplt.plot([0,1],[0,4])\nplt.show()\n```\n\n![图片16](Python之MatPlotLib使用教程/图片16.png)\n\n不均匀图中图\n\n```python\nplt.figure()\nplt.subplot(2,1,1)#将整个窗口分割成2行1列，当前位置表示第一个图\nplt.plot([0,1],[0,1])#横坐标变化为[0,1],竖坐标变化为[0,1]\n\nplt.subplot(2,3,4)#将整个窗口分割成2行3列，当前位置为4\nplt.plot([0,1],[0,2])\n\nplt.subplot(2,3,5)\nplt.plot([0,1],[0,3])\n\nplt.subplot(2,3,6)\nplt.plot([0,1],[0,4])\nplt.show()\n```\n\n![图片17](Python之MatPlotLib使用教程/图片17.png)\n\n#### 6.2SubPlot分格显示\n\n方法一\n\n```python\nimport matplotlib.gridspec as gridspec#引入新模块\nplt.figure()\n'''\n使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1\n'''\nax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3)  # stands for axes\nax1.plot([1, 2], [1, 2])\nax1.set_title('ax1_title')#设置图的标题\n\n#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2\nax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)\n\n#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2\nax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\n#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1\nax4 = plt.subplot2grid((3, 3), (2, 0))\nax4.scatter([1, 2], [2, 2])\nax4.set_xlabel('ax4_x')\nax4.set_ylabel('ax4_y')\nax5 = plt.subplot2grid((3, 3), (2, 1))\n```\n\n![图像18](Python之MatPlotLib使用教程/图像18.png)\n\n方法二\n\n```Python\nplt.figure()\ngs = gridspec.GridSpec(3, 3)#将图像分割成3行3列\nax6 = plt.subplot(gs[0, :])#gs[0:1]表示图占第0行和所有列\nax7 = plt.subplot(gs[1, :2])#gs[1,:2]表示图占第1行和第二列前的所有列\nax8 = plt.subplot(gs[1:, 2])\nax9 = plt.subplot(gs[-1, 0])\nax10 = plt.subplot(gs[-1, -2])#gs[-1.-2]表示这个图占倒数第1行和倒数第2行\nplt.show()\n```\n\n![图像19](Python之MatPlotLib使用教程/图像19.png)\n\n方法三\n\n```Python\n'''\n建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114\n'''\nf, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, sharex=True, sharey=True)\nax11.scatter([1,2], [1,2])ax11.scatter 坐标范围x为[1,2]，y为[1,2]\nplt.tight_layout()#表示紧凑显示图像\nplt.show()\n```\n\n![图像21](Python之MatPlotLib使用教程/图像20.png)\n\n#### 6.3图中图\n\n```Python\nfig=plt.figure()\n#创建数据\nx=[1,2,3,4,5,6,7]\ny=[1,3,4,2,5,8,6]\n\n#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。\nleft, bottom, width, height = 0.1, 0.1, 0.8, 0.8\nax1 = fig.add_axes([left, bottom, width, height])  # main axes\nax1.plot(x, y, 'r')#绘制大图，颜色为red\nax1.set_xlabel('x')#横坐标名称为x\nax1.set_ylabel('y')\nax1.set_title('title')#图名称为title\n\n#绘制小图，注意坐标系位置和大小的改变\nax2 = fig.add_axes([0.2, 0.6, 0.25, 0.25])\nax2.plot(y, x, 'b')#颜色为buue\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nax2.set_title('title inside 1')\n\n#绘制第二个小兔\nplt.axes([0.6, 0.2, 0.25, 0.25])\nplt.plot(y[::-1], x, 'g')#将y进行逆序\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('title inside 2')\nplt.show()\n```\n\n![图像21](Python之MatPlotLib使用教程/图像21.png)\n\n#### 6.4次坐标轴\n\n```Python\nx=np.arange(0,10,0.1)\ny1=0.5*x**2\ny2=-1*y1\nfig, ax1 = plt.subplots()\n\nax2 = ax1.twinx()#镜像显示\nax1.plot(x, y1, 'g-')\nax2.plot(x, y2, 'b-')\n\nax1.set_xlabel('X data')\nax1.set_ylabel('Y1 data', color='g')#第一个y坐标轴\nax2.set_ylabel('Y2 data', color='b')#第二个y坐标轴\nplt.show()\n```\n\n![图像22](Python之MatPlotLib使用教程/图像22.png)\n\n### 7.动画\n\n```Python\nfrom matplotlib import animation#引入新模块\nfig,ax=plt.subplots()\nx=np.arange(0,2*np.pi,0.01)#数据为0~2PI范围内的正弦曲线\nline,=ax.plot(x,np.sin(x))# line表示列表\n\n#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧\ndef animate(i):\n    line.set_ydata(np.sin(x+i/100))\n    return line,\n\n#构造开始帧函数init\ndef init():\n    line.set_ydata(np.sin(x))\n    return line,\n\n# frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 \n# blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。\nani=animation.FuncAnimation(fig=fig,func=animate,frames=200,init_func=init,interval=20,blit=False)\nplt.show()\n```\n\n![图像23](Python之MatPlotLib使用教程/图像23.png)\n\n**MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。**\n\n----------\n\n\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。\n\n","source":"_posts/Python之MatPlotLib使用教程.md","raw":"---\ntitle: Python之MatPlotLib使用教程\ndate: 2018-03-14 11:43:07\ntags: python\ntoc: true\ncategories: Python库\ncomments: true\n---\n### 1.Matplotlib简介\n\n 1. Matplotlib是非常强大的python画图工具\n 2. Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 \n\n### 2.Matplotlib安装\n```\npip3 install matplotlib#python3\n```\n### 3.Matplotlib引入\n```\nimport matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas\n```\n### 4.Matplotlib基本应用\n```\nx=np.linspace(-1,1,50)#定义x数据范围\ny1=2*x+1#定义y数据范围\ny2=x**2\nplt.figure()#定义一个图像窗口\nplt.plot(x,y)#plot()画出曲线\nplt.show()#显示图像\n```\n![图片01](http://img.blog.csdn.net/20180311144537863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 4.1figure图像\nmatplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。\n```\nx=np.linspace(-3,3,50)#50为生成的样本数\ny1=2*x+1\ny2=x**2\nplt.figure(num=1,figsize=(8,5))#定义编号为1 大小为(8,5)\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')#颜色为红色，线宽度为2，线风格为--\nplt.plot(x,y2)#进行画图\nplt.show()#显示图\n```\n![图片02](http://img.blog.csdn.net/20180311145427674?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 4.2设置坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='-')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(\"I'm x\")\nplt.ylabel(\"I'm y\")\nplt.show()\n```\n![图片03](http://img.blog.csdn.net/20180311151115803?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n自定义坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='-')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(\"I'm x\")\nplt.ylabel(\"I'm y\")\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nprint(new_ticks)\n#[-1.   -0.25  0.5   1.25  2.  ]\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nplt.show()\n```\n![图片04](http://img.blog.csdn.net/2018031115185255?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n设置边框属性\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nax=plt.gca()#gca=get current axis\nax.spines['right'].set_color('none')#边框属性设置为none 不显示\nax.spines['top'].set_color('none')\nplt.show()\n```\n![图片05](http://img.blog.csdn.net/20180311152822953?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n调整移动坐标轴\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color='red',linewidth=2,linestyle='--')\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\nax=plt.gca()#gca=get current axis\nax.spines['right'].set_color('none')#边框属性设置为none 不显示\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none\nax.spines['bottom'].set_position(('data', 0))#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))#坐标中心点在(0,0)位置\nplt.show()\n```\n![这里写图片描述](http://img.blog.csdn.net/20180311153109404?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 4.3添加图例\nmatplotlib中legend图例帮助我们展示数据对应的图像名称。\n```\nx=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])\n\nl1,=plt.plot(x,y1,color='red',linewidth=2,linestyle='--',label='linear line')\nl2,=plt.plot(x,y2,label='square line')#进行画图\nplt.legend(loc='best')#显示在最好的位置\nplt.show()#显示图\n```\n![图片07](http://img.blog.csdn.net/20180311163819992?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数\n```\nplt.legend(handles=[l1, l2], labels=['up', 'down'],  loc='best')\n#loc有很多参数 其中best自分配最佳位置\n'''\n 'best' : 0,          \n 'upper right'  : 1,\n 'upper left'   : 2,\n 'lower left'   : 3,\n 'lower right'  : 4,\n 'right'        : 5,\n 'center left'  : 6,\n 'center right' : 7,\n 'lower center' : 8,\n 'upper center' : 9,\n 'center'       : 10,\n '''\n```\n#### 4.4标注\n\n```\nx=np.linspace(-3,3,50)\ny = 2*x + 1\nplt.figure(num=1, figsize=(8, 5))\nplt.plot(x, y,)\n\n#移动坐标轴\nax = plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data', 0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data', 0))\n\n#标注信息\nx0=1\ny0=2*x0+1\nplt.scatter(x0,y0,s=50,color='b')\nplt.plot([x0,x0],[y0,0],'k--',lw=2.5)#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细\n#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置\nplt.annotate(r'$2x0+1=%s$' % y0, xy=(x0, y0), xycoords='data', xytext=(+30, -30),\n             textcoords='offset points', fontsize=16,\n             arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=.2\"))\n#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             \nplt.text(-3.7, 3, r'$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$',\n         fontdict={'size': 16, 'color': 'r'})\nplt.show()\n```\n![图片08](http://img.blog.csdn.net/2018031116474811?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 4.5能见度调整\n```\nx=np.linspace(-3, 3, 50)\ny=0.1*x\nplt.figure()\nplt.plot(x, y, linewidth=10, zorder=1)\nplt.ylim(-2, 2)\n\n#移动坐标轴\nax = plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data', 0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data', 0))\n\n#label.set_fontsize(12)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序\nfor label in ax.get_xticklabels() + ax.get_yticklabels():\n    label.set_fontsize(12)\n    label.set_bbox(dict(facecolor='red', edgecolor='None', alpha=0.7, zorder=2))\nplt.show()\n```\n![图片09](http://img.blog.csdn.net/20180311172352949?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n### 5.画图种类\n#### 5.1Scatter散点图\n```\nn=1024\nX=np.random.normal(0,1,n)#每一个点的X值\nY=np.random.normal(0,1,n)#每一个点的Y值\nT=np.arctan2(Y,X)#arctan2返回给定的X和Y值的反正切值\n#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴\nplt.scatter(X,Y,s=75,c=T,alpha=0.5)\nplt.xlim(-1.5,1.5)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.5,1.5)\nplt.yticks(())#忽略yticks\nplt.show()\n```\n\n![图片10](http://img.blog.csdn.net/20180311174847167?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 5.2条形图\n```\n#基本图形\nn=12\nX=np.arange(n)\nY1=(1-X/float(n))*np.random.uniform(0.5,1,n)\nY2=(1-X/float(n))*np.random.uniform(0.5,1,n)\nplt.bar(X,+Y1,facecolor='#9999ff',edgecolor='white')\nplt.bar(X,-Y2,facecolor='#ff9999',edgecolor='white')\n\n#标记值\nfor x,y in zip(X,Y1):#zip表示可以传递两个值\n    plt.text(x+0.4,y+0.05,'%.2f'%y,ha='center',va='bottom')#ha表示横向对齐 bottom表示向下对齐\nfor x,y in zip(X,Y2):\n    plt.text(x+0.4,-y-0.05,'%.2f'%y,ha='center',va='top')\nplt.xlim(-0.5,n)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.25,1.25)\nplt.yticks(())#忽略yticks\nplt.show()\n```\n![图片11](http://img.blog.csdn.net/20180311181815729?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 5.3等高线图\n\n```\nn=256\nx=np.linspace(-3,3,n)\ny=np.linspace(-3,3,n)\nX,Y=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵\n#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中\ndef f(x,y):\n    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)\nplt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map\n#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5\nC=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)\n#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10\nplt.clabel(C,inline=True,fontsize=10)\nplt.xticks(())#隐藏坐标轴\nplt.yticks(())\nplt.show()\n```\n![图片12](http://img.blog.csdn.net/20180311182506284?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 5.4Image图片\n利用matplotlib打印出图像\n```\na = np.array([0.313660827978, 0.365348418405, 0.423733120134,\n              0.365348418405, 0.439599930621, 0.525083754405,\n              0.423733120134, 0.525083754405, 0.651536351379]).reshape(3,3)\n#origin='lower'代表的就是选择的原点位置\nplt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color map\nplt.colorbar(shrink=.92)#右边颜色说明 shrink参数是将图片长度变为原来的92%\nplt.xticks(())\nplt.yticks(())\nplt.show()              \n```\n![图片13](http://img.blog.csdn.net/20180311192757270?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n出图方式 此处采用内插法中的nearest-neighbor\n![图片14](http://img.blog.csdn.net/20180311193343476?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n#### 5.53D图像\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D\nfig=plt.figure()#定义图像窗口\nax=Axes3D(fig)#在窗口上添加3D坐标轴\n#将X和Y值编织成栅格\nX=np.arange(-4,4,0.25)\nY=np.arange(-4,4,0.25)\nX,Y=np.meshgrid(X,Y)\nR=np.sqrt(X**2+Y**2)\nZ=np.sin(R)#高度值\n#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度\nax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图\n#添加XY平面等高线 投影到z平面\nax.contourf(X,Y,Z,zdir='z',offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置\nax.set_zlim(-2,2)\nplt.show()\n```\n![图片15](http://img.blog.csdn.net/20180311194735746?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n### 6.多图合并显示\n\n#### 6.1Subplot多合一显示\n\n均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。\n\n```python\nplt.figure()\nplt.subplot(2,1,1)#表示整个图像分割成2行2列，当前位置为1\nplt.plot([0,1],[0,1])#横坐标变化为[0,1] 竖坐标变化为[0,2]\n\nplt.subplot(2,3,4)\nplt.plot([0,1],[0,2])\n\nplt.subplot(2,3,5)\nplt.plot([0,1],[0,3])\n\nplt.subplot(2,3,6)\nplt.plot([0,1],[0,4])\nplt.show()\n```\n\n![图片16](Python之MatPlotLib使用教程/图片16.png)\n\n不均匀图中图\n\n```python\nplt.figure()\nplt.subplot(2,1,1)#将整个窗口分割成2行1列，当前位置表示第一个图\nplt.plot([0,1],[0,1])#横坐标变化为[0,1],竖坐标变化为[0,1]\n\nplt.subplot(2,3,4)#将整个窗口分割成2行3列，当前位置为4\nplt.plot([0,1],[0,2])\n\nplt.subplot(2,3,5)\nplt.plot([0,1],[0,3])\n\nplt.subplot(2,3,6)\nplt.plot([0,1],[0,4])\nplt.show()\n```\n\n![图片17](Python之MatPlotLib使用教程/图片17.png)\n\n#### 6.2SubPlot分格显示\n\n方法一\n\n```python\nimport matplotlib.gridspec as gridspec#引入新模块\nplt.figure()\n'''\n使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1\n'''\nax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3)  # stands for axes\nax1.plot([1, 2], [1, 2])\nax1.set_title('ax1_title')#设置图的标题\n\n#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2\nax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)\n\n#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2\nax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\n#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1\nax4 = plt.subplot2grid((3, 3), (2, 0))\nax4.scatter([1, 2], [2, 2])\nax4.set_xlabel('ax4_x')\nax4.set_ylabel('ax4_y')\nax5 = plt.subplot2grid((3, 3), (2, 1))\n```\n\n![图像18](Python之MatPlotLib使用教程/图像18.png)\n\n方法二\n\n```Python\nplt.figure()\ngs = gridspec.GridSpec(3, 3)#将图像分割成3行3列\nax6 = plt.subplot(gs[0, :])#gs[0:1]表示图占第0行和所有列\nax7 = plt.subplot(gs[1, :2])#gs[1,:2]表示图占第1行和第二列前的所有列\nax8 = plt.subplot(gs[1:, 2])\nax9 = plt.subplot(gs[-1, 0])\nax10 = plt.subplot(gs[-1, -2])#gs[-1.-2]表示这个图占倒数第1行和倒数第2行\nplt.show()\n```\n\n![图像19](Python之MatPlotLib使用教程/图像19.png)\n\n方法三\n\n```Python\n'''\n建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114\n'''\nf, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, sharex=True, sharey=True)\nax11.scatter([1,2], [1,2])ax11.scatter 坐标范围x为[1,2]，y为[1,2]\nplt.tight_layout()#表示紧凑显示图像\nplt.show()\n```\n\n![图像21](Python之MatPlotLib使用教程/图像20.png)\n\n#### 6.3图中图\n\n```Python\nfig=plt.figure()\n#创建数据\nx=[1,2,3,4,5,6,7]\ny=[1,3,4,2,5,8,6]\n\n#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。\nleft, bottom, width, height = 0.1, 0.1, 0.8, 0.8\nax1 = fig.add_axes([left, bottom, width, height])  # main axes\nax1.plot(x, y, 'r')#绘制大图，颜色为red\nax1.set_xlabel('x')#横坐标名称为x\nax1.set_ylabel('y')\nax1.set_title('title')#图名称为title\n\n#绘制小图，注意坐标系位置和大小的改变\nax2 = fig.add_axes([0.2, 0.6, 0.25, 0.25])\nax2.plot(y, x, 'b')#颜色为buue\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nax2.set_title('title inside 1')\n\n#绘制第二个小兔\nplt.axes([0.6, 0.2, 0.25, 0.25])\nplt.plot(y[::-1], x, 'g')#将y进行逆序\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('title inside 2')\nplt.show()\n```\n\n![图像21](Python之MatPlotLib使用教程/图像21.png)\n\n#### 6.4次坐标轴\n\n```Python\nx=np.arange(0,10,0.1)\ny1=0.5*x**2\ny2=-1*y1\nfig, ax1 = plt.subplots()\n\nax2 = ax1.twinx()#镜像显示\nax1.plot(x, y1, 'g-')\nax2.plot(x, y2, 'b-')\n\nax1.set_xlabel('X data')\nax1.set_ylabel('Y1 data', color='g')#第一个y坐标轴\nax2.set_ylabel('Y2 data', color='b')#第二个y坐标轴\nplt.show()\n```\n\n![图像22](Python之MatPlotLib使用教程/图像22.png)\n\n### 7.动画\n\n```Python\nfrom matplotlib import animation#引入新模块\nfig,ax=plt.subplots()\nx=np.arange(0,2*np.pi,0.01)#数据为0~2PI范围内的正弦曲线\nline,=ax.plot(x,np.sin(x))# line表示列表\n\n#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧\ndef animate(i):\n    line.set_ydata(np.sin(x+i/100))\n    return line,\n\n#构造开始帧函数init\ndef init():\n    line.set_ydata(np.sin(x))\n    return line,\n\n# frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 \n# blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。\nani=animation.FuncAnimation(fig=fig,func=animate,frames=200,init_func=init,interval=20,blit=False)\nplt.show()\n```\n\n![图像23](Python之MatPlotLib使用教程/图像23.png)\n\n**MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。**\n\n----------\n\n\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。\n\n","slug":"Python之MatPlotLib使用教程","published":1,"updated":"2018-03-25T11:59:25.077Z","layout":"post","photos":[],"link":"","_id":"cji4rdzd900072e01gu51z7ex","content":"<h3 id=\"1-Matplotlib简介\"><a href=\"#1-Matplotlib简介\" class=\"headerlink\" title=\"1.Matplotlib简介\"></a>1.Matplotlib简介</h3><ol>\n<li>Matplotlib是非常强大的python画图工具</li>\n<li>Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 </li>\n</ol>\n<h3 id=\"2-Matplotlib安装\"><a href=\"#2-Matplotlib安装\" class=\"headerlink\" title=\"2.Matplotlib安装\"></a>2.Matplotlib安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 <span class=\"keyword\">install</span> matplotlib<span class=\"comment\">#python3</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Matplotlib引入\"><a href=\"#3-Matplotlib引入\" class=\"headerlink\" title=\"3.Matplotlib引入\"></a>3.Matplotlib引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt<span class=\"comment\">#为方便简介为plt</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np<span class=\"comment\">#画图过程中会使用numpy</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd<span class=\"comment\">#画图过程中会使用pandas</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Matplotlib基本应用\"><a href=\"#4-Matplotlib基本应用\" class=\"headerlink\" title=\"4.Matplotlib基本应用\"></a>4.Matplotlib基本应用</h3><figure class=\"highlight makefile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-1,1,50)<span class=\"comment\">#定义x数据范围</span></span><br><span class=\"line\">y1=2*x+1<span class=\"comment\">#定义y数据范围</span></span><br><span class=\"line\">y2=x**2</span><br><span class=\"line\">plt.figure()<span class=\"comment\">#定义一个图像窗口</span></span><br><span class=\"line\">plt.plot(x,y)<span class=\"comment\">#plot()画出曲线</span></span><br><span class=\"line\">plt.show()<span class=\"comment\">#显示图像</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311144537863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片01\"></p>\n<h4 id=\"4-1figure图像\"><a href=\"#4-1figure图像\" class=\"headerlink\" title=\"4.1figure图像\"></a>4.1figure图像</h4><p>matplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)#<span class=\"number\">50</span>为生成的样本数</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">1</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))#定义编号为<span class=\"number\">1</span> 大小为(<span class=\"number\">8</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='--')#颜色为红色，线宽度为<span class=\"number\">2</span>，线风格为--</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.show()#显示图</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311145427674?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片02\"></p>\n<h4 id=\"4-2设置坐标轴\"><a href=\"#4-2设置坐标轴\" class=\"headerlink\" title=\"4.2设置坐标轴\"></a>4.2设置坐标轴</h4><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='-')</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"I'm x\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"I'm y\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311151115803?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片03\"><br>自定义坐标轴<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='-')</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"I'm x\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"I'm y\"</span>)</span><br><span class=\"line\">new_ticks=np.linspace(<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)#小标从<span class=\"number\">-1</span>到<span class=\"number\">2</span>分为<span class=\"number\">5</span>个单位</span><br><span class=\"line\">print(new_ticks)</span><br><span class=\"line\">#[<span class=\"number\">-1.</span>   <span class=\"number\">-0.25</span>  <span class=\"number\">0.5</span>   <span class=\"number\">1.25</span>  <span class=\"number\">2.</span>  ]</span><br><span class=\"line\">plt.xticks(new_ticks)#进行替换新下标</span><br><span class=\"line\">plt.yticks([<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/2018031115185255?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片04\"><br>设置边框属性<br><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"number\">2</span>,linestyle=<span class=\"string\">'--'</span>)</span><br><span class=\"line\">plt.plot(x,y2)<span class=\"comment\">#进行画图</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(-<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)<span class=\"comment\">#小标从-1到2分为5个单位</span></span><br><span class=\"line\">plt.xticks(new_ticks)<span class=\"comment\">#进行替换新下标</span></span><br><span class=\"line\">plt.yticks([-<span class=\"number\">2</span>,-<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [<span class=\"string\">r'$really\\ bad$'</span>,<span class=\"string\">'$bad$'</span>,<span class=\"string\">'$well$'</span>,<span class=\"string\">'$really\\ well$'</span>])</span><br><span class=\"line\">ax=plt.gca()<span class=\"comment\">#gca=get current axis</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)<span class=\"comment\">#边框属性设置为none 不显示</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311152822953?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片05\"><br>调整移动坐标轴<br><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"number\">2</span>,linestyle=<span class=\"string\">'--'</span>)</span><br><span class=\"line\">plt.plot(x,y2)<span class=\"comment\">#进行画图</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(-<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)<span class=\"comment\">#小标从-1到2分为5个单位</span></span><br><span class=\"line\">plt.xticks(new_ticks)<span class=\"comment\">#进行替换新下标</span></span><br><span class=\"line\">plt.yticks([-<span class=\"number\">2</span>,-<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [<span class=\"string\">r'$really\\ bad$'</span>,<span class=\"string\">'$bad$'</span>,<span class=\"string\">'$well$'</span>,<span class=\"string\">'$really\\ well$'</span>])</span><br><span class=\"line\">ax=plt.gca()<span class=\"comment\">#gca=get current axis</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)<span class=\"comment\">#边框属性设置为none 不显示</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)<span class=\"comment\">#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))<span class=\"comment\">#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data</span></span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))<span class=\"comment\">#坐标中心点在(0,0)位置</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311153109404?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"这里写图片描述\"></p>\n<h4 id=\"4-3添加图例\"><a href=\"#4-3添加图例\" class=\"headerlink\" title=\"4.3添加图例\"></a>4.3添加图例</h4><p>matplotlib中legend图例帮助我们展示数据对应的图像名称。<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)#小标从<span class=\"number\">-1</span>到<span class=\"number\">2</span>分为<span class=\"number\">5</span>个单位</span><br><span class=\"line\">plt.xticks(new_ticks)#进行替换新下标</span><br><span class=\"line\">plt.yticks([<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])</span><br><span class=\"line\"></span><br><span class=\"line\">l1,=plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='--',label='linear line')</span><br><span class=\"line\">l2,=plt.plot(x,y2,label='square line')#进行画图</span><br><span class=\"line\">plt.legend(loc='best')#显示在最好的位置</span><br><span class=\"line\">plt.show()#显示图</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311163819992?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片07\"><br>调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数<br><figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.legend(handles=[l1, l2], labels=[<span class=\"symbol\">'up</span>', <span class=\"symbol\">'down</span>'],  loc=<span class=\"symbol\">'best</span>')</span><br><span class=\"line\">#loc有很多参数 其中best自分配最佳位置</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"> <span class=\"symbol\">'best</span>' : 0,          </span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> right'  : 1,</span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> left'   : 2,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> left'   : 3,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> right'  : 4,</span><br><span class=\"line\"> <span class=\"symbol\">'right</span>'        : 5,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span> left'  : 6,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span> right' : 7,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> center' : 8,</span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> center' : 9,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span>'       : 10,</span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-4标注\"><a href=\"#4-4标注\" class=\"headerlink\" title=\"4.4标注\"></a>4.4标注</h4><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y = <span class=\"number\">2</span>*x + <span class=\"number\">1</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">1</span>, figsize=(<span class=\"number\">8</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x, y,)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#移动坐标轴</span></span><br><span class=\"line\">ax = plt.gca()</span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#标注信息</span></span><br><span class=\"line\">x0=<span class=\"number\">1</span></span><br><span class=\"line\">y0=<span class=\"number\">2</span>*x0+<span class=\"number\">1</span></span><br><span class=\"line\">plt.scatter(x0,y0,s=<span class=\"number\">50</span>,color=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">plt.plot([x0,x0],[y0,<span class=\"number\">0</span>],<span class=\"string\">'k--'</span>,lw=<span class=\"number\">2.5</span>)<span class=\"comment\">#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细</span></span><br><span class=\"line\"><span class=\"comment\">#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置</span></span><br><span class=\"line\">plt.annotate(<span class=\"string\">r'$2x0+1=%s$'</span> % y0, xy=(x0, y0), xycoords=<span class=\"string\">'data'</span>, xytext=(+<span class=\"number\">30</span>, -<span class=\"number\">30</span>),</span><br><span class=\"line\">             textcoords=<span class=\"string\">'offset points'</span>, fontsize=<span class=\"number\">16</span>,</span><br><span class=\"line\">             arrowprops=dict(arrowstyle=<span class=\"string\">'-&gt;'</span>, connectionstyle=<span class=\"string\">\"arc3,rad=.2\"</span>))</span><br><span class=\"line\"><span class=\"comment\">#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             </span></span><br><span class=\"line\">plt.text(-<span class=\"number\">3.7</span>, <span class=\"number\">3</span>, <span class=\"string\">r'$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$'</span>,</span><br><span class=\"line\">         fontdict=&#123;<span class=\"string\">'size'</span>: <span class=\"number\">16</span>, <span class=\"string\">'color'</span>: <span class=\"string\">'r'</span>&#125;)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/2018031116474811?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片08\"></p>\n<h4 id=\"4-5能见度调整\"><a href=\"#4-5能见度调整\" class=\"headerlink\" title=\"4.5能见度调整\"></a>4.5能见度调整</h4><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">50</span>)</span><br><span class=\"line\">y=<span class=\"number\">0.1</span>*x</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(x, y, linewidth=<span class=\"number\">10</span>, zorder=<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">#移动坐标轴</span><br><span class=\"line\">ax = plt.gca()</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.xaxis</span><span class=\"selector-class\">.set_ticks_position</span>(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax<span class=\"selector-class\">.yaxis</span><span class=\"selector-class\">.set_ticks_position</span>(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-id\">#label</span>.set_fontsize(<span class=\"number\">12</span>)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序</span><br><span class=\"line\"><span class=\"keyword\">for</span> <span class=\"selector-tag\">label</span> <span class=\"keyword\">in</span> ax.get_xticklabels() + ax.get_yticklabels():</span><br><span class=\"line\">    <span class=\"selector-tag\">label</span>.set_fontsize(<span class=\"number\">12</span>)</span><br><span class=\"line\">    <span class=\"selector-tag\">label</span>.set_bbox(dict(facecolor=<span class=\"string\">'red'</span>, edgecolor=<span class=\"string\">'None'</span>, alpha=<span class=\"number\">0.7</span>, zorder=<span class=\"number\">2</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311172352949?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片09\"></p>\n<h3 id=\"5-画图种类\"><a href=\"#5-画图种类\" class=\"headerlink\" title=\"5.画图种类\"></a>5.画图种类</h3><h4 id=\"5-1Scatter散点图\"><a href=\"#5-1Scatter散点图\" class=\"headerlink\" title=\"5.1Scatter散点图\"></a>5.1Scatter散点图</h4><figure class=\"highlight makefile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n=1024</span><br><span class=\"line\">X=np.random.normal(0,1,n)<span class=\"comment\">#每一个点的X值</span></span><br><span class=\"line\">Y=np.random.normal(0,1,n)<span class=\"comment\">#每一个点的Y值</span></span><br><span class=\"line\">T=np.arctan2(Y,X)<span class=\"comment\">#arctan2返回给定的X和Y值的反正切值</span></span><br><span class=\"line\"><span class=\"comment\">#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴</span></span><br><span class=\"line\">plt.scatter(X,Y,s=75,c=T,alpha=0.5)</span><br><span class=\"line\">plt.xlim(-1.5,1.5)</span><br><span class=\"line\">plt.xticks(())<span class=\"comment\">#忽略xticks</span></span><br><span class=\"line\">plt.ylim(-1.5,1.5)</span><br><span class=\"line\">plt.yticks(())<span class=\"comment\">#忽略yticks</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311174847167?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片10\"></p>\n<h4 id=\"5-2条形图\"><a href=\"#5-2条形图\" class=\"headerlink\" title=\"5.2条形图\"></a>5.2条形图</h4><figure class=\"highlight livecodeserver\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#基本图形</span></span><br><span class=\"line\">n=<span class=\"number\">12</span></span><br><span class=\"line\">X=np.arange(n)</span><br><span class=\"line\">Y1=(<span class=\"number\">1</span>-X/float(n))*np.<span class=\"built_in\">random</span>.uniform(<span class=\"number\">0.5</span>,<span class=\"number\">1</span>,n)</span><br><span class=\"line\">Y2=(<span class=\"number\">1</span>-X/float(n))*np.<span class=\"built_in\">random</span>.uniform(<span class=\"number\">0.5</span>,<span class=\"number\">1</span>,n)</span><br><span class=\"line\">plt.bar(X,+Y1,facecolor=<span class=\"string\">'#9999ff'</span>,edgecolor=<span class=\"string\">'white'</span>)</span><br><span class=\"line\">plt.bar(X,-Y2,facecolor=<span class=\"string\">'#ff9999'</span>,edgecolor=<span class=\"string\">'white'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#标记值</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x,y <span class=\"keyword\">in</span> zip(X,Y1):<span class=\"comment\">#zip表示可以传递两个值</span></span><br><span class=\"line\">    plt.<span class=\"keyword\">text</span>(x+<span class=\"number\">0.4</span>,y+<span class=\"number\">0.05</span>,<span class=\"string\">'%.2f'</span>%y,ha=<span class=\"string\">'center'</span>,va=<span class=\"string\">'bottom'</span>)<span class=\"comment\">#ha表示横向对齐 bottom表示向下对齐</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x,y <span class=\"keyword\">in</span> zip(X,Y2):</span><br><span class=\"line\">    plt.<span class=\"keyword\">text</span>(x+<span class=\"number\">0.4</span>,-y<span class=\"number\">-0.05</span>,<span class=\"string\">'%.2f'</span>%y,ha=<span class=\"string\">'center'</span>,va=<span class=\"string\">'top'</span>)</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-0.5</span>,n)</span><br><span class=\"line\">plt.xticks(())<span class=\"comment\">#忽略xticks</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">-1.25</span>,<span class=\"number\">1.25</span>)</span><br><span class=\"line\">plt.yticks(())<span class=\"comment\">#忽略yticks</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311181815729?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片11\"></p>\n<h4 id=\"5-3等高线图\"><a href=\"#5-3等高线图\" class=\"headerlink\" title=\"5.3等高线图\"></a>5.3等高线图</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">n</span>=256</span><br><span class=\"line\"><span class=\"attribute\">x</span>=np.linspace(-3,3,n)</span><br><span class=\"line\"><span class=\"attribute\">y</span>=np.linspace(-3,3,n)</span><br><span class=\"line\">X,<span class=\"attribute\">Y</span>=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵</span><br><span class=\"line\"><span class=\"comment\">#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中</span></span><br><span class=\"line\">def f(x,y):</span><br><span class=\"line\">    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)</span><br><span class=\"line\">plt.contourf(X,Y,f(X,Y),8,<span class=\"attribute\">alpha</span>=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map</span><br><span class=\"line\"><span class=\"comment\">#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5</span></span><br><span class=\"line\"><span class=\"attribute\">C</span>=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)</span><br><span class=\"line\"><span class=\"comment\">#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10</span></span><br><span class=\"line\">plt.clabel(C,<span class=\"attribute\">inline</span>=<span class=\"literal\">True</span>,fontsize=10)</span><br><span class=\"line\">plt.xticks(())#隐藏坐标轴</span><br><span class=\"line\">plt.yticks(())</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311182506284?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片12\"></p>\n<h4 id=\"5-4Image图片\"><a href=\"#5-4Image图片\" class=\"headerlink\" title=\"5.4Image图片\"></a>5.4Image图片</h4><p>利用matplotlib打印出图像<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = np.array([<span class=\"number\">0.313660827978</span>, <span class=\"number\">0.365348418405</span>, <span class=\"number\">0.423733120134</span>,</span><br><span class=\"line\">              <span class=\"number\">0.365348418405</span>, <span class=\"number\">0.439599930621</span>, <span class=\"number\">0.525083754405</span>,</span><br><span class=\"line\">              <span class=\"number\">0.423733120134</span>, <span class=\"number\">0.525083754405</span>, <span class=\"number\">0.651536351379</span>]).reshape(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">#origin='lower'代表的就是选择的原点位置</span><br><span class=\"line\">plt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color map</span><br><span class=\"line\">plt.colorbar(shrink=<span class=\"number\">.92</span>)#右边颜色说明 shrink参数是将图片长度变为原来的<span class=\"number\">92</span>%</span><br><span class=\"line\">plt.xticks(())</span><br><span class=\"line\">plt.yticks(())</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311192757270?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片13\"><br>出图方式 此处采用内插法中的nearest-neighbor<br><img src=\"http://img.blog.csdn.net/20180311193343476?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片14\"></p>\n<h4 id=\"5-53D图像\"><a href=\"#5-53D图像\" class=\"headerlink\" title=\"5.53D图像\"></a>5.53D图像</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D</span><br><span class=\"line\"><span class=\"attribute\">fig</span>=plt.figure()#定义图像窗口</span><br><span class=\"line\"><span class=\"attribute\">ax</span>=Axes3D(fig)#在窗口上添加3D坐标轴</span><br><span class=\"line\"><span class=\"comment\">#将X和Y值编织成栅格</span></span><br><span class=\"line\"><span class=\"attribute\">X</span>=np.arange(-4,4,0.25)</span><br><span class=\"line\"><span class=\"attribute\">Y</span>=np.arange(-4,4,0.25)</span><br><span class=\"line\">X,<span class=\"attribute\">Y</span>=np.meshgrid(X,Y)</span><br><span class=\"line\"><span class=\"attribute\">R</span>=np.sqrt(X**2+Y**2)</span><br><span class=\"line\"><span class=\"attribute\">Z</span>=np.sin(R)#高度值</span><br><span class=\"line\"><span class=\"comment\">#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度</span></span><br><span class=\"line\">ax.plot_surface(X,Y,Z,<span class=\"attribute\">rstride</span>=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图</span><br><span class=\"line\"><span class=\"comment\">#添加XY平面等高线 投影到z平面</span></span><br><span class=\"line\">ax.contourf(X,Y,Z,<span class=\"attribute\">zdir</span>=<span class=\"string\">'z'</span>,offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置</span><br><span class=\"line\">ax.set_zlim(-2,2)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311194735746?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片15\"></p>\n<h3 id=\"6-多图合并显示\"><a href=\"#6-多图合并显示\" class=\"headerlink\" title=\"6.多图合并显示\"></a>6.多图合并显示</h3><h4 id=\"6-1Subplot多合一显示\"><a href=\"#6-1Subplot多合一显示\" class=\"headerlink\" title=\"6.1Subplot多合一显示\"></a>6.1Subplot多合一显示</h4><p>均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>)<span class=\"comment\">#表示整个图像分割成2行2列，当前位置为1</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>])<span class=\"comment\">#横坐标变化为[0,1] 竖坐标变化为[0,2]</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图片16.png\" alt=\"图片16\"></p>\n<p>不均匀图中图</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>)<span class=\"comment\">#将整个窗口分割成2行1列，当前位置表示第一个图</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>])<span class=\"comment\">#横坐标变化为[0,1],竖坐标变化为[0,1]</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)<span class=\"comment\">#将整个窗口分割成2行3列，当前位置为4</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图片17.png\" alt=\"图片17\"></p>\n<h4 id=\"6-2SubPlot分格显示\"><a href=\"#6-2SubPlot分格显示\" class=\"headerlink\" title=\"6.2SubPlot分格显示\"></a>6.2SubPlot分格显示</h4><p>方法一</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.gridspec <span class=\"keyword\">as</span> gridspec<span class=\"comment\">#引入新模块</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">ax1 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">0</span>, <span class=\"number\">0</span>), colspan=<span class=\"number\">3</span>)  <span class=\"comment\"># stands for axes</span></span><br><span class=\"line\">ax1.plot([<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">'ax1_title'</span>)<span class=\"comment\">#设置图的标题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2</span></span><br><span class=\"line\">ax2 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">1</span>, <span class=\"number\">0</span>), colspan=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2</span></span><br><span class=\"line\">ax3 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">1</span>, <span class=\"number\">2</span>), rowspan=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1</span></span><br><span class=\"line\">ax4 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">2</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax4.scatter([<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax4.set_xlabel(<span class=\"string\">'ax4_x'</span>)</span><br><span class=\"line\">ax4.set_ylabel(<span class=\"string\">'ax4_y'</span>)</span><br><span class=\"line\">ax5 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像18.png\" alt=\"图像18\"></p>\n<p>方法二</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">gs = gridspec.GridSpec(<span class=\"number\">3</span>, <span class=\"number\">3</span>)<span class=\"comment\">#将图像分割成3行3列</span></span><br><span class=\"line\">ax6 = plt.subplot(gs[<span class=\"number\">0</span>, :])<span class=\"comment\">#gs[0:1]表示图占第0行和所有列</span></span><br><span class=\"line\">ax7 = plt.subplot(gs[<span class=\"number\">1</span>, :<span class=\"number\">2</span>])<span class=\"comment\">#gs[1,:2]表示图占第1行和第二列前的所有列</span></span><br><span class=\"line\">ax8 = plt.subplot(gs[<span class=\"number\">1</span>:, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax9 = plt.subplot(gs[<span class=\"number\">-1</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">ax10 = plt.subplot(gs[<span class=\"number\">-1</span>, <span class=\"number\">-2</span>])<span class=\"comment\">#gs[-1.-2]表示这个图占倒数第1行和倒数第2行</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像19.png\" alt=\"图像19\"></p>\n<p>方法三</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">f, ((ax11, ax12), (ax13, ax14)) = plt.subplots(<span class=\"number\">2</span>, <span class=\"number\">2</span>, sharex=<span class=\"keyword\">True</span>, sharey=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">ax11.scatter([<span class=\"number\">1</span>,<span class=\"number\">2</span>], [<span class=\"number\">1</span>,<span class=\"number\">2</span>])ax11.scatter 坐标范围x为[<span class=\"number\">1</span>,<span class=\"number\">2</span>]，y为[<span class=\"number\">1</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\">plt.tight_layout()<span class=\"comment\">#表示紧凑显示图像</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像20.png\" alt=\"图像21\"></p>\n<h4 id=\"6-3图中图\"><a href=\"#6-3图中图\" class=\"headerlink\" title=\"6.3图中图\"></a>6.3图中图</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig=plt.figure()</span><br><span class=\"line\"><span class=\"comment\">#创建数据</span></span><br><span class=\"line\">x=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>]</span><br><span class=\"line\">y=[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>,<span class=\"number\">8</span>,<span class=\"number\">6</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。</span></span><br><span class=\"line\">left, bottom, width, height = <span class=\"number\">0.1</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.8</span></span><br><span class=\"line\">ax1 = fig.add_axes([left, bottom, width, height])  <span class=\"comment\"># main axes</span></span><br><span class=\"line\">ax1.plot(x, y, <span class=\"string\">'r'</span>)<span class=\"comment\">#绘制大图，颜色为red</span></span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">'x'</span>)<span class=\"comment\">#横坐标名称为x</span></span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">'title'</span>)<span class=\"comment\">#图名称为title</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制小图，注意坐标系位置和大小的改变</span></span><br><span class=\"line\">ax2 = fig.add_axes([<span class=\"number\">0.2</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>])</span><br><span class=\"line\">ax2.plot(y, x, <span class=\"string\">'b'</span>)<span class=\"comment\">#颜色为buue</span></span><br><span class=\"line\">ax2.set_xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">ax2.set_title(<span class=\"string\">'title inside 1'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制第二个小兔</span></span><br><span class=\"line\">plt.axes([<span class=\"number\">0.6</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>])</span><br><span class=\"line\">plt.plot(y[::<span class=\"number\">-1</span>], x, <span class=\"string\">'g'</span>)<span class=\"comment\">#将y进行逆序</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'title inside 2'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像21.png\" alt=\"图像21\"></p>\n<h4 id=\"6-4次坐标轴\"><a href=\"#6-4次坐标轴\" class=\"headerlink\" title=\"6.4次坐标轴\"></a>6.4次坐标轴</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">10</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">y1=<span class=\"number\">0.5</span>*x**<span class=\"number\">2</span></span><br><span class=\"line\">y2=<span class=\"number\">-1</span>*y1</span><br><span class=\"line\">fig, ax1 = plt.subplots()</span><br><span class=\"line\"></span><br><span class=\"line\">ax2 = ax1.twinx()<span class=\"comment\">#镜像显示</span></span><br><span class=\"line\">ax1.plot(x, y1, <span class=\"string\">'g-'</span>)</span><br><span class=\"line\">ax2.plot(x, y2, <span class=\"string\">'b-'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">'X data'</span>)</span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">'Y1 data'</span>, color=<span class=\"string\">'g'</span>)<span class=\"comment\">#第一个y坐标轴</span></span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">'Y2 data'</span>, color=<span class=\"string\">'b'</span>)<span class=\"comment\">#第二个y坐标轴</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像22.png\" alt=\"图像22\"></p>\n<h3 id=\"7-动画\"><a href=\"#7-动画\" class=\"headerlink\" title=\"7.动画\"></a>7.动画</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> animation<span class=\"comment\">#引入新模块</span></span><br><span class=\"line\">fig,ax=plt.subplots()</span><br><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">2</span>*np.pi,<span class=\"number\">0.01</span>)<span class=\"comment\">#数据为0~2PI范围内的正弦曲线</span></span><br><span class=\"line\">line,=ax.plot(x,np.sin(x))<span class=\"comment\"># line表示列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">animate</span><span class=\"params\">(i)</span>:</span></span><br><span class=\"line\">    line.set_ydata(np.sin(x+i/<span class=\"number\">100</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> line,</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#构造开始帧函数init</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    line.set_ydata(np.sin(x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> line,</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 </span></span><br><span class=\"line\"><span class=\"comment\"># blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。</span></span><br><span class=\"line\">ani=animation.FuncAnimation(fig=fig,func=animate,frames=<span class=\"number\">200</span>,init_func=init,interval=<span class=\"number\">20</span>,blit=<span class=\"keyword\">False</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像23.png\" alt=\"图像23\"></p>\n<p><strong>MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。</strong></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Matplotlib简介\"><a href=\"#1-Matplotlib简介\" class=\"headerlink\" title=\"1.Matplotlib简介\"></a>1.Matplotlib简介</h3><ol>\n<li>Matplotlib是非常强大的python画图工具</li>\n<li>Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 </li>\n</ol>\n<h3 id=\"2-Matplotlib安装\"><a href=\"#2-Matplotlib安装\" class=\"headerlink\" title=\"2.Matplotlib安装\"></a>2.Matplotlib安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 <span class=\"keyword\">install</span> matplotlib<span class=\"comment\">#python3</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Matplotlib引入\"><a href=\"#3-Matplotlib引入\" class=\"headerlink\" title=\"3.Matplotlib引入\"></a>3.Matplotlib引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt<span class=\"comment\">#为方便简介为plt</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np<span class=\"comment\">#画图过程中会使用numpy</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd<span class=\"comment\">#画图过程中会使用pandas</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Matplotlib基本应用\"><a href=\"#4-Matplotlib基本应用\" class=\"headerlink\" title=\"4.Matplotlib基本应用\"></a>4.Matplotlib基本应用</h3><figure class=\"highlight makefile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-1,1,50)<span class=\"comment\">#定义x数据范围</span></span><br><span class=\"line\">y1=2*x+1<span class=\"comment\">#定义y数据范围</span></span><br><span class=\"line\">y2=x**2</span><br><span class=\"line\">plt.figure()<span class=\"comment\">#定义一个图像窗口</span></span><br><span class=\"line\">plt.plot(x,y)<span class=\"comment\">#plot()画出曲线</span></span><br><span class=\"line\">plt.show()<span class=\"comment\">#显示图像</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311144537863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片01\"></p>\n<h4 id=\"4-1figure图像\"><a href=\"#4-1figure图像\" class=\"headerlink\" title=\"4.1figure图像\"></a>4.1figure图像</h4><p>matplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)#<span class=\"number\">50</span>为生成的样本数</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">1</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))#定义编号为<span class=\"number\">1</span> 大小为(<span class=\"number\">8</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='--')#颜色为红色，线宽度为<span class=\"number\">2</span>，线风格为--</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.show()#显示图</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311145427674?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片02\"></p>\n<h4 id=\"4-2设置坐标轴\"><a href=\"#4-2设置坐标轴\" class=\"headerlink\" title=\"4.2设置坐标轴\"></a>4.2设置坐标轴</h4><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='-')</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"I'm x\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"I'm y\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311151115803?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片03\"><br>自定义坐标轴<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='-')</span><br><span class=\"line\">plt.plot(x,y2)#进行画图</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"I'm x\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"I'm y\"</span>)</span><br><span class=\"line\">new_ticks=np.linspace(<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)#小标从<span class=\"number\">-1</span>到<span class=\"number\">2</span>分为<span class=\"number\">5</span>个单位</span><br><span class=\"line\">print(new_ticks)</span><br><span class=\"line\">#[<span class=\"number\">-1.</span>   <span class=\"number\">-0.25</span>  <span class=\"number\">0.5</span>   <span class=\"number\">1.25</span>  <span class=\"number\">2.</span>  ]</span><br><span class=\"line\">plt.xticks(new_ticks)#进行替换新下标</span><br><span class=\"line\">plt.yticks([<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/2018031115185255?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片04\"><br>设置边框属性<br><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"number\">2</span>,linestyle=<span class=\"string\">'--'</span>)</span><br><span class=\"line\">plt.plot(x,y2)<span class=\"comment\">#进行画图</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(-<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)<span class=\"comment\">#小标从-1到2分为5个单位</span></span><br><span class=\"line\">plt.xticks(new_ticks)<span class=\"comment\">#进行替换新下标</span></span><br><span class=\"line\">plt.yticks([-<span class=\"number\">2</span>,-<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [<span class=\"string\">r'$really\\ bad$'</span>,<span class=\"string\">'$bad$'</span>,<span class=\"string\">'$well$'</span>,<span class=\"string\">'$really\\ well$'</span>])</span><br><span class=\"line\">ax=plt.gca()<span class=\"comment\">#gca=get current axis</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)<span class=\"comment\">#边框属性设置为none 不显示</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311152822953?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片05\"><br>调整移动坐标轴<br><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x,y1,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"number\">2</span>,linestyle=<span class=\"string\">'--'</span>)</span><br><span class=\"line\">plt.plot(x,y2)<span class=\"comment\">#进行画图</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(-<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)<span class=\"comment\">#小标从-1到2分为5个单位</span></span><br><span class=\"line\">plt.xticks(new_ticks)<span class=\"comment\">#进行替换新下标</span></span><br><span class=\"line\">plt.yticks([-<span class=\"number\">2</span>,-<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [<span class=\"string\">r'$really\\ bad$'</span>,<span class=\"string\">'$bad$'</span>,<span class=\"string\">'$well$'</span>,<span class=\"string\">'$really\\ well$'</span>])</span><br><span class=\"line\">ax=plt.gca()<span class=\"comment\">#gca=get current axis</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)<span class=\"comment\">#边框属性设置为none 不显示</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)<span class=\"comment\">#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none</span></span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))<span class=\"comment\">#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data</span></span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))<span class=\"comment\">#坐标中心点在(0,0)位置</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311153109404?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"这里写图片描述\"></p>\n<h4 id=\"4-3添加图例\"><a href=\"#4-3添加图例\" class=\"headerlink\" title=\"4.3添加图例\"></a>4.3添加图例</h4><p>matplotlib中legend图例帮助我们展示数据对应的图像名称。<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(<span class=\"number\">-3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y1=<span class=\"number\">2</span>*x+<span class=\"number\">1</span></span><br><span class=\"line\">y2=x**<span class=\"number\">2</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">2</span>,figsize=(<span class=\"number\">8</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">new_ticks=np.linspace(<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>)#小标从<span class=\"number\">-1</span>到<span class=\"number\">2</span>分为<span class=\"number\">5</span>个单位</span><br><span class=\"line\">plt.xticks(new_ticks)#进行替换新下标</span><br><span class=\"line\">plt.yticks([<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,],</span><br><span class=\"line\">           [r'$really\\ bad$','$bad$','$well$','$really\\ well$'])</span><br><span class=\"line\"></span><br><span class=\"line\">l1,=plt.plot(x,y1,color='red',linewidth=<span class=\"number\">2</span>,linestyle='--',label='linear line')</span><br><span class=\"line\">l2,=plt.plot(x,y2,label='square line')#进行画图</span><br><span class=\"line\">plt.legend(loc='best')#显示在最好的位置</span><br><span class=\"line\">plt.show()#显示图</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311163819992?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片07\"><br>调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数<br><figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.legend(handles=[l1, l2], labels=[<span class=\"symbol\">'up</span>', <span class=\"symbol\">'down</span>'],  loc=<span class=\"symbol\">'best</span>')</span><br><span class=\"line\">#loc有很多参数 其中best自分配最佳位置</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"> <span class=\"symbol\">'best</span>' : 0,          </span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> right'  : 1,</span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> left'   : 2,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> left'   : 3,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> right'  : 4,</span><br><span class=\"line\"> <span class=\"symbol\">'right</span>'        : 5,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span> left'  : 6,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span> right' : 7,</span><br><span class=\"line\"> <span class=\"symbol\">'lower</span> center' : 8,</span><br><span class=\"line\"> <span class=\"symbol\">'upper</span> center' : 9,</span><br><span class=\"line\"> <span class=\"symbol\">'center</span>'       : 10,</span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-4标注\"><a href=\"#4-4标注\" class=\"headerlink\" title=\"4.4标注\"></a>4.4标注</h4><figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">y = <span class=\"number\">2</span>*x + <span class=\"number\">1</span></span><br><span class=\"line\">plt.figure(num=<span class=\"number\">1</span>, figsize=(<span class=\"number\">8</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">plt.plot(x, y,)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#移动坐标轴</span></span><br><span class=\"line\">ax = plt.gca()</span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#标注信息</span></span><br><span class=\"line\">x0=<span class=\"number\">1</span></span><br><span class=\"line\">y0=<span class=\"number\">2</span>*x0+<span class=\"number\">1</span></span><br><span class=\"line\">plt.scatter(x0,y0,s=<span class=\"number\">50</span>,color=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">plt.plot([x0,x0],[y0,<span class=\"number\">0</span>],<span class=\"string\">'k--'</span>,lw=<span class=\"number\">2.5</span>)<span class=\"comment\">#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细</span></span><br><span class=\"line\"><span class=\"comment\">#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置</span></span><br><span class=\"line\">plt.annotate(<span class=\"string\">r'$2x0+1=%s$'</span> % y0, xy=(x0, y0), xycoords=<span class=\"string\">'data'</span>, xytext=(+<span class=\"number\">30</span>, -<span class=\"number\">30</span>),</span><br><span class=\"line\">             textcoords=<span class=\"string\">'offset points'</span>, fontsize=<span class=\"number\">16</span>,</span><br><span class=\"line\">             arrowprops=dict(arrowstyle=<span class=\"string\">'-&gt;'</span>, connectionstyle=<span class=\"string\">\"arc3,rad=.2\"</span>))</span><br><span class=\"line\"><span class=\"comment\">#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             </span></span><br><span class=\"line\">plt.text(-<span class=\"number\">3.7</span>, <span class=\"number\">3</span>, <span class=\"string\">r'$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$'</span>,</span><br><span class=\"line\">         fontdict=&#123;<span class=\"string\">'size'</span>: <span class=\"number\">16</span>, <span class=\"string\">'color'</span>: <span class=\"string\">'r'</span>&#125;)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/2018031116474811?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片08\"></p>\n<h4 id=\"4-5能见度调整\"><a href=\"#4-5能见度调整\" class=\"headerlink\" title=\"4.5能见度调整\"></a>4.5能见度调整</h4><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.linspace(-<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">50</span>)</span><br><span class=\"line\">y=<span class=\"number\">0.1</span>*x</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(x, y, linewidth=<span class=\"number\">10</span>, zorder=<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">#移动坐标轴</span><br><span class=\"line\">ax = plt.gca()</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.xaxis</span><span class=\"selector-class\">.set_ticks_position</span>(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax<span class=\"selector-class\">.yaxis</span><span class=\"selector-class\">.set_ticks_position</span>(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax<span class=\"selector-class\">.spines</span>[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-id\">#label</span>.set_fontsize(<span class=\"number\">12</span>)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序</span><br><span class=\"line\"><span class=\"keyword\">for</span> <span class=\"selector-tag\">label</span> <span class=\"keyword\">in</span> ax.get_xticklabels() + ax.get_yticklabels():</span><br><span class=\"line\">    <span class=\"selector-tag\">label</span>.set_fontsize(<span class=\"number\">12</span>)</span><br><span class=\"line\">    <span class=\"selector-tag\">label</span>.set_bbox(dict(facecolor=<span class=\"string\">'red'</span>, edgecolor=<span class=\"string\">'None'</span>, alpha=<span class=\"number\">0.7</span>, zorder=<span class=\"number\">2</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311172352949?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片09\"></p>\n<h3 id=\"5-画图种类\"><a href=\"#5-画图种类\" class=\"headerlink\" title=\"5.画图种类\"></a>5.画图种类</h3><h4 id=\"5-1Scatter散点图\"><a href=\"#5-1Scatter散点图\" class=\"headerlink\" title=\"5.1Scatter散点图\"></a>5.1Scatter散点图</h4><figure class=\"highlight makefile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n=1024</span><br><span class=\"line\">X=np.random.normal(0,1,n)<span class=\"comment\">#每一个点的X值</span></span><br><span class=\"line\">Y=np.random.normal(0,1,n)<span class=\"comment\">#每一个点的Y值</span></span><br><span class=\"line\">T=np.arctan2(Y,X)<span class=\"comment\">#arctan2返回给定的X和Y值的反正切值</span></span><br><span class=\"line\"><span class=\"comment\">#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴</span></span><br><span class=\"line\">plt.scatter(X,Y,s=75,c=T,alpha=0.5)</span><br><span class=\"line\">plt.xlim(-1.5,1.5)</span><br><span class=\"line\">plt.xticks(())<span class=\"comment\">#忽略xticks</span></span><br><span class=\"line\">plt.ylim(-1.5,1.5)</span><br><span class=\"line\">plt.yticks(())<span class=\"comment\">#忽略yticks</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311174847167?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片10\"></p>\n<h4 id=\"5-2条形图\"><a href=\"#5-2条形图\" class=\"headerlink\" title=\"5.2条形图\"></a>5.2条形图</h4><figure class=\"highlight livecodeserver\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#基本图形</span></span><br><span class=\"line\">n=<span class=\"number\">12</span></span><br><span class=\"line\">X=np.arange(n)</span><br><span class=\"line\">Y1=(<span class=\"number\">1</span>-X/float(n))*np.<span class=\"built_in\">random</span>.uniform(<span class=\"number\">0.5</span>,<span class=\"number\">1</span>,n)</span><br><span class=\"line\">Y2=(<span class=\"number\">1</span>-X/float(n))*np.<span class=\"built_in\">random</span>.uniform(<span class=\"number\">0.5</span>,<span class=\"number\">1</span>,n)</span><br><span class=\"line\">plt.bar(X,+Y1,facecolor=<span class=\"string\">'#9999ff'</span>,edgecolor=<span class=\"string\">'white'</span>)</span><br><span class=\"line\">plt.bar(X,-Y2,facecolor=<span class=\"string\">'#ff9999'</span>,edgecolor=<span class=\"string\">'white'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#标记值</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x,y <span class=\"keyword\">in</span> zip(X,Y1):<span class=\"comment\">#zip表示可以传递两个值</span></span><br><span class=\"line\">    plt.<span class=\"keyword\">text</span>(x+<span class=\"number\">0.4</span>,y+<span class=\"number\">0.05</span>,<span class=\"string\">'%.2f'</span>%y,ha=<span class=\"string\">'center'</span>,va=<span class=\"string\">'bottom'</span>)<span class=\"comment\">#ha表示横向对齐 bottom表示向下对齐</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> x,y <span class=\"keyword\">in</span> zip(X,Y2):</span><br><span class=\"line\">    plt.<span class=\"keyword\">text</span>(x+<span class=\"number\">0.4</span>,-y<span class=\"number\">-0.05</span>,<span class=\"string\">'%.2f'</span>%y,ha=<span class=\"string\">'center'</span>,va=<span class=\"string\">'top'</span>)</span><br><span class=\"line\">plt.xlim(<span class=\"number\">-0.5</span>,n)</span><br><span class=\"line\">plt.xticks(())<span class=\"comment\">#忽略xticks</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">-1.25</span>,<span class=\"number\">1.25</span>)</span><br><span class=\"line\">plt.yticks(())<span class=\"comment\">#忽略yticks</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311181815729?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片11\"></p>\n<h4 id=\"5-3等高线图\"><a href=\"#5-3等高线图\" class=\"headerlink\" title=\"5.3等高线图\"></a>5.3等高线图</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">n</span>=256</span><br><span class=\"line\"><span class=\"attribute\">x</span>=np.linspace(-3,3,n)</span><br><span class=\"line\"><span class=\"attribute\">y</span>=np.linspace(-3,3,n)</span><br><span class=\"line\">X,<span class=\"attribute\">Y</span>=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵</span><br><span class=\"line\"><span class=\"comment\">#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中</span></span><br><span class=\"line\">def f(x,y):</span><br><span class=\"line\">    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)</span><br><span class=\"line\">plt.contourf(X,Y,f(X,Y),8,<span class=\"attribute\">alpha</span>=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map</span><br><span class=\"line\"><span class=\"comment\">#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5</span></span><br><span class=\"line\"><span class=\"attribute\">C</span>=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)</span><br><span class=\"line\"><span class=\"comment\">#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10</span></span><br><span class=\"line\">plt.clabel(C,<span class=\"attribute\">inline</span>=<span class=\"literal\">True</span>,fontsize=10)</span><br><span class=\"line\">plt.xticks(())#隐藏坐标轴</span><br><span class=\"line\">plt.yticks(())</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311182506284?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片12\"></p>\n<h4 id=\"5-4Image图片\"><a href=\"#5-4Image图片\" class=\"headerlink\" title=\"5.4Image图片\"></a>5.4Image图片</h4><p>利用matplotlib打印出图像<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = np.array([<span class=\"number\">0.313660827978</span>, <span class=\"number\">0.365348418405</span>, <span class=\"number\">0.423733120134</span>,</span><br><span class=\"line\">              <span class=\"number\">0.365348418405</span>, <span class=\"number\">0.439599930621</span>, <span class=\"number\">0.525083754405</span>,</span><br><span class=\"line\">              <span class=\"number\">0.423733120134</span>, <span class=\"number\">0.525083754405</span>, <span class=\"number\">0.651536351379</span>]).reshape(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">#origin='lower'代表的就是选择的原点位置</span><br><span class=\"line\">plt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color map</span><br><span class=\"line\">plt.colorbar(shrink=<span class=\"number\">.92</span>)#右边颜色说明 shrink参数是将图片长度变为原来的<span class=\"number\">92</span>%</span><br><span class=\"line\">plt.xticks(())</span><br><span class=\"line\">plt.yticks(())</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://img.blog.csdn.net/20180311192757270?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片13\"><br>出图方式 此处采用内插法中的nearest-neighbor<br><img src=\"http://img.blog.csdn.net/20180311193343476?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片14\"></p>\n<h4 id=\"5-53D图像\"><a href=\"#5-53D图像\" class=\"headerlink\" title=\"5.53D图像\"></a>5.53D图像</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D</span><br><span class=\"line\"><span class=\"attribute\">fig</span>=plt.figure()#定义图像窗口</span><br><span class=\"line\"><span class=\"attribute\">ax</span>=Axes3D(fig)#在窗口上添加3D坐标轴</span><br><span class=\"line\"><span class=\"comment\">#将X和Y值编织成栅格</span></span><br><span class=\"line\"><span class=\"attribute\">X</span>=np.arange(-4,4,0.25)</span><br><span class=\"line\"><span class=\"attribute\">Y</span>=np.arange(-4,4,0.25)</span><br><span class=\"line\">X,<span class=\"attribute\">Y</span>=np.meshgrid(X,Y)</span><br><span class=\"line\"><span class=\"attribute\">R</span>=np.sqrt(X**2+Y**2)</span><br><span class=\"line\"><span class=\"attribute\">Z</span>=np.sin(R)#高度值</span><br><span class=\"line\"><span class=\"comment\">#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度</span></span><br><span class=\"line\">ax.plot_surface(X,Y,Z,<span class=\"attribute\">rstride</span>=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图</span><br><span class=\"line\"><span class=\"comment\">#添加XY平面等高线 投影到z平面</span></span><br><span class=\"line\">ax.contourf(X,Y,Z,<span class=\"attribute\">zdir</span>=<span class=\"string\">'z'</span>,offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置</span><br><span class=\"line\">ax.set_zlim(-2,2)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20180311194735746?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"图片15\"></p>\n<h3 id=\"6-多图合并显示\"><a href=\"#6-多图合并显示\" class=\"headerlink\" title=\"6.多图合并显示\"></a>6.多图合并显示</h3><h4 id=\"6-1Subplot多合一显示\"><a href=\"#6-1Subplot多合一显示\" class=\"headerlink\" title=\"6.1Subplot多合一显示\"></a>6.1Subplot多合一显示</h4><p>均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>)<span class=\"comment\">#表示整个图像分割成2行2列，当前位置为1</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>])<span class=\"comment\">#横坐标变化为[0,1] 竖坐标变化为[0,2]</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图片16.png\" alt=\"图片16\"></p>\n<p>不均匀图中图</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>)<span class=\"comment\">#将整个窗口分割成2行1列，当前位置表示第一个图</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>])<span class=\"comment\">#横坐标变化为[0,1],竖坐标变化为[0,1]</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)<span class=\"comment\">#将整个窗口分割成2行3列，当前位置为4</span></span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图片17.png\" alt=\"图片17\"></p>\n<h4 id=\"6-2SubPlot分格显示\"><a href=\"#6-2SubPlot分格显示\" class=\"headerlink\" title=\"6.2SubPlot分格显示\"></a>6.2SubPlot分格显示</h4><p>方法一</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.gridspec <span class=\"keyword\">as</span> gridspec<span class=\"comment\">#引入新模块</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">ax1 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">0</span>, <span class=\"number\">0</span>), colspan=<span class=\"number\">3</span>)  <span class=\"comment\"># stands for axes</span></span><br><span class=\"line\">ax1.plot([<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">'ax1_title'</span>)<span class=\"comment\">#设置图的标题</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2</span></span><br><span class=\"line\">ax2 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">1</span>, <span class=\"number\">0</span>), colspan=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2</span></span><br><span class=\"line\">ax3 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">1</span>, <span class=\"number\">2</span>), rowspan=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1</span></span><br><span class=\"line\">ax4 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">2</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">ax4.scatter([<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax4.set_xlabel(<span class=\"string\">'ax4_x'</span>)</span><br><span class=\"line\">ax4.set_ylabel(<span class=\"string\">'ax4_y'</span>)</span><br><span class=\"line\">ax5 = plt.subplot2grid((<span class=\"number\">3</span>, <span class=\"number\">3</span>), (<span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像18.png\" alt=\"图像18\"></p>\n<p>方法二</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure()</span><br><span class=\"line\">gs = gridspec.GridSpec(<span class=\"number\">3</span>, <span class=\"number\">3</span>)<span class=\"comment\">#将图像分割成3行3列</span></span><br><span class=\"line\">ax6 = plt.subplot(gs[<span class=\"number\">0</span>, :])<span class=\"comment\">#gs[0:1]表示图占第0行和所有列</span></span><br><span class=\"line\">ax7 = plt.subplot(gs[<span class=\"number\">1</span>, :<span class=\"number\">2</span>])<span class=\"comment\">#gs[1,:2]表示图占第1行和第二列前的所有列</span></span><br><span class=\"line\">ax8 = plt.subplot(gs[<span class=\"number\">1</span>:, <span class=\"number\">2</span>])</span><br><span class=\"line\">ax9 = plt.subplot(gs[<span class=\"number\">-1</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">ax10 = plt.subplot(gs[<span class=\"number\">-1</span>, <span class=\"number\">-2</span>])<span class=\"comment\">#gs[-1.-2]表示这个图占倒数第1行和倒数第2行</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像19.png\" alt=\"图像19\"></p>\n<p>方法三</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">f, ((ax11, ax12), (ax13, ax14)) = plt.subplots(<span class=\"number\">2</span>, <span class=\"number\">2</span>, sharex=<span class=\"keyword\">True</span>, sharey=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">ax11.scatter([<span class=\"number\">1</span>,<span class=\"number\">2</span>], [<span class=\"number\">1</span>,<span class=\"number\">2</span>])ax11.scatter 坐标范围x为[<span class=\"number\">1</span>,<span class=\"number\">2</span>]，y为[<span class=\"number\">1</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\">plt.tight_layout()<span class=\"comment\">#表示紧凑显示图像</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像20.png\" alt=\"图像21\"></p>\n<h4 id=\"6-3图中图\"><a href=\"#6-3图中图\" class=\"headerlink\" title=\"6.3图中图\"></a>6.3图中图</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig=plt.figure()</span><br><span class=\"line\"><span class=\"comment\">#创建数据</span></span><br><span class=\"line\">x=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>]</span><br><span class=\"line\">y=[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">5</span>,<span class=\"number\">8</span>,<span class=\"number\">6</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。</span></span><br><span class=\"line\">left, bottom, width, height = <span class=\"number\">0.1</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.8</span></span><br><span class=\"line\">ax1 = fig.add_axes([left, bottom, width, height])  <span class=\"comment\"># main axes</span></span><br><span class=\"line\">ax1.plot(x, y, <span class=\"string\">'r'</span>)<span class=\"comment\">#绘制大图，颜色为red</span></span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">'x'</span>)<span class=\"comment\">#横坐标名称为x</span></span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">'title'</span>)<span class=\"comment\">#图名称为title</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制小图，注意坐标系位置和大小的改变</span></span><br><span class=\"line\">ax2 = fig.add_axes([<span class=\"number\">0.2</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>])</span><br><span class=\"line\">ax2.plot(y, x, <span class=\"string\">'b'</span>)<span class=\"comment\">#颜色为buue</span></span><br><span class=\"line\">ax2.set_xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">ax2.set_title(<span class=\"string\">'title inside 1'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘制第二个小兔</span></span><br><span class=\"line\">plt.axes([<span class=\"number\">0.6</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>])</span><br><span class=\"line\">plt.plot(y[::<span class=\"number\">-1</span>], x, <span class=\"string\">'g'</span>)<span class=\"comment\">#将y进行逆序</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'title inside 2'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像21.png\" alt=\"图像21\"></p>\n<h4 id=\"6-4次坐标轴\"><a href=\"#6-4次坐标轴\" class=\"headerlink\" title=\"6.4次坐标轴\"></a>6.4次坐标轴</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">10</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">y1=<span class=\"number\">0.5</span>*x**<span class=\"number\">2</span></span><br><span class=\"line\">y2=<span class=\"number\">-1</span>*y1</span><br><span class=\"line\">fig, ax1 = plt.subplots()</span><br><span class=\"line\"></span><br><span class=\"line\">ax2 = ax1.twinx()<span class=\"comment\">#镜像显示</span></span><br><span class=\"line\">ax1.plot(x, y1, <span class=\"string\">'g-'</span>)</span><br><span class=\"line\">ax2.plot(x, y2, <span class=\"string\">'b-'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">'X data'</span>)</span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">'Y1 data'</span>, color=<span class=\"string\">'g'</span>)<span class=\"comment\">#第一个y坐标轴</span></span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">'Y2 data'</span>, color=<span class=\"string\">'b'</span>)<span class=\"comment\">#第二个y坐标轴</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像22.png\" alt=\"图像22\"></p>\n<h3 id=\"7-动画\"><a href=\"#7-动画\" class=\"headerlink\" title=\"7.动画\"></a>7.动画</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> animation<span class=\"comment\">#引入新模块</span></span><br><span class=\"line\">fig,ax=plt.subplots()</span><br><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">2</span>*np.pi,<span class=\"number\">0.01</span>)<span class=\"comment\">#数据为0~2PI范围内的正弦曲线</span></span><br><span class=\"line\">line,=ax.plot(x,np.sin(x))<span class=\"comment\"># line表示列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">animate</span><span class=\"params\">(i)</span>:</span></span><br><span class=\"line\">    line.set_ydata(np.sin(x+i/<span class=\"number\">100</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> line,</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#构造开始帧函数init</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    line.set_ydata(np.sin(x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> line,</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 </span></span><br><span class=\"line\"><span class=\"comment\"># blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。</span></span><br><span class=\"line\">ani=animation.FuncAnimation(fig=fig,func=animate,frames=<span class=\"number\">200</span>,init_func=init,interval=<span class=\"number\">20</span>,blit=<span class=\"keyword\">False</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之MatPlotLib使用教程/图像23.png\" alt=\"图像23\"></p>\n<p><strong>MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。</strong></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。</p>\n"},{"title":"Python之NumPy使用教程","date":"2018-03-13T09:35:25.000Z","toc":true,"comments":1,"_content":"### 1.NumPy概述\nNumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：\n 1. 强大的N维数组对象Array\n 2. 成熟的函数库\n 3. 用于集成C/C++和Fortran代码的工具\n 4. 实用的线性代数、傅立叶变换和随机生成函数\n\n### 2.NumPy安装\n```\npip install numpy或pip3 install numpy\n```\n### 3.NumPy引入\n```\nimport numpy as np#为了方便实用numpy 采用np简写\n```\n### 4.NumPy方法\n```\narray=np.array([[1,2,3],[4,5,6]])#将列表转换为矩阵 并转换为int类型\nprint(array)\n'''\n[[1 2 3]\n [4 5 6]]\n '''\n```\n#### 4.1NumPy属性\n```\nprint('array of dim:',array.ndim)#矩阵的维度\n#array of dim:2\nprint('array of shape',array.shape)#矩阵的行数和列数\n#array of shape:(2,3)\nprint('number of size:',array.size)#元素的个数\n#number of size:6\n```\n#### 4.2NumPy创建Array\n\n - array:创建数组\n - dtype:指定数据类型\n - zeros:创建数据全为0\n - ones:创建数据全为1\n - empty:创建数据接近0\n - arange:指定范围内创建数据\n - linspace创建线段\n\n创建数组\n```\na=np.array([1,2,3])\nprint(a)\n#[1,2,3]\n```\n指定数据dtype\n```\na=np.array([1,2,3],dtype=np.int)#指定为int类型\nprint(a.dtype)\n#int 64\nb=np.array([1,2,3],dtype=np.float)#指定为float类型\nprint(b.dtype)\n#float 64\n```\n创建特定数据\n```\na=np.array([[1,2,3],[4,5,6]])#矩阵 2行3列\nprint(a)\n'''\n[[1 2 3]\n [4 5 6]]\n '''\n```\n创建全0数组\n```\na=np.zeros((2,3))#数据全0 2行3列\nprint(a)\n'''\n[[0 0 0]\n [0 0 0]]\n '''\n```\n创建全1数组 指定特定类型dtype\n```\na=np.zeros((2,3),dtype=np.int)#数据全1 2行3列 同时指定类型\nprint(a)\n'''\n[[1 1 1]\n [1 1 1]]\n '''\n```\n创建全空数组 每个值接近0\n```\na=np.empty(2,3)#数据全为empty 3行4列\nprint(a)\n'''\n[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]\n [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]\n '''\n```\n用array创建连续数组\n```\na=np.arange(1,10,2)#1到10的数据 2步长\nprint(a)\n#[1 3 5 7 9]\n```\n用reshape改变数据形状\n```\na=np.arange(6).reshape(2,3)\nprint(a)\n'''\n[[0 1 2]\n [3 4 5]]\n '''\n```\n用linspace创建线段形数据\n```\na=np.linspace(1,10,20)#开始端1 结束端5 分割成10个数据 生成线段\nprint(a)\n'''\n[ 1.          1.44444444  1.88888889  2.33333333  2.77777778  3.22222222\n  3.66666667  4.11111111  4.55555556  5.        ]\n  '''\n```\n#### 4.3NumPy基础运算\n基础运算之加、减、三角函数等\n```\na=np.array([10,20,30,40])\nb=np.arange(4) #array[0,1,2,3]\n\nc=a+b#加法运算\nprint(c)\n#[10,21,32,43]\n\nc=a-b#减法运算\nprint(c)\n#[10.19,28,37]\n\nc=10*np.sin(a)#三角函数运算\n#[-5.44021111,  9.12945251, -9.88031624,  7.4511316 ]\n\nprint(b<3)#逻辑判断\n#[ True  True  True False]\n\nd=np.random.random((2,3))#随机生成2行3列的矩阵\nprint(d)\n'''\n[[ 0.21116981  0.0804489   0.51855475]\n [ 0.38359164  0.55852973  0.73218811]]\n'''\nprint(np.sum(d))#元素求和\n#2.48448292958\nprint(np.max(d))#元素求最大值\n#0.732188108709\nprint(np.min(d))#元素求最小值\n#0.0804488978886\n```\n多维矩阵运算\n```\na=np.array([[1,1],[0,1]])\nb=np.arange(4).reshape((2,2))\n\nc=np.dot(a,b)#或c=a.dot(b)矩阵运算\nprint(c)\n'''\n[[2 4]\n [2 3]]\n '''\n```\n对行或列执行查找运算\n```\na=np.array([[1,2],[3,4]])\nprint(a)\n'''\n[[1,2]\n [3,4]]\n '''\nprint(np.max(a,axis=0))#axis=0时是对列进行操作\n#[3,4]\nprint(np.min(a,axis=1))#axis=1是对行进行操作\n#[1,3]\n```\n矩阵索引操作\n```\nA=np.arange(2,14).reshape(3,4)\nprint(A)\n'''\n[[2,3,4,5]\n [6,7,8,9]\n [10,11,12,13]]\n '''\nprint(np.argmax(A))#矩阵中最大元素的索引\n#11\nprint(np.argmin(A))#矩阵中最小元素的索引\n#0\nprint(np.mean(A))#或者np.average(A)求解矩阵均值\n#7.5\nprint(np.cumsum(A))#矩阵累加函数\n#[2 5 9 14 20 27 35 44 54 65 77 90]\nprint(np.diff(A))#矩阵累差函数\n'''\n[[1 1 1]\n [1 1 1]\n [1 1 1]]\n '''\nprint(np.nonzero(A))#将非0元素的行与列坐标分割开来\n#(array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]))\n```\n矩阵排序、转置、替换操作\n```\nA=np.arange(14,2,-1).reshape((3,4))\nprint(A)\n'''\n[[14 13 12 11]\n [10  9  8  7]\n [ 6  5  4  3]]\n '''\nprint(np.sort(A))#排序\n'''\n[[11 12 13 14]\n [ 7  8  9 10]\n [ 3  4  5  6]]\n '''\n\nprint(np.transpose(A))\n'''\n[[14 10  6]\n [13  9  5]\n [12  8  4]\n [11  7  3]]\n '''\n\nprint(np.clip(A,5,9))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换\n'''\n[[9 9 9 9]\n [9 9 8 7]\n [6 5 5 5]]\n '''\n```\n### 5.索引\n一维索引\n```\nA=np.arange(0,12)\nprint(A)\n#[ 0  1  2  3  4  5  6  7  8  9 10 11]\nprint(A[1])#一维索引\n#1\n\nA=np.arange(0,12).reshape((3,4))\nprint(A[0])\n#[0,1,2,3]\n```\n二维索引\n```\nA=np.arange(0,12).reshape((3,4))\nprint(A)\n'''\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n '''\nprint(A[1][1])#或者A[1,1]\n#5\nprint(A[1,1:3])#切片处理\n#[5,6]\n\nfor row in A:\n    print(A)\n'''\n[0 1 2 3]\n[4 5 6 7]\n[ 8  9 10 11]\n '''\nfor col in A:\n    print(col)\n'''\n[0 4 8]\n[1 5 9]\n[ 2  6 10]\n[ 3  7 11]\n '''\n\nfor item in A.flat:\n    print(item)\n'''\n0\n1\n...\n10\n11\n'''\n```\n### 6.NumPy之Array合并\n```\nA=np.array([1,1,1])\nB=np.array([2,2,2])\nprint(np.vstack((A,B)))#上下合并\n'''\n[[1 1 1]\n [2 2 2]]\n '''\nprint(np.hstack((A,B)))#左右合并\n#[1 1 1 2 2 2]\n```\n增加维度\n```\nA=np.array([1,1,1])\nprint(A.shape)\n#(3,)\nprint(A[np.newaxis,:])\n#[[1 1 1]]\nprint(A[np.newaxis,:].shape)#newaxis增加维度\n#(1,3)\n\nprint(A[:,np.newaxis])\n'''\n[[1]\n [1]\n [1]]\n '''\nprint(A[:,np.newaxis].shape)\n#（3,1）\n```\n多矩阵合并\n```\nA = np.array([1,1,1])[:,np.newaxis]\nB = np.array([2,2,2])[:,np.newaxis]\nprint(np.concatenate((A,B,B,A),axis=0))#0表示上下合并\n'''\n[[1]\n [1]\n [1]\n [2]\n [2]\n [2]\n [2]\n [2]\n [2]\n [1]\n [1]\n [1]]\n '''\nprint(np.concatenate((A,B,B,A),axis=1))#1表示左右合并\n'''\n[[1 2 2 1]\n [1 2 2 1]\n [1 2 2 1]]\n '''\n```\n### 7.NumPy分割\n```\nA=np.arange(12).reshape((3,4))\nprint(A)\n'''\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n '''\nprint(np.split(A,3,axis=0))#横向分割成3部分 或者np.vsplit(A,3)\n#[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]\n\nprint(np.split(A,2,axis=1))#竖向分割成2部分 或者np.hsplit(A,2)\n'''\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2,  3],\n       [ 6,  7],\n       [10, 11]])]\n '''\n \nprint(np.array_split(A,3,axis=1))#不等量分割成3部分\n'''\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2],\n       [ 6],\n       [10]]), array([[ 3],\n       [ 7],\n       [11]])]\n'''  \n```\n### 8.NumPy中copy和deep copy\n'='赋值方式会带有关联性\n```\na=np.arange(4)\nprint(a)\n#[1 2 3 4]\nb=a\nc=a\nd=b\nprint(b is a)\n#True\nprint(c is a)\n#True\nprint(d is a)\n#True\n\nb[0]=5#改变b的值，a,c,d同样会进行改变\nprint(a)\n#[5 2 3 4]\n```\n'copy()'赋值方式没有关联性\n```\na=np.arange(4)#deep copy\nprint(a)\n#[0 1 2 3]\nb=a.copy()\na[0]=5\nprint(b)#值并不发生改变\n#[0 1 2 3]\n```\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!\n<img src=\"http://img.blog.csdn.net/20180309135000807?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width = \"400\" height = \"400\" alt=\"公众号\" align=center/> <img src=\"http://img.blog.csdn.net/20180308203132669?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width = \"400\" height = \"400\" alt=\"赞赏码\" align=center/>","source":"_posts/Python之NumPy使用教程.md","raw":"---\ntitle: Python之NumPy使用教程\ndate: 2018-03-13 17:35:25\ntags: python\ntoc: true\ncategories: Python库\ncomments: true\n---\n### 1.NumPy概述\nNumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：\n 1. 强大的N维数组对象Array\n 2. 成熟的函数库\n 3. 用于集成C/C++和Fortran代码的工具\n 4. 实用的线性代数、傅立叶变换和随机生成函数\n\n### 2.NumPy安装\n```\npip install numpy或pip3 install numpy\n```\n### 3.NumPy引入\n```\nimport numpy as np#为了方便实用numpy 采用np简写\n```\n### 4.NumPy方法\n```\narray=np.array([[1,2,3],[4,5,6]])#将列表转换为矩阵 并转换为int类型\nprint(array)\n'''\n[[1 2 3]\n [4 5 6]]\n '''\n```\n#### 4.1NumPy属性\n```\nprint('array of dim:',array.ndim)#矩阵的维度\n#array of dim:2\nprint('array of shape',array.shape)#矩阵的行数和列数\n#array of shape:(2,3)\nprint('number of size:',array.size)#元素的个数\n#number of size:6\n```\n#### 4.2NumPy创建Array\n\n - array:创建数组\n - dtype:指定数据类型\n - zeros:创建数据全为0\n - ones:创建数据全为1\n - empty:创建数据接近0\n - arange:指定范围内创建数据\n - linspace创建线段\n\n创建数组\n```\na=np.array([1,2,3])\nprint(a)\n#[1,2,3]\n```\n指定数据dtype\n```\na=np.array([1,2,3],dtype=np.int)#指定为int类型\nprint(a.dtype)\n#int 64\nb=np.array([1,2,3],dtype=np.float)#指定为float类型\nprint(b.dtype)\n#float 64\n```\n创建特定数据\n```\na=np.array([[1,2,3],[4,5,6]])#矩阵 2行3列\nprint(a)\n'''\n[[1 2 3]\n [4 5 6]]\n '''\n```\n创建全0数组\n```\na=np.zeros((2,3))#数据全0 2行3列\nprint(a)\n'''\n[[0 0 0]\n [0 0 0]]\n '''\n```\n创建全1数组 指定特定类型dtype\n```\na=np.zeros((2,3),dtype=np.int)#数据全1 2行3列 同时指定类型\nprint(a)\n'''\n[[1 1 1]\n [1 1 1]]\n '''\n```\n创建全空数组 每个值接近0\n```\na=np.empty(2,3)#数据全为empty 3行4列\nprint(a)\n'''\n[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]\n [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]\n '''\n```\n用array创建连续数组\n```\na=np.arange(1,10,2)#1到10的数据 2步长\nprint(a)\n#[1 3 5 7 9]\n```\n用reshape改变数据形状\n```\na=np.arange(6).reshape(2,3)\nprint(a)\n'''\n[[0 1 2]\n [3 4 5]]\n '''\n```\n用linspace创建线段形数据\n```\na=np.linspace(1,10,20)#开始端1 结束端5 分割成10个数据 生成线段\nprint(a)\n'''\n[ 1.          1.44444444  1.88888889  2.33333333  2.77777778  3.22222222\n  3.66666667  4.11111111  4.55555556  5.        ]\n  '''\n```\n#### 4.3NumPy基础运算\n基础运算之加、减、三角函数等\n```\na=np.array([10,20,30,40])\nb=np.arange(4) #array[0,1,2,3]\n\nc=a+b#加法运算\nprint(c)\n#[10,21,32,43]\n\nc=a-b#减法运算\nprint(c)\n#[10.19,28,37]\n\nc=10*np.sin(a)#三角函数运算\n#[-5.44021111,  9.12945251, -9.88031624,  7.4511316 ]\n\nprint(b<3)#逻辑判断\n#[ True  True  True False]\n\nd=np.random.random((2,3))#随机生成2行3列的矩阵\nprint(d)\n'''\n[[ 0.21116981  0.0804489   0.51855475]\n [ 0.38359164  0.55852973  0.73218811]]\n'''\nprint(np.sum(d))#元素求和\n#2.48448292958\nprint(np.max(d))#元素求最大值\n#0.732188108709\nprint(np.min(d))#元素求最小值\n#0.0804488978886\n```\n多维矩阵运算\n```\na=np.array([[1,1],[0,1]])\nb=np.arange(4).reshape((2,2))\n\nc=np.dot(a,b)#或c=a.dot(b)矩阵运算\nprint(c)\n'''\n[[2 4]\n [2 3]]\n '''\n```\n对行或列执行查找运算\n```\na=np.array([[1,2],[3,4]])\nprint(a)\n'''\n[[1,2]\n [3,4]]\n '''\nprint(np.max(a,axis=0))#axis=0时是对列进行操作\n#[3,4]\nprint(np.min(a,axis=1))#axis=1是对行进行操作\n#[1,3]\n```\n矩阵索引操作\n```\nA=np.arange(2,14).reshape(3,4)\nprint(A)\n'''\n[[2,3,4,5]\n [6,7,8,9]\n [10,11,12,13]]\n '''\nprint(np.argmax(A))#矩阵中最大元素的索引\n#11\nprint(np.argmin(A))#矩阵中最小元素的索引\n#0\nprint(np.mean(A))#或者np.average(A)求解矩阵均值\n#7.5\nprint(np.cumsum(A))#矩阵累加函数\n#[2 5 9 14 20 27 35 44 54 65 77 90]\nprint(np.diff(A))#矩阵累差函数\n'''\n[[1 1 1]\n [1 1 1]\n [1 1 1]]\n '''\nprint(np.nonzero(A))#将非0元素的行与列坐标分割开来\n#(array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]))\n```\n矩阵排序、转置、替换操作\n```\nA=np.arange(14,2,-1).reshape((3,4))\nprint(A)\n'''\n[[14 13 12 11]\n [10  9  8  7]\n [ 6  5  4  3]]\n '''\nprint(np.sort(A))#排序\n'''\n[[11 12 13 14]\n [ 7  8  9 10]\n [ 3  4  5  6]]\n '''\n\nprint(np.transpose(A))\n'''\n[[14 10  6]\n [13  9  5]\n [12  8  4]\n [11  7  3]]\n '''\n\nprint(np.clip(A,5,9))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换\n'''\n[[9 9 9 9]\n [9 9 8 7]\n [6 5 5 5]]\n '''\n```\n### 5.索引\n一维索引\n```\nA=np.arange(0,12)\nprint(A)\n#[ 0  1  2  3  4  5  6  7  8  9 10 11]\nprint(A[1])#一维索引\n#1\n\nA=np.arange(0,12).reshape((3,4))\nprint(A[0])\n#[0,1,2,3]\n```\n二维索引\n```\nA=np.arange(0,12).reshape((3,4))\nprint(A)\n'''\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n '''\nprint(A[1][1])#或者A[1,1]\n#5\nprint(A[1,1:3])#切片处理\n#[5,6]\n\nfor row in A:\n    print(A)\n'''\n[0 1 2 3]\n[4 5 6 7]\n[ 8  9 10 11]\n '''\nfor col in A:\n    print(col)\n'''\n[0 4 8]\n[1 5 9]\n[ 2  6 10]\n[ 3  7 11]\n '''\n\nfor item in A.flat:\n    print(item)\n'''\n0\n1\n...\n10\n11\n'''\n```\n### 6.NumPy之Array合并\n```\nA=np.array([1,1,1])\nB=np.array([2,2,2])\nprint(np.vstack((A,B)))#上下合并\n'''\n[[1 1 1]\n [2 2 2]]\n '''\nprint(np.hstack((A,B)))#左右合并\n#[1 1 1 2 2 2]\n```\n增加维度\n```\nA=np.array([1,1,1])\nprint(A.shape)\n#(3,)\nprint(A[np.newaxis,:])\n#[[1 1 1]]\nprint(A[np.newaxis,:].shape)#newaxis增加维度\n#(1,3)\n\nprint(A[:,np.newaxis])\n'''\n[[1]\n [1]\n [1]]\n '''\nprint(A[:,np.newaxis].shape)\n#（3,1）\n```\n多矩阵合并\n```\nA = np.array([1,1,1])[:,np.newaxis]\nB = np.array([2,2,2])[:,np.newaxis]\nprint(np.concatenate((A,B,B,A),axis=0))#0表示上下合并\n'''\n[[1]\n [1]\n [1]\n [2]\n [2]\n [2]\n [2]\n [2]\n [2]\n [1]\n [1]\n [1]]\n '''\nprint(np.concatenate((A,B,B,A),axis=1))#1表示左右合并\n'''\n[[1 2 2 1]\n [1 2 2 1]\n [1 2 2 1]]\n '''\n```\n### 7.NumPy分割\n```\nA=np.arange(12).reshape((3,4))\nprint(A)\n'''\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n '''\nprint(np.split(A,3,axis=0))#横向分割成3部分 或者np.vsplit(A,3)\n#[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]\n\nprint(np.split(A,2,axis=1))#竖向分割成2部分 或者np.hsplit(A,2)\n'''\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2,  3],\n       [ 6,  7],\n       [10, 11]])]\n '''\n \nprint(np.array_split(A,3,axis=1))#不等量分割成3部分\n'''\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2],\n       [ 6],\n       [10]]), array([[ 3],\n       [ 7],\n       [11]])]\n'''  \n```\n### 8.NumPy中copy和deep copy\n'='赋值方式会带有关联性\n```\na=np.arange(4)\nprint(a)\n#[1 2 3 4]\nb=a\nc=a\nd=b\nprint(b is a)\n#True\nprint(c is a)\n#True\nprint(d is a)\n#True\n\nb[0]=5#改变b的值，a,c,d同样会进行改变\nprint(a)\n#[5 2 3 4]\n```\n'copy()'赋值方式没有关联性\n```\na=np.arange(4)#deep copy\nprint(a)\n#[0 1 2 3]\nb=a.copy()\na[0]=5\nprint(b)#值并不发生改变\n#[0 1 2 3]\n```\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!\n<img src=\"http://img.blog.csdn.net/20180309135000807?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width = \"400\" height = \"400\" alt=\"公众号\" align=center/> <img src=\"http://img.blog.csdn.net/20180308203132669?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width = \"400\" height = \"400\" alt=\"赞赏码\" align=center/>","slug":"Python之NumPy使用教程","published":1,"updated":"2018-03-14T03:16:53.604Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdc00092e01l1xeovvs","content":"<h3 id=\"1-NumPy概述\"><a href=\"#1-NumPy概述\" class=\"headerlink\" title=\"1.NumPy概述\"></a>1.NumPy概述</h3><p>NumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：</p>\n<ol>\n<li>强大的N维数组对象Array</li>\n<li>成熟的函数库</li>\n<li>用于集成C/C++和Fortran代码的工具</li>\n<li>实用的线性代数、傅立叶变换和随机生成函数</li>\n</ol>\n<h3 id=\"2-NumPy安装\"><a href=\"#2-NumPy安装\" class=\"headerlink\" title=\"2.NumPy安装\"></a>2.NumPy安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip <span class=\"keyword\">install</span> numpy或pip3 <span class=\"keyword\">install</span> numpy</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-NumPy引入\"><a href=\"#3-NumPy引入\" class=\"headerlink\" title=\"3.NumPy引入\"></a>3.NumPy引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np<span class=\"comment\">#为了方便实用numpy 采用np简写</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-NumPy方法\"><a href=\"#4-NumPy方法\" class=\"headerlink\" title=\"4.NumPy方法\"></a>4.NumPy方法</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])#将列表转换为矩阵 并转换为int类型</span><br><span class=\"line\">print(array)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\"> [<span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure>\n<h4 id=\"4-1NumPy属性\"><a href=\"#4-1NumPy属性\" class=\"headerlink\" title=\"4.1NumPy属性\"></a>4.1NumPy属性</h4><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'array of dim:'</span>,array.ndim)</span></span>#矩阵的维度</span><br><span class=\"line\"><span class=\"selector-id\">#array</span> of dim:<span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'array of shape'</span>,array.shape)</span></span>#矩阵的行数和列数</span><br><span class=\"line\"><span class=\"selector-id\">#array</span> of shape:(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'number of size:'</span>,array.size)</span></span>#元素的个数</span><br><span class=\"line\"><span class=\"selector-id\">#number</span> of size:<span class=\"number\">6</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2NumPy创建Array\"><a href=\"#4-2NumPy创建Array\" class=\"headerlink\" title=\"4.2NumPy创建Array\"></a>4.2NumPy创建Array</h4><ul>\n<li>array:创建数组</li>\n<li>dtype:指定数据类型</li>\n<li>zeros:创建数据全为0</li>\n<li>ones:创建数据全为1</li>\n<li>empty:创建数据接近0</li>\n<li>arange:指定范围内创建数据</li>\n<li>linspace创建线段</li>\n</ul>\n<p>创建数组<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>指定数据dtype<br><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">a</span>=np.array([1,2,3],<span class=\"attribute\">dtype</span>=np.int)#指定为int类型</span><br><span class=\"line\"><span class=\"builtin-name\">print</span>(a.dtype)</span><br><span class=\"line\"><span class=\"comment\">#int 64</span></span><br><span class=\"line\"><span class=\"attribute\">b</span>=np.array([1,2,3],<span class=\"attribute\">dtype</span>=np.float)#指定为float类型</span><br><span class=\"line\"><span class=\"builtin-name\">print</span>(b.dtype)</span><br><span class=\"line\"><span class=\"comment\">#float 64</span></span><br></pre></td></tr></table></figure></p>\n<p>创建特定数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])#矩阵 <span class=\"number\">2</span>行<span class=\"number\">3</span>列</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\"> [<span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>创建全0数组<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.zeros((<span class=\"number\">2</span>,<span class=\"number\">3</span>))<span class=\"comment\">#数据全0 2行3列</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[0 0 0]</span></span><br><span class=\"line\"><span class=\"string\"> [0 0 0]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>创建全1数组 指定特定类型dtype<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.zeros((<span class=\"number\">2</span>,<span class=\"number\">3</span>),dtype=np.int)<span class=\"comment\">#数据全1 2行3列 同时指定类型</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1 1 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 1 1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>创建全空数组 每个值接近0<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>)<span class=\"comment\">#数据全为empty 3行4列</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]</span></span><br><span class=\"line\"><span class=\"string\"> [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>用array创建连续数组<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">2</span>)#<span class=\"number\">1</span>到<span class=\"number\">10</span>的数据 <span class=\"number\">2</span>步长</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">5</span> <span class=\"number\">7</span> <span class=\"number\">9</span>]</span><br></pre></td></tr></table></figure></p>\n<p>用reshape改变数据形状<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">6</span>).reshape(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[0 1 2]</span></span><br><span class=\"line\"><span class=\"string\"> [3 4 5]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>用linspace创建线段形数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.linspace(<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">20</span>)#开始端<span class=\"number\">1</span> 结束端<span class=\"number\">5</span> 分割成<span class=\"number\">10</span>个数据 生成线段</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[ <span class=\"number\">1.</span>          <span class=\"number\">1.44444444</span>  <span class=\"number\">1.88888889</span>  <span class=\"number\">2.33333333</span>  <span class=\"number\">2.77777778</span>  <span class=\"number\">3.22222222</span></span><br><span class=\"line\">  <span class=\"number\">3.66666667</span>  <span class=\"number\">4.11111111</span>  <span class=\"number\">4.55555556</span>  <span class=\"number\">5.</span>        ]</span><br><span class=\"line\">  '''</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-3NumPy基础运算\"><a href=\"#4-3NumPy基础运算\" class=\"headerlink\" title=\"4.3NumPy基础运算\"></a>4.3NumPy基础运算</h4><p>基础运算之加、减、三角函数等<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([<span class=\"number\">10</span>,<span class=\"number\">20</span>,<span class=\"number\">30</span>,<span class=\"number\">40</span>])</span><br><span class=\"line\">b=np.arange(<span class=\"number\">4</span>) #array[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=a+b#加法运算</span><br><span class=\"line\">print(c)</span><br><span class=\"line\">#[<span class=\"number\">10</span>,<span class=\"number\">21</span>,<span class=\"number\">32</span>,<span class=\"number\">43</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=a-b#减法运算</span><br><span class=\"line\">print(c)</span><br><span class=\"line\">#[<span class=\"number\">10.19</span>,<span class=\"number\">28</span>,<span class=\"number\">37</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=<span class=\"number\">10</span>*np.sin(a)#三角函数运算</span><br><span class=\"line\">#[<span class=\"number\">-5.44021111</span>,  <span class=\"number\">9.12945251</span>, <span class=\"number\">-9.88031624</span>,  <span class=\"number\">7.4511316</span> ]</span><br><span class=\"line\"></span><br><span class=\"line\">print(b&lt;<span class=\"number\">3</span>)#逻辑判断</span><br><span class=\"line\">#[ True  True  True False]</span><br><span class=\"line\"></span><br><span class=\"line\">d=np.random.random((<span class=\"number\">2</span>,<span class=\"number\">3</span>))#随机生成<span class=\"number\">2</span>行<span class=\"number\">3</span>列的矩阵</span><br><span class=\"line\">print(d)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[ <span class=\"number\">0.21116981</span>  <span class=\"number\">0.0804489</span>   <span class=\"number\">0.51855475</span>]</span><br><span class=\"line\"> [ <span class=\"number\">0.38359164</span>  <span class=\"number\">0.55852973</span>  <span class=\"number\">0.73218811</span>]]</span><br><span class=\"line\">'''</span><br><span class=\"line\">print(np.sum(d))#元素求和</span><br><span class=\"line\">#<span class=\"number\">2.48448292958</span></span><br><span class=\"line\">print(np.max(d))#元素求最大值</span><br><span class=\"line\">#<span class=\"number\">0.732188108709</span></span><br><span class=\"line\">print(np.min(d))#元素求最小值</span><br><span class=\"line\">#<span class=\"number\">0.0804488978886</span></span><br></pre></td></tr></table></figure></p>\n<p>多维矩阵运算<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>]])</span><br><span class=\"line\">b=np.arange(<span class=\"number\">4</span>).reshape((<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">c=np.dot(a,b)<span class=\"comment\">#或c=a.dot(b)矩阵运算</span></span><br><span class=\"line\">print(c)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[2 4]</span></span><br><span class=\"line\"><span class=\"string\"> [2 3]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>对行或列执行查找运算<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>],[<span class=\"number\">3</span>,<span class=\"number\">4</span>]])</span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1,2]</span></span><br><span class=\"line\"><span class=\"string\"> [3,4]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(np.max(a,axis=<span class=\"number\">0</span>))<span class=\"comment\">#axis=0时是对列进行操作</span></span><br><span class=\"line\"><span class=\"comment\">#[3,4]</span></span><br><span class=\"line\">print(np.min(a,axis=<span class=\"number\">1</span>))<span class=\"comment\">#axis=1是对行进行操作</span></span><br><span class=\"line\"><span class=\"comment\">#[1,3]</span></span><br></pre></td></tr></table></figure></p>\n<p>矩阵索引操作<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">2</span>,<span class=\"number\">14</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]</span><br><span class=\"line\"> [<span class=\"number\">10</span>,<span class=\"number\">11</span>,<span class=\"number\">12</span>,<span class=\"number\">13</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.argmax(A))#矩阵中最大元素的索引</span><br><span class=\"line\">#<span class=\"number\">11</span></span><br><span class=\"line\">print(np.argmin(A))#矩阵中最小元素的索引</span><br><span class=\"line\">#<span class=\"number\">0</span></span><br><span class=\"line\">print(np.mean(A))#或者np.average(A)求解矩阵均值</span><br><span class=\"line\">#<span class=\"number\">7.5</span></span><br><span class=\"line\">print(np.cumsum(A))#矩阵累加函数</span><br><span class=\"line\">#[<span class=\"number\">2</span> <span class=\"number\">5</span> <span class=\"number\">9</span> <span class=\"number\">14</span> <span class=\"number\">20</span> <span class=\"number\">27</span> <span class=\"number\">35</span> <span class=\"number\">44</span> <span class=\"number\">54</span> <span class=\"number\">65</span> <span class=\"number\">77</span> <span class=\"number\">90</span>]</span><br><span class=\"line\">print(np.diff(A))#矩阵累差函数</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.nonzero(A))#将非<span class=\"number\">0</span>元素的行与列坐标分割开来</span><br><span class=\"line\">#(array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]), array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]))</span><br></pre></td></tr></table></figure></p>\n<p>矩阵排序、转置、替换操作<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">14</span>,<span class=\"number\">2</span>,<span class=\"number\">-1</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">14</span> <span class=\"number\">13</span> <span class=\"number\">12</span> <span class=\"number\">11</span>]</span><br><span class=\"line\"> [<span class=\"number\">10</span>  <span class=\"number\">9</span>  <span class=\"number\">8</span>  <span class=\"number\">7</span>]</span><br><span class=\"line\"> [ <span class=\"number\">6</span>  <span class=\"number\">5</span>  <span class=\"number\">4</span>  <span class=\"number\">3</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.sort(A))#排序</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">11</span> <span class=\"number\">12</span> <span class=\"number\">13</span> <span class=\"number\">14</span>]</span><br><span class=\"line\"> [ <span class=\"number\">7</span>  <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span>]</span><br><span class=\"line\"> [ <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.transpose(A))</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">14</span> <span class=\"number\">10</span>  <span class=\"number\">6</span>]</span><br><span class=\"line\"> [<span class=\"number\">13</span>  <span class=\"number\">9</span>  <span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">12</span>  <span class=\"number\">8</span>  <span class=\"number\">4</span>]</span><br><span class=\"line\"> [<span class=\"number\">11</span>  <span class=\"number\">7</span>  <span class=\"number\">3</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.clip(A,<span class=\"number\">5</span>,<span class=\"number\">9</span>))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">9</span>]</span><br><span class=\"line\"> [<span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">8</span> <span class=\"number\">7</span>]</span><br><span class=\"line\"> [<span class=\"number\">6</span> <span class=\"number\">5</span> <span class=\"number\">5</span> <span class=\"number\">5</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-索引\"><a href=\"#5-索引\" class=\"headerlink\" title=\"5.索引\"></a>5.索引</h3><p>一维索引<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>)</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">#[ <span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>  <span class=\"number\">7</span>  <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]</span><br><span class=\"line\">print(A[<span class=\"number\">1</span>])#一维索引</span><br><span class=\"line\">#<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A[<span class=\"number\">0</span>])</span><br><span class=\"line\">#[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>二维索引<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[ 0  1  2  3]</span></span><br><span class=\"line\"><span class=\"string\"> [ 4  5  6  7]</span></span><br><span class=\"line\"><span class=\"string\"> [ 8  9 10 11]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(A[<span class=\"number\">1</span>][<span class=\"number\">1</span>])<span class=\"comment\">#或者A[1,1]</span></span><br><span class=\"line\"><span class=\"comment\">#5</span></span><br><span class=\"line\">print(A[<span class=\"number\">1</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>])<span class=\"comment\">#切片处理</span></span><br><span class=\"line\"><span class=\"comment\">#[5,6]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">    print(A)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 1 2 3]</span></span><br><span class=\"line\"><span class=\"string\">[4 5 6 7]</span></span><br><span class=\"line\"><span class=\"string\">[ 8  9 10 11]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> col <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">    print(col)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 4 8]</span></span><br><span class=\"line\"><span class=\"string\">[1 5 9]</span></span><br><span class=\"line\"><span class=\"string\">[ 2  6 10]</span></span><br><span class=\"line\"><span class=\"string\">[ 3  7 11]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> A.flat:</span><br><span class=\"line\">    print(item)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">10</span></span><br><span class=\"line\"><span class=\"string\">11</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-NumPy之Array合并\"><a href=\"#6-NumPy之Array合并\" class=\"headerlink\" title=\"6.NumPy之Array合并\"></a>6.NumPy之Array合并</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">B=np.array([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\">print(np.vstack((A,B)))#上下合并</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.hstack((A,B)))#左右合并</span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n<p>增加维度<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">print(A.shape)</span><br><span class=\"line\"><span class=\"comment\">#(3,)</span></span><br><span class=\"line\">print(A[np.newaxis,:])</span><br><span class=\"line\"><span class=\"comment\">#[[1 1 1]]</span></span><br><span class=\"line\">print(A[np.newaxis,:].shape)<span class=\"comment\">#newaxis增加维度</span></span><br><span class=\"line\"><span class=\"comment\">#(1,3)</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(A[:,np.newaxis])</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(A[:,np.newaxis].shape)</span><br><span class=\"line\"><span class=\"comment\">#（3,1）</span></span><br></pre></td></tr></table></figure></p>\n<p>多矩阵合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])[:,np.newaxis]</span><br><span class=\"line\">B = np.array([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])[:,np.newaxis]</span><br><span class=\"line\">print(np.concatenate((A,B,B,A),axis=<span class=\"number\">0</span>))<span class=\"comment\">#0表示上下合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(np.concatenate((A,B,B,A),axis=<span class=\"number\">1</span>))<span class=\"comment\">#1表示左右合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1 2 2 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 2 2 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 2 2 1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-NumPy分割\"><a href=\"#7-NumPy分割\" class=\"headerlink\" title=\"7.NumPy分割\"></a>7.NumPy分割</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[ <span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>]</span><br><span class=\"line\"> [ <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>  <span class=\"number\">7</span>]</span><br><span class=\"line\"> [ <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.split(A,<span class=\"number\">3</span>,axis=<span class=\"number\">0</span>))#横向分割成<span class=\"number\">3</span>部分 或者np.vsplit(A,<span class=\"number\">3</span>)</span><br><span class=\"line\">#[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]]), array([[<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>]]), array([[ <span class=\"number\">8</span>,  <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>]])]</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.split(A,<span class=\"number\">2</span>,axis=<span class=\"number\">1</span>))#竖向分割成<span class=\"number\">2</span>部分 或者np.hsplit(A,<span class=\"number\">2</span>)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">       [<span class=\"number\">8</span>, <span class=\"number\">9</span>]]), array([[ <span class=\"number\">2</span>,  <span class=\"number\">3</span>],</span><br><span class=\"line\">       [ <span class=\"number\">6</span>,  <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">10</span>, <span class=\"number\">11</span>]])]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"> </span><br><span class=\"line\">print(np.array_split(A,<span class=\"number\">3</span>,axis=<span class=\"number\">1</span>))#不等量分割成<span class=\"number\">3</span>部分</span><br><span class=\"line\">'''</span><br><span class=\"line\">[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">       [<span class=\"number\">8</span>, <span class=\"number\">9</span>]]), array([[ <span class=\"number\">2</span>],</span><br><span class=\"line\">       [ <span class=\"number\">6</span>],</span><br><span class=\"line\">       [<span class=\"number\">10</span>]]), array([[ <span class=\"number\">3</span>],</span><br><span class=\"line\">       [ <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">11</span>]])]</span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-NumPy中copy和deep-copy\"><a href=\"#8-NumPy中copy和deep-copy\" class=\"headerlink\" title=\"8.NumPy中copy和deep copy\"></a>8.NumPy中copy和deep copy</h3><p>‘=’赋值方式会带有关联性<br><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(a)</span></span></span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br><span class=\"line\">b=a</span><br><span class=\"line\">c=a</span><br><span class=\"line\">d=b</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(b is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(c is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(d is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">b</span>[<span class=\"number\">0</span>]=<span class=\"number\">5</span>#改变b的值，<span class=\"selector-tag\">a</span>,c,d同样会进行改变</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(a)</span></span></span><br><span class=\"line\">#[<span class=\"number\">5</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br></pre></td></tr></table></figure></p>\n<p>‘copy()’赋值方式没有关联性<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">4</span>)#deep copy</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\">b=a.copy()</span><br><span class=\"line\">a[<span class=\"number\">0</span>]=<span class=\"number\">5</span></span><br><span class=\"line\">print(b)#值并不发生改变</span><br><span class=\"line\">#[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!<br><img src=\"http://img.blog.csdn.net/20180309135000807?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width=\"400\" height=\"400\" alt=\"公众号\" align=\"center/\"> <img src=\"http://img.blog.csdn.net/20180308203132669?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width=\"400\" height=\"400\" alt=\"赞赏码\" align=\"center/\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-NumPy概述\"><a href=\"#1-NumPy概述\" class=\"headerlink\" title=\"1.NumPy概述\"></a>1.NumPy概述</h3><p>NumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：</p>\n<ol>\n<li>强大的N维数组对象Array</li>\n<li>成熟的函数库</li>\n<li>用于集成C/C++和Fortran代码的工具</li>\n<li>实用的线性代数、傅立叶变换和随机生成函数</li>\n</ol>\n<h3 id=\"2-NumPy安装\"><a href=\"#2-NumPy安装\" class=\"headerlink\" title=\"2.NumPy安装\"></a>2.NumPy安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip <span class=\"keyword\">install</span> numpy或pip3 <span class=\"keyword\">install</span> numpy</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-NumPy引入\"><a href=\"#3-NumPy引入\" class=\"headerlink\" title=\"3.NumPy引入\"></a>3.NumPy引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np<span class=\"comment\">#为了方便实用numpy 采用np简写</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-NumPy方法\"><a href=\"#4-NumPy方法\" class=\"headerlink\" title=\"4.NumPy方法\"></a>4.NumPy方法</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])#将列表转换为矩阵 并转换为int类型</span><br><span class=\"line\">print(array)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\"> [<span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure>\n<h4 id=\"4-1NumPy属性\"><a href=\"#4-1NumPy属性\" class=\"headerlink\" title=\"4.1NumPy属性\"></a>4.1NumPy属性</h4><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'array of dim:'</span>,array.ndim)</span></span>#矩阵的维度</span><br><span class=\"line\"><span class=\"selector-id\">#array</span> of dim:<span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'array of shape'</span>,array.shape)</span></span>#矩阵的行数和列数</span><br><span class=\"line\"><span class=\"selector-id\">#array</span> of shape:(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(<span class=\"string\">'number of size:'</span>,array.size)</span></span>#元素的个数</span><br><span class=\"line\"><span class=\"selector-id\">#number</span> of size:<span class=\"number\">6</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2NumPy创建Array\"><a href=\"#4-2NumPy创建Array\" class=\"headerlink\" title=\"4.2NumPy创建Array\"></a>4.2NumPy创建Array</h4><ul>\n<li>array:创建数组</li>\n<li>dtype:指定数据类型</li>\n<li>zeros:创建数据全为0</li>\n<li>ones:创建数据全为1</li>\n<li>empty:创建数据接近0</li>\n<li>arange:指定范围内创建数据</li>\n<li>linspace创建线段</li>\n</ul>\n<p>创建数组<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>指定数据dtype<br><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">a</span>=np.array([1,2,3],<span class=\"attribute\">dtype</span>=np.int)#指定为int类型</span><br><span class=\"line\"><span class=\"builtin-name\">print</span>(a.dtype)</span><br><span class=\"line\"><span class=\"comment\">#int 64</span></span><br><span class=\"line\"><span class=\"attribute\">b</span>=np.array([1,2,3],<span class=\"attribute\">dtype</span>=np.float)#指定为float类型</span><br><span class=\"line\"><span class=\"builtin-name\">print</span>(b.dtype)</span><br><span class=\"line\"><span class=\"comment\">#float 64</span></span><br></pre></td></tr></table></figure></p>\n<p>创建特定数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])#矩阵 <span class=\"number\">2</span>行<span class=\"number\">3</span>列</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\"> [<span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>创建全0数组<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.zeros((<span class=\"number\">2</span>,<span class=\"number\">3</span>))<span class=\"comment\">#数据全0 2行3列</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[0 0 0]</span></span><br><span class=\"line\"><span class=\"string\"> [0 0 0]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>创建全1数组 指定特定类型dtype<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.zeros((<span class=\"number\">2</span>,<span class=\"number\">3</span>),dtype=np.int)<span class=\"comment\">#数据全1 2行3列 同时指定类型</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1 1 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 1 1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>创建全空数组 每个值接近0<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.empty(<span class=\"number\">2</span>,<span class=\"number\">3</span>)<span class=\"comment\">#数据全为empty 3行4列</span></span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]</span></span><br><span class=\"line\"><span class=\"string\"> [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>用array创建连续数组<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">2</span>)#<span class=\"number\">1</span>到<span class=\"number\">10</span>的数据 <span class=\"number\">2</span>步长</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">5</span> <span class=\"number\">7</span> <span class=\"number\">9</span>]</span><br></pre></td></tr></table></figure></p>\n<p>用reshape改变数据形状<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">6</span>).reshape(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[0 1 2]</span></span><br><span class=\"line\"><span class=\"string\"> [3 4 5]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>用linspace创建线段形数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.linspace(<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">20</span>)#开始端<span class=\"number\">1</span> 结束端<span class=\"number\">5</span> 分割成<span class=\"number\">10</span>个数据 生成线段</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[ <span class=\"number\">1.</span>          <span class=\"number\">1.44444444</span>  <span class=\"number\">1.88888889</span>  <span class=\"number\">2.33333333</span>  <span class=\"number\">2.77777778</span>  <span class=\"number\">3.22222222</span></span><br><span class=\"line\">  <span class=\"number\">3.66666667</span>  <span class=\"number\">4.11111111</span>  <span class=\"number\">4.55555556</span>  <span class=\"number\">5.</span>        ]</span><br><span class=\"line\">  '''</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-3NumPy基础运算\"><a href=\"#4-3NumPy基础运算\" class=\"headerlink\" title=\"4.3NumPy基础运算\"></a>4.3NumPy基础运算</h4><p>基础运算之加、减、三角函数等<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([<span class=\"number\">10</span>,<span class=\"number\">20</span>,<span class=\"number\">30</span>,<span class=\"number\">40</span>])</span><br><span class=\"line\">b=np.arange(<span class=\"number\">4</span>) #array[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=a+b#加法运算</span><br><span class=\"line\">print(c)</span><br><span class=\"line\">#[<span class=\"number\">10</span>,<span class=\"number\">21</span>,<span class=\"number\">32</span>,<span class=\"number\">43</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=a-b#减法运算</span><br><span class=\"line\">print(c)</span><br><span class=\"line\">#[<span class=\"number\">10.19</span>,<span class=\"number\">28</span>,<span class=\"number\">37</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">c=<span class=\"number\">10</span>*np.sin(a)#三角函数运算</span><br><span class=\"line\">#[<span class=\"number\">-5.44021111</span>,  <span class=\"number\">9.12945251</span>, <span class=\"number\">-9.88031624</span>,  <span class=\"number\">7.4511316</span> ]</span><br><span class=\"line\"></span><br><span class=\"line\">print(b&lt;<span class=\"number\">3</span>)#逻辑判断</span><br><span class=\"line\">#[ True  True  True False]</span><br><span class=\"line\"></span><br><span class=\"line\">d=np.random.random((<span class=\"number\">2</span>,<span class=\"number\">3</span>))#随机生成<span class=\"number\">2</span>行<span class=\"number\">3</span>列的矩阵</span><br><span class=\"line\">print(d)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[ <span class=\"number\">0.21116981</span>  <span class=\"number\">0.0804489</span>   <span class=\"number\">0.51855475</span>]</span><br><span class=\"line\"> [ <span class=\"number\">0.38359164</span>  <span class=\"number\">0.55852973</span>  <span class=\"number\">0.73218811</span>]]</span><br><span class=\"line\">'''</span><br><span class=\"line\">print(np.sum(d))#元素求和</span><br><span class=\"line\">#<span class=\"number\">2.48448292958</span></span><br><span class=\"line\">print(np.max(d))#元素求最大值</span><br><span class=\"line\">#<span class=\"number\">0.732188108709</span></span><br><span class=\"line\">print(np.min(d))#元素求最小值</span><br><span class=\"line\">#<span class=\"number\">0.0804488978886</span></span><br></pre></td></tr></table></figure></p>\n<p>多维矩阵运算<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">1</span>],[<span class=\"number\">0</span>,<span class=\"number\">1</span>]])</span><br><span class=\"line\">b=np.arange(<span class=\"number\">4</span>).reshape((<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">c=np.dot(a,b)<span class=\"comment\">#或c=a.dot(b)矩阵运算</span></span><br><span class=\"line\">print(c)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[2 4]</span></span><br><span class=\"line\"><span class=\"string\"> [2 3]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>对行或列执行查找运算<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>],[<span class=\"number\">3</span>,<span class=\"number\">4</span>]])</span><br><span class=\"line\">print(a)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1,2]</span></span><br><span class=\"line\"><span class=\"string\"> [3,4]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(np.max(a,axis=<span class=\"number\">0</span>))<span class=\"comment\">#axis=0时是对列进行操作</span></span><br><span class=\"line\"><span class=\"comment\">#[3,4]</span></span><br><span class=\"line\">print(np.min(a,axis=<span class=\"number\">1</span>))<span class=\"comment\">#axis=1是对行进行操作</span></span><br><span class=\"line\"><span class=\"comment\">#[1,3]</span></span><br></pre></td></tr></table></figure></p>\n<p>矩阵索引操作<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">2</span>,<span class=\"number\">14</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]</span><br><span class=\"line\"> [<span class=\"number\">10</span>,<span class=\"number\">11</span>,<span class=\"number\">12</span>,<span class=\"number\">13</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.argmax(A))#矩阵中最大元素的索引</span><br><span class=\"line\">#<span class=\"number\">11</span></span><br><span class=\"line\">print(np.argmin(A))#矩阵中最小元素的索引</span><br><span class=\"line\">#<span class=\"number\">0</span></span><br><span class=\"line\">print(np.mean(A))#或者np.average(A)求解矩阵均值</span><br><span class=\"line\">#<span class=\"number\">7.5</span></span><br><span class=\"line\">print(np.cumsum(A))#矩阵累加函数</span><br><span class=\"line\">#[<span class=\"number\">2</span> <span class=\"number\">5</span> <span class=\"number\">9</span> <span class=\"number\">14</span> <span class=\"number\">20</span> <span class=\"number\">27</span> <span class=\"number\">35</span> <span class=\"number\">44</span> <span class=\"number\">54</span> <span class=\"number\">65</span> <span class=\"number\">77</span> <span class=\"number\">90</span>]</span><br><span class=\"line\">print(np.diff(A))#矩阵累差函数</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.nonzero(A))#将非<span class=\"number\">0</span>元素的行与列坐标分割开来</span><br><span class=\"line\">#(array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]), array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]))</span><br></pre></td></tr></table></figure></p>\n<p>矩阵排序、转置、替换操作<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">14</span>,<span class=\"number\">2</span>,<span class=\"number\">-1</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">14</span> <span class=\"number\">13</span> <span class=\"number\">12</span> <span class=\"number\">11</span>]</span><br><span class=\"line\"> [<span class=\"number\">10</span>  <span class=\"number\">9</span>  <span class=\"number\">8</span>  <span class=\"number\">7</span>]</span><br><span class=\"line\"> [ <span class=\"number\">6</span>  <span class=\"number\">5</span>  <span class=\"number\">4</span>  <span class=\"number\">3</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.sort(A))#排序</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">11</span> <span class=\"number\">12</span> <span class=\"number\">13</span> <span class=\"number\">14</span>]</span><br><span class=\"line\"> [ <span class=\"number\">7</span>  <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span>]</span><br><span class=\"line\"> [ <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.transpose(A))</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">14</span> <span class=\"number\">10</span>  <span class=\"number\">6</span>]</span><br><span class=\"line\"> [<span class=\"number\">13</span>  <span class=\"number\">9</span>  <span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">12</span>  <span class=\"number\">8</span>  <span class=\"number\">4</span>]</span><br><span class=\"line\"> [<span class=\"number\">11</span>  <span class=\"number\">7</span>  <span class=\"number\">3</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.clip(A,<span class=\"number\">5</span>,<span class=\"number\">9</span>))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">9</span>]</span><br><span class=\"line\"> [<span class=\"number\">9</span> <span class=\"number\">9</span> <span class=\"number\">8</span> <span class=\"number\">7</span>]</span><br><span class=\"line\"> [<span class=\"number\">6</span> <span class=\"number\">5</span> <span class=\"number\">5</span> <span class=\"number\">5</span>]]</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-索引\"><a href=\"#5-索引\" class=\"headerlink\" title=\"5.索引\"></a>5.索引</h3><p>一维索引<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>)</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">#[ <span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>  <span class=\"number\">7</span>  <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]</span><br><span class=\"line\">print(A[<span class=\"number\">1</span>])#一维索引</span><br><span class=\"line\">#<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A[<span class=\"number\">0</span>])</span><br><span class=\"line\">#[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>二维索引<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">0</span>,<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[ 0  1  2  3]</span></span><br><span class=\"line\"><span class=\"string\"> [ 4  5  6  7]</span></span><br><span class=\"line\"><span class=\"string\"> [ 8  9 10 11]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(A[<span class=\"number\">1</span>][<span class=\"number\">1</span>])<span class=\"comment\">#或者A[1,1]</span></span><br><span class=\"line\"><span class=\"comment\">#5</span></span><br><span class=\"line\">print(A[<span class=\"number\">1</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>])<span class=\"comment\">#切片处理</span></span><br><span class=\"line\"><span class=\"comment\">#[5,6]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">    print(A)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 1 2 3]</span></span><br><span class=\"line\"><span class=\"string\">[4 5 6 7]</span></span><br><span class=\"line\"><span class=\"string\">[ 8  9 10 11]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> col <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">    print(col)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 4 8]</span></span><br><span class=\"line\"><span class=\"string\">[1 5 9]</span></span><br><span class=\"line\"><span class=\"string\">[ 2  6 10]</span></span><br><span class=\"line\"><span class=\"string\">[ 3  7 11]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> A.flat:</span><br><span class=\"line\">    print(item)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">10</span></span><br><span class=\"line\"><span class=\"string\">11</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-NumPy之Array合并\"><a href=\"#6-NumPy之Array合并\" class=\"headerlink\" title=\"6.NumPy之Array合并\"></a>6.NumPy之Array合并</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">B=np.array([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\">print(np.vstack((A,B)))#上下合并</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.hstack((A,B)))#左右合并</span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n<p>增加维度<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">print(A.shape)</span><br><span class=\"line\"><span class=\"comment\">#(3,)</span></span><br><span class=\"line\">print(A[np.newaxis,:])</span><br><span class=\"line\"><span class=\"comment\">#[[1 1 1]]</span></span><br><span class=\"line\">print(A[np.newaxis,:].shape)<span class=\"comment\">#newaxis增加维度</span></span><br><span class=\"line\"><span class=\"comment\">#(1,3)</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(A[:,np.newaxis])</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(A[:,np.newaxis].shape)</span><br><span class=\"line\"><span class=\"comment\">#（3,1）</span></span><br></pre></td></tr></table></figure></p>\n<p>多矩阵合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = np.array([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>])[:,np.newaxis]</span><br><span class=\"line\">B = np.array([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])[:,np.newaxis]</span><br><span class=\"line\">print(np.concatenate((A,B,B,A),axis=<span class=\"number\">0</span>))<span class=\"comment\">#0表示上下合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [2]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]</span></span><br><span class=\"line\"><span class=\"string\"> [1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(np.concatenate((A,B,B,A),axis=<span class=\"number\">1</span>))<span class=\"comment\">#1表示左右合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1 2 2 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 2 2 1]</span></span><br><span class=\"line\"><span class=\"string\"> [1 2 2 1]]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-NumPy分割\"><a href=\"#7-NumPy分割\" class=\"headerlink\" title=\"7.NumPy分割\"></a>7.NumPy分割</h3><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=np.arange(<span class=\"number\">12</span>).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">print(A)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[[ <span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>]</span><br><span class=\"line\"> [ <span class=\"number\">4</span>  <span class=\"number\">5</span>  <span class=\"number\">6</span>  <span class=\"number\">7</span>]</span><br><span class=\"line\"> [ <span class=\"number\">8</span>  <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]]</span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(np.split(A,<span class=\"number\">3</span>,axis=<span class=\"number\">0</span>))#横向分割成<span class=\"number\">3</span>部分 或者np.vsplit(A,<span class=\"number\">3</span>)</span><br><span class=\"line\">#[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]]), array([[<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>]]), array([[ <span class=\"number\">8</span>,  <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>]])]</span><br><span class=\"line\"></span><br><span class=\"line\">print(np.split(A,<span class=\"number\">2</span>,axis=<span class=\"number\">1</span>))#竖向分割成<span class=\"number\">2</span>部分 或者np.hsplit(A,<span class=\"number\">2</span>)</span><br><span class=\"line\">'''</span><br><span class=\"line\">[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">       [<span class=\"number\">8</span>, <span class=\"number\">9</span>]]), array([[ <span class=\"number\">2</span>,  <span class=\"number\">3</span>],</span><br><span class=\"line\">       [ <span class=\"number\">6</span>,  <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">10</span>, <span class=\"number\">11</span>]])]</span><br><span class=\"line\"> '''</span><br><span class=\"line\"> </span><br><span class=\"line\">print(np.array_split(A,<span class=\"number\">3</span>,axis=<span class=\"number\">1</span>))#不等量分割成<span class=\"number\">3</span>部分</span><br><span class=\"line\">'''</span><br><span class=\"line\">[array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">       [<span class=\"number\">8</span>, <span class=\"number\">9</span>]]), array([[ <span class=\"number\">2</span>],</span><br><span class=\"line\">       [ <span class=\"number\">6</span>],</span><br><span class=\"line\">       [<span class=\"number\">10</span>]]), array([[ <span class=\"number\">3</span>],</span><br><span class=\"line\">       [ <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">11</span>]])]</span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-NumPy中copy和deep-copy\"><a href=\"#8-NumPy中copy和deep-copy\" class=\"headerlink\" title=\"8.NumPy中copy和deep copy\"></a>8.NumPy中copy和deep copy</h3><p>‘=’赋值方式会带有关联性<br><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(a)</span></span></span><br><span class=\"line\">#[<span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br><span class=\"line\">b=a</span><br><span class=\"line\">c=a</span><br><span class=\"line\">d=b</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(b is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(c is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(d is a)</span></span></span><br><span class=\"line\">#True</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">b</span>[<span class=\"number\">0</span>]=<span class=\"number\">5</span>#改变b的值，<span class=\"selector-tag\">a</span>,c,d同样会进行改变</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(a)</span></span></span><br><span class=\"line\">#[<span class=\"number\">5</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br></pre></td></tr></table></figure></p>\n<p>‘copy()’赋值方式没有关联性<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=np.arange(<span class=\"number\">4</span>)#deep copy</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">#[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br><span class=\"line\">b=a.copy()</span><br><span class=\"line\">a[<span class=\"number\">0</span>]=<span class=\"number\">5</span></span><br><span class=\"line\">print(b)#值并不发生改变</span><br><span class=\"line\">#[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!<br><img src=\"http://img.blog.csdn.net/20180309135000807?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width=\"400\" height=\"400\" alt=\"公众号\" align=\"center/\"> <img src=\"http://img.blog.csdn.net/20180308203132669?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" width=\"400\" height=\"400\" alt=\"赞赏码\" align=\"center/\"></p>\n"},{"title":"机器学习之Apriori算法","date":"2018-05-18T02:05:37.000Z","mathjax":true,"comments":1,"_content":"\n### 1.Apriori算法简介\n\n**Apriori**算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到**大部分顾客会在一次购物中同时购买面包和牛奶**，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。\n\n+ **事务数据库：**设$I=\\{ i_1,i_2,…,i_m \\}$是一个全局项的集合，事物数据库$D=\\{ t_1,t_2,..,t_n \\}$是一个事务的集合，每个事务$t_i(1\\le i \\le n)$都对应$I$上的一个子集，例如$t_1=\\{ i_1,i_3,i_7\\}$。\n+ **关联规则：**关联规则表示项之间的关系，是形如$X\\rightarrow Y​$的蕴含表达式，其中$X​$和$Y​$是不相交的项集，$X​$称为规则的前件，$Y​$称为规则的后件。例如$\\{ cereal,milk\\}\\rightarrow \\{fruit \\}​$关联规则表示购买谷类食品和牛奶的人也会购买水果。通常关联规则的强度可以用支持度和置信度来度量。\n\n+ **支持度：**支持度表示关联数据在数据集中出现的次数或所占的比重。\n\n$$\nsupport(X\\rightarrow Y)=P(X\\cup Y)=\\frac{|X\\cup Y|}{|D|}\n$$\n\n+ **置信度：**置信度表示$Y$数据出现后，$X$数据出现的可能性，也可以说是数据的条件概率。\n\n$$\nconfidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(XY)}{P(Y)}\n$$\n\n+ **提升度：**提升度体现$X$和$Y$之间的关联关系，提升度大于1表示$X$和$Y$之间具有强关联关系，提升度小于等于1表示$X$和$Y$之间无有效的强关联关系。\n\n$$\nlift(X \\Leftarrow Y)=\\frac{confidence(X \\Leftarrow Y)}{P(X)}=\\frac{P(XY)}{P(X)P(Y)}\n$$\n\n+ **强关联规则：**满足最小支持度和最小置信度的关联规则。\n\n关联规则的挖掘目标是**找出所有的频繁项集**和**根据频繁项集产生强关联规则**。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。\n\n### 2.Apriori算法原理\n\nApriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是**找到最多的K项频繁集**。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。\n\nApriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。\n\n![机器学习之Apriori算法图片01](机器学习之Apriori算法/机器学习之Apriori算法图片01.png)\n\n数据集包含4条记录{'134','235','1235','25'}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{'1','2','3','4','5'}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{'1','2','3','5'}。根据频繁1项集连接得到候选2项集{'12','13','15','23','25','35'}，其中数据{'12','15'}低于最低支持度，进行剪枝处理，得到频繁2项集为{'13','23','25','35'}。如此迭代下去，最终能够得到频繁3项集{'235'}，由于数据无法再进行连接，算法至此结束。\n\n### 3.Apriori算法流程\n\n从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。\n\n+ 扫描数据集，得到所有出现过的数据，作为候选1项集。\n+ 挖掘频繁k项集。\n  + 扫描计算候选k项集的支持度。\n  + 剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。\n  + 基于频繁k项集，连接生成候选k+1项集。\n+ 利用步骤2，迭代得到k=k+1项集结果。\n\n### 4.Apriori算法优缺点\n\n#### 4.1优点\n\n+ 适合稀疏数据集。\n+ 算法原理简单，易实现。\n+ 适合事务数据库的关联规则挖掘。\n\n#### 4.2缺点\n\n+ 可能产生庞大的候选集。\n+ 算法需多次遍历数据集，算法效率低，耗时。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n> [刘建平Pinard-Apriori算法原理总结](http://www.cnblogs.com/pinard/p/6293298.html)\n","source":"_posts/机器学习之Apriori算法.md","raw":"---\ntitle: 机器学习之Apriori算法\ndate: 2018-05-18 10:05:37\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.Apriori算法简介\n\n**Apriori**算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到**大部分顾客会在一次购物中同时购买面包和牛奶**，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。\n\n+ **事务数据库：**设$I=\\{ i_1,i_2,…,i_m \\}$是一个全局项的集合，事物数据库$D=\\{ t_1,t_2,..,t_n \\}$是一个事务的集合，每个事务$t_i(1\\le i \\le n)$都对应$I$上的一个子集，例如$t_1=\\{ i_1,i_3,i_7\\}$。\n+ **关联规则：**关联规则表示项之间的关系，是形如$X\\rightarrow Y​$的蕴含表达式，其中$X​$和$Y​$是不相交的项集，$X​$称为规则的前件，$Y​$称为规则的后件。例如$\\{ cereal,milk\\}\\rightarrow \\{fruit \\}​$关联规则表示购买谷类食品和牛奶的人也会购买水果。通常关联规则的强度可以用支持度和置信度来度量。\n\n+ **支持度：**支持度表示关联数据在数据集中出现的次数或所占的比重。\n\n$$\nsupport(X\\rightarrow Y)=P(X\\cup Y)=\\frac{|X\\cup Y|}{|D|}\n$$\n\n+ **置信度：**置信度表示$Y$数据出现后，$X$数据出现的可能性，也可以说是数据的条件概率。\n\n$$\nconfidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(XY)}{P(Y)}\n$$\n\n+ **提升度：**提升度体现$X$和$Y$之间的关联关系，提升度大于1表示$X$和$Y$之间具有强关联关系，提升度小于等于1表示$X$和$Y$之间无有效的强关联关系。\n\n$$\nlift(X \\Leftarrow Y)=\\frac{confidence(X \\Leftarrow Y)}{P(X)}=\\frac{P(XY)}{P(X)P(Y)}\n$$\n\n+ **强关联规则：**满足最小支持度和最小置信度的关联规则。\n\n关联规则的挖掘目标是**找出所有的频繁项集**和**根据频繁项集产生强关联规则**。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。\n\n### 2.Apriori算法原理\n\nApriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是**找到最多的K项频繁集**。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。\n\nApriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。\n\n![机器学习之Apriori算法图片01](机器学习之Apriori算法/机器学习之Apriori算法图片01.png)\n\n数据集包含4条记录{'134','235','1235','25'}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{'1','2','3','4','5'}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{'1','2','3','5'}。根据频繁1项集连接得到候选2项集{'12','13','15','23','25','35'}，其中数据{'12','15'}低于最低支持度，进行剪枝处理，得到频繁2项集为{'13','23','25','35'}。如此迭代下去，最终能够得到频繁3项集{'235'}，由于数据无法再进行连接，算法至此结束。\n\n### 3.Apriori算法流程\n\n从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。\n\n+ 扫描数据集，得到所有出现过的数据，作为候选1项集。\n+ 挖掘频繁k项集。\n  + 扫描计算候选k项集的支持度。\n  + 剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。\n  + 基于频繁k项集，连接生成候选k+1项集。\n+ 利用步骤2，迭代得到k=k+1项集结果。\n\n### 4.Apriori算法优缺点\n\n#### 4.1优点\n\n+ 适合稀疏数据集。\n+ 算法原理简单，易实现。\n+ 适合事务数据库的关联规则挖掘。\n\n#### 4.2缺点\n\n+ 可能产生庞大的候选集。\n+ 算法需多次遍历数据集，算法效率低，耗时。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n> [刘建平Pinard-Apriori算法原理总结](http://www.cnblogs.com/pinard/p/6293298.html)\n","slug":"机器学习之Apriori算法","published":1,"updated":"2018-06-07T04:45:14.805Z","layout":"post","photos":[],"link":"","_id":"cji4rdzde000a2e01z3lad7yd","content":"<h3 id=\"1-Apriori算法简介\"><a href=\"#1-Apriori算法简介\" class=\"headerlink\" title=\"1.Apriori算法简介\"></a>1.Apriori算法简介</h3><p><strong>Apriori</strong>算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到<strong>大部分顾客会在一次购物中同时购买面包和牛奶</strong>，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。</p>\n<ul>\n<li><strong>事务数据库：</strong>设$I={ i_1,i_2,…,i_m }$是一个全局项的集合，事物数据库$D={ t_1,t_2,..,t_n }$是一个事务的集合，每个事务$t_i(1\\le i \\le n)$都对应$I$上的一个子集，例如$t_1={ i_1,i_3,i_7}$。</li>\n<li><p><strong>关联规则：</strong>关联规则表示项之间的关系，是形如$X\\rightarrow Y​$的蕴含表达式，其中$X​$和$Y​$是不相交的项集，$X​$称为规则的前件，$Y​$称为规则的后件。例如${ cereal,milk}\\rightarrow {fruit }​$关联规则表示购买谷类食品和牛奶的人也会购买水果。通常关联规则的强度可以用支持度和置信度来度量。</p>\n</li>\n<li><p><strong>支持度：</strong>支持度表示关联数据在数据集中出现的次数或所占的比重。</p>\n</li>\n</ul>\n<p>$$<br>support(X\\rightarrow Y)=P(X\\cup Y)=\\frac{|X\\cup Y|}{|D|}<br>$$</p>\n<ul>\n<li><strong>置信度：</strong>置信度表示$Y$数据出现后，$X$数据出现的可能性，也可以说是数据的条件概率。</li>\n</ul>\n<p>$$<br>confidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(XY)}{P(Y)}<br>$$</p>\n<ul>\n<li><strong>提升度：</strong>提升度体现$X$和$Y$之间的关联关系，提升度大于1表示$X$和$Y$之间具有强关联关系，提升度小于等于1表示$X$和$Y$之间无有效的强关联关系。</li>\n</ul>\n<p>$$<br>lift(X \\Leftarrow Y)=\\frac{confidence(X \\Leftarrow Y)}{P(X)}=\\frac{P(XY)}{P(X)P(Y)}<br>$$</p>\n<ul>\n<li><strong>强关联规则：</strong>满足最小支持度和最小置信度的关联规则。</li>\n</ul>\n<p>关联规则的挖掘目标是<strong>找出所有的频繁项集</strong>和<strong>根据频繁项集产生强关联规则</strong>。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。</p>\n<h3 id=\"2-Apriori算法原理\"><a href=\"#2-Apriori算法原理\" class=\"headerlink\" title=\"2.Apriori算法原理\"></a>2.Apriori算法原理</h3><p>Apriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是<strong>找到最多的K项频繁集</strong>。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。</p>\n<p>Apriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。</p>\n<p><img src=\"机器学习之Apriori算法/机器学习之Apriori算法图片01.png\" alt=\"机器学习之Apriori算法图片01\"></p>\n<p>数据集包含4条记录{‘134’,’235’,’1235’,’25’}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{‘1’,’2’,’3’,’4’,’5’}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{‘1’,’2’,’3’,’5’}。根据频繁1项集连接得到候选2项集{‘12’,’13’,’15’,’23’,’25’,’35’}，其中数据{‘12’,’15’}低于最低支持度，进行剪枝处理，得到频繁2项集为{‘13’,’23’,’25’,’35’}。如此迭代下去，最终能够得到频繁3项集{‘235’}，由于数据无法再进行连接，算法至此结束。</p>\n<h3 id=\"3-Apriori算法流程\"><a href=\"#3-Apriori算法流程\" class=\"headerlink\" title=\"3.Apriori算法流程\"></a>3.Apriori算法流程</h3><p>从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。</p>\n<ul>\n<li>扫描数据集，得到所有出现过的数据，作为候选1项集。</li>\n<li>挖掘频繁k项集。<ul>\n<li>扫描计算候选k项集的支持度。</li>\n<li>剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。</li>\n<li>基于频繁k项集，连接生成候选k+1项集。</li>\n</ul>\n</li>\n<li>利用步骤2，迭代得到k=k+1项集结果。</li>\n</ul>\n<h3 id=\"4-Apriori算法优缺点\"><a href=\"#4-Apriori算法优缺点\" class=\"headerlink\" title=\"4.Apriori算法优缺点\"></a>4.Apriori算法优缺点</h3><h4 id=\"4-1优点\"><a href=\"#4-1优点\" class=\"headerlink\" title=\"4.1优点\"></a>4.1优点</h4><ul>\n<li>适合稀疏数据集。</li>\n<li>算法原理简单，易实现。</li>\n<li>适合事务数据库的关联规则挖掘。</li>\n</ul>\n<h4 id=\"4-2缺点\"><a href=\"#4-2缺点\" class=\"headerlink\" title=\"4.2缺点\"></a>4.2缺点</h4><ul>\n<li>可能产生庞大的候选集。</li>\n<li>算法需多次遍历数据集，算法效率低，耗时。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6293298.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard-Apriori算法原理总结</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Apriori算法简介\"><a href=\"#1-Apriori算法简介\" class=\"headerlink\" title=\"1.Apriori算法简介\"></a>1.Apriori算法简介</h3><p><strong>Apriori</strong>算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到<strong>大部分顾客会在一次购物中同时购买面包和牛奶</strong>，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。</p>\n<ul>\n<li><strong>事务数据库：</strong>设$I={ i_1,i_2,…,i_m }$是一个全局项的集合，事物数据库$D={ t_1,t_2,..,t_n }$是一个事务的集合，每个事务$t_i(1\\le i \\le n)$都对应$I$上的一个子集，例如$t_1={ i_1,i_3,i_7}$。</li>\n<li><p><strong>关联规则：</strong>关联规则表示项之间的关系，是形如$X\\rightarrow Y​$的蕴含表达式，其中$X​$和$Y​$是不相交的项集，$X​$称为规则的前件，$Y​$称为规则的后件。例如${ cereal,milk}\\rightarrow {fruit }​$关联规则表示购买谷类食品和牛奶的人也会购买水果。通常关联规则的强度可以用支持度和置信度来度量。</p>\n</li>\n<li><p><strong>支持度：</strong>支持度表示关联数据在数据集中出现的次数或所占的比重。</p>\n</li>\n</ul>\n<p>$$<br>support(X\\rightarrow Y)=P(X\\cup Y)=\\frac{|X\\cup Y|}{|D|}<br>$$</p>\n<ul>\n<li><strong>置信度：</strong>置信度表示$Y$数据出现后，$X$数据出现的可能性，也可以说是数据的条件概率。</li>\n</ul>\n<p>$$<br>confidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(XY)}{P(Y)}<br>$$</p>\n<ul>\n<li><strong>提升度：</strong>提升度体现$X$和$Y$之间的关联关系，提升度大于1表示$X$和$Y$之间具有强关联关系，提升度小于等于1表示$X$和$Y$之间无有效的强关联关系。</li>\n</ul>\n<p>$$<br>lift(X \\Leftarrow Y)=\\frac{confidence(X \\Leftarrow Y)}{P(X)}=\\frac{P(XY)}{P(X)P(Y)}<br>$$</p>\n<ul>\n<li><strong>强关联规则：</strong>满足最小支持度和最小置信度的关联规则。</li>\n</ul>\n<p>关联规则的挖掘目标是<strong>找出所有的频繁项集</strong>和<strong>根据频繁项集产生强关联规则</strong>。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。</p>\n<h3 id=\"2-Apriori算法原理\"><a href=\"#2-Apriori算法原理\" class=\"headerlink\" title=\"2.Apriori算法原理\"></a>2.Apriori算法原理</h3><p>Apriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是<strong>找到最多的K项频繁集</strong>。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。</p>\n<p>Apriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。</p>\n<p><img src=\"机器学习之Apriori算法/机器学习之Apriori算法图片01.png\" alt=\"机器学习之Apriori算法图片01\"></p>\n<p>数据集包含4条记录{‘134’,’235’,’1235’,’25’}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{‘1’,’2’,’3’,’4’,’5’}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{‘1’,’2’,’3’,’5’}。根据频繁1项集连接得到候选2项集{‘12’,’13’,’15’,’23’,’25’,’35’}，其中数据{‘12’,’15’}低于最低支持度，进行剪枝处理，得到频繁2项集为{‘13’,’23’,’25’,’35’}。如此迭代下去，最终能够得到频繁3项集{‘235’}，由于数据无法再进行连接，算法至此结束。</p>\n<h3 id=\"3-Apriori算法流程\"><a href=\"#3-Apriori算法流程\" class=\"headerlink\" title=\"3.Apriori算法流程\"></a>3.Apriori算法流程</h3><p>从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。</p>\n<ul>\n<li>扫描数据集，得到所有出现过的数据，作为候选1项集。</li>\n<li>挖掘频繁k项集。<ul>\n<li>扫描计算候选k项集的支持度。</li>\n<li>剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。</li>\n<li>基于频繁k项集，连接生成候选k+1项集。</li>\n</ul>\n</li>\n<li>利用步骤2，迭代得到k=k+1项集结果。</li>\n</ul>\n<h3 id=\"4-Apriori算法优缺点\"><a href=\"#4-Apriori算法优缺点\" class=\"headerlink\" title=\"4.Apriori算法优缺点\"></a>4.Apriori算法优缺点</h3><h4 id=\"4-1优点\"><a href=\"#4-1优点\" class=\"headerlink\" title=\"4.1优点\"></a>4.1优点</h4><ul>\n<li>适合稀疏数据集。</li>\n<li>算法原理简单，易实现。</li>\n<li>适合事务数据库的关联规则挖掘。</li>\n</ul>\n<h4 id=\"4-2缺点\"><a href=\"#4-2缺点\" class=\"headerlink\" title=\"4.2缺点\"></a>4.2缺点</h4><ul>\n<li>可能产生庞大的候选集。</li>\n<li>算法需多次遍历数据集，算法效率低，耗时。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6293298.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard-Apriori算法原理总结</a></p>\n</blockquote>\n"},{"title":"Python之Sklearn使用教程","date":"2018-04-15T03:43:18.000Z","_content":"\n### 1.Sklearn简介\n\nScikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：\n\n+ 简单高效的数据挖掘和数据分析工具\n\n\n+ 让每个人能够在复杂环境中重复使用\n+ 建立NumPy、Scipy、MatPlotLib之上\n\n![Python之Sklearn使用教程图片01](Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png)\n\n### 2.Sklearn安装\n\nSklearn安装要求`Python(>=2.7 or >=3.3)`、`NumPy (>= 1.8.2)`、`SciPy (>= 0.13.3)`。如果已经安装NumPy和SciPy，安装scikit-learn可以使用`pip install -U scikit-learn`。\n\n### 3.Sklearn通用学习模式\n\nSklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，`4.Sklearn datasets`中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过`MatPlotLib`等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。\n\n```python\nfrom sklearn import datasets#引入数据集,sklearn包含众多数据集\nfrom sklearn.model_selection import train_test_split#将数据分为测试集和训练集\nfrom sklearn.neighbors import KNeighborsClassifier#利用邻近点方式训练数据\n\n###引入数据###\niris=datasets.load_iris()#引入iris鸢尾花数据,iris数据包含4个特征变量\niris_X=iris.data#特征变量\niris_y=iris.target#目标值\nX_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)#利用train_test_split进行将训练集和测试集进行分开，test_size占30%\nprint(y_train)#我们看到训练数据的特征值分为3类\n'''\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n '''\n\n###训练数据###\nknn=KNeighborsClassifier()#引入训练方法\nknn.fit(X_train,y_train)#进行填充测试数据进行训练\n\n###预测数据###\nprint(knn.predict(X_test))#预测特征值\n'''\n[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n 1 2 1 0 0 1 0 2]\n'''\nprint(y_test)#真实特征值\n'''\n[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n 1 2 1 0 0 1 0 2]\n'''\n```\n\n### 4.Sklearn datasets\n\nSklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的`load_iris`数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过`load_sample_images()`来引入图片。\n\n![Python之Sklearn使用教程图片02](Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png)\n\n除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。\n\n```python\nfrom sklearn import datasets#引入数据集\n#构造的各种参数可以根据自己需要调整\nX,y=datasets.make_regression(n_samples=100,n_features=1,n_targets=1,noise=1)\n\n###绘制构造的数据###\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.scatter(X,y)\nplt.show()\n```\n\n![Python之Sklearn使用教程03](Python之Sklearn使用教程/Python之Sklearn使用教程03.png)\n\n### 5.Sklearn Model的属性和功能\n\n数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数`y=0.3x+1`，我们可通过`_coef`得到模型的系数为0.3，通过`_intercept`得到模型的截距为1。\n\n```Python\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression#引入线性回归模型\n\n###引入数据###\nload_data=datasets.load_boston()\ndata_X=load_data.data\ndata_y=load_data.target\nprint(data_X.shape)\n#(506, 13)data_X共13个特征变量\n\n###训练数据###\nmodel=LinearRegression()\nmodel.fit(data_X,data_y)\nmodel.predict(data_X[:4,:])#预测前4个数据\n\n###属性和功能###\nprint(model.coef_)\n'''\n[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00\n  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00\n   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03\n  -5.25466633e-01]\n'''\nprint(model.intercept_)\n#36.4911032804\nprint(model.get_params())#得到模型的参数\n#{'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True}\nprint(model.score(data_X,data_y))#对训练情况进行打分\n#0.740607742865\n```\n\n### 6.Sklearn数据预处理\n\n数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。\n\n例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。\n\n```python\nfrom sklearn import preprocessing\nimport numpy as np\na=np.array([[10,2.7,3.6],\n            [-100,5,-2],\n            [120,20,40]],dtype=np.float64)\nprint(a)\nprint(preprocessing.scale(a))#将值的相差度减小\n'''\n[[  10.     2.7    3.6]\n [-100.     5.    -2. ]\n [ 120.    20.    40\n[[ 0.         -0.85170713 -0.55138018]\n [-1.22474487 -0.55187146 -0.852133  ]\n [ 1.22474487  1.40357859  1.40351318]]\n'''\n```\n\n我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为`0.511111111111`，预处理后模型评分为`0.933333333333`，可以看到预处理对模型评分有很大程度的提升。\n\n```Python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets.samples_generator import make_classification\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\n\n###生成的数据如下图所示###\nplt.figure\nX,y=make_classification(n_samples=300,n_features=2,n_redundant=0,n_informative=2,             random_state=22,n_clusters_per_class=1,scale=100)\nplt.scatter(X[:,0],X[:,1],c=y)\nplt.show()\n\n###利用minmax方式对数据进行规范化###\nX=preprocessing.minmax_scale(X)#feature_range=(-1,1)可设置重置范围\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\nclf=SVC()\nclf.fit(X_train,y_train)\nprint(clf.score(X_test,y_test))\n#0.933333333333\n#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升\n```\n\n![Python之Sklearn使用教程04](Python之Sklearn使用教程/Python之Sklearn使用教程04.png)\n\n### 7.交叉验证\n\n交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。\n\n机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：**训练集、验证集和测试集**。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。\n\n以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。\n\n![Python之Sklearn使用教程05](Python之Sklearn使用教程/Python之Sklearn使用教程05.png)\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n###引入数据###\niris=load_iris()\nX=iris.data\ny=iris.target\n\n###训练数据###\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n#引入交叉验证,数据分为5组进行训练\nfrom sklearn.model_selection import cross_val_score\nknn=KNeighborsClassifier(n_neighbors=5)#选择邻近的5个点\nscores=cross_val_score(knn,X,y,cv=5,scoring='accuracy')#评分方式为accuracy\nprint(scores)#每组的评分结果\n#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据\nprint(scores.mean())#平均评分结果\n#0.973333333333\n```\n\n那么是否**n_neighbor=5**便是最好呢，我们来调整参数来看模型最终训练分数。\n\n```Python\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score#引入交叉验证\nimport  matplotlib.pyplot as plt\n###引入数据###\niris=datasets.load_iris()\nX=iris.data\ny=iris.target\n###设置n_neighbors的值为1到30,通过绘图来看训练分数###\nk_range=range(1,31)\nk_score=[]\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    scores=cross_val_score(knn,X,y,cv=10,scoring='accuracy')#for classfication\n    k_score.append(loss.mean())\nplt.figure()\nplt.plot(k_range,k_score)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('CrossValidation accuracy')\nplt.show()\n#K过大会带来过拟合问题,我们可以选择12-18之间的值\n```\n\n我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择`2-fold Cross Validation`,`Leave-One-Out Cross Validation`等方法来分割数据，比较不同方法和参数得到最优结果。\n\n![Python之Sklearn使用教程06](Python之Sklearn使用教程/Python之Sklearn使用教程06.png)\n\n我们将上述代码中的循环部分改变一下，评分函数改为`neg_mean_squared_error`，便得到对于不同参数时的损失函数。\n\n```\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    loss=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# for regression\n    k_score.append(loss.mean())\n```\n\n![Python之Sklearn使用教程07](Python之Sklearn使用教程/Python之Sklearn使用教程07.png)\n\n### 8.过拟合问题\n\n什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。\n\n![Python之Sklearn使用教程08](Python之Sklearn使用教程/Python之Sklearn使用教程08.png)\n\n我们先举例如何辨别**overfitting**问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。\n\n```Python\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#引入数据\ndigits=load_digits()\nX=digits.data\ny=digits.target\n\n#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下\ntrain_size,train_loss,test_loss=learning_curve(\n    SVC(gamma=0.1),X,y,cv=10,scoring='neg_mean_squared_error',\n    train_sizes=[0.1,0.25,0.5,0.75,1]\n)\ntrain_loss_mean=-np.mean(train_loss,axis=1)\ntest_loss_mean=-np.mean(test_loss,axis=1)\n\nplt.figure()\n#将每一步进行打印出来\nplt.plot(train_size,train_loss_mean,'o-',color='r',label='Training')\nplt.plot(train_size,test_loss_mean,'o-',color='g',label='Cross-validation')\nplt.legend('best')\nplt.show()\n```\n\n![Python之Sklearn使用教程09](Python之Sklearn使用教程/Python之Sklearn使用教程09.png)\n\n如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。\n\n![Python之Sklearn使用教程10](Python之Sklearn使用教程/Python之Sklearn使用教程10.png)\n\n\n\n下面我们通过修改gamma参数来修正过拟合问题。\n\n```Python\nfrom sklearn.model_selection import  validation_curve#将learning_curve改为validation_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n#引入数据\ndigits=load_digits()\nX=digits.data\ny=digits.target\n\n#改变param来观察Loss函数情况\nparam_range=np.logspace(-6,-2.3,5)\ntrain_loss,test_loss=validation_curve(\n    SVC(),X,y,param_name='gamma',param_range=param_range,cv=10,\n    scoring='neg_mean_squared_error'\n)\ntrain_loss_mean=-np.mean(train_loss,axis=1)\ntest_loss_mean=-np.mean(test_loss,axis=1)\n\nplt.figure()\nplt.plot(param_range,train_loss_mean,'o-',color='r',label='Training')\nplt.plot(param_range,test_loss_mean,'o-',color='g',label='Cross-validation')\nplt.xlabel('gamma')\nplt.ylabel('loss')\nplt.legend(loc='best')\nplt.show()\n```\n\n通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。\n\n![Python之Sklearn使用教程11.png](Python之Sklearn使用教程/Python之Sklearn使用教程11.png)\n\n### 9.保存模型\n\n我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。\n\n```python\nfrom sklearn import svm\nfrom sklearn import datasets\n\n#引入和训练数据\niris=datasets.load_iris()\nX,y=iris.data,iris.target\nclf=svm.SVC()\nclf.fit(X,y)\n\n#引入sklearn中自带的保存模块\nfrom sklearn.externals import joblib\n#保存model\njoblib.dump(clf,'sklearn_save/clf.pkl')\n\n#重新加载model，只有保存一次后才能加载model\nclf3=joblib.load('sklearn_save/clf.pkl')\nprint(clf3.predict(X[0:1]))\n#存放model能够更快的获得以前的结果\n```\n\n### 10.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![Python之Sklearn使用教程推广](Python之Sklearn使用教程/Python之Sklearn使用教程推广.png)","source":"_posts/Python之Sklearn使用教程.md","raw":"---\ntitle: Python之Sklearn使用教程\ndate: 2018-04-15 11:43:18\ntags: [Python,机器学习]\ncategories: Python库\n---\n\n### 1.Sklearn简介\n\nScikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：\n\n+ 简单高效的数据挖掘和数据分析工具\n\n\n+ 让每个人能够在复杂环境中重复使用\n+ 建立NumPy、Scipy、MatPlotLib之上\n\n![Python之Sklearn使用教程图片01](Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png)\n\n### 2.Sklearn安装\n\nSklearn安装要求`Python(>=2.7 or >=3.3)`、`NumPy (>= 1.8.2)`、`SciPy (>= 0.13.3)`。如果已经安装NumPy和SciPy，安装scikit-learn可以使用`pip install -U scikit-learn`。\n\n### 3.Sklearn通用学习模式\n\nSklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，`4.Sklearn datasets`中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过`MatPlotLib`等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。\n\n```python\nfrom sklearn import datasets#引入数据集,sklearn包含众多数据集\nfrom sklearn.model_selection import train_test_split#将数据分为测试集和训练集\nfrom sklearn.neighbors import KNeighborsClassifier#利用邻近点方式训练数据\n\n###引入数据###\niris=datasets.load_iris()#引入iris鸢尾花数据,iris数据包含4个特征变量\niris_X=iris.data#特征变量\niris_y=iris.target#目标值\nX_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)#利用train_test_split进行将训练集和测试集进行分开，test_size占30%\nprint(y_train)#我们看到训练数据的特征值分为3类\n'''\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n '''\n\n###训练数据###\nknn=KNeighborsClassifier()#引入训练方法\nknn.fit(X_train,y_train)#进行填充测试数据进行训练\n\n###预测数据###\nprint(knn.predict(X_test))#预测特征值\n'''\n[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n 1 2 1 0 0 1 0 2]\n'''\nprint(y_test)#真实特征值\n'''\n[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n 1 2 1 0 0 1 0 2]\n'''\n```\n\n### 4.Sklearn datasets\n\nSklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的`load_iris`数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过`load_sample_images()`来引入图片。\n\n![Python之Sklearn使用教程图片02](Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png)\n\n除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。\n\n```python\nfrom sklearn import datasets#引入数据集\n#构造的各种参数可以根据自己需要调整\nX,y=datasets.make_regression(n_samples=100,n_features=1,n_targets=1,noise=1)\n\n###绘制构造的数据###\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.scatter(X,y)\nplt.show()\n```\n\n![Python之Sklearn使用教程03](Python之Sklearn使用教程/Python之Sklearn使用教程03.png)\n\n### 5.Sklearn Model的属性和功能\n\n数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数`y=0.3x+1`，我们可通过`_coef`得到模型的系数为0.3，通过`_intercept`得到模型的截距为1。\n\n```Python\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression#引入线性回归模型\n\n###引入数据###\nload_data=datasets.load_boston()\ndata_X=load_data.data\ndata_y=load_data.target\nprint(data_X.shape)\n#(506, 13)data_X共13个特征变量\n\n###训练数据###\nmodel=LinearRegression()\nmodel.fit(data_X,data_y)\nmodel.predict(data_X[:4,:])#预测前4个数据\n\n###属性和功能###\nprint(model.coef_)\n'''\n[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00\n  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00\n   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03\n  -5.25466633e-01]\n'''\nprint(model.intercept_)\n#36.4911032804\nprint(model.get_params())#得到模型的参数\n#{'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True}\nprint(model.score(data_X,data_y))#对训练情况进行打分\n#0.740607742865\n```\n\n### 6.Sklearn数据预处理\n\n数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。\n\n例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。\n\n```python\nfrom sklearn import preprocessing\nimport numpy as np\na=np.array([[10,2.7,3.6],\n            [-100,5,-2],\n            [120,20,40]],dtype=np.float64)\nprint(a)\nprint(preprocessing.scale(a))#将值的相差度减小\n'''\n[[  10.     2.7    3.6]\n [-100.     5.    -2. ]\n [ 120.    20.    40\n[[ 0.         -0.85170713 -0.55138018]\n [-1.22474487 -0.55187146 -0.852133  ]\n [ 1.22474487  1.40357859  1.40351318]]\n'''\n```\n\n我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为`0.511111111111`，预处理后模型评分为`0.933333333333`，可以看到预处理对模型评分有很大程度的提升。\n\n```Python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets.samples_generator import make_classification\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\n\n###生成的数据如下图所示###\nplt.figure\nX,y=make_classification(n_samples=300,n_features=2,n_redundant=0,n_informative=2,             random_state=22,n_clusters_per_class=1,scale=100)\nplt.scatter(X[:,0],X[:,1],c=y)\nplt.show()\n\n###利用minmax方式对数据进行规范化###\nX=preprocessing.minmax_scale(X)#feature_range=(-1,1)可设置重置范围\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\nclf=SVC()\nclf.fit(X_train,y_train)\nprint(clf.score(X_test,y_test))\n#0.933333333333\n#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升\n```\n\n![Python之Sklearn使用教程04](Python之Sklearn使用教程/Python之Sklearn使用教程04.png)\n\n### 7.交叉验证\n\n交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。\n\n机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：**训练集、验证集和测试集**。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。\n\n以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。\n\n![Python之Sklearn使用教程05](Python之Sklearn使用教程/Python之Sklearn使用教程05.png)\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n###引入数据###\niris=load_iris()\nX=iris.data\ny=iris.target\n\n###训练数据###\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n#引入交叉验证,数据分为5组进行训练\nfrom sklearn.model_selection import cross_val_score\nknn=KNeighborsClassifier(n_neighbors=5)#选择邻近的5个点\nscores=cross_val_score(knn,X,y,cv=5,scoring='accuracy')#评分方式为accuracy\nprint(scores)#每组的评分结果\n#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据\nprint(scores.mean())#平均评分结果\n#0.973333333333\n```\n\n那么是否**n_neighbor=5**便是最好呢，我们来调整参数来看模型最终训练分数。\n\n```Python\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score#引入交叉验证\nimport  matplotlib.pyplot as plt\n###引入数据###\niris=datasets.load_iris()\nX=iris.data\ny=iris.target\n###设置n_neighbors的值为1到30,通过绘图来看训练分数###\nk_range=range(1,31)\nk_score=[]\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    scores=cross_val_score(knn,X,y,cv=10,scoring='accuracy')#for classfication\n    k_score.append(loss.mean())\nplt.figure()\nplt.plot(k_range,k_score)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('CrossValidation accuracy')\nplt.show()\n#K过大会带来过拟合问题,我们可以选择12-18之间的值\n```\n\n我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择`2-fold Cross Validation`,`Leave-One-Out Cross Validation`等方法来分割数据，比较不同方法和参数得到最优结果。\n\n![Python之Sklearn使用教程06](Python之Sklearn使用教程/Python之Sklearn使用教程06.png)\n\n我们将上述代码中的循环部分改变一下，评分函数改为`neg_mean_squared_error`，便得到对于不同参数时的损失函数。\n\n```\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    loss=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# for regression\n    k_score.append(loss.mean())\n```\n\n![Python之Sklearn使用教程07](Python之Sklearn使用教程/Python之Sklearn使用教程07.png)\n\n### 8.过拟合问题\n\n什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。\n\n![Python之Sklearn使用教程08](Python之Sklearn使用教程/Python之Sklearn使用教程08.png)\n\n我们先举例如何辨别**overfitting**问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。\n\n```Python\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#引入数据\ndigits=load_digits()\nX=digits.data\ny=digits.target\n\n#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下\ntrain_size,train_loss,test_loss=learning_curve(\n    SVC(gamma=0.1),X,y,cv=10,scoring='neg_mean_squared_error',\n    train_sizes=[0.1,0.25,0.5,0.75,1]\n)\ntrain_loss_mean=-np.mean(train_loss,axis=1)\ntest_loss_mean=-np.mean(test_loss,axis=1)\n\nplt.figure()\n#将每一步进行打印出来\nplt.plot(train_size,train_loss_mean,'o-',color='r',label='Training')\nplt.plot(train_size,test_loss_mean,'o-',color='g',label='Cross-validation')\nplt.legend('best')\nplt.show()\n```\n\n![Python之Sklearn使用教程09](Python之Sklearn使用教程/Python之Sklearn使用教程09.png)\n\n如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。\n\n![Python之Sklearn使用教程10](Python之Sklearn使用教程/Python之Sklearn使用教程10.png)\n\n\n\n下面我们通过修改gamma参数来修正过拟合问题。\n\n```Python\nfrom sklearn.model_selection import  validation_curve#将learning_curve改为validation_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n#引入数据\ndigits=load_digits()\nX=digits.data\ny=digits.target\n\n#改变param来观察Loss函数情况\nparam_range=np.logspace(-6,-2.3,5)\ntrain_loss,test_loss=validation_curve(\n    SVC(),X,y,param_name='gamma',param_range=param_range,cv=10,\n    scoring='neg_mean_squared_error'\n)\ntrain_loss_mean=-np.mean(train_loss,axis=1)\ntest_loss_mean=-np.mean(test_loss,axis=1)\n\nplt.figure()\nplt.plot(param_range,train_loss_mean,'o-',color='r',label='Training')\nplt.plot(param_range,test_loss_mean,'o-',color='g',label='Cross-validation')\nplt.xlabel('gamma')\nplt.ylabel('loss')\nplt.legend(loc='best')\nplt.show()\n```\n\n通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。\n\n![Python之Sklearn使用教程11.png](Python之Sklearn使用教程/Python之Sklearn使用教程11.png)\n\n### 9.保存模型\n\n我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。\n\n```python\nfrom sklearn import svm\nfrom sklearn import datasets\n\n#引入和训练数据\niris=datasets.load_iris()\nX,y=iris.data,iris.target\nclf=svm.SVC()\nclf.fit(X,y)\n\n#引入sklearn中自带的保存模块\nfrom sklearn.externals import joblib\n#保存model\njoblib.dump(clf,'sklearn_save/clf.pkl')\n\n#重新加载model，只有保存一次后才能加载model\nclf3=joblib.load('sklearn_save/clf.pkl')\nprint(clf3.predict(X[0:1]))\n#存放model能够更快的获得以前的结果\n```\n\n### 10.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![Python之Sklearn使用教程推广](Python之Sklearn使用教程/Python之Sklearn使用教程推广.png)","slug":"Python之Sklearn使用教程","published":1,"updated":"2018-04-19T01:26:26.055Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji4rdzdh000d2e01qxrefsqi","content":"<h3 id=\"1-Sklearn简介\"><a href=\"#1-Sklearn简介\" class=\"headerlink\" title=\"1.Sklearn简介\"></a>1.Sklearn简介</h3><p>Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：</p>\n<ul>\n<li>简单高效的数据挖掘和数据分析工具</li>\n</ul>\n<ul>\n<li>让每个人能够在复杂环境中重复使用</li>\n<li>建立NumPy、Scipy、MatPlotLib之上</li>\n</ul>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png\" alt=\"Python之Sklearn使用教程图片01\"></p>\n<h3 id=\"2-Sklearn安装\"><a href=\"#2-Sklearn安装\" class=\"headerlink\" title=\"2.Sklearn安装\"></a>2.Sklearn安装</h3><p>Sklearn安装要求<code>Python(&gt;=2.7 or &gt;=3.3)</code>、<code>NumPy (&gt;= 1.8.2)</code>、<code>SciPy (&gt;= 0.13.3)</code>。如果已经安装NumPy和SciPy，安装scikit-learn可以使用<code>pip install -U scikit-learn</code>。</p>\n<h3 id=\"3-Sklearn通用学习模式\"><a href=\"#3-Sklearn通用学习模式\" class=\"headerlink\" title=\"3.Sklearn通用学习模式\"></a>3.Sklearn通用学习模式</h3><p>Sklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，<code>4.Sklearn datasets</code>中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过<code>MatPlotLib</code>等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets<span class=\"comment\">#引入数据集,sklearn包含众多数据集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split<span class=\"comment\">#将数据分为测试集和训练集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier<span class=\"comment\">#利用邻近点方式训练数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=datasets.load_iris()<span class=\"comment\">#引入iris鸢尾花数据,iris数据包含4个特征变量</span></span><br><span class=\"line\">iris_X=iris.data<span class=\"comment\">#特征变量</span></span><br><span class=\"line\">iris_y=iris.target<span class=\"comment\">#目标值</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=<span class=\"number\">0.3</span>)<span class=\"comment\">#利用train_test_split进行将训练集和测试集进行分开，test_size占30%</span></span><br><span class=\"line\">print(y_train)<span class=\"comment\">#我们看到训练数据的特征值分为3类</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span></span><br><span class=\"line\"><span class=\"string\"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span></span><br><span class=\"line\"><span class=\"string\"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class=\"line\"><span class=\"string\"> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class=\"line\"><span class=\"string\"> 2 2]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">knn=KNeighborsClassifier()<span class=\"comment\">#引入训练方法</span></span><br><span class=\"line\">knn.fit(X_train,y_train)<span class=\"comment\">#进行填充测试数据进行训练</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###预测数据###</span></span><br><span class=\"line\">print(knn.predict(X_test))<span class=\"comment\">#预测特征值</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0</span></span><br><span class=\"line\"><span class=\"string\"> 1 2 1 0 0 1 0 2]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(y_test)<span class=\"comment\">#真实特征值</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0</span></span><br><span class=\"line\"><span class=\"string\"> 1 2 1 0 0 1 0 2]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Sklearn-datasets\"><a href=\"#4-Sklearn-datasets\" class=\"headerlink\" title=\"4.Sklearn datasets\"></a>4.Sklearn datasets</h3><p>Sklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的<code>load_iris</code>数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过<code>load_sample_images()</code>来引入图片。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png\" alt=\"Python之Sklearn使用教程图片02\"></p>\n<p>除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets<span class=\"comment\">#引入数据集</span></span><br><span class=\"line\"><span class=\"comment\">#构造的各种参数可以根据自己需要调整</span></span><br><span class=\"line\">X,y=datasets.make_regression(n_samples=<span class=\"number\">100</span>,n_features=<span class=\"number\">1</span>,n_targets=<span class=\"number\">1</span>,noise=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###绘制构造的数据###</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X,y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程03.png\" alt=\"Python之Sklearn使用教程03\"></p>\n<h3 id=\"5-Sklearn-Model的属性和功能\"><a href=\"#5-Sklearn-Model的属性和功能\" class=\"headerlink\" title=\"5.Sklearn Model的属性和功能\"></a>5.Sklearn Model的属性和功能</h3><p>数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数<code>y=0.3x+1</code>，我们可通过<code>_coef</code>得到模型的系数为0.3，通过<code>_intercept</code>得到模型的截距为1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression<span class=\"comment\">#引入线性回归模型</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">load_data=datasets.load_boston()</span><br><span class=\"line\">data_X=load_data.data</span><br><span class=\"line\">data_y=load_data.target</span><br><span class=\"line\">print(data_X.shape)</span><br><span class=\"line\"><span class=\"comment\">#(506, 13)data_X共13个特征变量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">model=LinearRegression()</span><br><span class=\"line\">model.fit(data_X,data_y)</span><br><span class=\"line\">model.predict(data_X[:<span class=\"number\">4</span>,:])<span class=\"comment\">#预测前4个数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###属性和功能###</span></span><br><span class=\"line\">print(model.coef_)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00</span></span><br><span class=\"line\"><span class=\"string\">  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00</span></span><br><span class=\"line\"><span class=\"string\">   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03</span></span><br><span class=\"line\"><span class=\"string\">  -5.25466633e-01]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(model.intercept_)</span><br><span class=\"line\"><span class=\"comment\">#36.4911032804</span></span><br><span class=\"line\">print(model.get_params())<span class=\"comment\">#得到模型的参数</span></span><br><span class=\"line\"><span class=\"comment\">#&#123;'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True&#125;</span></span><br><span class=\"line\">print(model.score(data_X,data_y))<span class=\"comment\">#对训练情况进行打分</span></span><br><span class=\"line\"><span class=\"comment\">#0.740607742865</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"6-Sklearn数据预处理\"><a href=\"#6-Sklearn数据预处理\" class=\"headerlink\" title=\"6.Sklearn数据预处理\"></a>6.Sklearn数据预处理</h3><p>数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。</p>\n<p>例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a=np.array([[<span class=\"number\">10</span>,<span class=\"number\">2.7</span>,<span class=\"number\">3.6</span>],</span><br><span class=\"line\">            [<span class=\"number\">-100</span>,<span class=\"number\">5</span>,<span class=\"number\">-2</span>],</span><br><span class=\"line\">            [<span class=\"number\">120</span>,<span class=\"number\">20</span>,<span class=\"number\">40</span>]],dtype=np.float64)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(preprocessing.scale(a))<span class=\"comment\">#将值的相差度减小</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[  10.     2.7    3.6]</span></span><br><span class=\"line\"><span class=\"string\"> [-100.     5.    -2. ]</span></span><br><span class=\"line\"><span class=\"string\"> [ 120.    20.    40</span></span><br><span class=\"line\"><span class=\"string\">[[ 0.         -0.85170713 -0.55138018]</span></span><br><span class=\"line\"><span class=\"string\"> [-1.22474487 -0.55187146 -0.852133  ]</span></span><br><span class=\"line\"><span class=\"string\"> [ 1.22474487  1.40357859  1.40351318]]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure>\n<p>我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为<code>0.511111111111</code>，预处理后模型评分为<code>0.933333333333</code>，可以看到预处理对模型评分有很大程度的提升。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###生成的数据如下图所示###</span></span><br><span class=\"line\">plt.figure</span><br><span class=\"line\">X,y=make_classification(n_samples=<span class=\"number\">300</span>,n_features=<span class=\"number\">2</span>,n_redundant=<span class=\"number\">0</span>,n_informative=<span class=\"number\">2</span>,             random_state=<span class=\"number\">22</span>,n_clusters_per_class=<span class=\"number\">1</span>,scale=<span class=\"number\">100</span>)</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],c=y)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###利用minmax方式对数据进行规范化###</span></span><br><span class=\"line\">X=preprocessing.minmax_scale(X)<span class=\"comment\">#feature_range=(-1,1)可设置重置范围</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\">clf=SVC()</span><br><span class=\"line\">clf.fit(X_train,y_train)</span><br><span class=\"line\">print(clf.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\">#0.933333333333</span></span><br><span class=\"line\"><span class=\"comment\">#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程04.png\" alt=\"Python之Sklearn使用教程04\"></p>\n<h3 id=\"7-交叉验证\"><a href=\"#7-交叉验证\" class=\"headerlink\" title=\"7.交叉验证\"></a>7.交叉验证</h3><p>交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。</p>\n<p>机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：<strong>训练集、验证集和测试集</strong>。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。</p>\n<p>以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程05.png\" alt=\"Python之Sklearn使用教程05\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\"><span class=\"comment\">#引入交叉验证,数据分为5组进行训练</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score</span><br><span class=\"line\">knn=KNeighborsClassifier(n_neighbors=<span class=\"number\">5</span>)<span class=\"comment\">#选择邻近的5个点</span></span><br><span class=\"line\">scores=cross_val_score(knn,X,y,cv=<span class=\"number\">5</span>,scoring=<span class=\"string\">'accuracy'</span>)<span class=\"comment\">#评分方式为accuracy</span></span><br><span class=\"line\">print(scores)<span class=\"comment\">#每组的评分结果</span></span><br><span class=\"line\"><span class=\"comment\">#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据</span></span><br><span class=\"line\">print(scores.mean())<span class=\"comment\">#平均评分结果</span></span><br><span class=\"line\"><span class=\"comment\">#0.973333333333</span></span><br></pre></td></tr></table></figure>\n<p>那么是否<strong>n_neighbor=5</strong>便是最好呢，我们来调整参数来看模型最终训练分数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score<span class=\"comment\">#引入交叉验证</span></span><br><span class=\"line\"><span class=\"keyword\">import</span>  matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=datasets.load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"><span class=\"comment\">###设置n_neighbors的值为1到30,通过绘图来看训练分数###</span></span><br><span class=\"line\">k_range=range(<span class=\"number\">1</span>,<span class=\"number\">31</span>)</span><br><span class=\"line\">k_score=[]</span><br><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_range:</span><br><span class=\"line\">    knn=KNeighborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    scores=cross_val_score(knn,X,y,cv=<span class=\"number\">10</span>,scoring=<span class=\"string\">'accuracy'</span>)<span class=\"comment\">#for classfication</span></span><br><span class=\"line\">    k_score.append(loss.mean())</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(k_range,k_score)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'Value of k for KNN'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'CrossValidation accuracy'</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"><span class=\"comment\">#K过大会带来过拟合问题,我们可以选择12-18之间的值</span></span><br></pre></td></tr></table></figure>\n<p>我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择<code>2-fold Cross Validation</code>,<code>Leave-One-Out Cross Validation</code>等方法来分割数据，比较不同方法和参数得到最优结果。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程06.png\" alt=\"Python之Sklearn使用教程06\"></p>\n<p>我们将上述代码中的循环部分改变一下，评分函数改为<code>neg_mean_squared_error</code>，便得到对于不同参数时的损失函数。</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_range:</span><br><span class=\"line\">    <span class=\"attribute\">knn</span>=KNeighborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    <span class=\"attribute\">loss</span>=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# <span class=\"keyword\">for</span> regression</span><br><span class=\"line\">    k_score.append(loss.mean())</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程07.png\" alt=\"Python之Sklearn使用教程07\"></p>\n<h3 id=\"8-过拟合问题\"><a href=\"#8-过拟合问题\" class=\"headerlink\" title=\"8.过拟合问题\"></a>8.过拟合问题</h3><p>什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程08.png\" alt=\"Python之Sklearn使用教程08\"></p>\n<p>我们先举例如何辨别<strong>overfitting</strong>问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> learning_curve</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">digits=load_digits()</span><br><span class=\"line\">X=digits.data</span><br><span class=\"line\">y=digits.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下</span></span><br><span class=\"line\">train_size,train_loss,test_loss=learning_curve(</span><br><span class=\"line\">    SVC(gamma=<span class=\"number\">0.1</span>),X,y,cv=<span class=\"number\">10</span>,scoring=<span class=\"string\">'neg_mean_squared_error'</span>,</span><br><span class=\"line\">    train_sizes=[<span class=\"number\">0.1</span>,<span class=\"number\">0.25</span>,<span class=\"number\">0.5</span>,<span class=\"number\">0.75</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">)</span><br><span class=\"line\">train_loss_mean=-np.mean(train_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">test_loss_mean=-np.mean(test_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"><span class=\"comment\">#将每一步进行打印出来</span></span><br><span class=\"line\">plt.plot(train_size,train_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'r'</span>,label=<span class=\"string\">'Training'</span>)</span><br><span class=\"line\">plt.plot(train_size,test_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'g'</span>,label=<span class=\"string\">'Cross-validation'</span>)</span><br><span class=\"line\">plt.legend(<span class=\"string\">'best'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程09.png\" alt=\"Python之Sklearn使用教程09\"></p>\n<p>如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程10.png\" alt=\"Python之Sklearn使用教程10\"></p>\n<p>下面我们通过修改gamma参数来修正过拟合问题。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span>  validation_curve<span class=\"comment\">#将learning_curve改为validation_curve</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">digits=load_digits()</span><br><span class=\"line\">X=digits.data</span><br><span class=\"line\">y=digits.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#改变param来观察Loss函数情况</span></span><br><span class=\"line\">param_range=np.logspace(<span class=\"number\">-6</span>,<span class=\"number\">-2.3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">train_loss,test_loss=validation_curve(</span><br><span class=\"line\">    SVC(),X,y,param_name=<span class=\"string\">'gamma'</span>,param_range=param_range,cv=<span class=\"number\">10</span>,</span><br><span class=\"line\">    scoring=<span class=\"string\">'neg_mean_squared_error'</span></span><br><span class=\"line\">)</span><br><span class=\"line\">train_loss_mean=-np.mean(train_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">test_loss_mean=-np.mean(test_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(param_range,train_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'r'</span>,label=<span class=\"string\">'Training'</span>)</span><br><span class=\"line\">plt.plot(param_range,test_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'g'</span>,label=<span class=\"string\">'Cross-validation'</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'gamma'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'loss'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">'best'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程11.png\" alt=\"Python之Sklearn使用教程11.png\"></p>\n<h3 id=\"9-保存模型\"><a href=\"#9-保存模型\" class=\"headerlink\" title=\"9.保存模型\"></a>9.保存模型</h3><p>我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> svm</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入和训练数据</span></span><br><span class=\"line\">iris=datasets.load_iris()</span><br><span class=\"line\">X,y=iris.data,iris.target</span><br><span class=\"line\">clf=svm.SVC()</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入sklearn中自带的保存模块</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.externals <span class=\"keyword\">import</span> joblib</span><br><span class=\"line\"><span class=\"comment\">#保存model</span></span><br><span class=\"line\">joblib.dump(clf,<span class=\"string\">'sklearn_save/clf.pkl'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#重新加载model，只有保存一次后才能加载model</span></span><br><span class=\"line\">clf3=joblib.load(<span class=\"string\">'sklearn_save/clf.pkl'</span>)</span><br><span class=\"line\">print(clf3.predict(X[<span class=\"number\">0</span>:<span class=\"number\">1</span>]))</span><br><span class=\"line\"><span class=\"comment\">#存放model能够更快的获得以前的结果</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"10-推广\"><a href=\"#10-推广\" class=\"headerlink\" title=\"10.推广\"></a>10.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程推广.png\" alt=\"Python之Sklearn使用教程推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Sklearn简介\"><a href=\"#1-Sklearn简介\" class=\"headerlink\" title=\"1.Sklearn简介\"></a>1.Sklearn简介</h3><p>Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：</p>\n<ul>\n<li>简单高效的数据挖掘和数据分析工具</li>\n</ul>\n<ul>\n<li>让每个人能够在复杂环境中重复使用</li>\n<li>建立NumPy、Scipy、MatPlotLib之上</li>\n</ul>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png\" alt=\"Python之Sklearn使用教程图片01\"></p>\n<h3 id=\"2-Sklearn安装\"><a href=\"#2-Sklearn安装\" class=\"headerlink\" title=\"2.Sklearn安装\"></a>2.Sklearn安装</h3><p>Sklearn安装要求<code>Python(&gt;=2.7 or &gt;=3.3)</code>、<code>NumPy (&gt;= 1.8.2)</code>、<code>SciPy (&gt;= 0.13.3)</code>。如果已经安装NumPy和SciPy，安装scikit-learn可以使用<code>pip install -U scikit-learn</code>。</p>\n<h3 id=\"3-Sklearn通用学习模式\"><a href=\"#3-Sklearn通用学习模式\" class=\"headerlink\" title=\"3.Sklearn通用学习模式\"></a>3.Sklearn通用学习模式</h3><p>Sklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，<code>4.Sklearn datasets</code>中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过<code>MatPlotLib</code>等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets<span class=\"comment\">#引入数据集,sklearn包含众多数据集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split<span class=\"comment\">#将数据分为测试集和训练集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier<span class=\"comment\">#利用邻近点方式训练数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=datasets.load_iris()<span class=\"comment\">#引入iris鸢尾花数据,iris数据包含4个特征变量</span></span><br><span class=\"line\">iris_X=iris.data<span class=\"comment\">#特征变量</span></span><br><span class=\"line\">iris_y=iris.target<span class=\"comment\">#目标值</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=<span class=\"number\">0.3</span>)<span class=\"comment\">#利用train_test_split进行将训练集和测试集进行分开，test_size占30%</span></span><br><span class=\"line\">print(y_train)<span class=\"comment\">#我们看到训练数据的特征值分为3类</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span></span><br><span class=\"line\"><span class=\"string\"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span></span><br><span class=\"line\"><span class=\"string\"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class=\"line\"><span class=\"string\"> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class=\"line\"><span class=\"string\"> 2 2]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">knn=KNeighborsClassifier()<span class=\"comment\">#引入训练方法</span></span><br><span class=\"line\">knn.fit(X_train,y_train)<span class=\"comment\">#进行填充测试数据进行训练</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###预测数据###</span></span><br><span class=\"line\">print(knn.predict(X_test))<span class=\"comment\">#预测特征值</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0</span></span><br><span class=\"line\"><span class=\"string\"> 1 2 1 0 0 1 0 2]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(y_test)<span class=\"comment\">#真实特征值</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0</span></span><br><span class=\"line\"><span class=\"string\"> 1 2 1 0 0 1 0 2]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Sklearn-datasets\"><a href=\"#4-Sklearn-datasets\" class=\"headerlink\" title=\"4.Sklearn datasets\"></a>4.Sklearn datasets</h3><p>Sklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的<code>load_iris</code>数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过<code>load_sample_images()</code>来引入图片。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png\" alt=\"Python之Sklearn使用教程图片02\"></p>\n<p>除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets<span class=\"comment\">#引入数据集</span></span><br><span class=\"line\"><span class=\"comment\">#构造的各种参数可以根据自己需要调整</span></span><br><span class=\"line\">X,y=datasets.make_regression(n_samples=<span class=\"number\">100</span>,n_features=<span class=\"number\">1</span>,n_targets=<span class=\"number\">1</span>,noise=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###绘制构造的数据###</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X,y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程03.png\" alt=\"Python之Sklearn使用教程03\"></p>\n<h3 id=\"5-Sklearn-Model的属性和功能\"><a href=\"#5-Sklearn-Model的属性和功能\" class=\"headerlink\" title=\"5.Sklearn Model的属性和功能\"></a>5.Sklearn Model的属性和功能</h3><p>数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数<code>y=0.3x+1</code>，我们可通过<code>_coef</code>得到模型的系数为0.3，通过<code>_intercept</code>得到模型的截距为1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression<span class=\"comment\">#引入线性回归模型</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">load_data=datasets.load_boston()</span><br><span class=\"line\">data_X=load_data.data</span><br><span class=\"line\">data_y=load_data.target</span><br><span class=\"line\">print(data_X.shape)</span><br><span class=\"line\"><span class=\"comment\">#(506, 13)data_X共13个特征变量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">model=LinearRegression()</span><br><span class=\"line\">model.fit(data_X,data_y)</span><br><span class=\"line\">model.predict(data_X[:<span class=\"number\">4</span>,:])<span class=\"comment\">#预测前4个数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###属性和功能###</span></span><br><span class=\"line\">print(model.coef_)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00</span></span><br><span class=\"line\"><span class=\"string\">  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00</span></span><br><span class=\"line\"><span class=\"string\">   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03</span></span><br><span class=\"line\"><span class=\"string\">  -5.25466633e-01]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(model.intercept_)</span><br><span class=\"line\"><span class=\"comment\">#36.4911032804</span></span><br><span class=\"line\">print(model.get_params())<span class=\"comment\">#得到模型的参数</span></span><br><span class=\"line\"><span class=\"comment\">#&#123;'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True&#125;</span></span><br><span class=\"line\">print(model.score(data_X,data_y))<span class=\"comment\">#对训练情况进行打分</span></span><br><span class=\"line\"><span class=\"comment\">#0.740607742865</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"6-Sklearn数据预处理\"><a href=\"#6-Sklearn数据预处理\" class=\"headerlink\" title=\"6.Sklearn数据预处理\"></a>6.Sklearn数据预处理</h3><p>数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。</p>\n<p>例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a=np.array([[<span class=\"number\">10</span>,<span class=\"number\">2.7</span>,<span class=\"number\">3.6</span>],</span><br><span class=\"line\">            [<span class=\"number\">-100</span>,<span class=\"number\">5</span>,<span class=\"number\">-2</span>],</span><br><span class=\"line\">            [<span class=\"number\">120</span>,<span class=\"number\">20</span>,<span class=\"number\">40</span>]],dtype=np.float64)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(preprocessing.scale(a))<span class=\"comment\">#将值的相差度减小</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[  10.     2.7    3.6]</span></span><br><span class=\"line\"><span class=\"string\"> [-100.     5.    -2. ]</span></span><br><span class=\"line\"><span class=\"string\"> [ 120.    20.    40</span></span><br><span class=\"line\"><span class=\"string\">[[ 0.         -0.85170713 -0.55138018]</span></span><br><span class=\"line\"><span class=\"string\"> [-1.22474487 -0.55187146 -0.852133  ]</span></span><br><span class=\"line\"><span class=\"string\"> [ 1.22474487  1.40357859  1.40351318]]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure>\n<p>我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为<code>0.511111111111</code>，预处理后模型评分为<code>0.933333333333</code>，可以看到预处理对模型评分有很大程度的提升。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###生成的数据如下图所示###</span></span><br><span class=\"line\">plt.figure</span><br><span class=\"line\">X,y=make_classification(n_samples=<span class=\"number\">300</span>,n_features=<span class=\"number\">2</span>,n_redundant=<span class=\"number\">0</span>,n_informative=<span class=\"number\">2</span>,             random_state=<span class=\"number\">22</span>,n_clusters_per_class=<span class=\"number\">1</span>,scale=<span class=\"number\">100</span>)</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],c=y)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###利用minmax方式对数据进行规范化###</span></span><br><span class=\"line\">X=preprocessing.minmax_scale(X)<span class=\"comment\">#feature_range=(-1,1)可设置重置范围</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\">clf=SVC()</span><br><span class=\"line\">clf.fit(X_train,y_train)</span><br><span class=\"line\">print(clf.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\">#0.933333333333</span></span><br><span class=\"line\"><span class=\"comment\">#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程04.png\" alt=\"Python之Sklearn使用教程04\"></p>\n<h3 id=\"7-交叉验证\"><a href=\"#7-交叉验证\" class=\"headerlink\" title=\"7.交叉验证\"></a>7.交叉验证</h3><p>交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。</p>\n<p>机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：<strong>训练集、验证集和测试集</strong>。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。</p>\n<p>以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程05.png\" alt=\"Python之Sklearn使用教程05\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###训练数据###</span></span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\"><span class=\"comment\">#引入交叉验证,数据分为5组进行训练</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score</span><br><span class=\"line\">knn=KNeighborsClassifier(n_neighbors=<span class=\"number\">5</span>)<span class=\"comment\">#选择邻近的5个点</span></span><br><span class=\"line\">scores=cross_val_score(knn,X,y,cv=<span class=\"number\">5</span>,scoring=<span class=\"string\">'accuracy'</span>)<span class=\"comment\">#评分方式为accuracy</span></span><br><span class=\"line\">print(scores)<span class=\"comment\">#每组的评分结果</span></span><br><span class=\"line\"><span class=\"comment\">#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据</span></span><br><span class=\"line\">print(scores.mean())<span class=\"comment\">#平均评分结果</span></span><br><span class=\"line\"><span class=\"comment\">#0.973333333333</span></span><br></pre></td></tr></table></figure>\n<p>那么是否<strong>n_neighbor=5</strong>便是最好呢，我们来调整参数来看模型最终训练分数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score<span class=\"comment\">#引入交叉验证</span></span><br><span class=\"line\"><span class=\"keyword\">import</span>  matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\">###引入数据###</span></span><br><span class=\"line\">iris=datasets.load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"><span class=\"comment\">###设置n_neighbors的值为1到30,通过绘图来看训练分数###</span></span><br><span class=\"line\">k_range=range(<span class=\"number\">1</span>,<span class=\"number\">31</span>)</span><br><span class=\"line\">k_score=[]</span><br><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_range:</span><br><span class=\"line\">    knn=KNeighborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    scores=cross_val_score(knn,X,y,cv=<span class=\"number\">10</span>,scoring=<span class=\"string\">'accuracy'</span>)<span class=\"comment\">#for classfication</span></span><br><span class=\"line\">    k_score.append(loss.mean())</span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(k_range,k_score)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'Value of k for KNN'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'CrossValidation accuracy'</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"><span class=\"comment\">#K过大会带来过拟合问题,我们可以选择12-18之间的值</span></span><br></pre></td></tr></table></figure>\n<p>我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择<code>2-fold Cross Validation</code>,<code>Leave-One-Out Cross Validation</code>等方法来分割数据，比较不同方法和参数得到最优结果。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程06.png\" alt=\"Python之Sklearn使用教程06\"></p>\n<p>我们将上述代码中的循环部分改变一下，评分函数改为<code>neg_mean_squared_error</code>，便得到对于不同参数时的损失函数。</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_range:</span><br><span class=\"line\">    <span class=\"attribute\">knn</span>=KNeighborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    <span class=\"attribute\">loss</span>=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# <span class=\"keyword\">for</span> regression</span><br><span class=\"line\">    k_score.append(loss.mean())</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程07.png\" alt=\"Python之Sklearn使用教程07\"></p>\n<h3 id=\"8-过拟合问题\"><a href=\"#8-过拟合问题\" class=\"headerlink\" title=\"8.过拟合问题\"></a>8.过拟合问题</h3><p>什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程08.png\" alt=\"Python之Sklearn使用教程08\"></p>\n<p>我们先举例如何辨别<strong>overfitting</strong>问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> learning_curve</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">digits=load_digits()</span><br><span class=\"line\">X=digits.data</span><br><span class=\"line\">y=digits.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下</span></span><br><span class=\"line\">train_size,train_loss,test_loss=learning_curve(</span><br><span class=\"line\">    SVC(gamma=<span class=\"number\">0.1</span>),X,y,cv=<span class=\"number\">10</span>,scoring=<span class=\"string\">'neg_mean_squared_error'</span>,</span><br><span class=\"line\">    train_sizes=[<span class=\"number\">0.1</span>,<span class=\"number\">0.25</span>,<span class=\"number\">0.5</span>,<span class=\"number\">0.75</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">)</span><br><span class=\"line\">train_loss_mean=-np.mean(train_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">test_loss_mean=-np.mean(test_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"><span class=\"comment\">#将每一步进行打印出来</span></span><br><span class=\"line\">plt.plot(train_size,train_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'r'</span>,label=<span class=\"string\">'Training'</span>)</span><br><span class=\"line\">plt.plot(train_size,test_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'g'</span>,label=<span class=\"string\">'Cross-validation'</span>)</span><br><span class=\"line\">plt.legend(<span class=\"string\">'best'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程09.png\" alt=\"Python之Sklearn使用教程09\"></p>\n<p>如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程10.png\" alt=\"Python之Sklearn使用教程10\"></p>\n<p>下面我们通过修改gamma参数来修正过拟合问题。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span>  validation_curve<span class=\"comment\">#将learning_curve改为validation_curve</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">digits=load_digits()</span><br><span class=\"line\">X=digits.data</span><br><span class=\"line\">y=digits.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#改变param来观察Loss函数情况</span></span><br><span class=\"line\">param_range=np.logspace(<span class=\"number\">-6</span>,<span class=\"number\">-2.3</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">train_loss,test_loss=validation_curve(</span><br><span class=\"line\">    SVC(),X,y,param_name=<span class=\"string\">'gamma'</span>,param_range=param_range,cv=<span class=\"number\">10</span>,</span><br><span class=\"line\">    scoring=<span class=\"string\">'neg_mean_squared_error'</span></span><br><span class=\"line\">)</span><br><span class=\"line\">train_loss_mean=-np.mean(train_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">test_loss_mean=-np.mean(test_loss,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(param_range,train_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'r'</span>,label=<span class=\"string\">'Training'</span>)</span><br><span class=\"line\">plt.plot(param_range,test_loss_mean,<span class=\"string\">'o-'</span>,color=<span class=\"string\">'g'</span>,label=<span class=\"string\">'Cross-validation'</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'gamma'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'loss'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">'best'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程11.png\" alt=\"Python之Sklearn使用教程11.png\"></p>\n<h3 id=\"9-保存模型\"><a href=\"#9-保存模型\" class=\"headerlink\" title=\"9.保存模型\"></a>9.保存模型</h3><p>我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> svm</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入和训练数据</span></span><br><span class=\"line\">iris=datasets.load_iris()</span><br><span class=\"line\">X,y=iris.data,iris.target</span><br><span class=\"line\">clf=svm.SVC()</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入sklearn中自带的保存模块</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.externals <span class=\"keyword\">import</span> joblib</span><br><span class=\"line\"><span class=\"comment\">#保存model</span></span><br><span class=\"line\">joblib.dump(clf,<span class=\"string\">'sklearn_save/clf.pkl'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#重新加载model，只有保存一次后才能加载model</span></span><br><span class=\"line\">clf3=joblib.load(<span class=\"string\">'sklearn_save/clf.pkl'</span>)</span><br><span class=\"line\">print(clf3.predict(X[<span class=\"number\">0</span>:<span class=\"number\">1</span>]))</span><br><span class=\"line\"><span class=\"comment\">#存放model能够更快的获得以前的结果</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"10-推广\"><a href=\"#10-推广\" class=\"headerlink\" title=\"10.推广\"></a>10.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"Python之Sklearn使用教程/Python之Sklearn使用教程推广.png\" alt=\"Python之Sklearn使用教程推广\"></p>\n"},{"title":"Python之Pandas使用教程","date":"2018-03-12T05:18:48.000Z","toc":true,"comments":1,"_content":"### 1.Pandas概述\n\n 1. Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。\n 2. Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。\n 3. Pandas提供大量能使我们快速便捷地处理数据的函数和方法。\n 4. Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。   \n### 2.Pandas安装\n```\npip3 install pandas\n```\n### 3.Pandas引入\n```\nimport pandas as pd#为了方便实用pandas 采用pd简写\n```\n### 4.Pandas数据结构\n#### 4.1Series\n```\nimport numpy as np\nimport pandas as pd\ns=pd.Series([1,2,3,np.nan,5,6])\nprint(s)#索引在左边 值在右边\n'''\n0    1.0\n1    2.0\n2    3.0\n3    NaN\n4    5.0\n5    6.0\ndtype: float64\n '''\n```\n#### 4.2DataFrame\nDataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。\n```\ndates=pd.date_range('20180310',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置\nprint(df)#输出6行4列的表格\n'''\n                   A         B         C         D\n2018-03-10 -0.092889 -0.503172  0.692763 -1.261313\n2018-03-11 -0.895628 -2.300249 -1.098069  0.468986\n2018-03-12  0.084732 -1.275078  1.638007 -0.291145\n2018-03-13 -0.561528  0.431088  0.430414  1.065939\n2018-03-14  1.485434 -0.341404  0.267613 -1.493366\n2018-03-15 -1.671474  0.110933  1.688264 -0.910599\n  '''\nprint(df['B'])\n'''\n2018-03-10   -0.927291\n2018-03-11   -0.406842\n2018-03-12   -0.088316\n2018-03-13   -1.631055\n2018-03-14   -0.929926\n2018-03-15   -0.010904\nFreq: D, Name: B, dtype: float64\n '''\n\n#创建特定数据的DataFrame\ndf_1=pd.DataFrame({'A' : 1.,\n                    'B' : pd.Timestamp('20180310'),\n                    'C' : pd.Series(1,index=list(range(4)),dtype='float32'),\n                    'D' : np.array([3] * 4,dtype='int32'),\n                    'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]),\n                    'F' : 'foo'\n                    })\nprint(df_1)\n'''\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n'''\nprint(df_1.dtypes)\n'''\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n'''\nprint(df_1.index)#行的序号\n#Int64Index([0, 1, 2, 3], dtype='int64')\nprint(df_1.columns)#列的序号名字\n#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')\nprint(df_1.values)#把每个值进行打印出来\n'''\n[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']]\n '''\nprint(df_1.describe())#数字总结\n'''\n         A    C    D\ncount  4.0  4.0  4.0\nmean   1.0  1.0  3.0\nstd    0.0  0.0  0.0\nmin    1.0  1.0  3.0\n25%    1.0  1.0  3.0\n50%    1.0  1.0  3.0\n75%    1.0  1.0  3.0\nmax    1.0  1.0  3.0\n'''\nprint(df_1.T)#翻转数据\n'''\n                     0                    1                    2  \\\nA                    1                    1                    1   \nB  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   \nC                    1                    1                    1   \nD                    3                    3                    3   \nE                 test                train                 test   \nF                  foo                  foo                  foo   \n\n                     3  \nA                    1  \nB  2018-03-10 00:00:00  \nC                    1  \nD                    3  \nE                train  \nF                  foo  \n'''\nprint(df_1.sort_index(axis=1, ascending=False))#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示\n'''\n     F      E  D    C          B    A\n0  foo   test  3  1.0 2018-03-10  1.0\n1  foo  train  3  1.0 2018-03-10  1.0\n2  foo   test  3  1.0 2018-03-10  1.0\n3  foo  train  3  1.0 2018-03-10  1.0\n'''\nprint(df_1.sort_values(by='E'))#按值进行排序\n'''\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n'''\n```\n###5.Pandas选择数据\n```\ndates=pd.date_range('20180310',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置\nprint(df)\n'''\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n2018-03-15 -0.188331 -0.578581 -0.845854 -0.056373\n '''\nprint(df['A'])#或者df.A 选择某列\n'''\n2018-03-10   -0.520509\n2018-03-11    0.332656\n2018-03-12    0.499960\n2018-03-13    0.540385\n2018-03-14    0.191962\n2018-03-15   -0.188331\n'''\n```\n切片选择\n```\nprint(df[0:3], df['20180310':'20180314'])#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择\n'''\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465                    \n                  A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n '''\n```\n根据标签loc-行标签进行选择数据\n```\nprint(df.loc['20180312', ['A','B']])#按照行标签进行选择 精确选择\n '''\nA    0.499960\nB    1.576897\nName: 2018-03-12 00:00:00, dtype: float64\n'''\n```\n根据序列iloc-行号进行选择数据\n```\nprint(df.iloc[3, 1])#输出第三行第一列的数据\n#0.427336827399\n\nprint(df.iloc[3:5,0:2])#进行切片选择\n '''\n                   A         B\n2018-03-13  0.540385  0.427337\n2018-03-14  0.191962  1.237843\n '''\n\nprint(df.iloc[[1,2,4],[0,2]])#进行不连续筛选\n'''\n                   A         C\n2018-03-11  0.332656  0.382384\n2018-03-12  0.499960  2.128730\n2018-03-14  0.191962  1.903370\n '''\n```\n根据混合的两种ix\n```\nprint(df.ix[:3, ['A', 'C']])\n'''\n                   A         C\n2018-03-10 -0.919275 -1.356037\n2018-03-11  0.010171 -0.380010\n2018-03-12  0.285251 -1.174265\n '''\n```\n\n根据判断筛选\n```\nprint(df[df.A > 0])#筛选出df.A大于0的元素 布尔条件筛选\n'''\n                   A         B         C         D\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n '''\n```\n### 6.Pandas设置数据\n根据loc和iloc设置\n```\ndates = pd.date_range('20180310', periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])\nprint(df)\n'''\n             A   B     C   D\n2018-03-10   0   1     2   3\n2018-03-11   4   5     6   7\n2018-03-12   8   9  1111  11\n2018-03-13  12  13    14  15\n2018-03-14  16  17    18  19\n2018-03-15  20  21    22  23\n'''\n\ndf.iloc[2,2] = 999#单点设置\ndf.loc['2018-03-13', 'D'] = 999\nprint(df)\n'''\n            A   B    C    D\n2018-03-10  0   1    2    3\n2018-03-11  0   5    6    7\n2018-03-12  0   9  999   11\n2018-03-13  0  13   14  999\n2018-03-14  0  17   18   19\n2018-03-15  0  21   22   23\n'''\n```\n根据条件设置\n```\ndf[df.A>0]=999#将df.A大于0的值改变\nprint(df)\n'''\n              A   B    C    D\n2018-03-10    0   1    2    3\n2018-03-11  999   5    6    7\n2018-03-12  999   9  999   11\n2018-03-13  999  13   14  999\n2018-03-14  999  17   18   19\n2018-03-15  999  21   22   23\n '''\n```\n根据行或列设置\n```\ndf['F']=np.nan\nprint(df)\n'''\n              A   B    C   D\n2018-03-10    0   1    2 NaN\n2018-03-11  999   5    6 NaN\n2018-03-12  999   9  999 NaN\n2018-03-13  999  13   14 NaN\n2018-03-14  999  17   18 NaN\n2018-03-15  999  21   22 NaN\n '''\n```\n添加数据\n```\ndf['E']  = pd.Series([1,2,3,4,5,6], index=pd.date_range('20180313', periods=6))#增加一列\nprint(df)\n'''\n              A   B    C   D    E\n2018-03-10    0   1    2 NaN  NaN\n2018-03-11  999   5    6 NaN  NaN\n2018-03-12  999   9  999 NaN  NaN\n2018-03-13  999  13   14 NaN  1.0\n2018-03-14  999  17   18 NaN  2.0\n2018-03-15  999  21   22 NaN  3.0\n'''\n```\n### 7.Pandas处理丢失数据\n处理数据中NaN数据\n```\ndates = pd.date_range('20180310', periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])\ndf.iloc[0,1]=np.nan\ndf.iloc[1,2]=np.nan\nprint(df)\n'''\n             A     B     C   D\n2018-03-10   0   NaN   2.0   3\n2018-03-11   4   5.0   NaN   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n'''\n```\n使用dropna（）函数去掉NaN的行或列\n```\nprint(df.dropna(axis=0,how='any'#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop\n'''\n             A     B     C   D\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n '''\n```\n使用fillna（）函数替换NaN值 \n```\nprint(df.fillna(value=0))#将NaN值替换为0\n'''\n             A     B     C   D\n2018-03-10   0   0.0   2.0   3\n2018-03-11   4   5.0   0.0   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n '''\n```\n使用isnull()函数判断数据是否丢失\n```\nprint(pd.isnull(df))#矩阵用布尔来进行表示 是nan为ture 不是nan为false\n'''\n                A      B      C      D\n2018-03-10  False   True  False  False\n2018-03-11  False  False   True  False\n2018-03-12  False  False  False  False\n2018-03-13  False  False  False  False\n2018-03-14  False  False  False  False\n2018-03-15  False  False  False  False\n '''\nprint(np.any(df.isnull()))#判断数据中是否会存在NaN值\n#True\n```\n### 8.Pandas导入导出\npandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看[官方资料](http://pandas.pydata.org/pandas-docs/stable/io.html)\n```\ndata=pd.read_csv('test1.csv')#读取csv文件\ndata.to_pickle('test2.pickle')#将资料存取成pickle文件 \n#其他文件导入导出方式相同\n```\n### 9.Pandas合并数据\naxis合并方向\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ndf3 = pd.DataFrame(np.ones((3,4))*2, columns=['a','b','c','d'])\nres = pd.concat([df1, df2, df3], axis=0, ignore_index=True)#0表示竖项合并 1表示横项合并 ingnore_index重置序列index index变为0 1 2 3 4 5 6 7 8\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n6  2.0  2.0  2.0  2.0\n7  2.0  2.0  2.0  2.0\n8  2.0  2.0  2.0  2.0\n '''\n```\njoin合并方式\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'], index=[1,2,3])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['b','c','d', 'e'], index=[2,3,4])\nprint(df1)\n'''\n     a    b    c    d\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n '''\nprint(df2)\n'''\n     b    c    d    e\n2  1.0  1.0  1.0  1.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n '''\nres=pd.concat([df1,df2],axis=1,join='outer')#行往外进行合并\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n4  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\n '''\n\nres=pd.concat([df1,df2],axis=1,join='outer')#行相同的进行合并\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n'''\n\nres=pd.concat([df1,df2],axis=1,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n'''\n```\nappend添加数据\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ndf3 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ns1 = pd.Series([1,2,3,4], index=['a','b','c','d'])\n\nres=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n'''\n\nres=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  2.0  3.0  4.0\n'''\n```\n### 10.Pandas合并merge\n依据一组key合并\n```\nleft = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                     'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3']})\nprint(left)\n'''\n    A   B key\n0  A0  B0  K0\n1  A1  B1  K1\n2  A2  B2  K2\n3  A3  B3  K3\n'''\nright = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                      'C': ['C0', 'C1', 'C2',  'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']})\nprint(right)\n'''\n    C   D key\n0  C0  D0  K0\n1  C1  D1  K1\n2  C2  D2  K2\n3  C3  D3  K3\n'''\nres=pd.merge(left,right,on='key')\nprint(res)\n'''\n    A   B key   C   D\n0  A0  B0  K0  C0  D0\n1  A1  B1  K1  C1  D1\n2  A2  B2  K2  C2  D2\n3  A3  B3  K3  C3  D3\n'''\n```\n依据两组key合并\n```\nleft = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n                             'key2': ['K0', 'K1', 'K0', 'K1'],\n                             'A': ['A0', 'A1', 'A2', 'A3'],\n                             'B': ['B0', 'B1', 'B2', 'B3']})\nprint(left)\n'''\n    A   B key1 key2\n0  A0  B0   K0   K0\n1  A1  B1   K0   K1\n2  A2  B2   K1   K0\n3  A3  B3   K2   K1\n '''\nright = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n                              'key2': ['K0', 'K0', 'K0', 'K0'],\n                              'C': ['C0', 'C1', 'C2', 'C3'],\n                              'D': ['D0', 'D1', 'D2', 'D3']})\nprint(right)\n'''\n    C   D key1 key2\n0  C0  D0   K0   K0\n1  C1  D1   K1   K0\n2  C2  D2   K1   K0\n3  C3  D3   K2   K0\n '''\n\nres=pd.merge(left,right,on=['key1','key2'],how='inner')#内联合并\nprint(res)\n'''\n    A   B key1 key2   C   D\n0  A0  B0   K0   K0  C0  D0\n1  A2  B2   K1   K0  C1  D1\n2  A2  B2   K1   K0  C2  D2\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='outer')#外联合并\nprint(res)\n'''\n     A    B key1 key2    C    D\n0   A0   B0   K0   K0   C0   D0\n1   A1   B1   K0   K1  NaN  NaN\n2   A2   B2   K1   K0   C1   D1\n3   A2   B2   K1   K0   C2   D2\n4   A3   B3   K2   K1  NaN  NaN\n5  NaN  NaN   K2   K0   C3   D3\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='left')#左联合并\n'''\n    A   B key1 key2    C    D\n0  A0  B0   K0   K0   C0   D0\n1  A1  B1   K0   K1  NaN  NaN\n2  A2  B2   K1   K0   C1   D1\n3  A2  B2   K1   K0   C2   D2\n4  A3  B3   K2   K1  NaN  NaN\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='right')#右联合并\nprint(res)\n'''\n     A    B key1 key2   C   D\n0   A0   B0   K0   K0  C0  D0\n1   A2   B2   K1   K0  C1  D1\n2   A2   B2   K1   K0  C2  D2\n3  NaN  NaN   K2   K0  C3  D3\n'''\n```\nIndicator合并\n```\ndf1 = pd.DataFrame({'col1':[0,1], 'col_left':['a','b']})\nprint(df1)\n'''\n   col1 col_left\n0     0        a\n1     1        b\n '''\ndf2 = pd.DataFrame({'col1':[1,2,2],'col_right':[2,2,2]})\nprint(df2)\n'''\n   col1  col_right\n0     1          2\n1     2          2\n2     2          2\n '''\n\nres=pd.merge(df1,df2,on='col1',how='outer',indicator=True)#依据col1进行合并 并启用indicator=True输出每项合并方式\nprint(res)\n'''\n   col1 col_left  col_right      _merge\n0     0        a        NaN   left_only\n1     1        b        2.0        both\n2     2      NaN        2.0  right_only\n3     2      NaN        2.0  right_only\n'''\n\nres = pd.merge(df1, df2, on='col1', how='outer', indicator='indicator_column')#自定义indicator column名称\nprint(res)\n'''\n   col1 col_left  col_right indicator_column\n0     0        a        NaN        left_only\n1     1        b        2.0             both\n2     2      NaN        2.0       right_only\n3     2      NaN        2.0       right_only\n'''\n```\n依据index合并\n```\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n                                  'B': ['B0', 'B1', 'B2']},\n                                  index=['K0', 'K1', 'K2'])\nprint(left)\n'''\n     A   B\nK0  A0  B0\nK1  A1  B1\nK2  A2  B2\n '''\nright = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n                                     'D': ['D0', 'D2', 'D3']},\n                                      index=['K0', 'K2', 'K3'])\nprint(right)\n'''\n     C   D\nK0  C0  D0\nK2  C2  D2\nK3  C3  D3\n'''\n\nres=pd.merge(left,right,left_index=True,right_index=True,how='outer')#根据index索引进行合并 并选择外联合并\nprint(res)\n'''\n      A    B    C    D\nK0   A0   B0   C0   D0\nK1   A1   B1  NaN  NaN\nK2   A2   B2   C2   D2\nK3  NaN  NaN   C3   D3\n'''\n\nres=pd.merge(left,right,left_index=True,right_index=True,how='inner')\nprint(res)\n'''\n     A   B   C   D\nK0  A0  B0  C0  D0\nK2  A2  B2  C2  D2\n'''\n```\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...\n<div align=center>![公众号](http://img.blog.csdn.net/20180311001720557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)","source":"_posts/Python之Pandas使用教程.md","raw":"---\ntitle: Python之Pandas使用教程\ndate: 2018-03-12 13:18:48\ntags: python\ntoc: true\ncategories: Python库\ncomments: true\n---\n### 1.Pandas概述\n\n 1. Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。\n 2. Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。\n 3. Pandas提供大量能使我们快速便捷地处理数据的函数和方法。\n 4. Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。   \n### 2.Pandas安装\n```\npip3 install pandas\n```\n### 3.Pandas引入\n```\nimport pandas as pd#为了方便实用pandas 采用pd简写\n```\n### 4.Pandas数据结构\n#### 4.1Series\n```\nimport numpy as np\nimport pandas as pd\ns=pd.Series([1,2,3,np.nan,5,6])\nprint(s)#索引在左边 值在右边\n'''\n0    1.0\n1    2.0\n2    3.0\n3    NaN\n4    5.0\n5    6.0\ndtype: float64\n '''\n```\n#### 4.2DataFrame\nDataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。\n```\ndates=pd.date_range('20180310',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置\nprint(df)#输出6行4列的表格\n'''\n                   A         B         C         D\n2018-03-10 -0.092889 -0.503172  0.692763 -1.261313\n2018-03-11 -0.895628 -2.300249 -1.098069  0.468986\n2018-03-12  0.084732 -1.275078  1.638007 -0.291145\n2018-03-13 -0.561528  0.431088  0.430414  1.065939\n2018-03-14  1.485434 -0.341404  0.267613 -1.493366\n2018-03-15 -1.671474  0.110933  1.688264 -0.910599\n  '''\nprint(df['B'])\n'''\n2018-03-10   -0.927291\n2018-03-11   -0.406842\n2018-03-12   -0.088316\n2018-03-13   -1.631055\n2018-03-14   -0.929926\n2018-03-15   -0.010904\nFreq: D, Name: B, dtype: float64\n '''\n\n#创建特定数据的DataFrame\ndf_1=pd.DataFrame({'A' : 1.,\n                    'B' : pd.Timestamp('20180310'),\n                    'C' : pd.Series(1,index=list(range(4)),dtype='float32'),\n                    'D' : np.array([3] * 4,dtype='int32'),\n                    'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]),\n                    'F' : 'foo'\n                    })\nprint(df_1)\n'''\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n'''\nprint(df_1.dtypes)\n'''\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n'''\nprint(df_1.index)#行的序号\n#Int64Index([0, 1, 2, 3], dtype='int64')\nprint(df_1.columns)#列的序号名字\n#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')\nprint(df_1.values)#把每个值进行打印出来\n'''\n[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']\n [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']]\n '''\nprint(df_1.describe())#数字总结\n'''\n         A    C    D\ncount  4.0  4.0  4.0\nmean   1.0  1.0  3.0\nstd    0.0  0.0  0.0\nmin    1.0  1.0  3.0\n25%    1.0  1.0  3.0\n50%    1.0  1.0  3.0\n75%    1.0  1.0  3.0\nmax    1.0  1.0  3.0\n'''\nprint(df_1.T)#翻转数据\n'''\n                     0                    1                    2  \\\nA                    1                    1                    1   \nB  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   \nC                    1                    1                    1   \nD                    3                    3                    3   \nE                 test                train                 test   \nF                  foo                  foo                  foo   \n\n                     3  \nA                    1  \nB  2018-03-10 00:00:00  \nC                    1  \nD                    3  \nE                train  \nF                  foo  \n'''\nprint(df_1.sort_index(axis=1, ascending=False))#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示\n'''\n     F      E  D    C          B    A\n0  foo   test  3  1.0 2018-03-10  1.0\n1  foo  train  3  1.0 2018-03-10  1.0\n2  foo   test  3  1.0 2018-03-10  1.0\n3  foo  train  3  1.0 2018-03-10  1.0\n'''\nprint(df_1.sort_values(by='E'))#按值进行排序\n'''\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n'''\n```\n###5.Pandas选择数据\n```\ndates=pd.date_range('20180310',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置\nprint(df)\n'''\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n2018-03-15 -0.188331 -0.578581 -0.845854 -0.056373\n '''\nprint(df['A'])#或者df.A 选择某列\n'''\n2018-03-10   -0.520509\n2018-03-11    0.332656\n2018-03-12    0.499960\n2018-03-13    0.540385\n2018-03-14    0.191962\n2018-03-15   -0.188331\n'''\n```\n切片选择\n```\nprint(df[0:3], df['20180310':'20180314'])#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择\n'''\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465                    \n                  A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n '''\n```\n根据标签loc-行标签进行选择数据\n```\nprint(df.loc['20180312', ['A','B']])#按照行标签进行选择 精确选择\n '''\nA    0.499960\nB    1.576897\nName: 2018-03-12 00:00:00, dtype: float64\n'''\n```\n根据序列iloc-行号进行选择数据\n```\nprint(df.iloc[3, 1])#输出第三行第一列的数据\n#0.427336827399\n\nprint(df.iloc[3:5,0:2])#进行切片选择\n '''\n                   A         B\n2018-03-13  0.540385  0.427337\n2018-03-14  0.191962  1.237843\n '''\n\nprint(df.iloc[[1,2,4],[0,2]])#进行不连续筛选\n'''\n                   A         C\n2018-03-11  0.332656  0.382384\n2018-03-12  0.499960  2.128730\n2018-03-14  0.191962  1.903370\n '''\n```\n根据混合的两种ix\n```\nprint(df.ix[:3, ['A', 'C']])\n'''\n                   A         C\n2018-03-10 -0.919275 -1.356037\n2018-03-11  0.010171 -0.380010\n2018-03-12  0.285251 -1.174265\n '''\n```\n\n根据判断筛选\n```\nprint(df[df.A > 0])#筛选出df.A大于0的元素 布尔条件筛选\n'''\n                   A         B         C         D\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n '''\n```\n### 6.Pandas设置数据\n根据loc和iloc设置\n```\ndates = pd.date_range('20180310', periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])\nprint(df)\n'''\n             A   B     C   D\n2018-03-10   0   1     2   3\n2018-03-11   4   5     6   7\n2018-03-12   8   9  1111  11\n2018-03-13  12  13    14  15\n2018-03-14  16  17    18  19\n2018-03-15  20  21    22  23\n'''\n\ndf.iloc[2,2] = 999#单点设置\ndf.loc['2018-03-13', 'D'] = 999\nprint(df)\n'''\n            A   B    C    D\n2018-03-10  0   1    2    3\n2018-03-11  0   5    6    7\n2018-03-12  0   9  999   11\n2018-03-13  0  13   14  999\n2018-03-14  0  17   18   19\n2018-03-15  0  21   22   23\n'''\n```\n根据条件设置\n```\ndf[df.A>0]=999#将df.A大于0的值改变\nprint(df)\n'''\n              A   B    C    D\n2018-03-10    0   1    2    3\n2018-03-11  999   5    6    7\n2018-03-12  999   9  999   11\n2018-03-13  999  13   14  999\n2018-03-14  999  17   18   19\n2018-03-15  999  21   22   23\n '''\n```\n根据行或列设置\n```\ndf['F']=np.nan\nprint(df)\n'''\n              A   B    C   D\n2018-03-10    0   1    2 NaN\n2018-03-11  999   5    6 NaN\n2018-03-12  999   9  999 NaN\n2018-03-13  999  13   14 NaN\n2018-03-14  999  17   18 NaN\n2018-03-15  999  21   22 NaN\n '''\n```\n添加数据\n```\ndf['E']  = pd.Series([1,2,3,4,5,6], index=pd.date_range('20180313', periods=6))#增加一列\nprint(df)\n'''\n              A   B    C   D    E\n2018-03-10    0   1    2 NaN  NaN\n2018-03-11  999   5    6 NaN  NaN\n2018-03-12  999   9  999 NaN  NaN\n2018-03-13  999  13   14 NaN  1.0\n2018-03-14  999  17   18 NaN  2.0\n2018-03-15  999  21   22 NaN  3.0\n'''\n```\n### 7.Pandas处理丢失数据\n处理数据中NaN数据\n```\ndates = pd.date_range('20180310', periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])\ndf.iloc[0,1]=np.nan\ndf.iloc[1,2]=np.nan\nprint(df)\n'''\n             A     B     C   D\n2018-03-10   0   NaN   2.0   3\n2018-03-11   4   5.0   NaN   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n'''\n```\n使用dropna（）函数去掉NaN的行或列\n```\nprint(df.dropna(axis=0,how='any'#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop\n'''\n             A     B     C   D\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n '''\n```\n使用fillna（）函数替换NaN值 \n```\nprint(df.fillna(value=0))#将NaN值替换为0\n'''\n             A     B     C   D\n2018-03-10   0   0.0   2.0   3\n2018-03-11   4   5.0   0.0   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n '''\n```\n使用isnull()函数判断数据是否丢失\n```\nprint(pd.isnull(df))#矩阵用布尔来进行表示 是nan为ture 不是nan为false\n'''\n                A      B      C      D\n2018-03-10  False   True  False  False\n2018-03-11  False  False   True  False\n2018-03-12  False  False  False  False\n2018-03-13  False  False  False  False\n2018-03-14  False  False  False  False\n2018-03-15  False  False  False  False\n '''\nprint(np.any(df.isnull()))#判断数据中是否会存在NaN值\n#True\n```\n### 8.Pandas导入导出\npandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看[官方资料](http://pandas.pydata.org/pandas-docs/stable/io.html)\n```\ndata=pd.read_csv('test1.csv')#读取csv文件\ndata.to_pickle('test2.pickle')#将资料存取成pickle文件 \n#其他文件导入导出方式相同\n```\n### 9.Pandas合并数据\naxis合并方向\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ndf3 = pd.DataFrame(np.ones((3,4))*2, columns=['a','b','c','d'])\nres = pd.concat([df1, df2, df3], axis=0, ignore_index=True)#0表示竖项合并 1表示横项合并 ingnore_index重置序列index index变为0 1 2 3 4 5 6 7 8\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n6  2.0  2.0  2.0  2.0\n7  2.0  2.0  2.0  2.0\n8  2.0  2.0  2.0  2.0\n '''\n```\njoin合并方式\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'], index=[1,2,3])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['b','c','d', 'e'], index=[2,3,4])\nprint(df1)\n'''\n     a    b    c    d\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n '''\nprint(df2)\n'''\n     b    c    d    e\n2  1.0  1.0  1.0  1.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n '''\nres=pd.concat([df1,df2],axis=1,join='outer')#行往外进行合并\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n4  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\n '''\n\nres=pd.concat([df1,df2],axis=1,join='outer')#行相同的进行合并\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n'''\n\nres=pd.concat([df1,df2],axis=1,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充\nprint(res)\n'''\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n'''\n```\nappend添加数据\n```\ndf1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ndf3 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])\ns1 = pd.Series([1,2,3,4], index=['a','b','c','d'])\n\nres=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n'''\n\nres=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index\nprint(res)\n'''\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  2.0  3.0  4.0\n'''\n```\n### 10.Pandas合并merge\n依据一组key合并\n```\nleft = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                     'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3']})\nprint(left)\n'''\n    A   B key\n0  A0  B0  K0\n1  A1  B1  K1\n2  A2  B2  K2\n3  A3  B3  K3\n'''\nright = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                      'C': ['C0', 'C1', 'C2',  'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']})\nprint(right)\n'''\n    C   D key\n0  C0  D0  K0\n1  C1  D1  K1\n2  C2  D2  K2\n3  C3  D3  K3\n'''\nres=pd.merge(left,right,on='key')\nprint(res)\n'''\n    A   B key   C   D\n0  A0  B0  K0  C0  D0\n1  A1  B1  K1  C1  D1\n2  A2  B2  K2  C2  D2\n3  A3  B3  K3  C3  D3\n'''\n```\n依据两组key合并\n```\nleft = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n                             'key2': ['K0', 'K1', 'K0', 'K1'],\n                             'A': ['A0', 'A1', 'A2', 'A3'],\n                             'B': ['B0', 'B1', 'B2', 'B3']})\nprint(left)\n'''\n    A   B key1 key2\n0  A0  B0   K0   K0\n1  A1  B1   K0   K1\n2  A2  B2   K1   K0\n3  A3  B3   K2   K1\n '''\nright = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n                              'key2': ['K0', 'K0', 'K0', 'K0'],\n                              'C': ['C0', 'C1', 'C2', 'C3'],\n                              'D': ['D0', 'D1', 'D2', 'D3']})\nprint(right)\n'''\n    C   D key1 key2\n0  C0  D0   K0   K0\n1  C1  D1   K1   K0\n2  C2  D2   K1   K0\n3  C3  D3   K2   K0\n '''\n\nres=pd.merge(left,right,on=['key1','key2'],how='inner')#内联合并\nprint(res)\n'''\n    A   B key1 key2   C   D\n0  A0  B0   K0   K0  C0  D0\n1  A2  B2   K1   K0  C1  D1\n2  A2  B2   K1   K0  C2  D2\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='outer')#外联合并\nprint(res)\n'''\n     A    B key1 key2    C    D\n0   A0   B0   K0   K0   C0   D0\n1   A1   B1   K0   K1  NaN  NaN\n2   A2   B2   K1   K0   C1   D1\n3   A2   B2   K1   K0   C2   D2\n4   A3   B3   K2   K1  NaN  NaN\n5  NaN  NaN   K2   K0   C3   D3\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='left')#左联合并\n'''\n    A   B key1 key2    C    D\n0  A0  B0   K0   K0   C0   D0\n1  A1  B1   K0   K1  NaN  NaN\n2  A2  B2   K1   K0   C1   D1\n3  A2  B2   K1   K0   C2   D2\n4  A3  B3   K2   K1  NaN  NaN\n'''\n\nres=pd.merge(left,right,on=['key1','key2'],how='right')#右联合并\nprint(res)\n'''\n     A    B key1 key2   C   D\n0   A0   B0   K0   K0  C0  D0\n1   A2   B2   K1   K0  C1  D1\n2   A2   B2   K1   K0  C2  D2\n3  NaN  NaN   K2   K0  C3  D3\n'''\n```\nIndicator合并\n```\ndf1 = pd.DataFrame({'col1':[0,1], 'col_left':['a','b']})\nprint(df1)\n'''\n   col1 col_left\n0     0        a\n1     1        b\n '''\ndf2 = pd.DataFrame({'col1':[1,2,2],'col_right':[2,2,2]})\nprint(df2)\n'''\n   col1  col_right\n0     1          2\n1     2          2\n2     2          2\n '''\n\nres=pd.merge(df1,df2,on='col1',how='outer',indicator=True)#依据col1进行合并 并启用indicator=True输出每项合并方式\nprint(res)\n'''\n   col1 col_left  col_right      _merge\n0     0        a        NaN   left_only\n1     1        b        2.0        both\n2     2      NaN        2.0  right_only\n3     2      NaN        2.0  right_only\n'''\n\nres = pd.merge(df1, df2, on='col1', how='outer', indicator='indicator_column')#自定义indicator column名称\nprint(res)\n'''\n   col1 col_left  col_right indicator_column\n0     0        a        NaN        left_only\n1     1        b        2.0             both\n2     2      NaN        2.0       right_only\n3     2      NaN        2.0       right_only\n'''\n```\n依据index合并\n```\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n                                  'B': ['B0', 'B1', 'B2']},\n                                  index=['K0', 'K1', 'K2'])\nprint(left)\n'''\n     A   B\nK0  A0  B0\nK1  A1  B1\nK2  A2  B2\n '''\nright = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n                                     'D': ['D0', 'D2', 'D3']},\n                                      index=['K0', 'K2', 'K3'])\nprint(right)\n'''\n     C   D\nK0  C0  D0\nK2  C2  D2\nK3  C3  D3\n'''\n\nres=pd.merge(left,right,left_index=True,right_index=True,how='outer')#根据index索引进行合并 并选择外联合并\nprint(res)\n'''\n      A    B    C    D\nK0   A0   B0   C0   D0\nK1   A1   B1  NaN  NaN\nK2   A2   B2   C2   D2\nK3  NaN  NaN   C3   D3\n'''\n\nres=pd.merge(left,right,left_index=True,right_index=True,how='inner')\nprint(res)\n'''\n     A   B   C   D\nK0  A0  B0  C0  D0\nK2  A2  B2  C2  D2\n'''\n```\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...\n<div align=center>![公众号](http://img.blog.csdn.net/20180311001720557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)","slug":"Python之Pandas使用教程","published":1,"updated":"2018-03-15T02:04:04.778Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdi000e2e01nfv5nofl","content":"<h3 id=\"1-Pandas概述\"><a href=\"#1-Pandas概述\" class=\"headerlink\" title=\"1.Pandas概述\"></a>1.Pandas概述</h3><ol>\n<li>Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。</li>\n<li>Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。</li>\n<li>Pandas提供大量能使我们快速便捷地处理数据的函数和方法。</li>\n<li>Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。   <h3 id=\"2-Pandas安装\"><a href=\"#2-Pandas安装\" class=\"headerlink\" title=\"2.Pandas安装\"></a>2.Pandas安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 <span class=\"keyword\">install</span> pandas</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"3-Pandas引入\"><a href=\"#3-Pandas引入\" class=\"headerlink\" title=\"3.Pandas引入\"></a>3.Pandas引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd<span class=\"comment\">#为了方便实用pandas 采用pd简写</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Pandas数据结构\"><a href=\"#4-Pandas数据结构\" class=\"headerlink\" title=\"4.Pandas数据结构\"></a>4.Pandas数据结构</h3><h4 id=\"4-1Series\"><a href=\"#4-1Series\" class=\"headerlink\" title=\"4.1Series\"></a>4.1Series</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">s=pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,np.nan,<span class=\"number\">5</span>,<span class=\"number\">6</span>])</span><br><span class=\"line\">print(s)<span class=\"comment\">#索引在左边 值在右边</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">0    1.0</span></span><br><span class=\"line\"><span class=\"string\">1    2.0</span></span><br><span class=\"line\"><span class=\"string\">2    3.0</span></span><br><span class=\"line\"><span class=\"string\">3    NaN</span></span><br><span class=\"line\"><span class=\"string\">4    5.0</span></span><br><span class=\"line\"><span class=\"string\">5    6.0</span></span><br><span class=\"line\"><span class=\"string\">dtype: float64</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2DataFrame\"><a href=\"#4-2DataFrame\" class=\"headerlink\" title=\"4.2DataFrame\"></a>4.2DataFrame</h4><p>DataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates=pd.date_range(<span class=\"string\">'20180310'</span>,periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">6</span>,<span class=\"number\">4</span>), index=dates, columns=[<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>,<span class=\"string\">'C'</span>,<span class=\"string\">'D'</span>])<span class=\"comment\">#生成6行4列位置</span></span><br><span class=\"line\">print(df)<span class=\"comment\">#输出6行4列的表格</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         B         C         D</span></span><br><span class=\"line\"><span class=\"string\">2018-03-10 -0.092889 -0.503172  0.692763 -1.261313</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11 -0.895628 -2.300249 -1.098069  0.468986</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12  0.084732 -1.275078  1.638007 -0.291145</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13 -0.561528  0.431088  0.430414  1.065939</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  1.485434 -0.341404  0.267613 -1.493366</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15 -1.671474  0.110933  1.688264 -0.910599</span></span><br><span class=\"line\"><span class=\"string\">  '''</span></span><br><span class=\"line\">print(df[<span class=\"string\">'B'</span>])</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">2018-03-10   -0.927291</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11   -0.406842</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12   -0.088316</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13   -1.631055</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14   -0.929926</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15   -0.010904</span></span><br><span class=\"line\"><span class=\"string\">Freq: D, Name: B, dtype: float64</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#创建特定数据的DataFrame</span></span><br><span class=\"line\">df_1=pd.DataFrame(&#123;<span class=\"string\">'A'</span> : <span class=\"number\">1.</span>,</span><br><span class=\"line\">                    <span class=\"string\">'B'</span> : pd.Timestamp(<span class=\"string\">'20180310'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'C'</span> : pd.Series(<span class=\"number\">1</span>,index=list(range(<span class=\"number\">4</span>)),dtype=<span class=\"string\">'float32'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'D'</span> : np.array([<span class=\"number\">3</span>] * <span class=\"number\">4</span>,dtype=<span class=\"string\">'int32'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'E'</span> : pd.Categorical([<span class=\"string\">\"test\"</span>,<span class=\"string\">\"train\"</span>,<span class=\"string\">\"test\"</span>,<span class=\"string\">\"train\"</span>]),</span><br><span class=\"line\">                    <span class=\"string\">'F'</span> : <span class=\"string\">'foo'</span></span><br><span class=\"line\">                    &#125;)</span><br><span class=\"line\">print(df_1)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A          B    C  D      E    F</span></span><br><span class=\"line\"><span class=\"string\">0  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">1  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">2  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">3  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.dtypes)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">A           float64</span></span><br><span class=\"line\"><span class=\"string\">B    datetime64[ns]</span></span><br><span class=\"line\"><span class=\"string\">C           float32</span></span><br><span class=\"line\"><span class=\"string\">D             int32</span></span><br><span class=\"line\"><span class=\"string\">E          category</span></span><br><span class=\"line\"><span class=\"string\">F            object</span></span><br><span class=\"line\"><span class=\"string\">dtype: object</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.index)<span class=\"comment\">#行的序号</span></span><br><span class=\"line\"><span class=\"comment\">#Int64Index([0, 1, 2, 3], dtype='int64')</span></span><br><span class=\"line\">print(df_1.columns)<span class=\"comment\">#列的序号名字</span></span><br><span class=\"line\"><span class=\"comment\">#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')</span></span><br><span class=\"line\">print(df_1.values)<span class=\"comment\">#把每个值进行打印出来</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(df_1.describe())<span class=\"comment\">#数字总结</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">         A    C    D</span></span><br><span class=\"line\"><span class=\"string\">count  4.0  4.0  4.0</span></span><br><span class=\"line\"><span class=\"string\">mean   1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">std    0.0  0.0  0.0</span></span><br><span class=\"line\"><span class=\"string\">min    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">25%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">50%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">75%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">max    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.T)<span class=\"comment\">#翻转数据</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                     0                    1                    2  \\</span></span><br><span class=\"line\"><span class=\"string\">A                    1                    1                    1   </span></span><br><span class=\"line\"><span class=\"string\">B  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   </span></span><br><span class=\"line\"><span class=\"string\">C                    1                    1                    1   </span></span><br><span class=\"line\"><span class=\"string\">D                    3                    3                    3   </span></span><br><span class=\"line\"><span class=\"string\">E                 test                train                 test   </span></span><br><span class=\"line\"><span class=\"string\">F                  foo                  foo                  foo   </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">                     3  </span></span><br><span class=\"line\"><span class=\"string\">A                    1  </span></span><br><span class=\"line\"><span class=\"string\">B  2018-03-10 00:00:00  </span></span><br><span class=\"line\"><span class=\"string\">C                    1  </span></span><br><span class=\"line\"><span class=\"string\">D                    3  </span></span><br><span class=\"line\"><span class=\"string\">E                train  </span></span><br><span class=\"line\"><span class=\"string\">F                  foo  </span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.sort_index(axis=<span class=\"number\">1</span>, ascending=<span class=\"keyword\">False</span>))<span class=\"comment\">#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     F      E  D    C          B    A</span></span><br><span class=\"line\"><span class=\"string\">0  foo   test  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">1  foo  train  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">2  foo   test  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">3  foo  train  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.sort_values(by=<span class=\"string\">'E'</span>))<span class=\"comment\">#按值进行排序</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A          B    C  D      E    F</span></span><br><span class=\"line\"><span class=\"string\">0  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">2  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">1  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">3  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>###5.Pandas选择数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates=pd.date_range('<span class=\"number\">20180310</span>',periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">6</span>,<span class=\"number\">4</span>), index=dates, columns=['A','B','C','D'])#生成<span class=\"number\">6</span>行<span class=\"number\">4</span>列位置</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span> <span class=\"number\">-0.520509</span> <span class=\"number\">-0.136602</span> <span class=\"number\">-0.516984</span>  <span class=\"number\">1.357505</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0.332656</span> <span class=\"number\">-0.094633</span>  <span class=\"number\">0.382384</span> <span class=\"number\">-0.914339</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0.499960</span>  <span class=\"number\">1.576897</span>  <span class=\"number\">2.128730</span>  <span class=\"number\">2.197465</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0.540385</span>  <span class=\"number\">0.427337</span> <span class=\"number\">-0.591381</span>  <span class=\"number\">0.126503</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0.191962</span>  <span class=\"number\">1.237843</span>  <span class=\"number\">1.903370</span>  <span class=\"number\">2.155366</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span> <span class=\"number\">-0.188331</span> <span class=\"number\">-0.578581</span> <span class=\"number\">-0.845854</span> <span class=\"number\">-0.056373</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(df['A'])#或者df.A 选择某列</span><br><span class=\"line\">'''</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">-0.520509</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>    <span class=\"number\">0.332656</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>    <span class=\"number\">0.499960</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>    <span class=\"number\">0.540385</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>    <span class=\"number\">0.191962</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>   <span class=\"number\">-0.188331</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>切片选择<br><figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df[<span class=\"number\">0</span>:<span class=\"number\">3</span>], df['<span class=\"number\">20180310</span>':'<span class=\"number\">20180314</span>'])<span class=\"meta\">#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择</span></span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">520509</span> -0.<span class=\"number\">136602</span> -0.<span class=\"number\">516984</span>  1.<span class=\"number\">357505</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">332656</span> -0.<span class=\"number\">094633</span>  0.<span class=\"number\">382384</span> -0.<span class=\"number\">914339</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">499960</span>  1.<span class=\"number\">576897</span>  2.<span class=\"number\">128730</span>  2.<span class=\"number\">197465</span>                    </span><br><span class=\"line\">                  A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">520509</span> -0.<span class=\"number\">136602</span> -0.<span class=\"number\">516984</span>  1.<span class=\"number\">357505</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">332656</span> -0.<span class=\"number\">094633</span>  0.<span class=\"number\">382384</span> -0.<span class=\"number\">914339</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">499960</span>  1.<span class=\"number\">576897</span>  2.<span class=\"number\">128730</span>  2.<span class=\"number\">197465</span></span><br><span class=\"line\"><span class=\"number\">2018-03-13</span>  0.<span class=\"number\">540385</span>  0.<span class=\"number\">427337</span> -0.<span class=\"number\">591381</span>  0.<span class=\"number\">126503</span></span><br><span class=\"line\"><span class=\"number\">2018-03-14</span>  0.<span class=\"number\">191962</span>  1.<span class=\"number\">237843</span>  1.<span class=\"number\">903370</span>  2.<span class=\"number\">155366</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据标签loc-行标签进行选择数据<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.loc[<span class=\"string\">'20180312'</span>, [<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>]])<span class=\"comment\">#按照行标签进行选择 精确选择</span></span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">A    0.499960</span></span><br><span class=\"line\"><span class=\"string\">B    1.576897</span></span><br><span class=\"line\"><span class=\"string\">Name: 2018-03-12 00:00:00, dtype: float64</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>根据序列iloc-行号进行选择数据<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.iloc[<span class=\"number\">3</span>, <span class=\"number\">1</span>])<span class=\"comment\">#输出第三行第一列的数据</span></span><br><span class=\"line\"><span class=\"comment\">#0.427336827399</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(df.iloc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,<span class=\"number\">0</span>:<span class=\"number\">2</span>])<span class=\"comment\">#进行切片选择</span></span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         B</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13  0.540385  0.427337</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  0.191962  1.237843</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(df.iloc[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>]])<span class=\"comment\">#进行不连续筛选</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         C</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11  0.332656  0.382384</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12  0.499960  2.128730</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  0.191962  1.903370</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>根据混合的两种ix<br><figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.ix[:<span class=\"number\">3</span>, ['A', 'C']])</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         C</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">919275</span> -1.<span class=\"number\">356037</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">010171</span> -0.<span class=\"number\">380010</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">285251</span> -1.<span class=\"number\">174265</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据判断筛选<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df[df.A &gt; <span class=\"number\">0</span>])#筛选出df.A大于<span class=\"number\">0</span>的元素 布尔条件筛选</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0.332656</span> <span class=\"number\">-0.094633</span>  <span class=\"number\">0.382384</span> <span class=\"number\">-0.914339</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0.499960</span>  <span class=\"number\">1.576897</span>  <span class=\"number\">2.128730</span>  <span class=\"number\">2.197465</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0.540385</span>  <span class=\"number\">0.427337</span> <span class=\"number\">-0.591381</span>  <span class=\"number\">0.126503</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0.191962</span>  <span class=\"number\">1.237843</span>  <span class=\"number\">1.903370</span>  <span class=\"number\">2.155366</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-Pandas设置数据\"><a href=\"#6-Pandas设置数据\" class=\"headerlink\" title=\"6.Pandas设置数据\"></a>6.Pandas设置数据</h3><p>根据loc和iloc设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates = pd.date_range('<span class=\"number\">20180310</span>', periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.arange(<span class=\"number\">24</span>).reshape((<span class=\"number\">6</span>,<span class=\"number\">4</span>)), index=dates, columns=['A', 'B', 'C', 'D'])</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">             A   B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   <span class=\"number\">1</span>     <span class=\"number\">2</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5</span>     <span class=\"number\">6</span>   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9</span>  <span class=\"number\">1111</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13</span>    <span class=\"number\">14</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17</span>    <span class=\"number\">18</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21</span>    <span class=\"number\">22</span>  <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">df.iloc[<span class=\"number\">2</span>,<span class=\"number\">2</span>] = <span class=\"number\">999</span>#单点设置</span><br><span class=\"line\">df.loc['<span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>', 'D'] = <span class=\"number\">999</span></span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">            A   B    C    D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>  <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span>    <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span>   <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span>  <span class=\"number\">999</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span>   <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">0</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span>   <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>根据条件设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df.A&gt;<span class=\"number\">0</span>]=<span class=\"number\">999</span>#将df.A大于<span class=\"number\">0</span>的值改变</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C    D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span>    <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span>   <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span>  <span class=\"number\">999</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span>   <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span>   <span class=\"number\">23</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据行或列设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df['F']=np.nan</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span> NaN</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>添加数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df['E']  = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>], index=pd.date_range('<span class=\"number\">20180313</span>', periods=<span class=\"number\">6</span>))#增加一列</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C   D    E</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span> NaN  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span> NaN  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span> NaN  <span class=\"number\">3.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-Pandas处理丢失数据\"><a href=\"#7-Pandas处理丢失数据\" class=\"headerlink\" title=\"7.Pandas处理丢失数据\"></a>7.Pandas处理丢失数据</h3><p>处理数据中NaN数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates = pd.date_range('<span class=\"number\">20180310</span>', periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.arange(<span class=\"number\">24</span>).reshape((<span class=\"number\">6</span>,<span class=\"number\">4</span>)), index=dates, columns=['A', 'B', 'C', 'D'])</span><br><span class=\"line\">df.iloc[<span class=\"number\">0</span>,<span class=\"number\">1</span>]=np.nan</span><br><span class=\"line\">df.iloc[<span class=\"number\">1</span>,<span class=\"number\">2</span>]=np.nan</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">             A     B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   NaN   <span class=\"number\">2.0</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5.0</span>   NaN   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9.0</span>  <span class=\"number\">10.0</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13.0</span>  <span class=\"number\">14.0</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17.0</span>  <span class=\"number\">18.0</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21.0</span>  <span class=\"number\">22.0</span>  <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>使用dropna（）函数去掉NaN的行或列<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.dropna(axis=<span class=\"number\">0</span>,how=<span class=\"string\">'any'</span><span class=\"comment\">#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">             A     B     C   D</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12   8   9.0  10.0  11</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13  12  13.0  14.0  15</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  16  17.0  18.0  19</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15  20  21.0  22.0  23</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>使用fillna（）函数替换NaN值<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.fillna(value=<span class=\"number\">0</span>))#将NaN值替换为<span class=\"number\">0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\">             A     B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   <span class=\"number\">0.0</span>   <span class=\"number\">2.0</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5.0</span>   <span class=\"number\">0.0</span>   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9.0</span>  <span class=\"number\">10.0</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13.0</span>  <span class=\"number\">14.0</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17.0</span>  <span class=\"number\">18.0</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21.0</span>  <span class=\"number\">22.0</span>  <span class=\"number\">23</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>使用isnull()函数判断数据是否丢失<br><figure class=\"highlight vbnet\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(pd.isnull(df))<span class=\"meta\">#矩阵用布尔来进行表示 是nan为ture 不是nan为false</span></span><br><span class=\"line\"><span class=\"comment\"><span class=\"doctag\">'''</span></span></span><br><span class=\"line\">                A      B      C      D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>  <span class=\"literal\">False</span>   <span class=\"literal\">True</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>   <span class=\"literal\">True</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"> <span class=\"comment\"><span class=\"doctag\">'''</span></span></span><br><span class=\"line\">print(np.any(df.isnull()))<span class=\"meta\">#判断数据中是否会存在NaN值</span></span><br><span class=\"line\"><span class=\"meta\">#True</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"8-Pandas导入导出\"><a href=\"#8-Pandas导入导出\" class=\"headerlink\" title=\"8.Pandas导入导出\"></a>8.Pandas导入导出</h3><p>pandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看<a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html\" target=\"_blank\" rel=\"noopener\">官方资料</a><br><figure class=\"highlight haskell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">data</span>=pd.read_csv('<span class=\"title\">test1</span>.<span class=\"title\">csv'</span>)#读取csv文件</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">data</span>.to_pickle('<span class=\"title\">test2</span>.<span class=\"title\">pickle'</span>)#将资料存取成pickle文件 </span></span><br><span class=\"line\"><span class=\"meta\">#其他文件导入导出方式相同</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"9-Pandas合并数据\"><a href=\"#9-Pandas合并数据\" class=\"headerlink\" title=\"9.Pandas合并数据\"></a>9.Pandas合并数据</h3><p>axis合并方向<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df3 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">2</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">res = pd.concat([df1, df2, df3], axis=<span class=\"number\">0</span>, ignore_index=True)#<span class=\"number\">0</span>表示竖项合并 <span class=\"number\">1</span>表示横项合并 ingnore_index重置序列index index变为<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span> <span class=\"number\">7</span> <span class=\"number\">8</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">6</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">7</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">8</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>join合并方式<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'], index=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['b','c','d', 'e'], index=[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     b    c    d    e</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join='outer')#行往外进行合并</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  NaN  NaN  NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  NaN  NaN  NaN  NaN  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join='outer')#行相同的进行合并</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  NaN  NaN  NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>append添加数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df3 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">s1 = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>], index=['a','b','c','d'])</span><br><span class=\"line\"></span><br><span class=\"line\">res=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">res=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">3.0</span>  <span class=\"number\">4.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"10-Pandas合并merge\"><a href=\"#10-Pandas合并merge\" class=\"headerlink\" title=\"10.Pandas合并merge\"></a>10.Pandas合并merge</h3><p>依据一组key合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'key'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>],</span><br><span class=\"line\">                     <span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>, <span class=\"string\">'A3'</span>],</span><br><span class=\"line\">                     <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>, <span class=\"string\">'B3'</span>]&#125;)</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0  K0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1  K1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2  K2</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3  K3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'key'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>],</span><br><span class=\"line\">                      <span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C1'</span>, <span class=\"string\">'C2'</span>,  <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                      <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D1'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;)</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    C   D key</span></span><br><span class=\"line\"><span class=\"string\">0  C0  D0  K0</span></span><br><span class=\"line\"><span class=\"string\">1  C1  D1  K1</span></span><br><span class=\"line\"><span class=\"string\">2  C2  D2  K2</span></span><br><span class=\"line\"><span class=\"string\">3  C3  D3  K3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">res=pd.merge(left,right,on=<span class=\"string\">'key'</span>)</span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key   C   D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0  K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1  K1  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2  K2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3  K3  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>依据两组key合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'key1'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'key2'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>, <span class=\"string\">'A3'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>, <span class=\"string\">'B3'</span>]&#125;)</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1   K0   K1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3   K2   K1</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'key1'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'key2'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C1'</span>, <span class=\"string\">'C2'</span>, <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D1'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;)</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    C   D key1 key2</span></span><br><span class=\"line\"><span class=\"string\">0  C0  D0   K0   K0</span></span><br><span class=\"line\"><span class=\"string\">1  C1  D1   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">2  C2  D2   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">3  C3  D3   K2   K0</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'inner'</span>)<span class=\"comment\">#内联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2   C   D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1  A2  B2   K1   K0  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'outer'</span>)<span class=\"comment\">#外联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A    B key1 key2    C    D</span></span><br><span class=\"line\"><span class=\"string\">0   A0   B0   K0   K0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">1   A1   B1   K0   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">2   A2   B2   K1   K0   C1   D1</span></span><br><span class=\"line\"><span class=\"string\">3   A2   B2   K1   K0   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">4   A3   B3   K2   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">5  NaN  NaN   K2   K0   C3   D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'left'</span>)<span class=\"comment\">#左联合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2    C    D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1   K0   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0   C1   D1</span></span><br><span class=\"line\"><span class=\"string\">3  A2  B2   K1   K0   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">4  A3  B3   K2   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'right'</span>)<span class=\"comment\">#右联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A    B key1 key2   C   D</span></span><br><span class=\"line\"><span class=\"string\">0   A0   B0   K0   K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1   A2   B2   K1   K0  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2   A2   B2   K1   K0  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">3  NaN  NaN   K2   K0  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>Indicator合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(&#123;<span class=\"string\">'col1'</span>:[<span class=\"number\">0</span>,<span class=\"number\">1</span>], <span class=\"string\">'col_left'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>]&#125;)</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left</span></span><br><span class=\"line\"><span class=\"string\">0     0        a</span></span><br><span class=\"line\"><span class=\"string\">1     1        b</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">df2 = pd.DataFrame(&#123;<span class=\"string\">'col1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>],<span class=\"string\">'col_right'</span>:[<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>]&#125;)</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1  col_right</span></span><br><span class=\"line\"><span class=\"string\">0     1          2</span></span><br><span class=\"line\"><span class=\"string\">1     2          2</span></span><br><span class=\"line\"><span class=\"string\">2     2          2</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(df1,df2,on=<span class=\"string\">'col1'</span>,how=<span class=\"string\">'outer'</span>,indicator=<span class=\"keyword\">True</span>)<span class=\"comment\">#依据col1进行合并 并启用indicator=True输出每项合并方式</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left  col_right      _merge</span></span><br><span class=\"line\"><span class=\"string\">0     0        a        NaN   left_only</span></span><br><span class=\"line\"><span class=\"string\">1     1        b        2.0        both</span></span><br><span class=\"line\"><span class=\"string\">2     2      NaN        2.0  right_only</span></span><br><span class=\"line\"><span class=\"string\">3     2      NaN        2.0  right_only</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res = pd.merge(df1, df2, on=<span class=\"string\">'col1'</span>, how=<span class=\"string\">'outer'</span>, indicator=<span class=\"string\">'indicator_column'</span>)<span class=\"comment\">#自定义indicator column名称</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left  col_right indicator_column</span></span><br><span class=\"line\"><span class=\"string\">0     0        a        NaN        left_only</span></span><br><span class=\"line\"><span class=\"string\">1     1        b        2.0             both</span></span><br><span class=\"line\"><span class=\"string\">2     2      NaN        2.0       right_only</span></span><br><span class=\"line\"><span class=\"string\">3     2      NaN        2.0       right_only</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>依据index合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>],</span><br><span class=\"line\">                                  <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>]&#125;,</span><br><span class=\"line\">                                  index=[<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>])</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A   B</span></span><br><span class=\"line\"><span class=\"string\">K0  A0  B0</span></span><br><span class=\"line\"><span class=\"string\">K1  A1  B1</span></span><br><span class=\"line\"><span class=\"string\">K2  A2  B2</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C2'</span>, <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                                     <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;,</span><br><span class=\"line\">                                      index=[<span class=\"string\">'K0'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>])</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     C   D</span></span><br><span class=\"line\"><span class=\"string\">K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">K2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">K3  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,left_index=<span class=\"keyword\">True</span>,right_index=<span class=\"keyword\">True</span>,how=<span class=\"string\">'outer'</span>)<span class=\"comment\">#根据index索引进行合并 并选择外联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">      A    B    C    D</span></span><br><span class=\"line\"><span class=\"string\">K0   A0   B0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">K1   A1   B1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">K2   A2   B2   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">K3  NaN  NaN   C3   D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,left_index=<span class=\"keyword\">True</span>,right_index=<span class=\"keyword\">True</span>,how=<span class=\"string\">'inner'</span>)</span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A   B   C   D</span></span><br><span class=\"line\"><span class=\"string\">K0  A0  B0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">K2  A2  B2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…</p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311001720557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"公众号\"></div></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Pandas概述\"><a href=\"#1-Pandas概述\" class=\"headerlink\" title=\"1.Pandas概述\"></a>1.Pandas概述</h3><ol>\n<li>Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。</li>\n<li>Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。</li>\n<li>Pandas提供大量能使我们快速便捷地处理数据的函数和方法。</li>\n<li>Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。   <h3 id=\"2-Pandas安装\"><a href=\"#2-Pandas安装\" class=\"headerlink\" title=\"2.Pandas安装\"></a>2.Pandas安装</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 <span class=\"keyword\">install</span> pandas</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"3-Pandas引入\"><a href=\"#3-Pandas引入\" class=\"headerlink\" title=\"3.Pandas引入\"></a>3.Pandas引入</h3><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd<span class=\"comment\">#为了方便实用pandas 采用pd简写</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-Pandas数据结构\"><a href=\"#4-Pandas数据结构\" class=\"headerlink\" title=\"4.Pandas数据结构\"></a>4.Pandas数据结构</h3><h4 id=\"4-1Series\"><a href=\"#4-1Series\" class=\"headerlink\" title=\"4.1Series\"></a>4.1Series</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">s=pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,np.nan,<span class=\"number\">5</span>,<span class=\"number\">6</span>])</span><br><span class=\"line\">print(s)<span class=\"comment\">#索引在左边 值在右边</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">0    1.0</span></span><br><span class=\"line\"><span class=\"string\">1    2.0</span></span><br><span class=\"line\"><span class=\"string\">2    3.0</span></span><br><span class=\"line\"><span class=\"string\">3    NaN</span></span><br><span class=\"line\"><span class=\"string\">4    5.0</span></span><br><span class=\"line\"><span class=\"string\">5    6.0</span></span><br><span class=\"line\"><span class=\"string\">dtype: float64</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"4-2DataFrame\"><a href=\"#4-2DataFrame\" class=\"headerlink\" title=\"4.2DataFrame\"></a>4.2DataFrame</h4><p>DataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates=pd.date_range(<span class=\"string\">'20180310'</span>,periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">6</span>,<span class=\"number\">4</span>), index=dates, columns=[<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>,<span class=\"string\">'C'</span>,<span class=\"string\">'D'</span>])<span class=\"comment\">#生成6行4列位置</span></span><br><span class=\"line\">print(df)<span class=\"comment\">#输出6行4列的表格</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         B         C         D</span></span><br><span class=\"line\"><span class=\"string\">2018-03-10 -0.092889 -0.503172  0.692763 -1.261313</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11 -0.895628 -2.300249 -1.098069  0.468986</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12  0.084732 -1.275078  1.638007 -0.291145</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13 -0.561528  0.431088  0.430414  1.065939</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  1.485434 -0.341404  0.267613 -1.493366</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15 -1.671474  0.110933  1.688264 -0.910599</span></span><br><span class=\"line\"><span class=\"string\">  '''</span></span><br><span class=\"line\">print(df[<span class=\"string\">'B'</span>])</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">2018-03-10   -0.927291</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11   -0.406842</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12   -0.088316</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13   -1.631055</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14   -0.929926</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15   -0.010904</span></span><br><span class=\"line\"><span class=\"string\">Freq: D, Name: B, dtype: float64</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#创建特定数据的DataFrame</span></span><br><span class=\"line\">df_1=pd.DataFrame(&#123;<span class=\"string\">'A'</span> : <span class=\"number\">1.</span>,</span><br><span class=\"line\">                    <span class=\"string\">'B'</span> : pd.Timestamp(<span class=\"string\">'20180310'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'C'</span> : pd.Series(<span class=\"number\">1</span>,index=list(range(<span class=\"number\">4</span>)),dtype=<span class=\"string\">'float32'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'D'</span> : np.array([<span class=\"number\">3</span>] * <span class=\"number\">4</span>,dtype=<span class=\"string\">'int32'</span>),</span><br><span class=\"line\">                    <span class=\"string\">'E'</span> : pd.Categorical([<span class=\"string\">\"test\"</span>,<span class=\"string\">\"train\"</span>,<span class=\"string\">\"test\"</span>,<span class=\"string\">\"train\"</span>]),</span><br><span class=\"line\">                    <span class=\"string\">'F'</span> : <span class=\"string\">'foo'</span></span><br><span class=\"line\">                    &#125;)</span><br><span class=\"line\">print(df_1)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A          B    C  D      E    F</span></span><br><span class=\"line\"><span class=\"string\">0  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">1  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">2  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">3  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.dtypes)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">A           float64</span></span><br><span class=\"line\"><span class=\"string\">B    datetime64[ns]</span></span><br><span class=\"line\"><span class=\"string\">C           float32</span></span><br><span class=\"line\"><span class=\"string\">D             int32</span></span><br><span class=\"line\"><span class=\"string\">E          category</span></span><br><span class=\"line\"><span class=\"string\">F            object</span></span><br><span class=\"line\"><span class=\"string\">dtype: object</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.index)<span class=\"comment\">#行的序号</span></span><br><span class=\"line\"><span class=\"comment\">#Int64Index([0, 1, 2, 3], dtype='int64')</span></span><br><span class=\"line\">print(df_1.columns)<span class=\"comment\">#列的序号名字</span></span><br><span class=\"line\"><span class=\"comment\">#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')</span></span><br><span class=\"line\">print(df_1.values)<span class=\"comment\">#把每个值进行打印出来</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo']</span></span><br><span class=\"line\"><span class=\"string\"> [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']]</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">print(df_1.describe())<span class=\"comment\">#数字总结</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">         A    C    D</span></span><br><span class=\"line\"><span class=\"string\">count  4.0  4.0  4.0</span></span><br><span class=\"line\"><span class=\"string\">mean   1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">std    0.0  0.0  0.0</span></span><br><span class=\"line\"><span class=\"string\">min    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">25%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">50%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">75%    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">max    1.0  1.0  3.0</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.T)<span class=\"comment\">#翻转数据</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                     0                    1                    2  \\</span></span><br><span class=\"line\"><span class=\"string\">A                    1                    1                    1   </span></span><br><span class=\"line\"><span class=\"string\">B  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   </span></span><br><span class=\"line\"><span class=\"string\">C                    1                    1                    1   </span></span><br><span class=\"line\"><span class=\"string\">D                    3                    3                    3   </span></span><br><span class=\"line\"><span class=\"string\">E                 test                train                 test   </span></span><br><span class=\"line\"><span class=\"string\">F                  foo                  foo                  foo   </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">                     3  </span></span><br><span class=\"line\"><span class=\"string\">A                    1  </span></span><br><span class=\"line\"><span class=\"string\">B  2018-03-10 00:00:00  </span></span><br><span class=\"line\"><span class=\"string\">C                    1  </span></span><br><span class=\"line\"><span class=\"string\">D                    3  </span></span><br><span class=\"line\"><span class=\"string\">E                train  </span></span><br><span class=\"line\"><span class=\"string\">F                  foo  </span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.sort_index(axis=<span class=\"number\">1</span>, ascending=<span class=\"keyword\">False</span>))<span class=\"comment\">#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     F      E  D    C          B    A</span></span><br><span class=\"line\"><span class=\"string\">0  foo   test  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">1  foo  train  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">2  foo   test  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">3  foo  train  3  1.0 2018-03-10  1.0</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">print(df_1.sort_values(by=<span class=\"string\">'E'</span>))<span class=\"comment\">#按值进行排序</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A          B    C  D      E    F</span></span><br><span class=\"line\"><span class=\"string\">0  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">2  1.0 2018-03-10  1.0  3   test  foo</span></span><br><span class=\"line\"><span class=\"string\">1  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">3  1.0 2018-03-10  1.0  3  train  foo</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>###5.Pandas选择数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates=pd.date_range('<span class=\"number\">20180310</span>',periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">6</span>,<span class=\"number\">4</span>), index=dates, columns=['A','B','C','D'])#生成<span class=\"number\">6</span>行<span class=\"number\">4</span>列位置</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span> <span class=\"number\">-0.520509</span> <span class=\"number\">-0.136602</span> <span class=\"number\">-0.516984</span>  <span class=\"number\">1.357505</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0.332656</span> <span class=\"number\">-0.094633</span>  <span class=\"number\">0.382384</span> <span class=\"number\">-0.914339</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0.499960</span>  <span class=\"number\">1.576897</span>  <span class=\"number\">2.128730</span>  <span class=\"number\">2.197465</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0.540385</span>  <span class=\"number\">0.427337</span> <span class=\"number\">-0.591381</span>  <span class=\"number\">0.126503</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0.191962</span>  <span class=\"number\">1.237843</span>  <span class=\"number\">1.903370</span>  <span class=\"number\">2.155366</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span> <span class=\"number\">-0.188331</span> <span class=\"number\">-0.578581</span> <span class=\"number\">-0.845854</span> <span class=\"number\">-0.056373</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(df['A'])#或者df.A 选择某列</span><br><span class=\"line\">'''</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">-0.520509</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>    <span class=\"number\">0.332656</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>    <span class=\"number\">0.499960</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>    <span class=\"number\">0.540385</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>    <span class=\"number\">0.191962</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>   <span class=\"number\">-0.188331</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>切片选择<br><figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df[<span class=\"number\">0</span>:<span class=\"number\">3</span>], df['<span class=\"number\">20180310</span>':'<span class=\"number\">20180314</span>'])<span class=\"meta\">#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择</span></span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">520509</span> -0.<span class=\"number\">136602</span> -0.<span class=\"number\">516984</span>  1.<span class=\"number\">357505</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">332656</span> -0.<span class=\"number\">094633</span>  0.<span class=\"number\">382384</span> -0.<span class=\"number\">914339</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">499960</span>  1.<span class=\"number\">576897</span>  2.<span class=\"number\">128730</span>  2.<span class=\"number\">197465</span>                    </span><br><span class=\"line\">                  A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">520509</span> -0.<span class=\"number\">136602</span> -0.<span class=\"number\">516984</span>  1.<span class=\"number\">357505</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">332656</span> -0.<span class=\"number\">094633</span>  0.<span class=\"number\">382384</span> -0.<span class=\"number\">914339</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">499960</span>  1.<span class=\"number\">576897</span>  2.<span class=\"number\">128730</span>  2.<span class=\"number\">197465</span></span><br><span class=\"line\"><span class=\"number\">2018-03-13</span>  0.<span class=\"number\">540385</span>  0.<span class=\"number\">427337</span> -0.<span class=\"number\">591381</span>  0.<span class=\"number\">126503</span></span><br><span class=\"line\"><span class=\"number\">2018-03-14</span>  0.<span class=\"number\">191962</span>  1.<span class=\"number\">237843</span>  1.<span class=\"number\">903370</span>  2.<span class=\"number\">155366</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据标签loc-行标签进行选择数据<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.loc[<span class=\"string\">'20180312'</span>, [<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>]])<span class=\"comment\">#按照行标签进行选择 精确选择</span></span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">A    0.499960</span></span><br><span class=\"line\"><span class=\"string\">B    1.576897</span></span><br><span class=\"line\"><span class=\"string\">Name: 2018-03-12 00:00:00, dtype: float64</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>根据序列iloc-行号进行选择数据<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.iloc[<span class=\"number\">3</span>, <span class=\"number\">1</span>])<span class=\"comment\">#输出第三行第一列的数据</span></span><br><span class=\"line\"><span class=\"comment\">#0.427336827399</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(df.iloc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,<span class=\"number\">0</span>:<span class=\"number\">2</span>])<span class=\"comment\">#进行切片选择</span></span><br><span class=\"line\"> <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         B</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13  0.540385  0.427337</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  0.191962  1.237843</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(df.iloc[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>],[<span class=\"number\">0</span>,<span class=\"number\">2</span>]])<span class=\"comment\">#进行不连续筛选</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">                   A         C</span></span><br><span class=\"line\"><span class=\"string\">2018-03-11  0.332656  0.382384</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12  0.499960  2.128730</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  0.191962  1.903370</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>根据混合的两种ix<br><figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.ix[:<span class=\"number\">3</span>, ['A', 'C']])</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         C</span><br><span class=\"line\"><span class=\"number\">2018-03-10</span> -0.<span class=\"number\">919275</span> -1.<span class=\"number\">356037</span></span><br><span class=\"line\"><span class=\"number\">2018-03-11</span>  0.<span class=\"number\">010171</span> -0.<span class=\"number\">380010</span></span><br><span class=\"line\"><span class=\"number\">2018-03-12</span>  0.<span class=\"number\">285251</span> -1.<span class=\"number\">174265</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据判断筛选<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df[df.A &gt; <span class=\"number\">0</span>])#筛选出df.A大于<span class=\"number\">0</span>的元素 布尔条件筛选</span><br><span class=\"line\">'''</span><br><span class=\"line\">                   A         B         C         D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0.332656</span> <span class=\"number\">-0.094633</span>  <span class=\"number\">0.382384</span> <span class=\"number\">-0.914339</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0.499960</span>  <span class=\"number\">1.576897</span>  <span class=\"number\">2.128730</span>  <span class=\"number\">2.197465</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0.540385</span>  <span class=\"number\">0.427337</span> <span class=\"number\">-0.591381</span>  <span class=\"number\">0.126503</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0.191962</span>  <span class=\"number\">1.237843</span>  <span class=\"number\">1.903370</span>  <span class=\"number\">2.155366</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-Pandas设置数据\"><a href=\"#6-Pandas设置数据\" class=\"headerlink\" title=\"6.Pandas设置数据\"></a>6.Pandas设置数据</h3><p>根据loc和iloc设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates = pd.date_range('<span class=\"number\">20180310</span>', periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.arange(<span class=\"number\">24</span>).reshape((<span class=\"number\">6</span>,<span class=\"number\">4</span>)), index=dates, columns=['A', 'B', 'C', 'D'])</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">             A   B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   <span class=\"number\">1</span>     <span class=\"number\">2</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5</span>     <span class=\"number\">6</span>   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9</span>  <span class=\"number\">1111</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13</span>    <span class=\"number\">14</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17</span>    <span class=\"number\">18</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21</span>    <span class=\"number\">22</span>  <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">df.iloc[<span class=\"number\">2</span>,<span class=\"number\">2</span>] = <span class=\"number\">999</span>#单点设置</span><br><span class=\"line\">df.loc['<span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>', 'D'] = <span class=\"number\">999</span></span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">            A   B    C    D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>  <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">0</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span>    <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">0</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span>   <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">0</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span>  <span class=\"number\">999</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">0</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span>   <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">0</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span>   <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>根据条件设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df.A&gt;<span class=\"number\">0</span>]=<span class=\"number\">999</span>#将df.A大于<span class=\"number\">0</span>的值改变</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C    D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span>    <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span>   <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span>  <span class=\"number\">999</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span>   <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span>   <span class=\"number\">23</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>根据行或列设置<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df['F']=np.nan</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span> NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span> NaN</span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>添加数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df['E']  = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>], index=pd.date_range('<span class=\"number\">20180313</span>', periods=<span class=\"number\">6</span>))#增加一列</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">              A   B    C   D    E</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>    <span class=\"number\">0</span>   <span class=\"number\">1</span>    <span class=\"number\">2</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"number\">999</span>   <span class=\"number\">5</span>    <span class=\"number\">6</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"number\">999</span>   <span class=\"number\">9</span>  <span class=\"number\">999</span> NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">999</span>  <span class=\"number\">13</span>   <span class=\"number\">14</span> NaN  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">999</span>  <span class=\"number\">17</span>   <span class=\"number\">18</span> NaN  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">999</span>  <span class=\"number\">21</span>   <span class=\"number\">22</span> NaN  <span class=\"number\">3.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-Pandas处理丢失数据\"><a href=\"#7-Pandas处理丢失数据\" class=\"headerlink\" title=\"7.Pandas处理丢失数据\"></a>7.Pandas处理丢失数据</h3><p>处理数据中NaN数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dates = pd.date_range('<span class=\"number\">20180310</span>', periods=<span class=\"number\">6</span>)</span><br><span class=\"line\">df = pd.DataFrame(np.arange(<span class=\"number\">24</span>).reshape((<span class=\"number\">6</span>,<span class=\"number\">4</span>)), index=dates, columns=['A', 'B', 'C', 'D'])</span><br><span class=\"line\">df.iloc[<span class=\"number\">0</span>,<span class=\"number\">1</span>]=np.nan</span><br><span class=\"line\">df.iloc[<span class=\"number\">1</span>,<span class=\"number\">2</span>]=np.nan</span><br><span class=\"line\">print(df)</span><br><span class=\"line\">'''</span><br><span class=\"line\">             A     B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   NaN   <span class=\"number\">2.0</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5.0</span>   NaN   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9.0</span>  <span class=\"number\">10.0</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13.0</span>  <span class=\"number\">14.0</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17.0</span>  <span class=\"number\">18.0</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21.0</span>  <span class=\"number\">22.0</span>  <span class=\"number\">23</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>使用dropna（）函数去掉NaN的行或列<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.dropna(axis=<span class=\"number\">0</span>,how=<span class=\"string\">'any'</span><span class=\"comment\">#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">             A     B     C   D</span></span><br><span class=\"line\"><span class=\"string\">2018-03-12   8   9.0  10.0  11</span></span><br><span class=\"line\"><span class=\"string\">2018-03-13  12  13.0  14.0  15</span></span><br><span class=\"line\"><span class=\"string\">2018-03-14  16  17.0  18.0  19</span></span><br><span class=\"line\"><span class=\"string\">2018-03-15  20  21.0  22.0  23</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br></pre></td></tr></table></figure></p>\n<p>使用fillna（）函数替换NaN值<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(df.fillna(value=<span class=\"number\">0</span>))#将NaN值替换为<span class=\"number\">0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\">             A     B     C   D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>   <span class=\"number\">0</span>   <span class=\"number\">0.0</span>   <span class=\"number\">2.0</span>   <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>   <span class=\"number\">4</span>   <span class=\"number\">5.0</span>   <span class=\"number\">0.0</span>   <span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>   <span class=\"number\">8</span>   <span class=\"number\">9.0</span>  <span class=\"number\">10.0</span>  <span class=\"number\">11</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"number\">12</span>  <span class=\"number\">13.0</span>  <span class=\"number\">14.0</span>  <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"number\">16</span>  <span class=\"number\">17.0</span>  <span class=\"number\">18.0</span>  <span class=\"number\">19</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"number\">20</span>  <span class=\"number\">21.0</span>  <span class=\"number\">22.0</span>  <span class=\"number\">23</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>使用isnull()函数判断数据是否丢失<br><figure class=\"highlight vbnet\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(pd.isnull(df))<span class=\"meta\">#矩阵用布尔来进行表示 是nan为ture 不是nan为false</span></span><br><span class=\"line\"><span class=\"comment\"><span class=\"doctag\">'''</span></span></span><br><span class=\"line\">                A      B      C      D</span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-10</span>  <span class=\"literal\">False</span>   <span class=\"literal\">True</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-11</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>   <span class=\"literal\">True</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-12</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-13</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-14</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"number\">2018</span><span class=\"number\">-03</span><span class=\"number\">-15</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span>  <span class=\"literal\">False</span></span><br><span class=\"line\"> <span class=\"comment\"><span class=\"doctag\">'''</span></span></span><br><span class=\"line\">print(np.any(df.isnull()))<span class=\"meta\">#判断数据中是否会存在NaN值</span></span><br><span class=\"line\"><span class=\"meta\">#True</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"8-Pandas导入导出\"><a href=\"#8-Pandas导入导出\" class=\"headerlink\" title=\"8.Pandas导入导出\"></a>8.Pandas导入导出</h3><p>pandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看<a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html\" target=\"_blank\" rel=\"noopener\">官方资料</a><br><figure class=\"highlight haskell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">data</span>=pd.read_csv('<span class=\"title\">test1</span>.<span class=\"title\">csv'</span>)#读取csv文件</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">data</span>.to_pickle('<span class=\"title\">test2</span>.<span class=\"title\">pickle'</span>)#将资料存取成pickle文件 </span></span><br><span class=\"line\"><span class=\"meta\">#其他文件导入导出方式相同</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"9-Pandas合并数据\"><a href=\"#9-Pandas合并数据\" class=\"headerlink\" title=\"9.Pandas合并数据\"></a>9.Pandas合并数据</h3><p>axis合并方向<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df3 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">2</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">res = pd.concat([df1, df2, df3], axis=<span class=\"number\">0</span>, ignore_index=True)#<span class=\"number\">0</span>表示竖项合并 <span class=\"number\">1</span>表示横项合并 ingnore_index重置序列index index变为<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">4</span> <span class=\"number\">5</span> <span class=\"number\">6</span> <span class=\"number\">7</span> <span class=\"number\">8</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">6</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">7</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"><span class=\"number\">8</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">2.0</span></span><br><span class=\"line\"> '''</span><br></pre></td></tr></table></figure></p>\n<p>join合并方式<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'], index=[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['b','c','d', 'e'], index=[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     b    c    d    e</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join='outer')#行往外进行合并</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  NaN  NaN  NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  NaN  NaN  NaN  NaN  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"> '''</span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join='outer')#行相同的进行合并</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.concat([df1,df2],axis=<span class=\"number\">1</span>,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d    b    c    d    e</span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  NaN  NaN  NaN  NaN</span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<p>append添加数据<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">0</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df2 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">df3 = pd.DataFrame(np.ones((<span class=\"number\">3</span>,<span class=\"number\">4</span>))*<span class=\"number\">1</span>, columns=['a','b','c','d'])</span><br><span class=\"line\">s1 = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>], index=['a','b','c','d'])</span><br><span class=\"line\"></span><br><span class=\"line\">res=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span>  <span class=\"number\">1.0</span></span><br><span class=\"line\">'''</span><br><span class=\"line\"></span><br><span class=\"line\">res=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index</span><br><span class=\"line\">print(res)</span><br><span class=\"line\">'''</span><br><span class=\"line\">     a    b    c    d</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span>  <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">1.0</span>  <span class=\"number\">2.0</span>  <span class=\"number\">3.0</span>  <span class=\"number\">4.0</span></span><br><span class=\"line\">'''</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"10-Pandas合并merge\"><a href=\"#10-Pandas合并merge\" class=\"headerlink\" title=\"10.Pandas合并merge\"></a>10.Pandas合并merge</h3><p>依据一组key合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'key'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>],</span><br><span class=\"line\">                     <span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>, <span class=\"string\">'A3'</span>],</span><br><span class=\"line\">                     <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>, <span class=\"string\">'B3'</span>]&#125;)</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0  K0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1  K1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2  K2</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3  K3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'key'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>],</span><br><span class=\"line\">                      <span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C1'</span>, <span class=\"string\">'C2'</span>,  <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                      <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D1'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;)</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    C   D key</span></span><br><span class=\"line\"><span class=\"string\">0  C0  D0  K0</span></span><br><span class=\"line\"><span class=\"string\">1  C1  D1  K1</span></span><br><span class=\"line\"><span class=\"string\">2  C2  D2  K2</span></span><br><span class=\"line\"><span class=\"string\">3  C3  D3  K3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">res=pd.merge(left,right,on=<span class=\"string\">'key'</span>)</span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key   C   D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0  K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1  K1  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2  K2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3  K3  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>依据两组key合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'key1'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'key2'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>, <span class=\"string\">'A3'</span>],</span><br><span class=\"line\">                             <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>, <span class=\"string\">'B3'</span>]&#125;)</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1   K0   K1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">3  A3  B3   K2   K1</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'key1'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'key2'</span>: [<span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>, <span class=\"string\">'K0'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C1'</span>, <span class=\"string\">'C2'</span>, <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                              <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D1'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;)</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    C   D key1 key2</span></span><br><span class=\"line\"><span class=\"string\">0  C0  D0   K0   K0</span></span><br><span class=\"line\"><span class=\"string\">1  C1  D1   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">2  C2  D2   K1   K0</span></span><br><span class=\"line\"><span class=\"string\">3  C3  D3   K2   K0</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'inner'</span>)<span class=\"comment\">#内联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2   C   D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1  A2  B2   K1   K0  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'outer'</span>)<span class=\"comment\">#外联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A    B key1 key2    C    D</span></span><br><span class=\"line\"><span class=\"string\">0   A0   B0   K0   K0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">1   A1   B1   K0   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">2   A2   B2   K1   K0   C1   D1</span></span><br><span class=\"line\"><span class=\"string\">3   A2   B2   K1   K0   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">4   A3   B3   K2   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">5  NaN  NaN   K2   K0   C3   D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'left'</span>)<span class=\"comment\">#左联合并</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    A   B key1 key2    C    D</span></span><br><span class=\"line\"><span class=\"string\">0  A0  B0   K0   K0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">1  A1  B1   K0   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">2  A2  B2   K1   K0   C1   D1</span></span><br><span class=\"line\"><span class=\"string\">3  A2  B2   K1   K0   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">4  A3  B3   K2   K1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,on=[<span class=\"string\">'key1'</span>,<span class=\"string\">'key2'</span>],how=<span class=\"string\">'right'</span>)<span class=\"comment\">#右联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A    B key1 key2   C   D</span></span><br><span class=\"line\"><span class=\"string\">0   A0   B0   K0   K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">1   A2   B2   K1   K0  C1  D1</span></span><br><span class=\"line\"><span class=\"string\">2   A2   B2   K1   K0  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">3  NaN  NaN   K2   K0  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>Indicator合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df1 = pd.DataFrame(&#123;<span class=\"string\">'col1'</span>:[<span class=\"number\">0</span>,<span class=\"number\">1</span>], <span class=\"string\">'col_left'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>]&#125;)</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left</span></span><br><span class=\"line\"><span class=\"string\">0     0        a</span></span><br><span class=\"line\"><span class=\"string\">1     1        b</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">df2 = pd.DataFrame(&#123;<span class=\"string\">'col1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>],<span class=\"string\">'col_right'</span>:[<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>]&#125;)</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1  col_right</span></span><br><span class=\"line\"><span class=\"string\">0     1          2</span></span><br><span class=\"line\"><span class=\"string\">1     2          2</span></span><br><span class=\"line\"><span class=\"string\">2     2          2</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(df1,df2,on=<span class=\"string\">'col1'</span>,how=<span class=\"string\">'outer'</span>,indicator=<span class=\"keyword\">True</span>)<span class=\"comment\">#依据col1进行合并 并启用indicator=True输出每项合并方式</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left  col_right      _merge</span></span><br><span class=\"line\"><span class=\"string\">0     0        a        NaN   left_only</span></span><br><span class=\"line\"><span class=\"string\">1     1        b        2.0        both</span></span><br><span class=\"line\"><span class=\"string\">2     2      NaN        2.0  right_only</span></span><br><span class=\"line\"><span class=\"string\">3     2      NaN        2.0  right_only</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res = pd.merge(df1, df2, on=<span class=\"string\">'col1'</span>, how=<span class=\"string\">'outer'</span>, indicator=<span class=\"string\">'indicator_column'</span>)<span class=\"comment\">#自定义indicator column名称</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">   col1 col_left  col_right indicator_column</span></span><br><span class=\"line\"><span class=\"string\">0     0        a        NaN        left_only</span></span><br><span class=\"line\"><span class=\"string\">1     1        b        2.0             both</span></span><br><span class=\"line\"><span class=\"string\">2     2      NaN        2.0       right_only</span></span><br><span class=\"line\"><span class=\"string\">3     2      NaN        2.0       right_only</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>依据index合并<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">left = pd.DataFrame(&#123;<span class=\"string\">'A'</span>: [<span class=\"string\">'A0'</span>, <span class=\"string\">'A1'</span>, <span class=\"string\">'A2'</span>],</span><br><span class=\"line\">                                  <span class=\"string\">'B'</span>: [<span class=\"string\">'B0'</span>, <span class=\"string\">'B1'</span>, <span class=\"string\">'B2'</span>]&#125;,</span><br><span class=\"line\">                                  index=[<span class=\"string\">'K0'</span>, <span class=\"string\">'K1'</span>, <span class=\"string\">'K2'</span>])</span><br><span class=\"line\">print(left)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A   B</span></span><br><span class=\"line\"><span class=\"string\">K0  A0  B0</span></span><br><span class=\"line\"><span class=\"string\">K1  A1  B1</span></span><br><span class=\"line\"><span class=\"string\">K2  A2  B2</span></span><br><span class=\"line\"><span class=\"string\"> '''</span></span><br><span class=\"line\">right = pd.DataFrame(&#123;<span class=\"string\">'C'</span>: [<span class=\"string\">'C0'</span>, <span class=\"string\">'C2'</span>, <span class=\"string\">'C3'</span>],</span><br><span class=\"line\">                                     <span class=\"string\">'D'</span>: [<span class=\"string\">'D0'</span>, <span class=\"string\">'D2'</span>, <span class=\"string\">'D3'</span>]&#125;,</span><br><span class=\"line\">                                      index=[<span class=\"string\">'K0'</span>, <span class=\"string\">'K2'</span>, <span class=\"string\">'K3'</span>])</span><br><span class=\"line\">print(right)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     C   D</span></span><br><span class=\"line\"><span class=\"string\">K0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">K2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">K3  C3  D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,left_index=<span class=\"keyword\">True</span>,right_index=<span class=\"keyword\">True</span>,how=<span class=\"string\">'outer'</span>)<span class=\"comment\">#根据index索引进行合并 并选择外联合并</span></span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">      A    B    C    D</span></span><br><span class=\"line\"><span class=\"string\">K0   A0   B0   C0   D0</span></span><br><span class=\"line\"><span class=\"string\">K1   A1   B1  NaN  NaN</span></span><br><span class=\"line\"><span class=\"string\">K2   A2   B2   C2   D2</span></span><br><span class=\"line\"><span class=\"string\">K3  NaN  NaN   C3   D3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\">res=pd.merge(left,right,left_index=<span class=\"keyword\">True</span>,right_index=<span class=\"keyword\">True</span>,how=<span class=\"string\">'inner'</span>)</span><br><span class=\"line\">print(res)</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">     A   B   C   D</span></span><br><span class=\"line\"><span class=\"string\">K0  A0  B0  C0  D0</span></span><br><span class=\"line\"><span class=\"string\">K2  A2  B2  C2  D2</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…</p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311001720557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"公众号\"></div></p>\n"},{"title":"机器学习之K近邻(KNN)算法","date":"2018-05-13T03:34:01.000Z","mathjax":true,"comments":1,"_content":"\n### 1.KNN简介\n\n**K近邻(K-Nearest Neighbors, KNN)**算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做**分类**预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做**回归**预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。\n\n如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设**K=3**，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设**K=5**，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。\n\n![机器学习之K近邻算法图片01](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png)\n\n从上面实例，我们可以总结下KNN算法过程\n\n1. 计算测试数据与各个训练数据之间的距离。\n2. 按照距离的递增关系进行排序，选取距离最小的K个点。\n3. 确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。\n\n从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。\n\n+ **距离度量方式：**KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。\n\n$$\nD(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\n$$\n\n$$\nD(x,y)={|x_1-y_1|+|x_2-y_2|+|x_3-y_3|...+|x_n-y_n|}={\\sum_{i=1}^{n}|x_i-y_i|}\n$$\n\n+ **K值的选取：**KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。\n+ **分类决策规则：**KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。\n\nKNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。\n\n### 2.KD树原理\n\nKD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。\n\n#### 2.1KD树建立\n\n下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。\n\n1. **寻找划分特征：**KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。\n2. **确定划分点：**选择特征nk的中位数nkv所对应的样本作为划分点。\n3. **确定左子空间和右子空间：**对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。\n4. **递归构建KD树：**对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。\n\n我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下\n\n1. **寻找划分特征：**6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。\n2. **确定划分点：**根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。\n3. **确定左子空间和右子空间：**分割超平面x=7将空间分为两部分。x<=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x>7的部分为右子空间，包含节点为{(9,6)，(8,1)}。\n4. **递归构建KD树：**用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。\n\n![机器学习之K近邻算法图片02](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png)\n\n![机器学习之K近邻算法图片03](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png)\n\n#### 2.2KD树搜索最近邻\n\n当我们生成KD树后，就可以预测测试样本集里面的样本目标点。\n\n1. **二叉搜索：**对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的**叶子节点**。\n2. **回溯：**为找到最近邻，还需要进行**回溯**操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。\n3. **更新最近邻：**返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。\n\n为方便理解上述过程，我们利用**2.1建立的KD树**来寻找(2,4.5)的最近邻。\n\n1. **二叉搜索：**首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)->(5,4)->(4,7)}。\n2. **回溯：**节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。\n3. **更新最近邻：**该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)->(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。\n\n![机器学习之K近邻算法图片04](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png)\n\n#### 2.3KD树预测\n\n根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。\n\n### 3.球树原理\n\nKD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。\n\n![机器学习之K近邻算法图片05](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png)\n\n为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。\n\n#### 3.1球树建立\n\n球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程\n\n1. **构建超球体：**超球体是可以包含所有样本的最小球体。\n2. **划分子超球体：**从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。\n3. **递归：**对上述两个子超球体，递归执行步骤2，最终得到球树。\n\n![机器学习之K近邻算法图片06](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png)\n\n#### 3.2球树搜索最近邻\n\nKD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。\n\n1. 自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。\n2. 然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。\n3. 检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。\n\n#### 3.3球树预测\n\n根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。\n\n### 4.KNN算法扩展\n\n有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。\n\n### 5.Sklearn实现KNN算法\n\n下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)。\n\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n#load iris data\niris=load_iris()\nX=iris.data\ny=iris.target\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nknn=KNeighborsClassifier(algorithm='kd_tree')\nknn.fit(X_train,y_train)\nprint(knn.predict(X_test))\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 1 2 1]\nprint(y_test)\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 2 2 1]\nprint(knn.score(X_test,y_test))\n# 0.977777777778\n```\n\n### 6.KNN优缺点\n\n#### 6.1优点\n\n+ 即可处理分类也可处理回归问题。\n+ 对数据没有假设，准确度高，对异常点不敏感。\n+ 比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。\n+ 主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。\n\n#### 6.2缺点\n\n+ 计算量大，尤其是特征维数较多时候。\n+ 样本不平衡时，对稀有类别的预测准确率低。\n+ KD树、球树之类的模型建立时需要大量的内存。\n+ 使用懒惰学习方法，基本上不学习，导致预测时速度较慢。\n\n### 7.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n>[刘建平Pinard_K近邻法(KNN)原理小结](http://www.cnblogs.com/pinard/p/6061661.html)\n>\n>[Yabea_K-近邻(KNN)算法](https://www.cnblogs.com/ybjourney/p/4702562.html)\n\n","source":"_posts/机器学习之K近邻-KNN-算法.md","raw":"---\ntitle: 机器学习之K近邻(KNN)算法\ndate: 2018-05-13 11:34:01\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.KNN简介\n\n**K近邻(K-Nearest Neighbors, KNN)**算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做**分类**预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做**回归**预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。\n\n如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设**K=3**，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设**K=5**，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。\n\n![机器学习之K近邻算法图片01](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png)\n\n从上面实例，我们可以总结下KNN算法过程\n\n1. 计算测试数据与各个训练数据之间的距离。\n2. 按照距离的递增关系进行排序，选取距离最小的K个点。\n3. 确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。\n\n从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。\n\n+ **距离度量方式：**KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。\n\n$$\nD(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\n$$\n\n$$\nD(x,y)={|x_1-y_1|+|x_2-y_2|+|x_3-y_3|...+|x_n-y_n|}={\\sum_{i=1}^{n}|x_i-y_i|}\n$$\n\n+ **K值的选取：**KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。\n+ **分类决策规则：**KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。\n\nKNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。\n\n### 2.KD树原理\n\nKD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。\n\n#### 2.1KD树建立\n\n下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。\n\n1. **寻找划分特征：**KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。\n2. **确定划分点：**选择特征nk的中位数nkv所对应的样本作为划分点。\n3. **确定左子空间和右子空间：**对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。\n4. **递归构建KD树：**对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。\n\n我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下\n\n1. **寻找划分特征：**6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。\n2. **确定划分点：**根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。\n3. **确定左子空间和右子空间：**分割超平面x=7将空间分为两部分。x<=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x>7的部分为右子空间，包含节点为{(9,6)，(8,1)}。\n4. **递归构建KD树：**用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。\n\n![机器学习之K近邻算法图片02](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png)\n\n![机器学习之K近邻算法图片03](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png)\n\n#### 2.2KD树搜索最近邻\n\n当我们生成KD树后，就可以预测测试样本集里面的样本目标点。\n\n1. **二叉搜索：**对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的**叶子节点**。\n2. **回溯：**为找到最近邻，还需要进行**回溯**操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。\n3. **更新最近邻：**返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。\n\n为方便理解上述过程，我们利用**2.1建立的KD树**来寻找(2,4.5)的最近邻。\n\n1. **二叉搜索：**首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)->(5,4)->(4,7)}。\n2. **回溯：**节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。\n3. **更新最近邻：**该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)->(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。\n\n![机器学习之K近邻算法图片04](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png)\n\n#### 2.3KD树预测\n\n根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。\n\n### 3.球树原理\n\nKD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。\n\n![机器学习之K近邻算法图片05](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png)\n\n为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。\n\n#### 3.1球树建立\n\n球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程\n\n1. **构建超球体：**超球体是可以包含所有样本的最小球体。\n2. **划分子超球体：**从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。\n3. **递归：**对上述两个子超球体，递归执行步骤2，最终得到球树。\n\n![机器学习之K近邻算法图片06](机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png)\n\n#### 3.2球树搜索最近邻\n\nKD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。\n\n1. 自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。\n2. 然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。\n3. 检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。\n\n#### 3.3球树预测\n\n根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。\n\n### 4.KNN算法扩展\n\n有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。\n\n### 5.Sklearn实现KNN算法\n\n下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)。\n\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n#load iris data\niris=load_iris()\nX=iris.data\ny=iris.target\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nknn=KNeighborsClassifier(algorithm='kd_tree')\nknn.fit(X_train,y_train)\nprint(knn.predict(X_test))\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 1 2 1]\nprint(y_test)\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 2 2 1]\nprint(knn.score(X_test,y_test))\n# 0.977777777778\n```\n\n### 6.KNN优缺点\n\n#### 6.1优点\n\n+ 即可处理分类也可处理回归问题。\n+ 对数据没有假设，准确度高，对异常点不敏感。\n+ 比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。\n+ 主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。\n\n#### 6.2缺点\n\n+ 计算量大，尤其是特征维数较多时候。\n+ 样本不平衡时，对稀有类别的预测准确率低。\n+ KD树、球树之类的模型建立时需要大量的内存。\n+ 使用懒惰学习方法，基本上不学习，导致预测时速度较慢。\n\n### 7.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n>[刘建平Pinard_K近邻法(KNN)原理小结](http://www.cnblogs.com/pinard/p/6061661.html)\n>\n>[Yabea_K-近邻(KNN)算法](https://www.cnblogs.com/ybjourney/p/4702562.html)\n\n","slug":"机器学习之K近邻-KNN-算法","published":1,"updated":"2018-05-20T04:02:14.778Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdn000i2e01e0e80w3t","content":"<h3 id=\"1-KNN简介\"><a href=\"#1-KNN简介\" class=\"headerlink\" title=\"1.KNN简介\"></a>1.KNN简介</h3><p><strong>K近邻(K-Nearest Neighbors, KNN)</strong>算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做<strong>分类</strong>预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做<strong>回归</strong>预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。</p>\n<p>如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设<strong>K=3</strong>，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设<strong>K=5</strong>，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。</p>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png\" alt=\"机器学习之K近邻算法图片01\"></p>\n<p>从上面实例，我们可以总结下KNN算法过程</p>\n<ol>\n<li>计算测试数据与各个训练数据之间的距离。</li>\n<li>按照距离的递增关系进行排序，选取距离最小的K个点。</li>\n<li>确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。</li>\n</ol>\n<p>从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。</p>\n<ul>\n<li><strong>距离度量方式：</strong>KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。</li>\n</ul>\n<p>$$<br>D(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+…+(x_n-y_n)^2}=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}<br>$$</p>\n<p>$$<br>D(x,y)={|x_1-y_1|+|x_2-y_2|+|x_3-y_3|…+|x_n-y_n|}={\\sum_{i=1}^{n}|x_i-y_i|}<br>$$</p>\n<ul>\n<li><strong>K值的选取：</strong>KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。</li>\n<li><strong>分类决策规则：</strong>KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。</li>\n</ul>\n<p>KNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。</p>\n<h3 id=\"2-KD树原理\"><a href=\"#2-KD树原理\" class=\"headerlink\" title=\"2.KD树原理\"></a>2.KD树原理</h3><p>KD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。</p>\n<h4 id=\"2-1KD树建立\"><a href=\"#2-1KD树建立\" class=\"headerlink\" title=\"2.1KD树建立\"></a>2.1KD树建立</h4><p>下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。</p>\n<ol>\n<li><strong>寻找划分特征：</strong>KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。</li>\n<li><strong>确定划分点：</strong>选择特征nk的中位数nkv所对应的样本作为划分点。</li>\n<li><strong>确定左子空间和右子空间：</strong>对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。</li>\n<li><strong>递归构建KD树：</strong>对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。</li>\n</ol>\n<p>我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下</p>\n<ol>\n<li><strong>寻找划分特征：</strong>6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。</li>\n<li><strong>确定划分点：</strong>根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。</li>\n<li><strong>确定左子空间和右子空间：</strong>分割超平面x=7将空间分为两部分。x&lt;=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x&gt;7的部分为右子空间，包含节点为{(9,6)，(8,1)}。</li>\n<li><strong>递归构建KD树：</strong>用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。</li>\n</ol>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png\" alt=\"机器学习之K近邻算法图片02\"></p>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png\" alt=\"机器学习之K近邻算法图片03\"></p>\n<h4 id=\"2-2KD树搜索最近邻\"><a href=\"#2-2KD树搜索最近邻\" class=\"headerlink\" title=\"2.2KD树搜索最近邻\"></a>2.2KD树搜索最近邻</h4><p>当我们生成KD树后，就可以预测测试样本集里面的样本目标点。</p>\n<ol>\n<li><strong>二叉搜索：</strong>对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的<strong>叶子节点</strong>。</li>\n<li><strong>回溯：</strong>为找到最近邻，还需要进行<strong>回溯</strong>操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。</li>\n<li><strong>更新最近邻：</strong>返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</li>\n</ol>\n<p>为方便理解上述过程，我们利用<strong>2.1建立的KD树</strong>来寻找(2,4.5)的最近邻。</p>\n<ol>\n<li><strong>二叉搜索：</strong>首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)-&gt;(5,4)-&gt;(4,7)}。</li>\n<li><strong>回溯：</strong>节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。</li>\n<li><strong>更新最近邻：</strong>该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)-&gt;(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。</li>\n</ol>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png\" alt=\"机器学习之K近邻算法图片04\"></p>\n<h4 id=\"2-3KD树预测\"><a href=\"#2-3KD树预测\" class=\"headerlink\" title=\"2.3KD树预测\"></a>2.3KD树预测</h4><p>根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p>\n<h3 id=\"3-球树原理\"><a href=\"#3-球树原理\" class=\"headerlink\" title=\"3.球树原理\"></a>3.球树原理</h3><p>KD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。</p>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png\" alt=\"机器学习之K近邻算法图片05\"></p>\n<p>为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。</p>\n<h4 id=\"3-1球树建立\"><a href=\"#3-1球树建立\" class=\"headerlink\" title=\"3.1球树建立\"></a>3.1球树建立</h4><p>球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程</p>\n<ol>\n<li><strong>构建超球体：</strong>超球体是可以包含所有样本的最小球体。</li>\n<li><strong>划分子超球体：</strong>从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。</li>\n<li><strong>递归：</strong>对上述两个子超球体，递归执行步骤2，最终得到球树。</li>\n</ol>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png\" alt=\"机器学习之K近邻算法图片06\"></p>\n<h4 id=\"3-2球树搜索最近邻\"><a href=\"#3-2球树搜索最近邻\" class=\"headerlink\" title=\"3.2球树搜索最近邻\"></a>3.2球树搜索最近邻</h4><p>KD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。</p>\n<ol>\n<li>自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。</li>\n<li>然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。</li>\n<li>检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。</li>\n</ol>\n<h4 id=\"3-3球树预测\"><a href=\"#3-3球树预测\" class=\"headerlink\" title=\"3.3球树预测\"></a>3.3球树预测</h4><p>根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p>\n<h3 id=\"4-KNN算法扩展\"><a href=\"#4-KNN算法扩展\" class=\"headerlink\" title=\"4.KNN算法扩展\"></a>4.KNN算法扩展</h3><p>有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。</p>\n<h3 id=\"5-Sklearn实现KNN算法\"><a href=\"#5-Sklearn实现KNN算法\" class=\"headerlink\" title=\"5.Sklearn实现KNN算法\"></a>5.Sklearn实现KNN算法</h3><p>下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load iris data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">knn=KNeighborsClassifier(algorithm=<span class=\"string\">'kd_tree'</span>)</span><br><span class=\"line\">knn.fit(X_train,y_train)</span><br><span class=\"line\">print(knn.predict(X_test))</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 1 2 1]</span></span><br><span class=\"line\">print(y_test)</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 2 2 1]</span></span><br><span class=\"line\">print(knn.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.977777777778</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"6-KNN优缺点\"><a href=\"#6-KNN优缺点\" class=\"headerlink\" title=\"6.KNN优缺点\"></a>6.KNN优缺点</h3><h4 id=\"6-1优点\"><a href=\"#6-1优点\" class=\"headerlink\" title=\"6.1优点\"></a>6.1优点</h4><ul>\n<li>即可处理分类也可处理回归问题。</li>\n<li>对数据没有假设，准确度高，对异常点不敏感。</li>\n<li>比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。</li>\n<li>主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。</li>\n</ul>\n<h4 id=\"6-2缺点\"><a href=\"#6-2缺点\" class=\"headerlink\" title=\"6.2缺点\"></a>6.2缺点</h4><ul>\n<li>计算量大，尤其是特征维数较多时候。</li>\n<li>样本不平衡时，对稀有类别的预测准确率低。</li>\n<li>KD树、球树之类的模型建立时需要大量的内存。</li>\n<li>使用懒惰学习方法，基本上不学习，导致预测时速度较慢。</li>\n</ul>\n<h3 id=\"7-推广\"><a href=\"#7-推广\" class=\"headerlink\" title=\"7.推广\"></a>7.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6061661.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_K近邻法(KNN)原理小结</a></p>\n<p><a href=\"https://www.cnblogs.com/ybjourney/p/4702562.html\" target=\"_blank\" rel=\"noopener\">Yabea_K-近邻(KNN)算法</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-KNN简介\"><a href=\"#1-KNN简介\" class=\"headerlink\" title=\"1.KNN简介\"></a>1.KNN简介</h3><p><strong>K近邻(K-Nearest Neighbors, KNN)</strong>算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做<strong>分类</strong>预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做<strong>回归</strong>预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。</p>\n<p>如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设<strong>K=3</strong>，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设<strong>K=5</strong>，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。</p>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png\" alt=\"机器学习之K近邻算法图片01\"></p>\n<p>从上面实例，我们可以总结下KNN算法过程</p>\n<ol>\n<li>计算测试数据与各个训练数据之间的距离。</li>\n<li>按照距离的递增关系进行排序，选取距离最小的K个点。</li>\n<li>确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。</li>\n</ol>\n<p>从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。</p>\n<ul>\n<li><strong>距离度量方式：</strong>KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。</li>\n</ul>\n<p>$$<br>D(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+…+(x_n-y_n)^2}=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}<br>$$</p>\n<p>$$<br>D(x,y)={|x_1-y_1|+|x_2-y_2|+|x_3-y_3|…+|x_n-y_n|}={\\sum_{i=1}^{n}|x_i-y_i|}<br>$$</p>\n<ul>\n<li><strong>K值的选取：</strong>KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。</li>\n<li><strong>分类决策规则：</strong>KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。</li>\n</ul>\n<p>KNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。</p>\n<h3 id=\"2-KD树原理\"><a href=\"#2-KD树原理\" class=\"headerlink\" title=\"2.KD树原理\"></a>2.KD树原理</h3><p>KD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。</p>\n<h4 id=\"2-1KD树建立\"><a href=\"#2-1KD树建立\" class=\"headerlink\" title=\"2.1KD树建立\"></a>2.1KD树建立</h4><p>下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。</p>\n<ol>\n<li><strong>寻找划分特征：</strong>KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。</li>\n<li><strong>确定划分点：</strong>选择特征nk的中位数nkv所对应的样本作为划分点。</li>\n<li><strong>确定左子空间和右子空间：</strong>对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。</li>\n<li><strong>递归构建KD树：</strong>对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。</li>\n</ol>\n<p>我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下</p>\n<ol>\n<li><strong>寻找划分特征：</strong>6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。</li>\n<li><strong>确定划分点：</strong>根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。</li>\n<li><strong>确定左子空间和右子空间：</strong>分割超平面x=7将空间分为两部分。x&lt;=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x&gt;7的部分为右子空间，包含节点为{(9,6)，(8,1)}。</li>\n<li><strong>递归构建KD树：</strong>用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。</li>\n</ol>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png\" alt=\"机器学习之K近邻算法图片02\"></p>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png\" alt=\"机器学习之K近邻算法图片03\"></p>\n<h4 id=\"2-2KD树搜索最近邻\"><a href=\"#2-2KD树搜索最近邻\" class=\"headerlink\" title=\"2.2KD树搜索最近邻\"></a>2.2KD树搜索最近邻</h4><p>当我们生成KD树后，就可以预测测试样本集里面的样本目标点。</p>\n<ol>\n<li><strong>二叉搜索：</strong>对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的<strong>叶子节点</strong>。</li>\n<li><strong>回溯：</strong>为找到最近邻，还需要进行<strong>回溯</strong>操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。</li>\n<li><strong>更新最近邻：</strong>返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</li>\n</ol>\n<p>为方便理解上述过程，我们利用<strong>2.1建立的KD树</strong>来寻找(2,4.5)的最近邻。</p>\n<ol>\n<li><strong>二叉搜索：</strong>首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)-&gt;(5,4)-&gt;(4,7)}。</li>\n<li><strong>回溯：</strong>节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。</li>\n<li><strong>更新最近邻：</strong>该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)-&gt;(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。</li>\n</ol>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png\" alt=\"机器学习之K近邻算法图片04\"></p>\n<h4 id=\"2-3KD树预测\"><a href=\"#2-3KD树预测\" class=\"headerlink\" title=\"2.3KD树预测\"></a>2.3KD树预测</h4><p>根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p>\n<h3 id=\"3-球树原理\"><a href=\"#3-球树原理\" class=\"headerlink\" title=\"3.球树原理\"></a>3.球树原理</h3><p>KD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。</p>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png\" alt=\"机器学习之K近邻算法图片05\"></p>\n<p>为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。</p>\n<h4 id=\"3-1球树建立\"><a href=\"#3-1球树建立\" class=\"headerlink\" title=\"3.1球树建立\"></a>3.1球树建立</h4><p>球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程</p>\n<ol>\n<li><strong>构建超球体：</strong>超球体是可以包含所有样本的最小球体。</li>\n<li><strong>划分子超球体：</strong>从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。</li>\n<li><strong>递归：</strong>对上述两个子超球体，递归执行步骤2，最终得到球树。</li>\n</ol>\n<p><img src=\"机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png\" alt=\"机器学习之K近邻算法图片06\"></p>\n<h4 id=\"3-2球树搜索最近邻\"><a href=\"#3-2球树搜索最近邻\" class=\"headerlink\" title=\"3.2球树搜索最近邻\"></a>3.2球树搜索最近邻</h4><p>KD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。</p>\n<ol>\n<li>自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。</li>\n<li>然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。</li>\n<li>检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。</li>\n</ol>\n<h4 id=\"3-3球树预测\"><a href=\"#3-3球树预测\" class=\"headerlink\" title=\"3.3球树预测\"></a>3.3球树预测</h4><p>根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p>\n<h3 id=\"4-KNN算法扩展\"><a href=\"#4-KNN算法扩展\" class=\"headerlink\" title=\"4.KNN算法扩展\"></a>4.KNN算法扩展</h3><p>有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。</p>\n<h3 id=\"5-Sklearn实现KNN算法\"><a href=\"#5-Sklearn实现KNN算法\" class=\"headerlink\" title=\"5.Sklearn实现KNN算法\"></a>5.Sklearn实现KNN算法</h3><p>下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load iris data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">knn=KNeighborsClassifier(algorithm=<span class=\"string\">'kd_tree'</span>)</span><br><span class=\"line\">knn.fit(X_train,y_train)</span><br><span class=\"line\">print(knn.predict(X_test))</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 1 2 1]</span></span><br><span class=\"line\">print(y_test)</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 2 2 1]</span></span><br><span class=\"line\">print(knn.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.977777777778</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"6-KNN优缺点\"><a href=\"#6-KNN优缺点\" class=\"headerlink\" title=\"6.KNN优缺点\"></a>6.KNN优缺点</h3><h4 id=\"6-1优点\"><a href=\"#6-1优点\" class=\"headerlink\" title=\"6.1优点\"></a>6.1优点</h4><ul>\n<li>即可处理分类也可处理回归问题。</li>\n<li>对数据没有假设，准确度高，对异常点不敏感。</li>\n<li>比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。</li>\n<li>主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。</li>\n</ul>\n<h4 id=\"6-2缺点\"><a href=\"#6-2缺点\" class=\"headerlink\" title=\"6.2缺点\"></a>6.2缺点</h4><ul>\n<li>计算量大，尤其是特征维数较多时候。</li>\n<li>样本不平衡时，对稀有类别的预测准确率低。</li>\n<li>KD树、球树之类的模型建立时需要大量的内存。</li>\n<li>使用懒惰学习方法，基本上不学习，导致预测时速度较慢。</li>\n</ul>\n<h3 id=\"7-推广\"><a href=\"#7-推广\" class=\"headerlink\" title=\"7.推广\"></a>7.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"http://www.cnblogs.com/pinard/p/6061661.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_K近邻法(KNN)原理小结</a></p>\n<p><a href=\"https://www.cnblogs.com/ybjourney/p/4702562.html\" target=\"_blank\" rel=\"noopener\">Yabea_K-近邻(KNN)算法</a></p>\n</blockquote>\n"},{"title":"机器学习之K均值(K-Means)算法","date":"2018-05-12T05:44:19.000Z","mathjax":true,"comments":1,"_content":"\n### 1.K-Means简介\n\n**K均值(K-Means)**算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化**K-Means++**算法，距离计算优化**Elkan K-Means**算法和大样本情况下**Mini Batch K-Means**算法。\n\nK-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。\n\n假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。\n$$\nE=\\sum _{i=1}^{k} \\sum _{x \\in C_i}||x-\\mu _i||^2\n$$\n其中μi是簇Ci的均值向量，也可称作质心，表达式为\n$$\n\\mu _i=\\frac{1}{|C_i|}\\sum _{x \\in C_i}x\n$$\n如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。\n\n+ 如图(a)所示：表示初始化数据集。\n+ 如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。\n+ 如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。\n+ 如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。\n+ 如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。\n+ 如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。\n\n![机器学习值K均值算法图片01](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png)\n\n### 2.K-Means算法流程\n\n假设输入样本集$D={x_1,x_2,…,x_m}$，聚类簇数为K，最大迭代次数为N。输出的簇划分为$C={C_1,C_2,…,C_m}$。\n\n+ 从数据集D中随机选择K个样本作为初始的质心向量$ \\mu=\\{ \\mu_1,\\mu_2,\\mu_3,...,\\mu_k \\}$。\n\n+ 迭代$n=1,2,…,N$。\n\n  + 划分初始化簇$C_t=\\varnothing ;\\ t=1,2,…,k$。\n  + 对于$i=1,2,…,m$，计算样本$x_i$和各个质心向量$\\mu_j(\\ j=1,2,…,k)$的距离$d_{ij}$。将$x_i$标记为最小的$d_{ij}$所对应的类别$\\lambda_i$，此时更新$C_{\\lambda i}=C_{\\lambda i} \\cup{x_i}$。\n\n  $$\n  d_{ij}=||x_i-\\mu_j||^2\n  $$\n\n  + 对于$j=1,2,…,k$，对$C_j$中所有样本点重新计算新的质心。\n\n  $$\n  \\mu_j=\\frac{1}{|C_j|}\\sum_{x\\in C_j}x\n  $$\n\n  + 如果K个质心向量都不再发生变化，则结束迭代。\n\n+ 输出K个划分簇$C$，$C=\\{C_1,C_2,C_3,…,C_k \\}$。\n\n对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。\n\n+ **对于K值的选择:**我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。\n+ **对于K个初始化质心:**由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。\n\n### 3.初始化优化K-Means++\n\n如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。\n\n+ 从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。\n+ 对于数据集中的每个点xi，计算与他最近的聚类中心距离。\n\n$$\nD(x)=\\arg \\min_{r=1}^{k_{selected}}||x_i-\\mu_r||^2\n$$\n\n+ 选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。\n+ 重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。\n\n### 4.距离计算优化Elkan K-Means算法\n\n传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。\n\n+ 对于一个样本点$x$和两个质心$\\mu_{j1},\\mu_{j2}$，如果我们预先计算出这两个质心之间的距离$D(j_1,j_2)$，如果发现$2D(x,j_1)≤D(j_1,j_2)$，那么我们便能得到$D(x,j_1)≤D(x,j_2)$。此时我们不再计算$D(x,j_2)$，也就节省了一步距离计算。\n+ 对于一个样本点$x$和两个质心$μ_{j1},μ_{j2}$，我们能够得到$D(x,j_2)≥max{0,D(x,j_1)−D(j_1,j_2)}$。\n\nElkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。\n\n### 5.大样本优化Mini Batch K-Means算法\n\n传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。\n\nMini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。\n\nMini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。\n\n### 6.Sklearn实现K-Means算法\n\n我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)。\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n#load iris\niris=load_iris()\nX=iris.data[:,:2]\nprint(X.shape)\n#150,2\n\n#plot data\nplt.figure()\nplt.scatter(X[:,0],X[:,1],c='blue',\n            marker='o',label='point')\nplt.legend(loc=2)\nplt.show()\n```\n\n![机器学习值K均值算法图片02](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png)\n\n```python\n# fit data\nkmeans=KMeans(n_clusters=3)\nkmeans.fit(X)\nlabel_pred=kmeans.labels_\n\n#plot answer\nplt.figure()\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\nplt.scatter(x0[:, 0], x0[:, 1], c = \"red\",\n            marker='o', label='label0')\nplt.scatter(x1[:, 0], x1[:, 1], c = \"green\",\n            marker='*', label='label1')\nplt.scatter(x2[:, 0], x2[:, 1], c = \"blue\",\n            marker='+', label='label2')\nplt.legend(loc=2)\nplt.show()\n```\n\n![机器学习值K均值算法图片03](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png)\n\n### 7.K-Means算法优缺点\n\n#### 7.1优点\n\n+ 聚类效果较优。\n\n+ 原理简单，实现容易，收敛速度快。\n+ 需要调整的参数较少，通常只需要调整簇数K。\n\n#### 7.2缺点\n\n+ K值选取不好把握。\n+ 对噪音和异常点比较敏感。\n+ 采用迭代方法，得到的结果是局部最优。\n+ 如果各隐含类别的数据不平衡，则聚类效果不佳。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n+ [刘建平Pinard_K-Means聚类算法原理](http://www.cnblogs.com/pinard/p/6164214.html)","source":"_posts/机器学习之K均值-K-Means-算法.md","raw":"---\ntitle: 机器学习之K均值(K-Means)算法\ndate: 2018-05-12 13:44:19\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.K-Means简介\n\n**K均值(K-Means)**算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化**K-Means++**算法，距离计算优化**Elkan K-Means**算法和大样本情况下**Mini Batch K-Means**算法。\n\nK-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。\n\n假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。\n$$\nE=\\sum _{i=1}^{k} \\sum _{x \\in C_i}||x-\\mu _i||^2\n$$\n其中μi是簇Ci的均值向量，也可称作质心，表达式为\n$$\n\\mu _i=\\frac{1}{|C_i|}\\sum _{x \\in C_i}x\n$$\n如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。\n\n+ 如图(a)所示：表示初始化数据集。\n+ 如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。\n+ 如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。\n+ 如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。\n+ 如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。\n+ 如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。\n\n![机器学习值K均值算法图片01](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png)\n\n### 2.K-Means算法流程\n\n假设输入样本集$D={x_1,x_2,…,x_m}$，聚类簇数为K，最大迭代次数为N。输出的簇划分为$C={C_1,C_2,…,C_m}$。\n\n+ 从数据集D中随机选择K个样本作为初始的质心向量$ \\mu=\\{ \\mu_1,\\mu_2,\\mu_3,...,\\mu_k \\}$。\n\n+ 迭代$n=1,2,…,N$。\n\n  + 划分初始化簇$C_t=\\varnothing ;\\ t=1,2,…,k$。\n  + 对于$i=1,2,…,m$，计算样本$x_i$和各个质心向量$\\mu_j(\\ j=1,2,…,k)$的距离$d_{ij}$。将$x_i$标记为最小的$d_{ij}$所对应的类别$\\lambda_i$，此时更新$C_{\\lambda i}=C_{\\lambda i} \\cup{x_i}$。\n\n  $$\n  d_{ij}=||x_i-\\mu_j||^2\n  $$\n\n  + 对于$j=1,2,…,k$，对$C_j$中所有样本点重新计算新的质心。\n\n  $$\n  \\mu_j=\\frac{1}{|C_j|}\\sum_{x\\in C_j}x\n  $$\n\n  + 如果K个质心向量都不再发生变化，则结束迭代。\n\n+ 输出K个划分簇$C$，$C=\\{C_1,C_2,C_3,…,C_k \\}$。\n\n对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。\n\n+ **对于K值的选择:**我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。\n+ **对于K个初始化质心:**由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。\n\n### 3.初始化优化K-Means++\n\n如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。\n\n+ 从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。\n+ 对于数据集中的每个点xi，计算与他最近的聚类中心距离。\n\n$$\nD(x)=\\arg \\min_{r=1}^{k_{selected}}||x_i-\\mu_r||^2\n$$\n\n+ 选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。\n+ 重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。\n\n### 4.距离计算优化Elkan K-Means算法\n\n传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。\n\n+ 对于一个样本点$x$和两个质心$\\mu_{j1},\\mu_{j2}$，如果我们预先计算出这两个质心之间的距离$D(j_1,j_2)$，如果发现$2D(x,j_1)≤D(j_1,j_2)$，那么我们便能得到$D(x,j_1)≤D(x,j_2)$。此时我们不再计算$D(x,j_2)$，也就节省了一步距离计算。\n+ 对于一个样本点$x$和两个质心$μ_{j1},μ_{j2}$，我们能够得到$D(x,j_2)≥max{0,D(x,j_1)−D(j_1,j_2)}$。\n\nElkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。\n\n### 5.大样本优化Mini Batch K-Means算法\n\n传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。\n\nMini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。\n\nMini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。\n\n### 6.Sklearn实现K-Means算法\n\n我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)。\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n#load iris\niris=load_iris()\nX=iris.data[:,:2]\nprint(X.shape)\n#150,2\n\n#plot data\nplt.figure()\nplt.scatter(X[:,0],X[:,1],c='blue',\n            marker='o',label='point')\nplt.legend(loc=2)\nplt.show()\n```\n\n![机器学习值K均值算法图片02](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png)\n\n```python\n# fit data\nkmeans=KMeans(n_clusters=3)\nkmeans.fit(X)\nlabel_pred=kmeans.labels_\n\n#plot answer\nplt.figure()\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\nplt.scatter(x0[:, 0], x0[:, 1], c = \"red\",\n            marker='o', label='label0')\nplt.scatter(x1[:, 0], x1[:, 1], c = \"green\",\n            marker='*', label='label1')\nplt.scatter(x2[:, 0], x2[:, 1], c = \"blue\",\n            marker='+', label='label2')\nplt.legend(loc=2)\nplt.show()\n```\n\n![机器学习值K均值算法图片03](机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png)\n\n### 7.K-Means算法优缺点\n\n#### 7.1优点\n\n+ 聚类效果较优。\n\n+ 原理简单，实现容易，收敛速度快。\n+ 需要调整的参数较少，通常只需要调整簇数K。\n\n#### 7.2缺点\n\n+ K值选取不好把握。\n+ 对噪音和异常点比较敏感。\n+ 采用迭代方法，得到的结果是局部最优。\n+ 如果各隐含类别的数据不平衡，则聚类效果不佳。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n+ [刘建平Pinard_K-Means聚类算法原理](http://www.cnblogs.com/pinard/p/6164214.html)","slug":"机器学习之K均值-K-Means-算法","published":1,"updated":"2018-05-20T04:02:21.696Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdp000j2e01pi5l4zxb","content":"<h3 id=\"1-K-Means简介\"><a href=\"#1-K-Means简介\" class=\"headerlink\" title=\"1.K-Means简介\"></a>1.K-Means简介</h3><p><strong>K均值(K-Means)</strong>算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化<strong>K-Means++</strong>算法，距离计算优化<strong>Elkan K-Means</strong>算法和大样本情况下<strong>Mini Batch K-Means</strong>算法。</p>\n<p>K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。</p>\n<p>假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。<br>$$<br>E=\\sum _{i=1}^{k} \\sum _{x \\in C_i}||x-\\mu _i||^2<br>$$<br>其中μi是簇Ci的均值向量，也可称作质心，表达式为<br>$$<br>\\mu _i=\\frac{1}{|C_i|}\\sum _{x \\in C_i}x<br>$$<br>如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。</p>\n<ul>\n<li>如图(a)所示：表示初始化数据集。</li>\n<li>如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。</li>\n<li>如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。</li>\n<li>如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。</li>\n<li>如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。</li>\n<li>如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。</li>\n</ul>\n<p><img src=\"机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png\" alt=\"机器学习值K均值算法图片01\"></p>\n<h3 id=\"2-K-Means算法流程\"><a href=\"#2-K-Means算法流程\" class=\"headerlink\" title=\"2.K-Means算法流程\"></a>2.K-Means算法流程</h3><p>假设输入样本集$D={x_1,x_2,…,x_m}$，聚类簇数为K，最大迭代次数为N。输出的簇划分为$C={C_1,C_2,…,C_m}$。</p>\n<ul>\n<li><p>从数据集D中随机选择K个样本作为初始的质心向量$ \\mu={ \\mu_1,\\mu_2,\\mu_3,…,\\mu_k }$。</p>\n</li>\n<li><p>迭代$n=1,2,…,N$。</p>\n<ul>\n<li>划分初始化簇$C_t=\\varnothing ;\\ t=1,2,…,k$。</li>\n<li>对于$i=1,2,…,m$，计算样本$x_i$和各个质心向量$\\mu_j(\\ j=1,2,…,k)$的距离$d_{ij}$。将$x_i$标记为最小的$d_{ij}$所对应的类别$\\lambda_i$，此时更新$C_{\\lambda i}=C_{\\lambda i} \\cup{x_i}$。</li>\n</ul>\n<p>$$<br>d_{ij}=||x_i-\\mu_j||^2<br>$$</p>\n<ul>\n<li>对于$j=1,2,…,k$，对$C_j$中所有样本点重新计算新的质心。</li>\n</ul>\n<p>$$<br>\\mu_j=\\frac{1}{|C_j|}\\sum_{x\\in C_j}x<br>$$</p>\n<ul>\n<li>如果K个质心向量都不再发生变化，则结束迭代。</li>\n</ul>\n</li>\n<li><p>输出K个划分簇$C$，$C={C_1,C_2,C_3,…,C_k }$。</p>\n</li>\n</ul>\n<p>对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。</p>\n<ul>\n<li><strong>对于K值的选择:</strong>我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。</li>\n<li><strong>对于K个初始化质心:</strong>由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。</li>\n</ul>\n<h3 id=\"3-初始化优化K-Means\"><a href=\"#3-初始化优化K-Means\" class=\"headerlink\" title=\"3.初始化优化K-Means++\"></a>3.初始化优化K-Means++</h3><p>如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。</p>\n<ul>\n<li>从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。</li>\n<li>对于数据集中的每个点xi，计算与他最近的聚类中心距离。</li>\n</ul>\n<p>$$<br>D(x)=\\arg \\min_{r=1}^{k_{selected}}||x_i-\\mu_r||^2<br>$$</p>\n<ul>\n<li>选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。</li>\n<li>重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。</li>\n</ul>\n<h3 id=\"4-距离计算优化Elkan-K-Means算法\"><a href=\"#4-距离计算优化Elkan-K-Means算法\" class=\"headerlink\" title=\"4.距离计算优化Elkan K-Means算法\"></a>4.距离计算优化Elkan K-Means算法</h3><p>传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。</p>\n<ul>\n<li>对于一个样本点$x$和两个质心$\\mu_{j1},\\mu_{j2}$，如果我们预先计算出这两个质心之间的距离$D(j_1,j_2)$，如果发现$2D(x,j_1)≤D(j_1,j_2)$，那么我们便能得到$D(x,j_1)≤D(x,j_2)$。此时我们不再计算$D(x,j_2)$，也就节省了一步距离计算。</li>\n<li>对于一个样本点$x$和两个质心$μ<em>{j1},μ</em>{j2}$，我们能够得到$D(x,j_2)≥max{0,D(x,j_1)−D(j_1,j_2)}$。</li>\n</ul>\n<p>Elkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。</p>\n<h3 id=\"5-大样本优化Mini-Batch-K-Means算法\"><a href=\"#5-大样本优化Mini-Batch-K-Means算法\" class=\"headerlink\" title=\"5.大样本优化Mini Batch K-Means算法\"></a>5.大样本优化Mini Batch K-Means算法</h3><p>传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。</p>\n<p>Mini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。</p>\n<p>Mini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。</p>\n<h3 id=\"6-Sklearn实现K-Means算法\"><a href=\"#6-Sklearn实现K-Means算法\" class=\"headerlink\" title=\"6.Sklearn实现K-Means算法\"></a>6.Sklearn实现K-Means算法</h3><p>我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load iris</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data[:,:<span class=\"number\">2</span>]</span><br><span class=\"line\">print(X.shape)</span><br><span class=\"line\"><span class=\"comment\">#150,2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot data</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],c=<span class=\"string\">'blue'</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'o'</span>,label=<span class=\"string\">'point'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png\" alt=\"机器学习值K均值算法图片02\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># fit data</span></span><br><span class=\"line\">kmeans=KMeans(n_clusters=<span class=\"number\">3</span>)</span><br><span class=\"line\">kmeans.fit(X)</span><br><span class=\"line\">label_pred=kmeans.labels_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot answer</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">x0 = X[label_pred == <span class=\"number\">0</span>]</span><br><span class=\"line\">x1 = X[label_pred == <span class=\"number\">1</span>]</span><br><span class=\"line\">x2 = X[label_pred == <span class=\"number\">2</span>]</span><br><span class=\"line\">plt.scatter(x0[:, <span class=\"number\">0</span>], x0[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"red\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'o'</span>, label=<span class=\"string\">'label0'</span>)</span><br><span class=\"line\">plt.scatter(x1[:, <span class=\"number\">0</span>], x1[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"green\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'*'</span>, label=<span class=\"string\">'label1'</span>)</span><br><span class=\"line\">plt.scatter(x2[:, <span class=\"number\">0</span>], x2[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"blue\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'+'</span>, label=<span class=\"string\">'label2'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png\" alt=\"机器学习值K均值算法图片03\"></p>\n<h3 id=\"7-K-Means算法优缺点\"><a href=\"#7-K-Means算法优缺点\" class=\"headerlink\" title=\"7.K-Means算法优缺点\"></a>7.K-Means算法优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li><p>聚类效果较优。</p>\n</li>\n<li><p>原理简单，实现容易，收敛速度快。</p>\n</li>\n<li>需要调整的参数较少，通常只需要调整簇数K。</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li>K值选取不好把握。</li>\n<li>对噪音和异常点比较敏感。</li>\n<li>采用迭代方法，得到的结果是局部最优。</li>\n<li>如果各隐含类别的数据不平衡，则聚类效果不佳。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<ul>\n<li><a href=\"http://www.cnblogs.com/pinard/p/6164214.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_K-Means聚类算法原理</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-K-Means简介\"><a href=\"#1-K-Means简介\" class=\"headerlink\" title=\"1.K-Means简介\"></a>1.K-Means简介</h3><p><strong>K均值(K-Means)</strong>算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化<strong>K-Means++</strong>算法，距离计算优化<strong>Elkan K-Means</strong>算法和大样本情况下<strong>Mini Batch K-Means</strong>算法。</p>\n<p>K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。</p>\n<p>假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。<br>$$<br>E=\\sum _{i=1}^{k} \\sum _{x \\in C_i}||x-\\mu _i||^2<br>$$<br>其中μi是簇Ci的均值向量，也可称作质心，表达式为<br>$$<br>\\mu _i=\\frac{1}{|C_i|}\\sum _{x \\in C_i}x<br>$$<br>如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。</p>\n<ul>\n<li>如图(a)所示：表示初始化数据集。</li>\n<li>如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。</li>\n<li>如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。</li>\n<li>如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。</li>\n<li>如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。</li>\n<li>如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。</li>\n</ul>\n<p><img src=\"机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png\" alt=\"机器学习值K均值算法图片01\"></p>\n<h3 id=\"2-K-Means算法流程\"><a href=\"#2-K-Means算法流程\" class=\"headerlink\" title=\"2.K-Means算法流程\"></a>2.K-Means算法流程</h3><p>假设输入样本集$D={x_1,x_2,…,x_m}$，聚类簇数为K，最大迭代次数为N。输出的簇划分为$C={C_1,C_2,…,C_m}$。</p>\n<ul>\n<li><p>从数据集D中随机选择K个样本作为初始的质心向量$ \\mu={ \\mu_1,\\mu_2,\\mu_3,…,\\mu_k }$。</p>\n</li>\n<li><p>迭代$n=1,2,…,N$。</p>\n<ul>\n<li>划分初始化簇$C_t=\\varnothing ;\\ t=1,2,…,k$。</li>\n<li>对于$i=1,2,…,m$，计算样本$x_i$和各个质心向量$\\mu_j(\\ j=1,2,…,k)$的距离$d_{ij}$。将$x_i$标记为最小的$d_{ij}$所对应的类别$\\lambda_i$，此时更新$C_{\\lambda i}=C_{\\lambda i} \\cup{x_i}$。</li>\n</ul>\n<p>$$<br>d_{ij}=||x_i-\\mu_j||^2<br>$$</p>\n<ul>\n<li>对于$j=1,2,…,k$，对$C_j$中所有样本点重新计算新的质心。</li>\n</ul>\n<p>$$<br>\\mu_j=\\frac{1}{|C_j|}\\sum_{x\\in C_j}x<br>$$</p>\n<ul>\n<li>如果K个质心向量都不再发生变化，则结束迭代。</li>\n</ul>\n</li>\n<li><p>输出K个划分簇$C$，$C={C_1,C_2,C_3,…,C_k }$。</p>\n</li>\n</ul>\n<p>对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。</p>\n<ul>\n<li><strong>对于K值的选择:</strong>我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。</li>\n<li><strong>对于K个初始化质心:</strong>由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。</li>\n</ul>\n<h3 id=\"3-初始化优化K-Means\"><a href=\"#3-初始化优化K-Means\" class=\"headerlink\" title=\"3.初始化优化K-Means++\"></a>3.初始化优化K-Means++</h3><p>如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。</p>\n<ul>\n<li>从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。</li>\n<li>对于数据集中的每个点xi，计算与他最近的聚类中心距离。</li>\n</ul>\n<p>$$<br>D(x)=\\arg \\min_{r=1}^{k_{selected}}||x_i-\\mu_r||^2<br>$$</p>\n<ul>\n<li>选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。</li>\n<li>重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。</li>\n</ul>\n<h3 id=\"4-距离计算优化Elkan-K-Means算法\"><a href=\"#4-距离计算优化Elkan-K-Means算法\" class=\"headerlink\" title=\"4.距离计算优化Elkan K-Means算法\"></a>4.距离计算优化Elkan K-Means算法</h3><p>传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。</p>\n<ul>\n<li>对于一个样本点$x$和两个质心$\\mu_{j1},\\mu_{j2}$，如果我们预先计算出这两个质心之间的距离$D(j_1,j_2)$，如果发现$2D(x,j_1)≤D(j_1,j_2)$，那么我们便能得到$D(x,j_1)≤D(x,j_2)$。此时我们不再计算$D(x,j_2)$，也就节省了一步距离计算。</li>\n<li>对于一个样本点$x$和两个质心$μ<em>{j1},μ</em>{j2}$，我们能够得到$D(x,j_2)≥max{0,D(x,j_1)−D(j_1,j_2)}$。</li>\n</ul>\n<p>Elkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。</p>\n<h3 id=\"5-大样本优化Mini-Batch-K-Means算法\"><a href=\"#5-大样本优化Mini-Batch-K-Means算法\" class=\"headerlink\" title=\"5.大样本优化Mini Batch K-Means算法\"></a>5.大样本优化Mini Batch K-Means算法</h3><p>传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。</p>\n<p>Mini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。</p>\n<p>Mini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。</p>\n<h3 id=\"6-Sklearn实现K-Means算法\"><a href=\"#6-Sklearn实现K-Means算法\" class=\"headerlink\" title=\"6.Sklearn实现K-Means算法\"></a>6.Sklearn实现K-Means算法</h3><p>我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load iris</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data[:,:<span class=\"number\">2</span>]</span><br><span class=\"line\">print(X.shape)</span><br><span class=\"line\"><span class=\"comment\">#150,2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot data</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],c=<span class=\"string\">'blue'</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'o'</span>,label=<span class=\"string\">'point'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png\" alt=\"机器学习值K均值算法图片02\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># fit data</span></span><br><span class=\"line\">kmeans=KMeans(n_clusters=<span class=\"number\">3</span>)</span><br><span class=\"line\">kmeans.fit(X)</span><br><span class=\"line\">label_pred=kmeans.labels_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot answer</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">x0 = X[label_pred == <span class=\"number\">0</span>]</span><br><span class=\"line\">x1 = X[label_pred == <span class=\"number\">1</span>]</span><br><span class=\"line\">x2 = X[label_pred == <span class=\"number\">2</span>]</span><br><span class=\"line\">plt.scatter(x0[:, <span class=\"number\">0</span>], x0[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"red\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'o'</span>, label=<span class=\"string\">'label0'</span>)</span><br><span class=\"line\">plt.scatter(x1[:, <span class=\"number\">0</span>], x1[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"green\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'*'</span>, label=<span class=\"string\">'label1'</span>)</span><br><span class=\"line\">plt.scatter(x2[:, <span class=\"number\">0</span>], x2[:, <span class=\"number\">1</span>], c = <span class=\"string\">\"blue\"</span>,</span><br><span class=\"line\">            marker=<span class=\"string\">'+'</span>, label=<span class=\"string\">'label2'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png\" alt=\"机器学习值K均值算法图片03\"></p>\n<h3 id=\"7-K-Means算法优缺点\"><a href=\"#7-K-Means算法优缺点\" class=\"headerlink\" title=\"7.K-Means算法优缺点\"></a>7.K-Means算法优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li><p>聚类效果较优。</p>\n</li>\n<li><p>原理简单，实现容易，收敛速度快。</p>\n</li>\n<li>需要调整的参数较少，通常只需要调整簇数K。</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li>K值选取不好把握。</li>\n<li>对噪音和异常点比较敏感。</li>\n<li>采用迭代方法，得到的结果是局部最优。</li>\n<li>如果各隐含类别的数据不平衡，则聚类效果不佳。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<ul>\n<li><a href=\"http://www.cnblogs.com/pinard/p/6164214.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_K-Means聚类算法原理</a></li>\n</ul>\n"},{"title":"机器学习之Logistic回归","date":"2018-03-27T09:49:33.000Z","comments":1,"mathjax":true,"_content":"\n### 1.Logistic回归简介\n\n线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。\n\n### 2.Sigmoid函数\n\n为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于\n$$\nexp(w_{k1}x_1+…+w_{kn}x_n)。\n$$\n因为一个数据点属于各类的概率之和为1，所以可以得到\n$$\nP(y=k)=\\frac{exp(\\sum_{i=1}^{n}w_{ki}{x_i})}{\\sum_{k'}exp(\\sum_{i=1}^{n}w_{k'i}x_i)}\n$$\n现在回到两类（0,1）的情况，此时分母上只有两项\n$$\nP(y=1)=\\frac{exp(\\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\\sum_{i=1}^{n}w_{1i}x_i)+exp(\\sum_{i=1}^{n}w_{0i}x_i)}\n$$\n公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有\n$$\nP(y=1)=\\frac{1}{1+exp(-\\sum_{i=1}^{n}w_ix_i)}\n$$\n上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。\n$$\nSigmoid Function: f(x)=\\frac{1}{1+e^{-x}}\n$$\nSigmoid函数具有如下性质\n\n+ 函数连续且单调递增\n+ 函数关于（0,0.5）对称\n+ $x\\in(-\\infty,\\infty)$时$y\\in(0,1)$\n\n```python\n#plot sigmoid function \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n##sigmoid function\nx=np.arange(-5,5,0.1)\ny=1/(1+np.exp(-x))\n\n#plot\nplt.figure()\nplt.plot(x,y,color='red',linewidth='2')\nax=plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data',0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()\n```\n\n![机器学习之Logistic回归01](机器学习之Logistic回归/机器学习之Logistic回归01.png)\n\n### 3.Logistic回归推导\n\n+ 特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。\n+ $\\theta=(\\theta_0,\\theta_1,\\theta_2…\\theta_n)$\n+ $n$表示特征数量\n+ $m$表示训练数据数量\n\n线性回归函数为$z=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n=\\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。\n$$\nh(X)=\\frac{1}{1+e^{-\\theta^TX}}\n$$\n我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)<0.5$是判断当前数据属于A类，当$h(X)>0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\\theta$。\n\n条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为\n$$\nP(y=1|X)=\\pi(X)=\\frac{1}{1+e^{-\\theta^TX}}\n$$\n条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为\n$$\nP(y=0|X)=1-\\pi(X)=\\frac{1}{1+e^{\\theta^TX}}\n$$\n因此我们可以得到事件的发生比为\n$$\nodds=\\frac{P(y=1|X)}{P(y=0|X)}\n$$\n事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。\n\n各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数\n$$\nL(\\theta)=\\prod_{i=1}^{m}[\\pi(X_i)]^{y_i}[1-\\pi(X_i)]^{1-y_i}\n$$\n目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到\n$$\nlnL(\\theta)=\\sum_{i=1}^{m}\\left \\{ y_iln[\\pi(X_i)] +(1-y_i)ln[1-\\pi(X_i)] \\right \\}\n$$\n\n$$\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_iln\\frac{\\pi(X_i)}{1-\\pi(X_i)}\n$$\n\n$$\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_i\\theta^TX\n$$\n\n$$\n=\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX\n$$\n\n通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\\theta$。首先在前面乘上负的系数$-\\frac{1}{m}$，所以$J(\\theta)$最小时的$\\theta$为最佳参数。\n$$\nJ(\\theta)=-\\frac{1}{m}lnL(\\theta)\n$$\n\n$$\n=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}\n$$\n\n### 4.梯度下降算法\n\n#### 4.1梯度下降算法简述\n\n实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。\n\n![器学习之Logistic回归0](机器学习之Logistic回归/机器学习之Logistic回归02.png)\n\n从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。\n\n#### 4.2 梯度下降算法相关概念\n\n求解梯度下降算法之前，我们先了解相关概念。\n\n- **步长（Learning Rate）**：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。\n- **特征（Feature）**：即上述描述的$X$\n- **假设函数（Hypothesis Function）**：监督学习中，为了拟合输入样本，而使用假设函数。\n- **损失函数（Loss Function）**：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为\n\n$$\nJ(\\theta)=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}\n$$\n\n我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。\n\n#### 4.3梯度下降算法过程\n\n- 随机选取一组$\\theta$。\n- 不断变化$\\theta$，让$J(\\theta)$变小，$\\alpha$为学习步长。\n\n$$\n\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n$$\n\n+ 直到$J(\\theta)$得到最小值，$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)$为$J(\\theta)$对$\\theta_k$的偏导。\n\n$$\n\\frac{\\partial J(\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{1+e^{\\theta^TX}}e^{\\theta^TX}X_{ij}-\\sum_{i=1}^{m}y_iX_{ij}\n$$\n\n$$\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\frac{e^{\\theta^TX}}{1+e^{\\theta^Tx}}-y_i]\n$$\n\n$$\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]\n$$\n\n因此梯度下降算法的迭代最终表述为\n$$\n\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]\n$$\n梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到Logistic函数。\n\n### 5.Logistic回归实现\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan', 'gray')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl)\n    # highlight test samples\n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        plt.scatter(X_test[:, 0], X_test[:, 1], c='blue', alpha=1.0, linewidth=1, marker='o', s=55, label='test set')\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n#为了追求机器学习的最佳性能，我们将特征缩放\nsc = StandardScaler()\nsc.fit(X_train)#估算每个特征的平均值和标准差\nX_train_std=sc.transform(X_train)#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性\nX_test_std=sc.transform(X_test)\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\n#训练感知机模型\nlr = LogisticRegression(C=1000.0,random_state=0)#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序\nlr.fit(X_train_std, y_train)\nlr.predict_proba(X_test_std)\n\n#绘图\nplot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105,150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\n```\n\n![机器学习之Logistic回归03](机器学习之Logistic回归/机器学习之Logistic回归03.png)\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![](机器学习之Logistic回归/推广.png)\n\n+ 参考\n\n[^1]: https://www.zhihu.com/people/maigo/activities\n[^2]: https://blog.csdn.net/programmer_wei/article/details/52072939\n[^3]: https://blog.csdn.net/javaisnotgood/article/details/78873819\n\n","source":"_posts/机器学习之Logistic回归.md","raw":"---\ntitle: 机器学习之Logistic回归\ndate: 2018-03-27 17:49:33\ntags: [机器学习,算法]\ncategories: 机器学习\ncomments: true\nmathjax: true\n---\n\n### 1.Logistic回归简介\n\n线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。\n\n### 2.Sigmoid函数\n\n为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于\n$$\nexp(w_{k1}x_1+…+w_{kn}x_n)。\n$$\n因为一个数据点属于各类的概率之和为1，所以可以得到\n$$\nP(y=k)=\\frac{exp(\\sum_{i=1}^{n}w_{ki}{x_i})}{\\sum_{k'}exp(\\sum_{i=1}^{n}w_{k'i}x_i)}\n$$\n现在回到两类（0,1）的情况，此时分母上只有两项\n$$\nP(y=1)=\\frac{exp(\\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\\sum_{i=1}^{n}w_{1i}x_i)+exp(\\sum_{i=1}^{n}w_{0i}x_i)}\n$$\n公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有\n$$\nP(y=1)=\\frac{1}{1+exp(-\\sum_{i=1}^{n}w_ix_i)}\n$$\n上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。\n$$\nSigmoid Function: f(x)=\\frac{1}{1+e^{-x}}\n$$\nSigmoid函数具有如下性质\n\n+ 函数连续且单调递增\n+ 函数关于（0,0.5）对称\n+ $x\\in(-\\infty,\\infty)$时$y\\in(0,1)$\n\n```python\n#plot sigmoid function \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n##sigmoid function\nx=np.arange(-5,5,0.1)\ny=1/(1+np.exp(-x))\n\n#plot\nplt.figure()\nplt.plot(x,y,color='red',linewidth='2')\nax=plt.gca()\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data',0))\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))\nplt.xlabel('independent variable')\nplt.ylabel('dependent variable')\nplt.show()\n```\n\n![机器学习之Logistic回归01](机器学习之Logistic回归/机器学习之Logistic回归01.png)\n\n### 3.Logistic回归推导\n\n+ 特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。\n+ $\\theta=(\\theta_0,\\theta_1,\\theta_2…\\theta_n)$\n+ $n$表示特征数量\n+ $m$表示训练数据数量\n\n线性回归函数为$z=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n=\\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。\n$$\nh(X)=\\frac{1}{1+e^{-\\theta^TX}}\n$$\n我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)<0.5$是判断当前数据属于A类，当$h(X)>0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\\theta$。\n\n条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为\n$$\nP(y=1|X)=\\pi(X)=\\frac{1}{1+e^{-\\theta^TX}}\n$$\n条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为\n$$\nP(y=0|X)=1-\\pi(X)=\\frac{1}{1+e^{\\theta^TX}}\n$$\n因此我们可以得到事件的发生比为\n$$\nodds=\\frac{P(y=1|X)}{P(y=0|X)}\n$$\n事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。\n\n各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数\n$$\nL(\\theta)=\\prod_{i=1}^{m}[\\pi(X_i)]^{y_i}[1-\\pi(X_i)]^{1-y_i}\n$$\n目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到\n$$\nlnL(\\theta)=\\sum_{i=1}^{m}\\left \\{ y_iln[\\pi(X_i)] +(1-y_i)ln[1-\\pi(X_i)] \\right \\}\n$$\n\n$$\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_iln\\frac{\\pi(X_i)}{1-\\pi(X_i)}\n$$\n\n$$\n=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_i\\theta^TX\n$$\n\n$$\n=\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX\n$$\n\n通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\\theta$。首先在前面乘上负的系数$-\\frac{1}{m}$，所以$J(\\theta)$最小时的$\\theta$为最佳参数。\n$$\nJ(\\theta)=-\\frac{1}{m}lnL(\\theta)\n$$\n\n$$\n=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}\n$$\n\n### 4.梯度下降算法\n\n#### 4.1梯度下降算法简述\n\n实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。\n\n![器学习之Logistic回归0](机器学习之Logistic回归/机器学习之Logistic回归02.png)\n\n从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。\n\n#### 4.2 梯度下降算法相关概念\n\n求解梯度下降算法之前，我们先了解相关概念。\n\n- **步长（Learning Rate）**：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。\n- **特征（Feature）**：即上述描述的$X$\n- **假设函数（Hypothesis Function）**：监督学习中，为了拟合输入样本，而使用假设函数。\n- **损失函数（Loss Function）**：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为\n\n$$\nJ(\\theta)=-\\frac{1}{m}\\left \\{\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right\\}\n$$\n\n我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。\n\n#### 4.3梯度下降算法过程\n\n- 随机选取一组$\\theta$。\n- 不断变化$\\theta$，让$J(\\theta)$变小，$\\alpha$为学习步长。\n\n$$\n\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n$$\n\n+ 直到$J(\\theta)$得到最小值，$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)$为$J(\\theta)$对$\\theta_k$的偏导。\n\n$$\n\\frac{\\partial J(\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{1+e^{\\theta^TX}}e^{\\theta^TX}X_{ij}-\\sum_{i=1}^{m}y_iX_{ij}\n$$\n\n$$\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\frac{e^{\\theta^TX}}{1+e^{\\theta^Tx}}-y_i]\n$$\n\n$$\n=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]\n$$\n\n因此梯度下降算法的迭代最终表述为\n$$\n\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]\n$$\n梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到Logistic函数。\n\n### 5.Logistic回归实现\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan', 'gray')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl)\n    # highlight test samples\n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        plt.scatter(X_test[:, 0], X_test[:, 1], c='blue', alpha=1.0, linewidth=1, marker='o', s=55, label='test set')\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n#为了追求机器学习的最佳性能，我们将特征缩放\nsc = StandardScaler()\nsc.fit(X_train)#估算每个特征的平均值和标准差\nX_train_std=sc.transform(X_train)#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性\nX_test_std=sc.transform(X_test)\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\n#训练感知机模型\nlr = LogisticRegression(C=1000.0,random_state=0)#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序\nlr.fit(X_train_std, y_train)\nlr.predict_proba(X_test_std)\n\n#绘图\nplot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105,150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\n```\n\n![机器学习之Logistic回归03](机器学习之Logistic回归/机器学习之Logistic回归03.png)\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![](机器学习之Logistic回归/推广.png)\n\n+ 参考\n\n[^1]: https://www.zhihu.com/people/maigo/activities\n[^2]: https://blog.csdn.net/programmer_wei/article/details/52072939\n[^3]: https://blog.csdn.net/javaisnotgood/article/details/78873819\n\n","slug":"机器学习之Logistic回归","published":1,"updated":"2018-03-28T16:43:35.044Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdr000n2e01u6930lhf","content":"<h3 id=\"1-Logistic回归简介\"><a href=\"#1-Logistic回归简介\" class=\"headerlink\" title=\"1.Logistic回归简介\"></a>1.Logistic回归简介</h3><p>线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。</p>\n<h3 id=\"2-Sigmoid函数\"><a href=\"#2-Sigmoid函数\" class=\"headerlink\" title=\"2.Sigmoid函数\"></a>2.Sigmoid函数</h3><p>为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于<br>$$<br>exp(w_{k1}x_1+…+w_{kn}x_n)。<br>$$<br>因为一个数据点属于各类的概率之和为1，所以可以得到<br>$$<br>P(y=k)=\\frac{exp(\\sum_{i=1}^{n}w_{ki}{x_i})}{\\sum_{k’}exp(\\sum_{i=1}^{n}w_{k’i}x_i)}<br>$$<br>现在回到两类（0,1）的情况，此时分母上只有两项<br>$$<br>P(y=1)=\\frac{exp(\\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\\sum_{i=1}^{n}w_{1i}x_i)+exp(\\sum_{i=1}^{n}w_{0i}x_i)}<br>$$<br>公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有<br>$$<br>P(y=1)=\\frac{1}{1+exp(-\\sum_{i=1}^{n}w_ix_i)}<br>$$<br>上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。<br>$$<br>Sigmoid Function: f(x)=\\frac{1}{1+e^{-x}}<br>$$<br>Sigmoid函数具有如下性质</p>\n<ul>\n<li>函数连续且单调递增</li>\n<li>函数关于（0,0.5）对称</li>\n<li>$x\\in(-\\infty,\\infty)$时$y\\in(0,1)$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#plot sigmoid function </span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">##sigmoid function</span></span><br><span class=\"line\">x=np.arange(<span class=\"number\">-5</span>,<span class=\"number\">5</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">y=<span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(x,y,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"string\">'2'</span>)</span><br><span class=\"line\">ax=plt.gca()</span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'independent variable'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'dependent variable'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之Logistic回归/机器学习之Logistic回归01.png\" alt=\"机器学习之Logistic回归01\"></p>\n<h3 id=\"3-Logistic回归推导\"><a href=\"#3-Logistic回归推导\" class=\"headerlink\" title=\"3.Logistic回归推导\"></a>3.Logistic回归推导</h3><ul>\n<li>特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。</li>\n<li>$\\theta=(\\theta_0,\\theta_1,\\theta_2…\\theta_n)$</li>\n<li>$n$表示特征数量</li>\n<li>$m$表示训练数据数量</li>\n</ul>\n<p>线性回归函数为$z=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n=\\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。<br>$$<br>h(X)=\\frac{1}{1+e^{-\\theta^TX}}<br>$$<br>我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)&lt;0.5$是判断当前数据属于A类，当$h(X)&gt;0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\\theta$。</p>\n<p>条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为<br>$$<br>P(y=1|X)=\\pi(X)=\\frac{1}{1+e^{-\\theta^TX}}<br>$$<br>条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为<br>$$<br>P(y=0|X)=1-\\pi(X)=\\frac{1}{1+e^{\\theta^TX}}<br>$$<br>因此我们可以得到事件的发生比为<br>$$<br>odds=\\frac{P(y=1|X)}{P(y=0|X)}<br>$$<br>事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。</p>\n<p>各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数<br>$$<br>L(\\theta)=\\prod_{i=1}^{m}[\\pi(X_i)]^{y_i}[1-\\pi(X_i)]^{1-y_i}<br>$$<br>目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到<br>$$<br>lnL(\\theta)=\\sum_{i=1}^{m}\\left { y_iln[\\pi(X_i)] +(1-y_i)ln[1-\\pi(X_i)] \\right }<br>$$</p>\n<p>$$<br>=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_iln\\frac{\\pi(X_i)}{1-\\pi(X_i)}<br>$$</p>\n<p>$$<br>=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_i\\theta^TX<br>$$</p>\n<p>$$<br>=\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX<br>$$</p>\n<p>通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\\theta$。首先在前面乘上负的系数$-\\frac{1}{m}$，所以$J(\\theta)$最小时的$\\theta$为最佳参数。<br>$$<br>J(\\theta)=-\\frac{1}{m}lnL(\\theta)<br>$$</p>\n<p>$$<br>=-\\frac{1}{m}\\left {\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right}<br>$$</p>\n<h3 id=\"4-梯度下降算法\"><a href=\"#4-梯度下降算法\" class=\"headerlink\" title=\"4.梯度下降算法\"></a>4.梯度下降算法</h3><h4 id=\"4-1梯度下降算法简述\"><a href=\"#4-1梯度下降算法简述\" class=\"headerlink\" title=\"4.1梯度下降算法简述\"></a>4.1梯度下降算法简述</h4><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。</p>\n<p><img src=\"机器学习之Logistic回归/机器学习之Logistic回归02.png\" alt=\"器学习之Logistic回归0\"></p>\n<p>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p>\n<h4 id=\"4-2-梯度下降算法相关概念\"><a href=\"#4-2-梯度下降算法相关概念\" class=\"headerlink\" title=\"4.2 梯度下降算法相关概念\"></a>4.2 梯度下降算法相关概念</h4><p>求解梯度下降算法之前，我们先了解相关概念。</p>\n<ul>\n<li><strong>步长（Learning Rate）</strong>：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。</li>\n<li><strong>特征（Feature）</strong>：即上述描述的$X$</li>\n<li><strong>假设函数（Hypothesis Function）</strong>：监督学习中，为了拟合输入样本，而使用假设函数。</li>\n<li><strong>损失函数（Loss Function）</strong>：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为</li>\n</ul>\n<p>$$<br>J(\\theta)=-\\frac{1}{m}\\left {\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right}<br>$$</p>\n<p>我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。</p>\n<h4 id=\"4-3梯度下降算法过程\"><a href=\"#4-3梯度下降算法过程\" class=\"headerlink\" title=\"4.3梯度下降算法过程\"></a>4.3梯度下降算法过程</h4><ul>\n<li>随机选取一组$\\theta$。</li>\n<li>不断变化$\\theta$，让$J(\\theta)$变小，$\\alpha$为学习步长。</li>\n</ul>\n<p>$$<br>\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)<br>$$</p>\n<ul>\n<li>直到$J(\\theta)$得到最小值，$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)$为$J(\\theta)$对$\\theta_k$的偏导。</li>\n</ul>\n<p>$$<br>\\frac{\\partial J(\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{1+e^{\\theta^TX}}e^{\\theta^TX}X_{ij}-\\sum_{i=1}^{m}y_iX_{ij}<br>$$</p>\n<p>$$<br>=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\frac{e^{\\theta^TX}}{1+e^{\\theta^Tx}}-y_i]<br>$$</p>\n<p>$$<br>=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]<br>$$</p>\n<p>因此梯度下降算法的迭代最终表述为<br>$$<br>\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]<br>$$<br>梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到Logistic函数。</p>\n<h3 id=\"5-Logistic回归实现\"><a href=\"#5-Logistic回归实现\" class=\"headerlink\" title=\"5.Logistic回归实现\"></a>5.Logistic回归实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib.colors <span class=\"keyword\">import</span> ListedColormap</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_regions</span><span class=\"params\">(X, y, classifier, test_idx=None, resolution=<span class=\"number\">0.02</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># setup marker generator and color map</span></span><br><span class=\"line\">    markers = (<span class=\"string\">'s'</span>, <span class=\"string\">'x'</span>, <span class=\"string\">'o'</span>, <span class=\"string\">'^'</span>, <span class=\"string\">'v'</span>)</span><br><span class=\"line\">    colors = (<span class=\"string\">'red'</span>, <span class=\"string\">'blue'</span>, <span class=\"string\">'lightgreen'</span>, <span class=\"string\">'cyan'</span>, <span class=\"string\">'gray'</span>)</span><br><span class=\"line\">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class=\"line\">    <span class=\"comment\"># plot the decision surface</span></span><br><span class=\"line\">    x1_min, x1_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    x2_min, x2_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))</span><br><span class=\"line\">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class=\"line\">    Z = Z.reshape(xx1.shape)</span><br><span class=\"line\">    plt.contourf(xx1, xx2, Z, alpha=<span class=\"number\">0.4</span>, cmap=cmap)</span><br><span class=\"line\">    plt.xlim(xx1.min(), xx1.max())</span><br><span class=\"line\">    plt.ylim(xx2.min(), xx2.max())</span><br><span class=\"line\">    <span class=\"comment\"># plot class samples</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx, cl <span class=\"keyword\">in</span> enumerate(np.unique(y)):</span><br><span class=\"line\">        plt.scatter(x=X[y == cl, <span class=\"number\">0</span>], y=X[y == cl, <span class=\"number\">1</span>],alpha=<span class=\"number\">0.8</span>, c=cmap(idx),marker=markers[idx], label=cl)</span><br><span class=\"line\">    <span class=\"comment\"># highlight test samples</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> test_idx:</span><br><span class=\"line\">        X_test, y_test = X[test_idx, :], y[test_idx]</span><br><span class=\"line\">        plt.scatter(X_test[:, <span class=\"number\">0</span>], X_test[:, <span class=\"number\">1</span>], c=<span class=\"string\">'blue'</span>, alpha=<span class=\"number\">1.0</span>, linewidth=<span class=\"number\">1</span>, marker=<span class=\"string\">'o'</span>, s=<span class=\"number\">55</span>, label=<span class=\"string\">'test set'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data[:, [<span class=\"number\">2</span>, <span class=\"number\">3</span>]]</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.3</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"comment\">#为了追求机器学习的最佳性能，我们将特征缩放</span></span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit(X_train)<span class=\"comment\">#估算每个特征的平均值和标准差</span></span><br><span class=\"line\">X_train_std=sc.transform(X_train)<span class=\"comment\">#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性</span></span><br><span class=\"line\">X_test_std=sc.transform(X_test)</span><br><span class=\"line\">X_combined_std = np.vstack((X_train_std, X_test_std))</span><br><span class=\"line\">y_combined = np.hstack((y_train, y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练感知机模型</span></span><br><span class=\"line\">lr = LogisticRegression(C=<span class=\"number\">1000.0</span>,random_state=<span class=\"number\">0</span>)<span class=\"comment\">#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序</span></span><br><span class=\"line\">lr.fit(X_train_std, y_train)</span><br><span class=\"line\">lr.predict_proba(X_test_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘图</span></span><br><span class=\"line\">plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(<span class=\"number\">105</span>,<span class=\"number\">150</span>))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'petal length [standardized]'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'petal width [standardized]'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">'upper left'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之Logistic回归/机器学习之Logistic回归03.png\" alt=\"机器学习之Logistic回归03\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"机器学习之Logistic回归/推广.png\" alt=\"\"></p>\n<ul>\n<li>参考</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Logistic回归简介\"><a href=\"#1-Logistic回归简介\" class=\"headerlink\" title=\"1.Logistic回归简介\"></a>1.Logistic回归简介</h3><p>线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。</p>\n<h3 id=\"2-Sigmoid函数\"><a href=\"#2-Sigmoid函数\" class=\"headerlink\" title=\"2.Sigmoid函数\"></a>2.Sigmoid函数</h3><p>为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于<br>$$<br>exp(w_{k1}x_1+…+w_{kn}x_n)。<br>$$<br>因为一个数据点属于各类的概率之和为1，所以可以得到<br>$$<br>P(y=k)=\\frac{exp(\\sum_{i=1}^{n}w_{ki}{x_i})}{\\sum_{k’}exp(\\sum_{i=1}^{n}w_{k’i}x_i)}<br>$$<br>现在回到两类（0,1）的情况，此时分母上只有两项<br>$$<br>P(y=1)=\\frac{exp(\\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\\sum_{i=1}^{n}w_{1i}x_i)+exp(\\sum_{i=1}^{n}w_{0i}x_i)}<br>$$<br>公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有<br>$$<br>P(y=1)=\\frac{1}{1+exp(-\\sum_{i=1}^{n}w_ix_i)}<br>$$<br>上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。<br>$$<br>Sigmoid Function: f(x)=\\frac{1}{1+e^{-x}}<br>$$<br>Sigmoid函数具有如下性质</p>\n<ul>\n<li>函数连续且单调递增</li>\n<li>函数关于（0,0.5）对称</li>\n<li>$x\\in(-\\infty,\\infty)$时$y\\in(0,1)$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#plot sigmoid function </span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">##sigmoid function</span></span><br><span class=\"line\">x=np.arange(<span class=\"number\">-5</span>,<span class=\"number\">5</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">y=<span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#plot</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.plot(x,y,color=<span class=\"string\">'red'</span>,linewidth=<span class=\"string\">'2'</span>)</span><br><span class=\"line\">ax=plt.gca()</span><br><span class=\"line\">ax.spines[<span class=\"string\">'right'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'top'</span>].set_color(<span class=\"string\">'none'</span>)</span><br><span class=\"line\">ax.xaxis.set_ticks_position(<span class=\"string\">'bottom'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'bottom'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">ax.yaxis.set_ticks_position(<span class=\"string\">'left'</span>)</span><br><span class=\"line\">ax.spines[<span class=\"string\">'left'</span>].set_position((<span class=\"string\">'data'</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'independent variable'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'dependent variable'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之Logistic回归/机器学习之Logistic回归01.png\" alt=\"机器学习之Logistic回归01\"></p>\n<h3 id=\"3-Logistic回归推导\"><a href=\"#3-Logistic回归推导\" class=\"headerlink\" title=\"3.Logistic回归推导\"></a>3.Logistic回归推导</h3><ul>\n<li>特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。</li>\n<li>$\\theta=(\\theta_0,\\theta_1,\\theta_2…\\theta_n)$</li>\n<li>$n$表示特征数量</li>\n<li>$m$表示训练数据数量</li>\n</ul>\n<p>线性回归函数为$z=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n=\\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。<br>$$<br>h(X)=\\frac{1}{1+e^{-\\theta^TX}}<br>$$<br>我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)&lt;0.5$是判断当前数据属于A类，当$h(X)&gt;0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\\theta$。</p>\n<p>条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为<br>$$<br>P(y=1|X)=\\pi(X)=\\frac{1}{1+e^{-\\theta^TX}}<br>$$<br>条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为<br>$$<br>P(y=0|X)=1-\\pi(X)=\\frac{1}{1+e^{\\theta^TX}}<br>$$<br>因此我们可以得到事件的发生比为<br>$$<br>odds=\\frac{P(y=1|X)}{P(y=0|X)}<br>$$<br>事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。</p>\n<p>各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数<br>$$<br>L(\\theta)=\\prod_{i=1}^{m}[\\pi(X_i)]^{y_i}[1-\\pi(X_i)]^{1-y_i}<br>$$<br>目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到<br>$$<br>lnL(\\theta)=\\sum_{i=1}^{m}\\left { y_iln[\\pi(X_i)] +(1-y_i)ln[1-\\pi(X_i)] \\right }<br>$$</p>\n<p>$$<br>=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_iln\\frac{\\pi(X_i)}{1-\\pi(X_i)}<br>$$</p>\n<p>$$<br>=\\sum_{i=1}^{m}ln[1-\\pi(X_i)]+\\sum_{i=1}^{m}y_i\\theta^TX<br>$$</p>\n<p>$$<br>=\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX<br>$$</p>\n<p>通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\\theta$。首先在前面乘上负的系数$-\\frac{1}{m}$，所以$J(\\theta)$最小时的$\\theta$为最佳参数。<br>$$<br>J(\\theta)=-\\frac{1}{m}lnL(\\theta)<br>$$</p>\n<p>$$<br>=-\\frac{1}{m}\\left {\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right}<br>$$</p>\n<h3 id=\"4-梯度下降算法\"><a href=\"#4-梯度下降算法\" class=\"headerlink\" title=\"4.梯度下降算法\"></a>4.梯度下降算法</h3><h4 id=\"4-1梯度下降算法简述\"><a href=\"#4-1梯度下降算法简述\" class=\"headerlink\" title=\"4.1梯度下降算法简述\"></a>4.1梯度下降算法简述</h4><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。</p>\n<p><img src=\"机器学习之Logistic回归/机器学习之Logistic回归02.png\" alt=\"器学习之Logistic回归0\"></p>\n<p>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p>\n<h4 id=\"4-2-梯度下降算法相关概念\"><a href=\"#4-2-梯度下降算法相关概念\" class=\"headerlink\" title=\"4.2 梯度下降算法相关概念\"></a>4.2 梯度下降算法相关概念</h4><p>求解梯度下降算法之前，我们先了解相关概念。</p>\n<ul>\n<li><strong>步长（Learning Rate）</strong>：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。</li>\n<li><strong>特征（Feature）</strong>：即上述描述的$X$</li>\n<li><strong>假设函数（Hypothesis Function）</strong>：监督学习中，为了拟合输入样本，而使用假设函数。</li>\n<li><strong>损失函数（Loss Function）</strong>：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为</li>\n</ul>\n<p>$$<br>J(\\theta)=-\\frac{1}{m}\\left {\\sum_{i=1}^{m}-ln[1+e^{\\theta^Tx}]+\\sum_{i=1}^{m}y_i\\theta^TX \\right}<br>$$</p>\n<p>我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。</p>\n<h4 id=\"4-3梯度下降算法过程\"><a href=\"#4-3梯度下降算法过程\" class=\"headerlink\" title=\"4.3梯度下降算法过程\"></a>4.3梯度下降算法过程</h4><ul>\n<li>随机选取一组$\\theta$。</li>\n<li>不断变化$\\theta$，让$J(\\theta)$变小，$\\alpha$为学习步长。</li>\n</ul>\n<p>$$<br>\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)<br>$$</p>\n<ul>\n<li>直到$J(\\theta)$得到最小值，$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)$为$J(\\theta)$对$\\theta_k$的偏导。</li>\n</ul>\n<p>$$<br>\\frac{\\partial J(\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{1+e^{\\theta^TX}}e^{\\theta^TX}X_{ij}-\\sum_{i=1}^{m}y_iX_{ij}<br>$$</p>\n<p>$$<br>=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\frac{e^{\\theta^TX}}{1+e^{\\theta^Tx}}-y_i]<br>$$</p>\n<p>$$<br>=\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]<br>$$</p>\n<p>因此梯度下降算法的迭代最终表述为<br>$$<br>\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[\\pi(X_i)-y_i]<br>$$<br>梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到Logistic函数。</p>\n<h3 id=\"5-Logistic回归实现\"><a href=\"#5-Logistic回归实现\" class=\"headerlink\" title=\"5.Logistic回归实现\"></a>5.Logistic回归实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib.colors <span class=\"keyword\">import</span> ListedColormap</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_regions</span><span class=\"params\">(X, y, classifier, test_idx=None, resolution=<span class=\"number\">0.02</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># setup marker generator and color map</span></span><br><span class=\"line\">    markers = (<span class=\"string\">'s'</span>, <span class=\"string\">'x'</span>, <span class=\"string\">'o'</span>, <span class=\"string\">'^'</span>, <span class=\"string\">'v'</span>)</span><br><span class=\"line\">    colors = (<span class=\"string\">'red'</span>, <span class=\"string\">'blue'</span>, <span class=\"string\">'lightgreen'</span>, <span class=\"string\">'cyan'</span>, <span class=\"string\">'gray'</span>)</span><br><span class=\"line\">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class=\"line\">    <span class=\"comment\"># plot the decision surface</span></span><br><span class=\"line\">    x1_min, x1_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    x2_min, x2_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))</span><br><span class=\"line\">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class=\"line\">    Z = Z.reshape(xx1.shape)</span><br><span class=\"line\">    plt.contourf(xx1, xx2, Z, alpha=<span class=\"number\">0.4</span>, cmap=cmap)</span><br><span class=\"line\">    plt.xlim(xx1.min(), xx1.max())</span><br><span class=\"line\">    plt.ylim(xx2.min(), xx2.max())</span><br><span class=\"line\">    <span class=\"comment\"># plot class samples</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx, cl <span class=\"keyword\">in</span> enumerate(np.unique(y)):</span><br><span class=\"line\">        plt.scatter(x=X[y == cl, <span class=\"number\">0</span>], y=X[y == cl, <span class=\"number\">1</span>],alpha=<span class=\"number\">0.8</span>, c=cmap(idx),marker=markers[idx], label=cl)</span><br><span class=\"line\">    <span class=\"comment\"># highlight test samples</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> test_idx:</span><br><span class=\"line\">        X_test, y_test = X[test_idx, :], y[test_idx]</span><br><span class=\"line\">        plt.scatter(X_test[:, <span class=\"number\">0</span>], X_test[:, <span class=\"number\">1</span>], c=<span class=\"string\">'blue'</span>, alpha=<span class=\"number\">1.0</span>, linewidth=<span class=\"number\">1</span>, marker=<span class=\"string\">'o'</span>, s=<span class=\"number\">55</span>, label=<span class=\"string\">'test set'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data[:, [<span class=\"number\">2</span>, <span class=\"number\">3</span>]]</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.3</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"comment\">#为了追求机器学习的最佳性能，我们将特征缩放</span></span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit(X_train)<span class=\"comment\">#估算每个特征的平均值和标准差</span></span><br><span class=\"line\">X_train_std=sc.transform(X_train)<span class=\"comment\">#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性</span></span><br><span class=\"line\">X_test_std=sc.transform(X_test)</span><br><span class=\"line\">X_combined_std = np.vstack((X_train_std, X_test_std))</span><br><span class=\"line\">y_combined = np.hstack((y_train, y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练感知机模型</span></span><br><span class=\"line\">lr = LogisticRegression(C=<span class=\"number\">1000.0</span>,random_state=<span class=\"number\">0</span>)<span class=\"comment\">#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序</span></span><br><span class=\"line\">lr.fit(X_train_std, y_train)</span><br><span class=\"line\">lr.predict_proba(X_test_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘图</span></span><br><span class=\"line\">plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(<span class=\"number\">105</span>,<span class=\"number\">150</span>))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'petal length [standardized]'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'petal width [standardized]'</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">'upper left'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之Logistic回归/机器学习之Logistic回归03.png\" alt=\"机器学习之Logistic回归03\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"机器学习之Logistic回归/推广.png\" alt=\"\"></p>\n<ul>\n<li>参考</li>\n</ul>\n"},{"title":"机器学习之SVM支持向量机（一）","date":"2018-03-29T07:51:41.000Z","comment":true,"mathjax":true,"_content":"\n我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。\n\n![器学习之SVM支持向量机0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png)\n\n### 1.SVM损失函数\n\n针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。\n\n首先我们先复习下Logistic Regression Function\n$$\nh_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}\n$$\n如果$y=1$，我们希望$h_{\\theta}\\approx1$，那么$\\theta^Tx\\gg0$。如果$y=0$，我们希望$h_{\\theta}\\approx0$，那么$\\theta^Tx\\ll0$。我们以Logistic Regression为例\n\n$$\nLR Cost Example=-\\left( (ylogh_\\theta(x))+(1-y)log(1-h_\\theta(x))\\right)\n$$\n\n$$\n=-ylog\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})\n$$\n\n+ 当$y=1$时，此时$\\theta^Tx\\gg0$，上述公式为$-ylog\\frac{1}{1+e^{-\\theta^Tx}}$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。\n+ 当$y=0$时，此时$\\theta^Tx\\ll0$，上述公式为$-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。\n+ $cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。\n\n![幕快照 2018-04-02 下午9.57.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png)\n\nLogistic Regression的损失函数:\n$$\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}(-logh_{\\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n因此对于SVM，我们得到:\n$$\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\\lambda B$，我们通过设置$\\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\\frac{1}{\\lambda}$作用一致，因此我们便得到SVM的损失函数。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n\n### 2.最大间隔分类\n\nSVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n\n+ 当为正样本时$y=1$，我们希望$\\theta^Tx\\ge1$，而不是 $\\theta^Tx\\ge0$。\n+ 当为正样本时$y=0$，我们希望$\\theta^Tx\\le-1$，而不是$\\theta^Tx\\le0$。\n\n![幕快照 2018-04-02 下午10.00.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png)\n\n我们设**C为非常大的值**，例如1000000。\n\n+ 当$y^{(i)}=1$时，$\\theta^Tx^{(i)}\\ge1$，此时SVM损失函数中第一项为0。\n+ 当$y^{(i)}=0$时，$\\theta^Tx^{(i)}\\le-1$，此时SVM损失函数中第一项为0。\n\n那么我们便得到:\n$$\nminC*0+\\frac{1}{2}\\sum_{i=1}^{m}\\theta_j^2\n$$\n\n+ 约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。\n+ 约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。\n\nSVM是一个**最大间隔分类器**，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在**模块3**中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。\n\n![幕快照 2018-04-02 下午3.57.3](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png)\n\n我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。\n\n![幕快照 2018-04-02 下午4.02.5](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png)\n\n### 3.SVM最大间隔分类\n\n首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为\n$$\nu^Tv=||u||\\cdot||v||\\cdot cos\\theta=||u||\\cdot p=u_1v_1+u_2v_2\n$$\n![幕快照 2018-04-02 下午4.11.5](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png)\n\n现在我们来看SVM损失函数:\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n由于C设置的非常大，那么SVM损失函数为:\n$$\nmin_{\\theta} \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2\n$$\n\n- 约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。\n- 约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。\n\n下面我们举例说明SVM，为了简化，假设n=2,$\\theta_0=0$，我们得到\n$$\nmin\\frac{1}{2}\\sum_{j=1}^{2}\\theta_j^2=\\frac{1}{2}{(\\theta_1^2+\\theta_2^2)}=\\frac{1}{2}\\left (\\sqrt{\\theta_1^2+\\theta_2^2}  \\right )^2=\\frac{1}{2}||\\theta||^2\n$$\n我们来更深层次的理解$\\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为\n$$\n\\theta^Tx^{(i)}=p^{(i)}\\cdot||\\theta||=\\theta_1x_1^{(i)}+\\theta_2x_1^{(2)}\n$$\n$\\theta^Tx^{(i)}$我们可以利用$p^{(i)}\\cdot||\\theta||$表示，同时SVM随时函数目标是极小化$||\\theta^2||$。\n\n![幕快照 2018-04-02 下午4.52.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png)\n\n下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\\theta||$，并且要满足$p^{(i)}\\cdot ||\\theta||\\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\\theta||$更大，不满足最小化$||\\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\\theta||$便较小，满足最小化$||\\theta||$的需求。SVM便是通过最大化分类间隔来让$||\\theta||$更小，这便是SVM中为什么要最大化分类间隔。\n\n![幕快照 2018-04-02 下午5.14.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png)\n\n### 4.核函数\n\n上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,...$\n\n+ $f_1,f_2,f_3…$为提取出来的特征。\n+ 定义预测方程$h_{\\theta}(x)$为多项式sigmoid函数。$h_{\\theta}(x)=g(\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n)$，其中$f_n$为x的幂次项组合。\n+ 当$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n\\ge0$时$h_{\\theta}(x)=1$，否则$h_{\\theta}(x)=0$。\n\n![幕快照 2018-04-02 下午5.40.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png)\n\n那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。\n\n![幕快照 2018-04-02 下午5.53.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png)\n\nx和l越相似，f越接近于1。x和l相差越远，f越接近于0。\n\n![幕快照 2018-04-02 下午6.02.1](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png)\n\n下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。\n\n![幕快照 2018-04-02 下午6.36.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png)\n\n下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。\n\n+ 假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\\theta(\\theta_0,\\theta_1,\\theta_2,\\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\\theta$值在下面会详细介绍。\n+ 对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。\n\n![幕快照 2018-04-02 下午6.44.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png)\n\n### 5.SVM中Gaussian Kernel的使用\n\n上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。\n$$\nf_i=similarity(x,l^{(i)})=exp\\left ( -\\frac{||x-l^{(i)}||^2}{2\\sigma^2}\\right)\n$$\n我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。\n\n![幕快照 2018-04-02 下午7.16.3](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png)\n\n![幕快照 2018-04-02 下午7.17.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png)\n\n那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\\theta$，并根据该参数$\\theta$进行预测。\n\n+ $\\theta^Tf\\ge0$，预测 y=1。\n+ $\\theta^Tf\\le0$，预测y=0。\n\n如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。\n\n![幕快照 2018-04-02 下午7.26.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png)\n\n最后我们介绍下如何选取C和$\\sigma^2$。由于$C=\\frac{1}{\\lambda}$，所以\n\n+ C大，$\\lambda$小，overfit，产生low bias，high variance。\n+ C小，$\\lambda$大，underfoot，underfoot，产生high bias，low variance。\n\n对于方差$\\sigma^2$\n\n+ $\\sigma^2$大，x-f相似性图像较为扁平。\n+ $\\sigma^2小$，x-f相似性图像较为窄尖。\n\n![幕快照 2018-04-02 下午8.03.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png)\n\n通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。\n\n由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。\n\n### 5.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png)","source":"_posts/机器学习之SVM支持向量机（一）.md","raw":"---\ntitle: 机器学习之SVM支持向量机（一）\ndate: 2018-03-29 15:51:41\ntags: [机器学习,算法]\ncategories: 机器学习\ncomment: true\nmathjax: true\n---\n\n我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。\n\n![器学习之SVM支持向量机0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png)\n\n### 1.SVM损失函数\n\n针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。\n\n首先我们先复习下Logistic Regression Function\n$$\nh_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}\n$$\n如果$y=1$，我们希望$h_{\\theta}\\approx1$，那么$\\theta^Tx\\gg0$。如果$y=0$，我们希望$h_{\\theta}\\approx0$，那么$\\theta^Tx\\ll0$。我们以Logistic Regression为例\n\n$$\nLR Cost Example=-\\left( (ylogh_\\theta(x))+(1-y)log(1-h_\\theta(x))\\right)\n$$\n\n$$\n=-ylog\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})\n$$\n\n+ 当$y=1$时，此时$\\theta^Tx\\gg0$，上述公式为$-ylog\\frac{1}{1+e^{-\\theta^Tx}}$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。\n+ 当$y=0$时，此时$\\theta^Tx\\ll0$，上述公式为$-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。\n+ $cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。\n\n![幕快照 2018-04-02 下午9.57.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png)\n\nLogistic Regression的损失函数:\n$$\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}(-logh_{\\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n因此对于SVM，我们得到:\n$$\nmin_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\\lambda B$，我们通过设置$\\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\\frac{1}{\\lambda}$作用一致，因此我们便得到SVM的损失函数。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n\n### 2.最大间隔分类\n\nSVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n\n+ 当为正样本时$y=1$，我们希望$\\theta^Tx\\ge1$，而不是 $\\theta^Tx\\ge0$。\n+ 当为正样本时$y=0$，我们希望$\\theta^Tx\\le-1$，而不是$\\theta^Tx\\le0$。\n\n![幕快照 2018-04-02 下午10.00.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png)\n\n我们设**C为非常大的值**，例如1000000。\n\n+ 当$y^{(i)}=1$时，$\\theta^Tx^{(i)}\\ge1$，此时SVM损失函数中第一项为0。\n+ 当$y^{(i)}=0$时，$\\theta^Tx^{(i)}\\le-1$，此时SVM损失函数中第一项为0。\n\n那么我们便得到:\n$$\nminC*0+\\frac{1}{2}\\sum_{i=1}^{m}\\theta_j^2\n$$\n\n+ 约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。\n+ 约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。\n\nSVM是一个**最大间隔分类器**，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在**模块3**中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。\n\n![幕快照 2018-04-02 下午3.57.3](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png)\n\n我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。\n\n![幕快照 2018-04-02 下午4.02.5](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png)\n\n### 3.SVM最大间隔分类\n\n首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为\n$$\nu^Tv=||u||\\cdot||v||\\cdot cos\\theta=||u||\\cdot p=u_1v_1+u_2v_2\n$$\n![幕快照 2018-04-02 下午4.11.5](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png)\n\n现在我们来看SVM损失函数:\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n由于C设置的非常大，那么SVM损失函数为:\n$$\nmin_{\\theta} \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2\n$$\n\n- 约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。\n- 约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。\n\n下面我们举例说明SVM，为了简化，假设n=2,$\\theta_0=0$，我们得到\n$$\nmin\\frac{1}{2}\\sum_{j=1}^{2}\\theta_j^2=\\frac{1}{2}{(\\theta_1^2+\\theta_2^2)}=\\frac{1}{2}\\left (\\sqrt{\\theta_1^2+\\theta_2^2}  \\right )^2=\\frac{1}{2}||\\theta||^2\n$$\n我们来更深层次的理解$\\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为\n$$\n\\theta^Tx^{(i)}=p^{(i)}\\cdot||\\theta||=\\theta_1x_1^{(i)}+\\theta_2x_1^{(2)}\n$$\n$\\theta^Tx^{(i)}$我们可以利用$p^{(i)}\\cdot||\\theta||$表示，同时SVM随时函数目标是极小化$||\\theta^2||$。\n\n![幕快照 2018-04-02 下午4.52.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png)\n\n下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\\theta||$，并且要满足$p^{(i)}\\cdot ||\\theta||\\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\\theta||$更大，不满足最小化$||\\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\\theta||$便较小，满足最小化$||\\theta||$的需求。SVM便是通过最大化分类间隔来让$||\\theta||$更小，这便是SVM中为什么要最大化分类间隔。\n\n![幕快照 2018-04-02 下午5.14.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png)\n\n### 4.核函数\n\n上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,...$\n\n+ $f_1,f_2,f_3…$为提取出来的特征。\n+ 定义预测方程$h_{\\theta}(x)$为多项式sigmoid函数。$h_{\\theta}(x)=g(\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n)$，其中$f_n$为x的幂次项组合。\n+ 当$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n\\ge0$时$h_{\\theta}(x)=1$，否则$h_{\\theta}(x)=0$。\n\n![幕快照 2018-04-02 下午5.40.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png)\n\n那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。\n\n![幕快照 2018-04-02 下午5.53.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png)\n\nx和l越相似，f越接近于1。x和l相差越远，f越接近于0。\n\n![幕快照 2018-04-02 下午6.02.1](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png)\n\n下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。\n\n![幕快照 2018-04-02 下午6.36.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png)\n\n下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。\n\n+ 假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\\theta(\\theta_0,\\theta_1,\\theta_2,\\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\\theta$值在下面会详细介绍。\n+ 对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。\n\n![幕快照 2018-04-02 下午6.44.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png)\n\n### 5.SVM中Gaussian Kernel的使用\n\n上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。\n$$\nf_i=similarity(x,l^{(i)})=exp\\left ( -\\frac{||x-l^{(i)}||^2}{2\\sigma^2}\\right)\n$$\n我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。\n\n![幕快照 2018-04-02 下午7.16.3](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png)\n\n![幕快照 2018-04-02 下午7.17.2](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png)\n\n那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\\theta$，并根据该参数$\\theta$进行预测。\n\n+ $\\theta^Tf\\ge0$，预测 y=1。\n+ $\\theta^Tf\\le0$，预测y=0。\n\n如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。\n\n![幕快照 2018-04-02 下午7.26.4](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png)\n\n最后我们介绍下如何选取C和$\\sigma^2$。由于$C=\\frac{1}{\\lambda}$，所以\n\n+ C大，$\\lambda$小，overfit，产生low bias，high variance。\n+ C小，$\\lambda$大，underfoot，underfoot，产生high bias，low variance。\n\n对于方差$\\sigma^2$\n\n+ $\\sigma^2$大，x-f相似性图像较为扁平。\n+ $\\sigma^2小$，x-f相似性图像较为窄尖。\n\n![幕快照 2018-04-02 下午8.03.0](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png)\n\n通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。\n\n由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。\n\n### 5.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![](机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png)","slug":"机器学习之SVM支持向量机（一）","published":1,"updated":"2018-04-03T02:52:42.498Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji4rdzds000p2e012599lzfd","content":"<p>我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png\" alt=\"器学习之SVM支持向量机0\"></p>\n<h3 id=\"1-SVM损失函数\"><a href=\"#1-SVM损失函数\" class=\"headerlink\" title=\"1.SVM损失函数\"></a>1.SVM损失函数</h3><p>针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。</p>\n<p>首先我们先复习下Logistic Regression Function<br>$$<br>h_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}<br>$$<br>如果$y=1$，我们希望$h_{\\theta}\\approx1$，那么$\\theta^Tx\\gg0$。如果$y=0$，我们希望$h_{\\theta}\\approx0$，那么$\\theta^Tx\\ll0$。我们以Logistic Regression为例</p>\n<p>$$<br>LR Cost Example=-\\left( (ylogh_\\theta(x))+(1-y)log(1-h_\\theta(x))\\right)<br>$$</p>\n<p>$$<br>=-ylog\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})<br>$$</p>\n<ul>\n<li>当$y=1$时，此时$\\theta^Tx\\gg0$，上述公式为$-ylog\\frac{1}{1+e^{-\\theta^Tx}}$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。</li>\n<li>当$y=0$时，此时$\\theta^Tx\\ll0$，上述公式为$-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。</li>\n<li>$cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png\" alt=\"幕快照 2018-04-02 下午9.57.2\"></p>\n<p>Logistic Regression的损失函数:<br>$$<br>min_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}(-logh_{\\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$<br>因此对于SVM，我们得到:<br>$$<br>min_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$<br>因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\\lambda B$，我们通过设置$\\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\\frac{1}{\\lambda}$作用一致，因此我们便得到SVM的损失函数。<br>$$<br>min_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$</p>\n<h3 id=\"2-最大间隔分类\"><a href=\"#2-最大间隔分类\" class=\"headerlink\" title=\"2.最大间隔分类\"></a>2.最大间隔分类</h3><p>SVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。<br>$$<br>min_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$</p>\n<ul>\n<li>当为正样本时$y=1$，我们希望$\\theta^Tx\\ge1$，而不是 $\\theta^Tx\\ge0$。</li>\n<li>当为正样本时$y=0$，我们希望$\\theta^Tx\\le-1$，而不是$\\theta^Tx\\le0$。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png\" alt=\"幕快照 2018-04-02 下午10.00.0\"></p>\n<p>我们设<strong>C为非常大的值</strong>，例如1000000。</p>\n<ul>\n<li>当$y^{(i)}=1$时，$\\theta^Tx^{(i)}\\ge1$，此时SVM损失函数中第一项为0。</li>\n<li>当$y^{(i)}=0$时，$\\theta^Tx^{(i)}\\le-1$，此时SVM损失函数中第一项为0。</li>\n</ul>\n<p>那么我们便得到:<br>$$<br>minC*0+\\frac{1}{2}\\sum_{i=1}^{m}\\theta_j^2<br>$$</p>\n<ul>\n<li>约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。</li>\n<li>约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。</li>\n</ul>\n<p>SVM是一个<strong>最大间隔分类器</strong>，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在<strong>模块3</strong>中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png\" alt=\"幕快照 2018-04-02 下午3.57.3\"></p>\n<p>我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png\" alt=\"幕快照 2018-04-02 下午4.02.5\"></p>\n<h3 id=\"3-SVM最大间隔分类\"><a href=\"#3-SVM最大间隔分类\" class=\"headerlink\" title=\"3.SVM最大间隔分类\"></a>3.SVM最大间隔分类</h3><p>首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为<br>$$<br>u^Tv=||u||\\cdot||v||\\cdot cos\\theta=||u||\\cdot p=u_1v_1+u_2v_2<br>$$<br><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png\" alt=\"幕快照 2018-04-02 下午4.11.5\"></p>\n<p>现在我们来看SVM损失函数:<br>$$<br>min_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$<br>由于C设置的非常大，那么SVM损失函数为:<br>$$<br>min_{\\theta} \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2<br>$$</p>\n<ul>\n<li>约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。</li>\n<li>约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。</li>\n</ul>\n<p>下面我们举例说明SVM，为了简化，假设n=2,$\\theta_0=0$，我们得到<br>$$<br>min\\frac{1}{2}\\sum_{j=1}^{2}\\theta_j^2=\\frac{1}{2}{(\\theta_1^2+\\theta_2^2)}=\\frac{1}{2}\\left (\\sqrt{\\theta_1^2+\\theta_2^2}  \\right )^2=\\frac{1}{2}||\\theta||^2<br>$$<br>我们来更深层次的理解$\\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为<br>$$<br>\\theta^Tx^{(i)}=p^{(i)}\\cdot||\\theta||=\\theta_1x_1^{(i)}+\\theta_2x_1^{(2)}<br>$$<br>$\\theta^Tx^{(i)}$我们可以利用$p^{(i)}\\cdot||\\theta||$表示，同时SVM随时函数目标是极小化$||\\theta^2||$。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png\" alt=\"幕快照 2018-04-02 下午4.52.4\"></p>\n<p>下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\\theta||$，并且要满足$p^{(i)}\\cdot ||\\theta||\\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\\theta||$更大，不满足最小化$||\\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\\theta||$便较小，满足最小化$||\\theta||$的需求。SVM便是通过最大化分类间隔来让$||\\theta||$更小，这便是SVM中为什么要最大化分类间隔。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png\" alt=\"幕快照 2018-04-02 下午5.14.2\"></p>\n<h3 id=\"4-核函数\"><a href=\"#4-核函数\" class=\"headerlink\" title=\"4.核函数\"></a>4.核函数</h3><p>上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,…$</p>\n<ul>\n<li>$f_1,f_2,f_3…$为提取出来的特征。</li>\n<li>定义预测方程$h_{\\theta}(x)$为多项式sigmoid函数。$h_{\\theta}(x)=g(\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n)$，其中$f_n$为x的幂次项组合。</li>\n<li>当$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n\\ge0$时$h_{\\theta}(x)=1$，否则$h_{\\theta}(x)=0$。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png\" alt=\"幕快照 2018-04-02 下午5.40.4\"></p>\n<p>那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png\" alt=\"幕快照 2018-04-02 下午5.53.4\"></p>\n<p>x和l越相似，f越接近于1。x和l相差越远，f越接近于0。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png\" alt=\"幕快照 2018-04-02 下午6.02.1\"></p>\n<p>下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png\" alt=\"幕快照 2018-04-02 下午6.36.0\"></p>\n<p>下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。</p>\n<ul>\n<li>假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\\theta(\\theta_0,\\theta_1,\\theta_2,\\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\\theta$值在下面会详细介绍。</li>\n<li>对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png\" alt=\"幕快照 2018-04-02 下午6.44.2\"></p>\n<h3 id=\"5-SVM中Gaussian-Kernel的使用\"><a href=\"#5-SVM中Gaussian-Kernel的使用\" class=\"headerlink\" title=\"5.SVM中Gaussian Kernel的使用\"></a>5.SVM中Gaussian Kernel的使用</h3><p>上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。<br>$$<br>f_i=similarity(x,l^{(i)})=exp\\left ( -\\frac{||x-l^{(i)}||^2}{2\\sigma^2}\\right)<br>$$<br>我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png\" alt=\"幕快照 2018-04-02 下午7.16.3\"></p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png\" alt=\"幕快照 2018-04-02 下午7.17.2\"></p>\n<p>那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\\theta$，并根据该参数$\\theta$进行预测。</p>\n<ul>\n<li>$\\theta^Tf\\ge0$，预测 y=1。</li>\n<li>$\\theta^Tf\\le0$，预测y=0。</li>\n</ul>\n<p>如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png\" alt=\"幕快照 2018-04-02 下午7.26.4\"></p>\n<p>最后我们介绍下如何选取C和$\\sigma^2$。由于$C=\\frac{1}{\\lambda}$，所以</p>\n<ul>\n<li>C大，$\\lambda$小，overfit，产生low bias，high variance。</li>\n<li>C小，$\\lambda$大，underfoot，underfoot，产生high bias，low variance。</li>\n</ul>\n<p>对于方差$\\sigma^2$</p>\n<ul>\n<li>$\\sigma^2$大，x-f相似性图像较为扁平。</li>\n<li>$\\sigma^2小$，x-f相似性图像较为窄尖。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png\" alt=\"幕快照 2018-04-02 下午8.03.0\"></p>\n<p>通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。</p>\n<p>由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。</p>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png\" alt=\"器学习之SVM支持向量机0\"></p>\n<h3 id=\"1-SVM损失函数\"><a href=\"#1-SVM损失函数\" class=\"headerlink\" title=\"1.SVM损失函数\"></a>1.SVM损失函数</h3><p>针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。</p>\n<p>首先我们先复习下Logistic Regression Function<br>$$<br>h_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}<br>$$<br>如果$y=1$，我们希望$h_{\\theta}\\approx1$，那么$\\theta^Tx\\gg0$。如果$y=0$，我们希望$h_{\\theta}\\approx0$，那么$\\theta^Tx\\ll0$。我们以Logistic Regression为例</p>\n<p>$$<br>LR Cost Example=-\\left( (ylogh_\\theta(x))+(1-y)log(1-h_\\theta(x))\\right)<br>$$</p>\n<p>$$<br>=-ylog\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})<br>$$</p>\n<ul>\n<li>当$y=1$时，此时$\\theta^Tx\\gg0$，上述公式为$-ylog\\frac{1}{1+e^{-\\theta^Tx}}$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。</li>\n<li>当$y=0$时，此时$\\theta^Tx\\ll0$，上述公式为$-(1-y)log(1-\\frac{1}{1+e^{-\\theta^Tx}})$，其中$z=\\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。</li>\n<li>$cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png\" alt=\"幕快照 2018-04-02 下午9.57.2\"></p>\n<p>Logistic Regression的损失函数:<br>$$<br>min_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}(-logh_{\\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$<br>因此对于SVM，我们得到:<br>$$<br>min_{\\theta}\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$<br>因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\\lambda B$，我们通过设置$\\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\\frac{1}{\\lambda}$作用一致，因此我们便得到SVM的损失函数。<br>$$<br>min_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$</p>\n<h3 id=\"2-最大间隔分类\"><a href=\"#2-最大间隔分类\" class=\"headerlink\" title=\"2.最大间隔分类\"></a>2.最大间隔分类</h3><p>SVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。<br>$$<br>min_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$</p>\n<ul>\n<li>当为正样本时$y=1$，我们希望$\\theta^Tx\\ge1$，而不是 $\\theta^Tx\\ge0$。</li>\n<li>当为正样本时$y=0$，我们希望$\\theta^Tx\\le-1$，而不是$\\theta^Tx\\le0$。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png\" alt=\"幕快照 2018-04-02 下午10.00.0\"></p>\n<p>我们设<strong>C为非常大的值</strong>，例如1000000。</p>\n<ul>\n<li>当$y^{(i)}=1$时，$\\theta^Tx^{(i)}\\ge1$，此时SVM损失函数中第一项为0。</li>\n<li>当$y^{(i)}=0$时，$\\theta^Tx^{(i)}\\le-1$，此时SVM损失函数中第一项为0。</li>\n</ul>\n<p>那么我们便得到:<br>$$<br>minC*0+\\frac{1}{2}\\sum_{i=1}^{m}\\theta_j^2<br>$$</p>\n<ul>\n<li>约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。</li>\n<li>约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。</li>\n</ul>\n<p>SVM是一个<strong>最大间隔分类器</strong>，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在<strong>模块3</strong>中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png\" alt=\"幕快照 2018-04-02 下午3.57.3\"></p>\n<p>我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png\" alt=\"幕快照 2018-04-02 下午4.02.5\"></p>\n<h3 id=\"3-SVM最大间隔分类\"><a href=\"#3-SVM最大间隔分类\" class=\"headerlink\" title=\"3.SVM最大间隔分类\"></a>3.SVM最大间隔分类</h3><p>首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为<br>$$<br>u^Tv=||u||\\cdot||v||\\cdot cos\\theta=||u||\\cdot p=u_1v_1+u_2v_2<br>$$<br><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png\" alt=\"幕快照 2018-04-02 下午4.11.5\"></p>\n<p>现在我们来看SVM损失函数:<br>$$<br>min_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$<br>由于C设置的非常大，那么SVM损失函数为:<br>$$<br>min_{\\theta} \\frac{1}{2}\\sum_{j=1}^{n}\\theta_j^2<br>$$</p>\n<ul>\n<li>约束条件1:如果$y^{(i)}=1$，$\\theta^Tx^{(i)}\\ge1$。</li>\n<li>约束条件2:如果$y^{(i)}=0$，$\\theta^Tx^{(i)}\\le-1$。</li>\n</ul>\n<p>下面我们举例说明SVM，为了简化，假设n=2,$\\theta_0=0$，我们得到<br>$$<br>min\\frac{1}{2}\\sum_{j=1}^{2}\\theta_j^2=\\frac{1}{2}{(\\theta_1^2+\\theta_2^2)}=\\frac{1}{2}\\left (\\sqrt{\\theta_1^2+\\theta_2^2}  \\right )^2=\\frac{1}{2}||\\theta||^2<br>$$<br>我们来更深层次的理解$\\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为<br>$$<br>\\theta^Tx^{(i)}=p^{(i)}\\cdot||\\theta||=\\theta_1x_1^{(i)}+\\theta_2x_1^{(2)}<br>$$<br>$\\theta^Tx^{(i)}$我们可以利用$p^{(i)}\\cdot||\\theta||$表示，同时SVM随时函数目标是极小化$||\\theta^2||$。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png\" alt=\"幕快照 2018-04-02 下午4.52.4\"></p>\n<p>下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\\theta||$，并且要满足$p^{(i)}\\cdot ||\\theta||\\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\\theta||$更大，不满足最小化$||\\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\\theta||$便较小，满足最小化$||\\theta||$的需求。SVM便是通过最大化分类间隔来让$||\\theta||$更小，这便是SVM中为什么要最大化分类间隔。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png\" alt=\"幕快照 2018-04-02 下午5.14.2\"></p>\n<h3 id=\"4-核函数\"><a href=\"#4-核函数\" class=\"headerlink\" title=\"4.核函数\"></a>4.核函数</h3><p>上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,…$</p>\n<ul>\n<li>$f_1,f_2,f_3…$为提取出来的特征。</li>\n<li>定义预测方程$h_{\\theta}(x)$为多项式sigmoid函数。$h_{\\theta}(x)=g(\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n)$，其中$f_n$为x的幂次项组合。</li>\n<li>当$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n\\ge0$时$h_{\\theta}(x)=1$，否则$h_{\\theta}(x)=0$。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png\" alt=\"幕快照 2018-04-02 下午5.40.4\"></p>\n<p>那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png\" alt=\"幕快照 2018-04-02 下午5.53.4\"></p>\n<p>x和l越相似，f越接近于1。x和l相差越远，f越接近于0。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png\" alt=\"幕快照 2018-04-02 下午6.02.1\"></p>\n<p>下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png\" alt=\"幕快照 2018-04-02 下午6.36.0\"></p>\n<p>下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。</p>\n<ul>\n<li>假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\\theta(\\theta_0,\\theta_1,\\theta_2,\\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\\theta$值在下面会详细介绍。</li>\n<li>对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\\theta_0f_0+\\theta_1f_1+\\theta_2f_2+…+\\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png\" alt=\"幕快照 2018-04-02 下午6.44.2\"></p>\n<h3 id=\"5-SVM中Gaussian-Kernel的使用\"><a href=\"#5-SVM中Gaussian-Kernel的使用\" class=\"headerlink\" title=\"5.SVM中Gaussian Kernel的使用\"></a>5.SVM中Gaussian Kernel的使用</h3><p>上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。<br>$$<br>f_i=similarity(x,l^{(i)})=exp\\left ( -\\frac{||x-l^{(i)}||^2}{2\\sigma^2}\\right)<br>$$<br>我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png\" alt=\"幕快照 2018-04-02 下午7.16.3\"></p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png\" alt=\"幕快照 2018-04-02 下午7.17.2\"></p>\n<p>那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\\theta$，并根据该参数$\\theta$进行预测。</p>\n<ul>\n<li>$\\theta^Tf\\ge0$，预测 y=1。</li>\n<li>$\\theta^Tf\\le0$，预测y=0。</li>\n</ul>\n<p>如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png\" alt=\"幕快照 2018-04-02 下午7.26.4\"></p>\n<p>最后我们介绍下如何选取C和$\\sigma^2$。由于$C=\\frac{1}{\\lambda}$，所以</p>\n<ul>\n<li>C大，$\\lambda$小，overfit，产生low bias，high variance。</li>\n<li>C小，$\\lambda$大，underfoot，underfoot，产生high bias，low variance。</li>\n</ul>\n<p>对于方差$\\sigma^2$</p>\n<ul>\n<li>$\\sigma^2$大，x-f相似性图像较为扁平。</li>\n<li>$\\sigma^2小$，x-f相似性图像较为窄尖。</li>\n</ul>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png\" alt=\"幕快照 2018-04-02 下午8.03.0\"></p>\n<p>通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。</p>\n<p>由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。</p>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png\" alt=\"\"></p>\n"},{"title":"机器学习之SVM支持向量机（二）","date":"2018-04-04T13:06:26.000Z","mathjax":true,"comments":1,"_content":"\n### 1.知识回顾\n\n[机器学习之SVM支持向量机（一）](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483818&idx=1&sn=50c634d8b00877134558125c4a718fd7&chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd)中我们介绍了**SVM损失函数**、**最大间隔分类**、**为什么SVM能形成最大间隔分类器**、**核函数**、**SVM中Gaussian Kernel的使用**知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。\n\n### 2.函数间隔和几何间隔\n\n上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示：\n\n![机器学习之SVM支持向量机（二）图像01](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F01.png)\n\n在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。\n\n定义函数间隔$\\hat{\\gamma}$：\n$$\n\\hat{\\gamma}=y(w^Tx+b)=yf(x)\n$$\n而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔：\n$$\n\\hat{\\gamma}=min\\hat{\\gamma} \n$$\n但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。\n\n但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。\n\n![机器学习之SVM支持向量机（二）图像02](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F02.png)\n$$\nx=x_0+\\gamma\\frac{w}{||w||}\n$$\n其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到：\n$$\n\\gamma=\\frac{w^T+b}{||w||}=\\frac{f(x)}{||w||}\n$$\n为了得到$\\gamma$绝对值，将$\\gamma$乘上相应类别$y$，即可得到几何间隔：\n$$\n\\tilde{r}=yr=\\frac{\\hat{\\gamma}}{||w||}\n$$\n从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y*(w^Tx+b)=y*f(x)$，实际上就是$|f(x)|$，几何间隔$\\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。\n\n对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\\tilde{\\gamma}$。同时需满足如下条件\n$$\ny_i(w^Tx_i+b)=\\hat{\\gamma_i}\\ge\\hat{\\gamma},i=1,2,3,...,n\n$$\n此处令函数间隔$\\hat{\\gamma}$等于1（之所以令$\\hat{\\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成：\n$$\nmax\\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n\n$$\n\n### 3.原始问题到对偶问题的求解\n\n接着考虑我们之前的目标函数，由于求$\\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以目标函数转换为\n$$\nmin\\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n\n$$\n现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。\n\n那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\\alpha$，定义拉格朗日函数为：\n$$\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)\n$$\n然后令：\n$$\n\\theta(w)=\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)\n$$\n当某个条件不满足时，例如$y_i(w^Tx+b)<1$，那么有$\\theta(w)=\\infty$。而当所有约束条件都满足时，则有$\\theta(w)=\\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为：\n$$\n\\min_{w,b}\\theta(w)=\\min_{w,b}\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)=p^*\n$$\n这里用$p^*$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下：\n$$\n\\max_{\\alpha_i\\ge0}\\min_{w,b}L(w,b,\\alpha)=d^*\n$$\n交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^*$表示，而且有$d^*\\le p^*$，在**满足某些条件**的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。\n\n此处**满足某些条件**的情况下，两者等价，此处的**满足某些条件**便是满足KKT条件。KKT最优化数学模型表示成下列标准形式:\n$$\n\\min f(x)\n$$\n\n$$\ns.t. h_j(x)=0,j=1,2,3,...,p\n$$\n\n$$\ng_k(x)\\le0,k=1,2,3,...,q\n$$\n\n$$\nx\\in X\\subset R^n\n$$\n\n其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。\n\n> 凸优化概念:$X\\subset R^n$为一凸集，$f:X->R$为一凸函数。凸优化便是寻找一点$x^*\\in X$，是的每一$x\\in X$满足$f(x^*)\\le f(x)$。\n>\n> KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。\n\nKKT条件就是上面最优化数学模型的标准形式中的最小点$x^*$必须满足下面的条件:\n$$\nh_j(x^*)=0,j=1,2,3,...,p\n$$\n\n$$\ng_k(x^*)\\le 0,k=1,2,3,...,q\n$$\n\n$$\n\\nabla f(x^*)+\\sum_{j=1}^{p}\\lambda_j\\nabla h_j(x^*)+\\sum_{k=1}^{q}\\mu_k \\nabla g_k(x^*)=0\n$$\n\n$$\n\\lambda_j \\neq0,\\mu \\ge0,\\mu_k g_k(x^*)=0\n$$\n\n此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\\alpha)$关于$w,b$的最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。\n\n首先固定$\\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。\n$$\n\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w= \\sum_{i=1}^{n}\\alpha_iy_ix_i\n$$\n\n$$\n\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n}\\alpha_iy_i=0\n$$\n\n将上述结果代入到之前的L得到：\n$$\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)\n$$\n\n$$\n=\\frac{1}{2}w^Tw-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_iy_ix_i)^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}\\sum_{i=1}^{n}\\alpha_iy_ix_i^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\n$$\n\n然后求对$\\alpha$的极大，即是关于对偶问题的最优化问题。\n$$\n\\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j\n$$\n\n$$\ns.t.,\\alpha_i\\ge0,i=1,2,3,...,n\n$$\n\n$$\n\\sum_{i=1}^{n}\\alpha_iy_i=0\n$$\n\n我们已经知道$x_i,x_j$的值，便可利用**SMO算法**求解$\\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。\n$$\nb^*=-\\frac{\\max_{i:y(i)=-1}w^Tx_i+\\min_{i:y(i)=1}w^Tx_i}{2}\n$$\n至此我们便可得出分类超平面和分类决策函数。\n\n### 4.松弛变量处理outliers方法\n\n实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。\n\n![机器学习之SVM支持向量机（二）图像03](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F03.png)\n\n为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即\n$$\ny_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n\n$$\n其中$\\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\\varepsilon_i$的总和也要尽量小。\n$$\n\\min\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i\n$$\n\n$$\ns.t.,y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n\n$$\n\n$$\n\\varepsilon_i\\ge0,i=1,2,3,...,n\n$$\n\n此处和[机器学习之SVM支持向量机（一）](https://weizhixiaoyi.com/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/)中的损失函数不同的是加入$\\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\\varepsilon_i$。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示:\n$$\nL(w,b,\\varepsilon,\\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1+\\varepsilon_i)-\\sum_{i=1}^{n}r_i\\varepsilon_i\n$$\n分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。\n\n### 5.Sklearn实现SVM支持向量机\n\n我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在**机器学习之SVM支持向量机（一）**中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇**SVM核函数的应用**。\n\n#### 5.1线性\n\n```Python\nfrom sklearn import svm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nx=np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]#正态分布产生数字20行2列\ny=[0]*20+[1]*20#20个class0,20个class1\nclf=svm.SVC(kernel='linear')#使用线性核\nclf.fit(x,y)\nw=clf.coef_[0]#获取w\na=-w[0]/w[1]#斜率\n\n#画图\nxx=np.linspace(-5,5)\nyy=a*xx-(clf.intercept_[0])/w[1]\nb=clf.support_vectors_[0]\nyy_down=a*xx+(b[1]-a*b[0])\nb=clf.support_vectors_[-1]\nyy_up=a*xx+(b[1]-a*b[0])\nplt.figure(figsize=(8,4))\nplt.plot(xx,yy)\nplt.plot(xx,yy_down)\nplt.plot(xx,yy_up)\nplt.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=80)\nplt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.Paired)\nplt.axis('tight')\nplt.show()\n```\n\n![机器学习之SVM支持向量机（二）图像05](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png)\n\n#### 5.2非线性\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import  Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nX, y = make_moons( n_samples=100, noise=0.15, random_state=42 )\n\ndef plot_dataset(X, y, axes):\n    plt.plot( X[:,0][y==0], X[:,1][y==0], \"bs\" )\n    plt.plot( X[:,0][y==1], X[:,1][y==1], \"g^\" )\n    plt.axis( axes )\n    plt.grid( True, which=\"both\" )\n    plt.xlabel(r\"$x_l$\")\n    plt.ylabel(r\"$x_2$\")\n\n# contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）\ndef plot_predict(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid( x0s, x1s )\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict( X ).reshape( x0.shape )\n    y_decision = clf.decision_function( X ).reshape( x0.shape )\n    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=0.5 )\n    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=0.2 )\n\npolynomial_svm_clf = Pipeline([ (\"poly_featutres\", PolynomialFeatures(degree=3)),\n                                (\"scaler\", StandardScaler()),\n                                (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42)  )\n                            ])#多项式核函数\npolynomial_svm_clf.fit( X, y )\nplot_dataset( X, y, [-1.5, 2.5, -1, 1.5] )\nplot_predict( polynomial_svm_clf, [-1.5, 2.5, -1, 1.5] )\nplt.show()\n```\n\n![机器学习之SVM支持向量机（二）图像06](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png)\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![机器学习之SVM支持向量机（二）图片推广](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png)\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/机器学习之SVM支持向量机（二）.md","raw":"---\ntitle: 机器学习之SVM支持向量机（二）\ndate: 2018-04-04 21:06:26\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.知识回顾\n\n[机器学习之SVM支持向量机（一）](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483818&idx=1&sn=50c634d8b00877134558125c4a718fd7&chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd)中我们介绍了**SVM损失函数**、**最大间隔分类**、**为什么SVM能形成最大间隔分类器**、**核函数**、**SVM中Gaussian Kernel的使用**知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。\n\n### 2.函数间隔和几何间隔\n\n上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示：\n\n![机器学习之SVM支持向量机（二）图像01](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F01.png)\n\n在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。\n\n定义函数间隔$\\hat{\\gamma}$：\n$$\n\\hat{\\gamma}=y(w^Tx+b)=yf(x)\n$$\n而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔：\n$$\n\\hat{\\gamma}=min\\hat{\\gamma} \n$$\n但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。\n\n但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。\n\n![机器学习之SVM支持向量机（二）图像02](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F02.png)\n$$\nx=x_0+\\gamma\\frac{w}{||w||}\n$$\n其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到：\n$$\n\\gamma=\\frac{w^T+b}{||w||}=\\frac{f(x)}{||w||}\n$$\n为了得到$\\gamma$绝对值，将$\\gamma$乘上相应类别$y$，即可得到几何间隔：\n$$\n\\tilde{r}=yr=\\frac{\\hat{\\gamma}}{||w||}\n$$\n从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y*(w^Tx+b)=y*f(x)$，实际上就是$|f(x)|$，几何间隔$\\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。\n\n对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\\tilde{\\gamma}$。同时需满足如下条件\n$$\ny_i(w^Tx_i+b)=\\hat{\\gamma_i}\\ge\\hat{\\gamma},i=1,2,3,...,n\n$$\n此处令函数间隔$\\hat{\\gamma}$等于1（之所以令$\\hat{\\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成：\n$$\nmax\\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n\n$$\n\n### 3.原始问题到对偶问题的求解\n\n接着考虑我们之前的目标函数，由于求$\\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以目标函数转换为\n$$\nmin\\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,...,n\n$$\n现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。\n\n那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\\alpha$，定义拉格朗日函数为：\n$$\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)\n$$\n然后令：\n$$\n\\theta(w)=\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)\n$$\n当某个条件不满足时，例如$y_i(w^Tx+b)<1$，那么有$\\theta(w)=\\infty$。而当所有约束条件都满足时，则有$\\theta(w)=\\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为：\n$$\n\\min_{w,b}\\theta(w)=\\min_{w,b}\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)=p^*\n$$\n这里用$p^*$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下：\n$$\n\\max_{\\alpha_i\\ge0}\\min_{w,b}L(w,b,\\alpha)=d^*\n$$\n交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^*$表示，而且有$d^*\\le p^*$，在**满足某些条件**的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。\n\n此处**满足某些条件**的情况下，两者等价，此处的**满足某些条件**便是满足KKT条件。KKT最优化数学模型表示成下列标准形式:\n$$\n\\min f(x)\n$$\n\n$$\ns.t. h_j(x)=0,j=1,2,3,...,p\n$$\n\n$$\ng_k(x)\\le0,k=1,2,3,...,q\n$$\n\n$$\nx\\in X\\subset R^n\n$$\n\n其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。\n\n> 凸优化概念:$X\\subset R^n$为一凸集，$f:X->R$为一凸函数。凸优化便是寻找一点$x^*\\in X$，是的每一$x\\in X$满足$f(x^*)\\le f(x)$。\n>\n> KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。\n\nKKT条件就是上面最优化数学模型的标准形式中的最小点$x^*$必须满足下面的条件:\n$$\nh_j(x^*)=0,j=1,2,3,...,p\n$$\n\n$$\ng_k(x^*)\\le 0,k=1,2,3,...,q\n$$\n\n$$\n\\nabla f(x^*)+\\sum_{j=1}^{p}\\lambda_j\\nabla h_j(x^*)+\\sum_{k=1}^{q}\\mu_k \\nabla g_k(x^*)=0\n$$\n\n$$\n\\lambda_j \\neq0,\\mu \\ge0,\\mu_k g_k(x^*)=0\n$$\n\n此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\\alpha)$关于$w,b$的最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。\n\n首先固定$\\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。\n$$\n\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w= \\sum_{i=1}^{n}\\alpha_iy_ix_i\n$$\n\n$$\n\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n}\\alpha_iy_i=0\n$$\n\n将上述结果代入到之前的L得到：\n$$\nL(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)\n$$\n\n$$\n=\\frac{1}{2}w^Tw-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_iy_ix_i)^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}\\sum_{i=1}^{n}\\alpha_iy_ix_i^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i\n$$\n\n$$\n=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\n$$\n\n然后求对$\\alpha$的极大，即是关于对偶问题的最优化问题。\n$$\n\\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j\n$$\n\n$$\ns.t.,\\alpha_i\\ge0,i=1,2,3,...,n\n$$\n\n$$\n\\sum_{i=1}^{n}\\alpha_iy_i=0\n$$\n\n我们已经知道$x_i,x_j$的值，便可利用**SMO算法**求解$\\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。\n$$\nb^*=-\\frac{\\max_{i:y(i)=-1}w^Tx_i+\\min_{i:y(i)=1}w^Tx_i}{2}\n$$\n至此我们便可得出分类超平面和分类决策函数。\n\n### 4.松弛变量处理outliers方法\n\n实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。\n\n![机器学习之SVM支持向量机（二）图像03](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F03.png)\n\n为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即\n$$\ny_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n\n$$\n其中$\\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\\varepsilon_i$的总和也要尽量小。\n$$\n\\min\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i\n$$\n\n$$\ns.t.,y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,...,n\n$$\n\n$$\n\\varepsilon_i\\ge0,i=1,2,3,...,n\n$$\n\n此处和[机器学习之SVM支持向量机（一）](https://weizhixiaoyi.com/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/)中的损失函数不同的是加入$\\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\\varepsilon_i$。\n$$\nmin_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n$$\n那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示:\n$$\nL(w,b,\\varepsilon,\\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1+\\varepsilon_i)-\\sum_{i=1}^{n}r_i\\varepsilon_i\n$$\n分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。\n\n### 5.Sklearn实现SVM支持向量机\n\n我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在**机器学习之SVM支持向量机（一）**中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇**SVM核函数的应用**。\n\n#### 5.1线性\n\n```Python\nfrom sklearn import svm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nx=np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]#正态分布产生数字20行2列\ny=[0]*20+[1]*20#20个class0,20个class1\nclf=svm.SVC(kernel='linear')#使用线性核\nclf.fit(x,y)\nw=clf.coef_[0]#获取w\na=-w[0]/w[1]#斜率\n\n#画图\nxx=np.linspace(-5,5)\nyy=a*xx-(clf.intercept_[0])/w[1]\nb=clf.support_vectors_[0]\nyy_down=a*xx+(b[1]-a*b[0])\nb=clf.support_vectors_[-1]\nyy_up=a*xx+(b[1]-a*b[0])\nplt.figure(figsize=(8,4))\nplt.plot(xx,yy)\nplt.plot(xx,yy_down)\nplt.plot(xx,yy_up)\nplt.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=80)\nplt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.Paired)\nplt.axis('tight')\nplt.show()\n```\n\n![机器学习之SVM支持向量机（二）图像05](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png)\n\n#### 5.2非线性\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import  Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nX, y = make_moons( n_samples=100, noise=0.15, random_state=42 )\n\ndef plot_dataset(X, y, axes):\n    plt.plot( X[:,0][y==0], X[:,1][y==0], \"bs\" )\n    plt.plot( X[:,0][y==1], X[:,1][y==1], \"g^\" )\n    plt.axis( axes )\n    plt.grid( True, which=\"both\" )\n    plt.xlabel(r\"$x_l$\")\n    plt.ylabel(r\"$x_2$\")\n\n# contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）\ndef plot_predict(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid( x0s, x1s )\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict( X ).reshape( x0.shape )\n    y_decision = clf.decision_function( X ).reshape( x0.shape )\n    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=0.5 )\n    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=0.2 )\n\npolynomial_svm_clf = Pipeline([ (\"poly_featutres\", PolynomialFeatures(degree=3)),\n                                (\"scaler\", StandardScaler()),\n                                (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42)  )\n                            ])#多项式核函数\npolynomial_svm_clf.fit( X, y )\nplot_dataset( X, y, [-1.5, 2.5, -1, 1.5] )\nplot_predict( polynomial_svm_clf, [-1.5, 2.5, -1, 1.5] )\nplt.show()\n```\n\n![机器学习之SVM支持向量机（二）图像06](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png)\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![机器学习之SVM支持向量机（二）图片推广](机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png)\n\n\n\n\n\n\n\n\n\n\n\n","slug":"机器学习之SVM支持向量机（二）","published":1,"updated":"2018-04-06T13:11:23.223Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdu000t2e01koqemz5r","content":"<h3 id=\"1-知识回顾\"><a href=\"#1-知识回顾\" class=\"headerlink\" title=\"1.知识回顾\"></a>1.知识回顾</h3><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483818&amp;idx=1&amp;sn=50c634d8b00877134558125c4a718fd7&amp;chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd\" target=\"_blank\" rel=\"noopener\">机器学习之SVM支持向量机（一）</a>中我们介绍了<strong>SVM损失函数</strong>、<strong>最大间隔分类</strong>、<strong>为什么SVM能形成最大间隔分类器</strong>、<strong>核函数</strong>、<strong>SVM中Gaussian Kernel的使用</strong>知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。</p>\n<h3 id=\"2-函数间隔和几何间隔\"><a href=\"#2-函数间隔和几何间隔\" class=\"headerlink\" title=\"2.函数间隔和几何间隔\"></a>2.函数间隔和几何间隔</h3><p>上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示：</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F01.png\" alt=\"机器学习之SVM支持向量机（二）图像01\"></p>\n<p>在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。</p>\n<p>定义函数间隔$\\hat{\\gamma}$：<br>$$<br>\\hat{\\gamma}=y(w^Tx+b)=yf(x)<br>$$<br>而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔：<br>$$<br>\\hat{\\gamma}=min\\hat{\\gamma}<br>$$<br>但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。</p>\n<p>但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F02.png\" alt=\"机器学习之SVM支持向量机（二）图像02\"><br>$$<br>x=x_0+\\gamma\\frac{w}{||w||}<br>$$<br>其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到：<br>$$<br>\\gamma=\\frac{w^T+b}{||w||}=\\frac{f(x)}{||w||}<br>$$<br>为了得到$\\gamma$绝对值，将$\\gamma$乘上相应类别$y$，即可得到几何间隔：<br>$$<br>\\tilde{r}=yr=\\frac{\\hat{\\gamma}}{||w||}<br>$$<br>从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y<em>(w^Tx+b)=y</em>f(x)$，实际上就是$|f(x)|$，几何间隔$\\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。</p>\n<p>对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\\tilde{\\gamma}$。同时需满足如下条件<br>$$<br>y_i(w^Tx_i+b)=\\hat{\\gamma_i}\\ge\\hat{\\gamma},i=1,2,3,…,n<br>$$<br>此处令函数间隔$\\hat{\\gamma}$等于1（之所以令$\\hat{\\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成：<br>$$<br>max\\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,…,n<br>$$</p>\n<h3 id=\"3-原始问题到对偶问题的求解\"><a href=\"#3-原始问题到对偶问题的求解\" class=\"headerlink\" title=\"3.原始问题到对偶问题的求解\"></a>3.原始问题到对偶问题的求解</h3><p>接着考虑我们之前的目标函数，由于求$\\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以目标函数转换为<br>$$<br>min\\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,…,n<br>$$<br>现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。</p>\n<p>那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\\alpha$，定义拉格朗日函数为：<br>$$<br>L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)<br>$$<br>然后令：<br>$$<br>\\theta(w)=\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)<br>$$<br>当某个条件不满足时，例如$y_i(w^Tx+b)&lt;1$，那么有$\\theta(w)=\\infty$。而当所有约束条件都满足时，则有$\\theta(w)=\\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为：<br>$$<br>\\min_{w,b}\\theta(w)=\\min_{w,b}\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)=p^<em><br>$$<br>这里用$p^</em>$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下：<br>$$<br>\\max_{\\alpha_i\\ge0}\\min_{w,b}L(w,b,\\alpha)=d^<em><br>$$<br>交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^</em>$表示，而且有$d^<em>\\le p^</em>$，在<strong>满足某些条件</strong>的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。</p>\n<p>此处<strong>满足某些条件</strong>的情况下，两者等价，此处的<strong>满足某些条件</strong>便是满足KKT条件。KKT最优化数学模型表示成下列标准形式:<br>$$<br>\\min f(x)<br>$$</p>\n<p>$$<br>s.t. h_j(x)=0,j=1,2,3,…,p<br>$$</p>\n<p>$$<br>g_k(x)\\le0,k=1,2,3,…,q<br>$$</p>\n<p>$$<br>x\\in X\\subset R^n<br>$$</p>\n<p>其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。</p>\n<blockquote>\n<p>凸优化概念:$X\\subset R^n$为一凸集，$f:X-&gt;R$为一凸函数。凸优化便是寻找一点$x^<em>\\in X$，是的每一$x\\in X$满足$f(x^</em>)\\le f(x)$。</p>\n<p>KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。</p>\n</blockquote>\n<p>KKT条件就是上面最优化数学模型的标准形式中的最小点$x^<em>$必须满足下面的条件:<br>$$<br>h_j(x^</em>)=0,j=1,2,3,…,p<br>$$</p>\n<p>$$<br>g_k(x^*)\\le 0,k=1,2,3,…,q<br>$$</p>\n<p>$$<br>\\nabla f(x^<em>)+\\sum_{j=1}^{p}\\lambda_j\\nabla h_j(x^</em>)+\\sum_{k=1}^{q}\\mu_k \\nabla g_k(x^*)=0<br>$$</p>\n<p>$$<br>\\lambda_j \\neq0,\\mu \\ge0,\\mu_k g_k(x^*)=0<br>$$</p>\n<p>此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\\alpha)$关于$w,b$的最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。</p>\n<p>首先固定$\\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。<br>$$<br>\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w= \\sum_{i=1}^{n}\\alpha_iy_ix_i<br>$$</p>\n<p>$$<br>\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n}\\alpha_iy_i=0<br>$$</p>\n<p>将上述结果代入到之前的L得到：<br>$$<br>L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)<br>$$</p>\n<p>$$<br>=\\frac{1}{2}w^Tw-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_iy_ix_i)^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}\\sum_{i=1}^{n}\\alpha_iy_ix_i^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j<br>$$</p>\n<p>然后求对$\\alpha$的极大，即是关于对偶问题的最优化问题。<br>$$<br>\\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j<br>$$</p>\n<p>$$<br>s.t.,\\alpha_i\\ge0,i=1,2,3,…,n<br>$$</p>\n<p>$$<br>\\sum_{i=1}^{n}\\alpha_iy_i=0<br>$$</p>\n<p>我们已经知道$x_i,x_j$的值，便可利用<strong>SMO算法</strong>求解$\\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。<br>$$<br>b^*=-\\frac{\\max_{i:y(i)=-1}w^Tx_i+\\min_{i:y(i)=1}w^Tx_i}{2}<br>$$<br>至此我们便可得出分类超平面和分类决策函数。</p>\n<h3 id=\"4-松弛变量处理outliers方法\"><a href=\"#4-松弛变量处理outliers方法\" class=\"headerlink\" title=\"4.松弛变量处理outliers方法\"></a>4.松弛变量处理outliers方法</h3><p>实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F03.png\" alt=\"机器学习之SVM支持向量机（二）图像03\"></p>\n<p>为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即<br>$$<br>y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,…,n<br>$$<br>其中$\\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\\varepsilon_i$的总和也要尽量小。<br>$$<br>\\min\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i<br>$$</p>\n<p>$$<br>s.t.,y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,…,n<br>$$</p>\n<p>$$<br>\\varepsilon_i\\ge0,i=1,2,3,…,n<br>$$</p>\n<p>此处和<a href=\"https://weizhixiaoyi.com/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/\">机器学习之SVM支持向量机（一）</a>中的损失函数不同的是加入$\\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\\varepsilon_i$。<br>$$<br>min_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$<br>那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示:<br>$$<br>L(w,b,\\varepsilon,\\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1+\\varepsilon_i)-\\sum_{i=1}^{n}r_i\\varepsilon_i<br>$$<br>分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。</p>\n<h3 id=\"5-Sklearn实现SVM支持向量机\"><a href=\"#5-Sklearn实现SVM支持向量机\" class=\"headerlink\" title=\"5.Sklearn实现SVM支持向量机\"></a>5.Sklearn实现SVM支持向量机</h3><p>我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在<strong>机器学习之SVM支持向量机（一）</strong>中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇<strong>SVM核函数的应用</strong>。</p>\n<h4 id=\"5-1线性\"><a href=\"#5-1线性\" class=\"headerlink\" title=\"5.1线性\"></a>5.1线性</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> svm</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">x=np.r_[np.random.randn(<span class=\"number\">20</span>,<span class=\"number\">2</span>)-[<span class=\"number\">2</span>,<span class=\"number\">2</span>],np.random.randn(<span class=\"number\">20</span>,<span class=\"number\">2</span>)+[<span class=\"number\">2</span>,<span class=\"number\">2</span>]]<span class=\"comment\">#正态分布产生数字20行2列</span></span><br><span class=\"line\">y=[<span class=\"number\">0</span>]*<span class=\"number\">20</span>+[<span class=\"number\">1</span>]*<span class=\"number\">20</span><span class=\"comment\">#20个class0,20个class1</span></span><br><span class=\"line\">clf=svm.SVC(kernel=<span class=\"string\">'linear'</span>)<span class=\"comment\">#使用线性核</span></span><br><span class=\"line\">clf.fit(x,y)</span><br><span class=\"line\">w=clf.coef_[<span class=\"number\">0</span>]<span class=\"comment\">#获取w</span></span><br><span class=\"line\">a=-w[<span class=\"number\">0</span>]/w[<span class=\"number\">1</span>]<span class=\"comment\">#斜率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">xx=np.linspace(<span class=\"number\">-5</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">yy=a*xx-(clf.intercept_[<span class=\"number\">0</span>])/w[<span class=\"number\">1</span>]</span><br><span class=\"line\">b=clf.support_vectors_[<span class=\"number\">0</span>]</span><br><span class=\"line\">yy_down=a*xx+(b[<span class=\"number\">1</span>]-a*b[<span class=\"number\">0</span>])</span><br><span class=\"line\">b=clf.support_vectors_[<span class=\"number\">-1</span>]</span><br><span class=\"line\">yy_up=a*xx+(b[<span class=\"number\">1</span>]-a*b[<span class=\"number\">0</span>])</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">plt.plot(xx,yy)</span><br><span class=\"line\">plt.plot(xx,yy_down)</span><br><span class=\"line\">plt.plot(xx,yy_up)</span><br><span class=\"line\">plt.scatter(clf.support_vectors_[:,<span class=\"number\">0</span>],clf.support_vectors_[:,<span class=\"number\">1</span>],s=<span class=\"number\">80</span>)</span><br><span class=\"line\">plt.scatter(x[:,<span class=\"number\">0</span>],x[:,<span class=\"number\">1</span>],c=y,cmap=plt.cm.Paired)</span><br><span class=\"line\">plt.axis(<span class=\"string\">'tight'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png\" alt=\"机器学习之SVM支持向量机（二）图像05\"></p>\n<h4 id=\"5-2非线性\"><a href=\"#5-2非线性\" class=\"headerlink\" title=\"5.2非线性\"></a>5.2非线性</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> PolynomialFeatures</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.pipeline <span class=\"keyword\">import</span>  Pipeline</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> LinearSVC</span><br><span class=\"line\">X, y = make_moons( n_samples=<span class=\"number\">100</span>, noise=<span class=\"number\">0.15</span>, random_state=<span class=\"number\">42</span> )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_dataset</span><span class=\"params\">(X, y, axes)</span>:</span></span><br><span class=\"line\">    plt.plot( X[:,<span class=\"number\">0</span>][y==<span class=\"number\">0</span>], X[:,<span class=\"number\">1</span>][y==<span class=\"number\">0</span>], <span class=\"string\">\"bs\"</span> )</span><br><span class=\"line\">    plt.plot( X[:,<span class=\"number\">0</span>][y==<span class=\"number\">1</span>], X[:,<span class=\"number\">1</span>][y==<span class=\"number\">1</span>], <span class=\"string\">\"g^\"</span> )</span><br><span class=\"line\">    plt.axis( axes )</span><br><span class=\"line\">    plt.grid( <span class=\"keyword\">True</span>, which=<span class=\"string\">\"both\"</span> )</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">r\"$x_l$\"</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">r\"$x_2$\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_predict</span><span class=\"params\">(clf, axes)</span>:</span></span><br><span class=\"line\">    x0s = np.linspace(axes[<span class=\"number\">0</span>], axes[<span class=\"number\">1</span>], <span class=\"number\">100</span>)</span><br><span class=\"line\">    x1s = np.linspace(axes[<span class=\"number\">2</span>], axes[<span class=\"number\">3</span>], <span class=\"number\">100</span>)</span><br><span class=\"line\">    x0, x1 = np.meshgrid( x0s, x1s )</span><br><span class=\"line\">    X = np.c_[x0.ravel(), x1.ravel()]</span><br><span class=\"line\">    y_pred = clf.predict( X ).reshape( x0.shape )</span><br><span class=\"line\">    y_decision = clf.decision_function( X ).reshape( x0.shape )</span><br><span class=\"line\">    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=<span class=\"number\">0.5</span> )</span><br><span class=\"line\">    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=<span class=\"number\">0.2</span> )</span><br><span class=\"line\"></span><br><span class=\"line\">polynomial_svm_clf = Pipeline([ (<span class=\"string\">\"poly_featutres\"</span>, PolynomialFeatures(degree=<span class=\"number\">3</span>)),</span><br><span class=\"line\">                                (<span class=\"string\">\"scaler\"</span>, StandardScaler()),</span><br><span class=\"line\">                                (<span class=\"string\">\"svm_clf\"</span>, LinearSVC(C=<span class=\"number\">10</span>, loss=<span class=\"string\">\"hinge\"</span>, random_state=<span class=\"number\">42</span>)  )</span><br><span class=\"line\">                            ])<span class=\"comment\">#多项式核函数</span></span><br><span class=\"line\">polynomial_svm_clf.fit( X, y )</span><br><span class=\"line\">plot_dataset( X, y, [<span class=\"number\">-1.5</span>, <span class=\"number\">2.5</span>, <span class=\"number\">-1</span>, <span class=\"number\">1.5</span>] )</span><br><span class=\"line\">plot_predict( polynomial_svm_clf, [<span class=\"number\">-1.5</span>, <span class=\"number\">2.5</span>, <span class=\"number\">-1</span>, <span class=\"number\">1.5</span>] )</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png\" alt=\"机器学习之SVM支持向量机（二）图像06\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png\" alt=\"机器学习之SVM支持向量机（二）图片推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-知识回顾\"><a href=\"#1-知识回顾\" class=\"headerlink\" title=\"1.知识回顾\"></a>1.知识回顾</h3><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483818&amp;idx=1&amp;sn=50c634d8b00877134558125c4a718fd7&amp;chksm=fcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48#rd\" target=\"_blank\" rel=\"noopener\">机器学习之SVM支持向量机（一）</a>中我们介绍了<strong>SVM损失函数</strong>、<strong>最大间隔分类</strong>、<strong>为什么SVM能形成最大间隔分类器</strong>、<strong>核函数</strong>、<strong>SVM中Gaussian Kernel的使用</strong>知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。</p>\n<h3 id=\"2-函数间隔和几何间隔\"><a href=\"#2-函数间隔和几何间隔\" class=\"headerlink\" title=\"2.函数间隔和几何间隔\"></a>2.函数间隔和几何间隔</h3><p>上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示：</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F01.png\" alt=\"机器学习之SVM支持向量机（二）图像01\"></p>\n<p>在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。</p>\n<p>定义函数间隔$\\hat{\\gamma}$：<br>$$<br>\\hat{\\gamma}=y(w^Tx+b)=yf(x)<br>$$<br>而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔：<br>$$<br>\\hat{\\gamma}=min\\hat{\\gamma}<br>$$<br>但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。</p>\n<p>但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F02.png\" alt=\"机器学习之SVM支持向量机（二）图像02\"><br>$$<br>x=x_0+\\gamma\\frac{w}{||w||}<br>$$<br>其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到：<br>$$<br>\\gamma=\\frac{w^T+b}{||w||}=\\frac{f(x)}{||w||}<br>$$<br>为了得到$\\gamma$绝对值，将$\\gamma$乘上相应类别$y$，即可得到几何间隔：<br>$$<br>\\tilde{r}=yr=\\frac{\\hat{\\gamma}}{||w||}<br>$$<br>从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y<em>(w^Tx+b)=y</em>f(x)$，实际上就是$|f(x)|$，几何间隔$\\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。</p>\n<p>对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\\tilde{\\gamma}$。同时需满足如下条件<br>$$<br>y_i(w^Tx_i+b)=\\hat{\\gamma_i}\\ge\\hat{\\gamma},i=1,2,3,…,n<br>$$<br>此处令函数间隔$\\hat{\\gamma}$等于1（之所以令$\\hat{\\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成：<br>$$<br>max\\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,…,n<br>$$</p>\n<h3 id=\"3-原始问题到对偶问题的求解\"><a href=\"#3-原始问题到对偶问题的求解\" class=\"headerlink\" title=\"3.原始问题到对偶问题的求解\"></a>3.原始问题到对偶问题的求解</h3><p>接着考虑我们之前的目标函数，由于求$\\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以目标函数转换为<br>$$<br>min\\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\\ge1,i=1,2,3,…,n<br>$$<br>现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。</p>\n<p>那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\\alpha$，定义拉格朗日函数为：<br>$$<br>L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)<br>$$<br>然后令：<br>$$<br>\\theta(w)=\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)<br>$$<br>当某个条件不满足时，例如$y_i(w^Tx+b)&lt;1$，那么有$\\theta(w)=\\infty$。而当所有约束条件都满足时，则有$\\theta(w)=\\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为：<br>$$<br>\\min_{w,b}\\theta(w)=\\min_{w,b}\\max_{\\alpha_i\\ge0}L(w,b,\\alpha)=p^<em><br>$$<br>这里用$p^</em>$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下：<br>$$<br>\\max_{\\alpha_i\\ge0}\\min_{w,b}L(w,b,\\alpha)=d^<em><br>$$<br>交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^</em>$表示，而且有$d^<em>\\le p^</em>$，在<strong>满足某些条件</strong>的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。</p>\n<p>此处<strong>满足某些条件</strong>的情况下，两者等价，此处的<strong>满足某些条件</strong>便是满足KKT条件。KKT最优化数学模型表示成下列标准形式:<br>$$<br>\\min f(x)<br>$$</p>\n<p>$$<br>s.t. h_j(x)=0,j=1,2,3,…,p<br>$$</p>\n<p>$$<br>g_k(x)\\le0,k=1,2,3,…,q<br>$$</p>\n<p>$$<br>x\\in X\\subset R^n<br>$$</p>\n<p>其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。</p>\n<blockquote>\n<p>凸优化概念:$X\\subset R^n$为一凸集，$f:X-&gt;R$为一凸函数。凸优化便是寻找一点$x^<em>\\in X$，是的每一$x\\in X$满足$f(x^</em>)\\le f(x)$。</p>\n<p>KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。</p>\n</blockquote>\n<p>KKT条件就是上面最优化数学模型的标准形式中的最小点$x^<em>$必须满足下面的条件:<br>$$<br>h_j(x^</em>)=0,j=1,2,3,…,p<br>$$</p>\n<p>$$<br>g_k(x^*)\\le 0,k=1,2,3,…,q<br>$$</p>\n<p>$$<br>\\nabla f(x^<em>)+\\sum_{j=1}^{p}\\lambda_j\\nabla h_j(x^</em>)+\\sum_{k=1}^{q}\\mu_k \\nabla g_k(x^*)=0<br>$$</p>\n<p>$$<br>\\lambda_j \\neq0,\\mu \\ge0,\\mu_k g_k(x^*)=0<br>$$</p>\n<p>此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\\alpha)$关于$w,b$的最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。</p>\n<p>首先固定$\\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。<br>$$<br>\\frac{\\partial L}{\\partial w}=0 \\Rightarrow w= \\sum_{i=1}^{n}\\alpha_iy_ix_i<br>$$</p>\n<p>$$<br>\\frac{\\partial L}{\\partial b}=0 \\Rightarrow \\sum_{i=1}^{n}\\alpha_iy_i=0<br>$$</p>\n<p>将上述结果代入到之前的L得到：<br>$$<br>L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1)<br>$$</p>\n<p>$$<br>=\\frac{1}{2}w^Tw-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_iw^Tx_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-\\sum_{i=1}^{n}\\alpha_iy_ib+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}w^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_iy_ix_i)^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}\\sum_{i=1}^{n}\\alpha_iy_ix_i^T\\sum_{i=1}^{n}\\alpha_iy_ix_i-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i<br>$$</p>\n<p>$$<br>=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j<br>$$</p>\n<p>然后求对$\\alpha$的极大，即是关于对偶问题的最优化问题。<br>$$<br>\\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i=1,j=1}^{n}\\alpha_iy_ix_i^T\\alpha_jy_jx_j<br>$$</p>\n<p>$$<br>s.t.,\\alpha_i\\ge0,i=1,2,3,…,n<br>$$</p>\n<p>$$<br>\\sum_{i=1}^{n}\\alpha_iy_i=0<br>$$</p>\n<p>我们已经知道$x_i,x_j$的值，便可利用<strong>SMO算法</strong>求解$\\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。<br>$$<br>b^*=-\\frac{\\max_{i:y(i)=-1}w^Tx_i+\\min_{i:y(i)=1}w^Tx_i}{2}<br>$$<br>至此我们便可得出分类超平面和分类决策函数。</p>\n<h3 id=\"4-松弛变量处理outliers方法\"><a href=\"#4-松弛变量处理outliers方法\" class=\"headerlink\" title=\"4.松弛变量处理outliers方法\"></a>4.松弛变量处理outliers方法</h3><p>实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9B%BE%E5%83%8F03.png\" alt=\"机器学习之SVM支持向量机（二）图像03\"></p>\n<p>为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即<br>$$<br>y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,…,n<br>$$<br>其中$\\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\\varepsilon_i$的总和也要尽量小。<br>$$<br>\\min\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i<br>$$</p>\n<p>$$<br>s.t.,y_i(w^Tx_i+b)\\ge1-\\varepsilon_i,i=1,2,3,…,n<br>$$</p>\n<p>$$<br>\\varepsilon_i\\ge0,i=1,2,3,…,n<br>$$</p>\n<p>此处和<a href=\"https://weizhixiaoyi.com/2018/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/\">机器学习之SVM支持向量机（一）</a>中的损失函数不同的是加入$\\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\\varepsilon_i$。<br>$$<br>min_{\\theta}C\\sum_{i=1}^{m}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}<br>$$<br>那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示:<br>$$<br>L(w,b,\\varepsilon,\\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i(y_i(w^Tx_i+b)-1+\\varepsilon_i)-\\sum_{i=1}^{n}r_i\\varepsilon_i<br>$$<br>分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。</p>\n<h3 id=\"5-Sklearn实现SVM支持向量机\"><a href=\"#5-Sklearn实现SVM支持向量机\" class=\"headerlink\" title=\"5.Sklearn实现SVM支持向量机\"></a>5.Sklearn实现SVM支持向量机</h3><p>我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在<strong>机器学习之SVM支持向量机（一）</strong>中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇<strong>SVM核函数的应用</strong>。</p>\n<h4 id=\"5-1线性\"><a href=\"#5-1线性\" class=\"headerlink\" title=\"5.1线性\"></a>5.1线性</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> svm</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">x=np.r_[np.random.randn(<span class=\"number\">20</span>,<span class=\"number\">2</span>)-[<span class=\"number\">2</span>,<span class=\"number\">2</span>],np.random.randn(<span class=\"number\">20</span>,<span class=\"number\">2</span>)+[<span class=\"number\">2</span>,<span class=\"number\">2</span>]]<span class=\"comment\">#正态分布产生数字20行2列</span></span><br><span class=\"line\">y=[<span class=\"number\">0</span>]*<span class=\"number\">20</span>+[<span class=\"number\">1</span>]*<span class=\"number\">20</span><span class=\"comment\">#20个class0,20个class1</span></span><br><span class=\"line\">clf=svm.SVC(kernel=<span class=\"string\">'linear'</span>)<span class=\"comment\">#使用线性核</span></span><br><span class=\"line\">clf.fit(x,y)</span><br><span class=\"line\">w=clf.coef_[<span class=\"number\">0</span>]<span class=\"comment\">#获取w</span></span><br><span class=\"line\">a=-w[<span class=\"number\">0</span>]/w[<span class=\"number\">1</span>]<span class=\"comment\">#斜率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">xx=np.linspace(<span class=\"number\">-5</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\">yy=a*xx-(clf.intercept_[<span class=\"number\">0</span>])/w[<span class=\"number\">1</span>]</span><br><span class=\"line\">b=clf.support_vectors_[<span class=\"number\">0</span>]</span><br><span class=\"line\">yy_down=a*xx+(b[<span class=\"number\">1</span>]-a*b[<span class=\"number\">0</span>])</span><br><span class=\"line\">b=clf.support_vectors_[<span class=\"number\">-1</span>]</span><br><span class=\"line\">yy_up=a*xx+(b[<span class=\"number\">1</span>]-a*b[<span class=\"number\">0</span>])</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">plt.plot(xx,yy)</span><br><span class=\"line\">plt.plot(xx,yy_down)</span><br><span class=\"line\">plt.plot(xx,yy_up)</span><br><span class=\"line\">plt.scatter(clf.support_vectors_[:,<span class=\"number\">0</span>],clf.support_vectors_[:,<span class=\"number\">1</span>],s=<span class=\"number\">80</span>)</span><br><span class=\"line\">plt.scatter(x[:,<span class=\"number\">0</span>],x[:,<span class=\"number\">1</span>],c=y,cmap=plt.cm.Paired)</span><br><span class=\"line\">plt.axis(<span class=\"string\">'tight'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png\" alt=\"机器学习之SVM支持向量机（二）图像05\"></p>\n<h4 id=\"5-2非线性\"><a href=\"#5-2非线性\" class=\"headerlink\" title=\"5.2非线性\"></a>5.2非线性</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> PolynomialFeatures</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.pipeline <span class=\"keyword\">import</span>  Pipeline</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> LinearSVC</span><br><span class=\"line\">X, y = make_moons( n_samples=<span class=\"number\">100</span>, noise=<span class=\"number\">0.15</span>, random_state=<span class=\"number\">42</span> )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_dataset</span><span class=\"params\">(X, y, axes)</span>:</span></span><br><span class=\"line\">    plt.plot( X[:,<span class=\"number\">0</span>][y==<span class=\"number\">0</span>], X[:,<span class=\"number\">1</span>][y==<span class=\"number\">0</span>], <span class=\"string\">\"bs\"</span> )</span><br><span class=\"line\">    plt.plot( X[:,<span class=\"number\">0</span>][y==<span class=\"number\">1</span>], X[:,<span class=\"number\">1</span>][y==<span class=\"number\">1</span>], <span class=\"string\">\"g^\"</span> )</span><br><span class=\"line\">    plt.axis( axes )</span><br><span class=\"line\">    plt.grid( <span class=\"keyword\">True</span>, which=<span class=\"string\">\"both\"</span> )</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">r\"$x_l$\"</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">r\"$x_2$\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_predict</span><span class=\"params\">(clf, axes)</span>:</span></span><br><span class=\"line\">    x0s = np.linspace(axes[<span class=\"number\">0</span>], axes[<span class=\"number\">1</span>], <span class=\"number\">100</span>)</span><br><span class=\"line\">    x1s = np.linspace(axes[<span class=\"number\">2</span>], axes[<span class=\"number\">3</span>], <span class=\"number\">100</span>)</span><br><span class=\"line\">    x0, x1 = np.meshgrid( x0s, x1s )</span><br><span class=\"line\">    X = np.c_[x0.ravel(), x1.ravel()]</span><br><span class=\"line\">    y_pred = clf.predict( X ).reshape( x0.shape )</span><br><span class=\"line\">    y_decision = clf.decision_function( X ).reshape( x0.shape )</span><br><span class=\"line\">    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=<span class=\"number\">0.5</span> )</span><br><span class=\"line\">    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=<span class=\"number\">0.2</span> )</span><br><span class=\"line\"></span><br><span class=\"line\">polynomial_svm_clf = Pipeline([ (<span class=\"string\">\"poly_featutres\"</span>, PolynomialFeatures(degree=<span class=\"number\">3</span>)),</span><br><span class=\"line\">                                (<span class=\"string\">\"scaler\"</span>, StandardScaler()),</span><br><span class=\"line\">                                (<span class=\"string\">\"svm_clf\"</span>, LinearSVC(C=<span class=\"number\">10</span>, loss=<span class=\"string\">\"hinge\"</span>, random_state=<span class=\"number\">42</span>)  )</span><br><span class=\"line\">                            ])<span class=\"comment\">#多项式核函数</span></span><br><span class=\"line\">polynomial_svm_clf.fit( X, y )</span><br><span class=\"line\">plot_dataset( X, y, [<span class=\"number\">-1.5</span>, <span class=\"number\">2.5</span>, <span class=\"number\">-1</span>, <span class=\"number\">1.5</span>] )</span><br><span class=\"line\">plot_predict( polynomial_svm_clf, [<span class=\"number\">-1.5</span>, <span class=\"number\">2.5</span>, <span class=\"number\">-1</span>, <span class=\"number\">1.5</span>] )</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png\" alt=\"机器学习之SVM支持向量机（二）图像06\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png\" alt=\"机器学习之SVM支持向量机（二）图片推广\"></p>\n"},{"title":"机器学习之决策树(C4.5算法)","date":"2018-04-19T03:03:26.000Z","mathjax":true,"comments":1,"_content":"\n### 1.决策树简介\n\n我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。\n\n| 序号 | 天气 | 温度 | 湿度 | 风速 | 高尔夫 |\n| ---- | :--: | :--: | :--: | :--: | :----: |\n| 1    |  晴  | 炎热 |  高  |  弱  |  进行  |\n| 2    |  晴  | 炎热 |  高  |  强  |  进行  |\n| 3    |  阴  | 炎热 |  高  |  弱  |  取消  |\n| 4    |  雨  | 适中 |  高  |  弱  |  取消  |\n| 5    |  雨  | 寒冷 | 正常 |  弱  |  取消  |\n| 6    |  雨  | 寒冷 | 正常 |  强  |  进行  |\n| 7    |  阴  | 寒冷 | 正常 |  强  |  进行  |\n| 8    |  晴  | 适中 |  高  |  弱  |  进行  |\n| 9    |  晴  | 寒冷 | 正常 |  弱  |  进行  |\n| 10   |  雨  | 适中 | 正常 |  弱  |  进行  |\n| 11   |  晴  | 适中 | 正常 |  强  |  进行  |\n| 12   |  阴  | 适中 |  高  |  强  |  进行  |\n| 13   |  阴  | 炎热 | 正常 |  弱  |  取消  |\n| 14   |  雨  | 适中 |  高  |  强  |  取消  |\n\n正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。\n\n![机器学习之决策树图片01](机器学习之决策树-C4-5算法/机器学习之决策树图片01.png)\n\n### 2.C4.5算法\n\n**上古之神赐予你智慧**：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。\n\nC4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设\n\n+ 类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。\n+ 类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。\n+ 属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。\n+ 属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。\n+ $p_i$表示类别$i$的概率，比如$p(进行)=\\frac{9}{14}$。\n\n#### 2.1信息增益\n\n信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。\n\n**计算类别信息熵**:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息\n$$\nInfo(D)=-\\sum_{i=1}^{m}p_ilog_2(p_i)\n$$\n\n$$\nInfo(D)=-\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14}=0.940\n$$\n\n**计算每个属性的信息熵**:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。\n$$\nInfo_A(D)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*Info(D_j)\n$$\n\n$$\nInfo(天气)=\\frac{5}{14}*[-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5}]+\\frac{4}{14}*[-\\frac{4}{4}log_2\\frac{4}{4}]+\\frac{5}{14}*[-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}]=0.694\n$$\n\n$$\nInfo(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892\n$$\n\n**计算信息增益**:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。\n$$\nGain(A)=Info(D)-Info_A(D)\n$$\n\n$$\nGain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246\n$$\n\n$$\nGain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048\n$$\n\n但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。\n\n#### 2.2信息增益率\n\n**计算属性分裂信息度量**:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用**信息增益 / 内在信息**表示，信息增益率会导致属性的重要性随着内在信息的增大而减小**（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）**，这样算是对单纯用信息增益有所补偿。信息增益率定义如下\n$$\nSplitInfo_A(D)=-\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*log_2\\frac{|D_j|}{|D|}\n$$\n\n$$\nSplitInfo(天气)=-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{4}{14}*log_2\\frac{4}{14}=1.577\n$$\n\n$$\nSplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985\n$$\n\n$$\nGainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}\n$$\n\n$$\nGainRatio(天气)=\\frac{Gain(天气)}{SplitInfo(天气)}=\\frac{0.246}{1.577}=0.155\n$$\n\n$$\nGainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048\n$$\n\n天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。\n\n### 3.树剪枝\n\n决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。\n\n#### 3.1先剪枝\n\n先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法\n\n+ 当决策树达到一定的高度就停止决策树的生长。\n+ 到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。\n+ 计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。\n\n#### 3.2后剪枝\n\n后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。\n\n把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为\n$$\n\\frac{\\sum (E_i+0.5*L)}{\\sum N_i}\n$$\n这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。\n\n假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(**e为分布的固有属性，可以统计出来**)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。\n$$\nE(subtree\\_err\\_count)=N*e\n$$\n\n$$\nvar(subtree\\_err\\_count)=\\sqrt{N*e*(1-e)}\n$$\n\n把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为\n$$\nE(leaf\\_err\\_count)=N*e\n$$\n使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝\n$$\nE(subtree\\_err\\_count)-var(subtree\\_err\\_count)>E(leaf\\_err\\_count)\n$$\n上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。\n\n### 4.Sklearn实现决策树\n\n我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\n#引入数据\niris=load_iris()\nX=iris.data\ny=iris.target\n\n#训练数据和模型,采用ID3或C4.5训练\nclf=tree.DecisionTreeClassifier(criterion='entropy')\nclf=clf.fit(X,y)\n\n\n#引入graphviz模块用来导出图,结果图如下所示\nimport graphviz\ndot_data=tree.export_graphviz(clf,out_file=None,\n                              feature_names=iris.feature_names,\n                              class_names=iris.target_names,\n                              filled=True,rounded=True,\n                              special_characters=True)\n\ngraph=graphviz.Source(dot_data)\ngraph.view()\n```\n\n![机器学习之决策树图片02](机器学习之决策树-C4-5算法/机器学习之决策树图片02.png)\n\n### 5.实际使用技巧\n\n- 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。\n- 训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (`sample_weight`) 的和归一化为相同的值。\n- 考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。\n- 通过 `export` 功能可以可视化您的决策树。使用 `max_depth=3`作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。\n- 填充树的样本数量会增加树的每个附加级别。使用 `max_depth` 来控制树的大小防止过拟合。\n- 通过使用 `min_samples_split` 和 `min_samples_leaf` 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/机器学习之决策树-C4-5算法.md","raw":"---\ntitle: 机器学习之决策树(C4.5算法)\ndate: 2018-04-19 11:03:26\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.决策树简介\n\n我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。\n\n| 序号 | 天气 | 温度 | 湿度 | 风速 | 高尔夫 |\n| ---- | :--: | :--: | :--: | :--: | :----: |\n| 1    |  晴  | 炎热 |  高  |  弱  |  进行  |\n| 2    |  晴  | 炎热 |  高  |  强  |  进行  |\n| 3    |  阴  | 炎热 |  高  |  弱  |  取消  |\n| 4    |  雨  | 适中 |  高  |  弱  |  取消  |\n| 5    |  雨  | 寒冷 | 正常 |  弱  |  取消  |\n| 6    |  雨  | 寒冷 | 正常 |  强  |  进行  |\n| 7    |  阴  | 寒冷 | 正常 |  强  |  进行  |\n| 8    |  晴  | 适中 |  高  |  弱  |  进行  |\n| 9    |  晴  | 寒冷 | 正常 |  弱  |  进行  |\n| 10   |  雨  | 适中 | 正常 |  弱  |  进行  |\n| 11   |  晴  | 适中 | 正常 |  强  |  进行  |\n| 12   |  阴  | 适中 |  高  |  强  |  进行  |\n| 13   |  阴  | 炎热 | 正常 |  弱  |  取消  |\n| 14   |  雨  | 适中 |  高  |  强  |  取消  |\n\n正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。\n\n![机器学习之决策树图片01](机器学习之决策树-C4-5算法/机器学习之决策树图片01.png)\n\n### 2.C4.5算法\n\n**上古之神赐予你智慧**：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。\n\nC4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设\n\n+ 类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。\n+ 类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。\n+ 属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。\n+ 属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。\n+ $p_i$表示类别$i$的概率，比如$p(进行)=\\frac{9}{14}$。\n\n#### 2.1信息增益\n\n信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。\n\n**计算类别信息熵**:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息\n$$\nInfo(D)=-\\sum_{i=1}^{m}p_ilog_2(p_i)\n$$\n\n$$\nInfo(D)=-\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14}=0.940\n$$\n\n**计算每个属性的信息熵**:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。\n$$\nInfo_A(D)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*Info(D_j)\n$$\n\n$$\nInfo(天气)=\\frac{5}{14}*[-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5}]+\\frac{4}{14}*[-\\frac{4}{4}log_2\\frac{4}{4}]+\\frac{5}{14}*[-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}]=0.694\n$$\n\n$$\nInfo(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892\n$$\n\n**计算信息增益**:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。\n$$\nGain(A)=Info(D)-Info_A(D)\n$$\n\n$$\nGain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246\n$$\n\n$$\nGain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048\n$$\n\n但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。\n\n#### 2.2信息增益率\n\n**计算属性分裂信息度量**:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用**信息增益 / 内在信息**表示，信息增益率会导致属性的重要性随着内在信息的增大而减小**（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）**，这样算是对单纯用信息增益有所补偿。信息增益率定义如下\n$$\nSplitInfo_A(D)=-\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*log_2\\frac{|D_j|}{|D|}\n$$\n\n$$\nSplitInfo(天气)=-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{5}{14}*log_2\\frac{5}{14}-\\frac{4}{14}*log_2\\frac{4}{14}=1.577\n$$\n\n$$\nSplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985\n$$\n\n$$\nGainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}\n$$\n\n$$\nGainRatio(天气)=\\frac{Gain(天气)}{SplitInfo(天气)}=\\frac{0.246}{1.577}=0.155\n$$\n\n$$\nGainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048\n$$\n\n天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。\n\n### 3.树剪枝\n\n决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。\n\n#### 3.1先剪枝\n\n先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法\n\n+ 当决策树达到一定的高度就停止决策树的生长。\n+ 到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。\n+ 计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。\n\n#### 3.2后剪枝\n\n后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。\n\n把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为\n$$\n\\frac{\\sum (E_i+0.5*L)}{\\sum N_i}\n$$\n这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。\n\n假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(**e为分布的固有属性，可以统计出来**)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。\n$$\nE(subtree\\_err\\_count)=N*e\n$$\n\n$$\nvar(subtree\\_err\\_count)=\\sqrt{N*e*(1-e)}\n$$\n\n把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为\n$$\nE(leaf\\_err\\_count)=N*e\n$$\n使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝\n$$\nE(subtree\\_err\\_count)-var(subtree\\_err\\_count)>E(leaf\\_err\\_count)\n$$\n上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。\n\n### 4.Sklearn实现决策树\n\n我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\n#引入数据\niris=load_iris()\nX=iris.data\ny=iris.target\n\n#训练数据和模型,采用ID3或C4.5训练\nclf=tree.DecisionTreeClassifier(criterion='entropy')\nclf=clf.fit(X,y)\n\n\n#引入graphviz模块用来导出图,结果图如下所示\nimport graphviz\ndot_data=tree.export_graphviz(clf,out_file=None,\n                              feature_names=iris.feature_names,\n                              class_names=iris.target_names,\n                              filled=True,rounded=True,\n                              special_characters=True)\n\ngraph=graphviz.Source(dot_data)\ngraph.view()\n```\n\n![机器学习之决策树图片02](机器学习之决策树-C4-5算法/机器学习之决策树图片02.png)\n\n### 5.实际使用技巧\n\n- 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。\n- 训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (`sample_weight`) 的和归一化为相同的值。\n- 考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。\n- 通过 `export` 功能可以可视化您的决策树。使用 `max_depth=3`作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。\n- 填充树的样本数量会增加树的每个附加级别。使用 `max_depth` 来控制树的大小防止过拟合。\n- 通过使用 `min_samples_split` 和 `min_samples_leaf` 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。\n\n### 6.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"机器学习之决策树-C4-5算法","published":1,"updated":"2018-04-20T10:10:03.597Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdw000w2e018zqafriy","content":"<h3 id=\"1-决策树简介\"><a href=\"#1-决策树简介\" class=\"headerlink\" title=\"1.决策树简介\"></a>1.决策树简介</h3><p>我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。</p>\n<table>\n<thead>\n<tr>\n<th>序号</th>\n<th style=\"text-align:center\">天气</th>\n<th style=\"text-align:center\">温度</th>\n<th style=\"text-align:center\">湿度</th>\n<th style=\"text-align:center\">风速</th>\n<th style=\"text-align:center\">高尔夫</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>2</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>3</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>4</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>5</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>6</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>7</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>8</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>9</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>10</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>11</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>12</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>13</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>14</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n</tbody>\n</table>\n<p>正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。</p>\n<p><img src=\"机器学习之决策树-C4-5算法/机器学习之决策树图片01.png\" alt=\"机器学习之决策树图片01\"></p>\n<h3 id=\"2-C4-5算法\"><a href=\"#2-C4-5算法\" class=\"headerlink\" title=\"2.C4.5算法\"></a>2.C4.5算法</h3><p><strong>上古之神赐予你智慧</strong>：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。</p>\n<p>C4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设</p>\n<ul>\n<li>类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。</li>\n<li>类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。</li>\n<li>属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。</li>\n<li>属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。</li>\n<li>$p_i$表示类别$i$的概率，比如$p(进行)=\\frac{9}{14}$。</li>\n</ul>\n<h4 id=\"2-1信息增益\"><a href=\"#2-1信息增益\" class=\"headerlink\" title=\"2.1信息增益\"></a>2.1信息增益</h4><p>信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。</p>\n<p><strong>计算类别信息熵</strong>:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息<br>$$<br>Info(D)=-\\sum_{i=1}^{m}p_ilog_2(p_i)<br>$$</p>\n<p>$$<br>Info(D)=-\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14}=0.940<br>$$</p>\n<p><strong>计算每个属性的信息熵</strong>:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。<br>$$<br>Info_A(D)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*Info(D_j)<br>$$</p>\n<p>$$<br>Info(天气)=\\frac{5}{14}<em>[-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5}]+\\frac{4}{14}</em>[-\\frac{4}{4}log_2\\frac{4}{4}]+\\frac{5}{14}*[-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}]=0.694<br>$$</p>\n<p>$$<br>Info(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892<br>$$</p>\n<p><strong>计算信息增益</strong>:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。<br>$$<br>Gain(A)=Info(D)-Info_A(D)<br>$$</p>\n<p>$$<br>Gain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246<br>$$</p>\n<p>$$<br>Gain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048<br>$$</p>\n<p>但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。</p>\n<h4 id=\"2-2信息增益率\"><a href=\"#2-2信息增益率\" class=\"headerlink\" title=\"2.2信息增益率\"></a>2.2信息增益率</h4><p><strong>计算属性分裂信息度量</strong>:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用<strong>信息增益 / 内在信息</strong>表示，信息增益率会导致属性的重要性随着内在信息的增大而减小<strong>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</strong>，这样算是对单纯用信息增益有所补偿。信息增益率定义如下<br>$$<br>SplitInfo_A(D)=-\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*log_2\\frac{|D_j|}{|D|}<br>$$</p>\n<p>$$<br>SplitInfo(天气)=-\\frac{5}{14}<em>log_2\\frac{5}{14}-\\frac{5}{14}</em>log_2\\frac{5}{14}-\\frac{4}{14}*log_2\\frac{4}{14}=1.577<br>$$</p>\n<p>$$<br>SplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985<br>$$</p>\n<p>$$<br>GainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}<br>$$</p>\n<p>$$<br>GainRatio(天气)=\\frac{Gain(天气)}{SplitInfo(天气)}=\\frac{0.246}{1.577}=0.155<br>$$</p>\n<p>$$<br>GainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048<br>$$</p>\n<p>天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。</p>\n<h3 id=\"3-树剪枝\"><a href=\"#3-树剪枝\" class=\"headerlink\" title=\"3.树剪枝\"></a>3.树剪枝</h3><p>决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。</p>\n<h4 id=\"3-1先剪枝\"><a href=\"#3-1先剪枝\" class=\"headerlink\" title=\"3.1先剪枝\"></a>3.1先剪枝</h4><p>先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法</p>\n<ul>\n<li>当决策树达到一定的高度就停止决策树的生长。</li>\n<li>到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。</li>\n<li>计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。</li>\n</ul>\n<h4 id=\"3-2后剪枝\"><a href=\"#3-2后剪枝\" class=\"headerlink\" title=\"3.2后剪枝\"></a>3.2后剪枝</h4><p>后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。</p>\n<p>把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为<br>$$<br>\\frac{\\sum (E_i+0.5*L)}{\\sum N_i}<br>$$<br>这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。</p>\n<p>假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(<strong>e为分布的固有属性，可以统计出来</strong>)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。<br>$$<br>E(subtree_err_count)=N*e<br>$$</p>\n<p>$$<br>var(subtree_err_count)=\\sqrt{N<em>e</em>(1-e)}<br>$$</p>\n<p>把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为<br>$$<br>E(leaf_err_count)=N*e<br>$$<br>使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝<br>$$<br>E(subtree_err_count)-var(subtree_err_count)&gt;E(leaf_err_count)<br>$$<br>上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。</p>\n<h3 id=\"4-Sklearn实现决策树\"><a href=\"#4-Sklearn实现决策树\" class=\"headerlink\" title=\"4.Sklearn实现决策树\"></a>4.Sklearn实现决策树</h3><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练数据和模型,采用ID3或C4.5训练</span></span><br><span class=\"line\">clf=tree.DecisionTreeClassifier(criterion=<span class=\"string\">'entropy'</span>)</span><br><span class=\"line\">clf=clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入graphviz模块用来导出图,结果图如下所示</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> graphviz</span><br><span class=\"line\">dot_data=tree.export_graphviz(clf,out_file=<span class=\"keyword\">None</span>,</span><br><span class=\"line\">                              feature_names=iris.feature_names,</span><br><span class=\"line\">                              class_names=iris.target_names,</span><br><span class=\"line\">                              filled=<span class=\"keyword\">True</span>,rounded=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                              special_characters=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph=graphviz.Source(dot_data)</span><br><span class=\"line\">graph.view()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之决策树-C4-5算法/机器学习之决策树图片02.png\" alt=\"机器学习之决策树图片02\"></p>\n<h3 id=\"5-实际使用技巧\"><a href=\"#5-实际使用技巧\" class=\"headerlink\" title=\"5.实际使用技巧\"></a>5.实际使用技巧</h3><ul>\n<li>对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。</li>\n<li>训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code>sample_weight</code>) 的和归一化为相同的值。</li>\n<li>考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。</li>\n<li>通过 <code>export</code> 功能可以可视化您的决策树。使用 <code>max_depth=3</code>作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。</li>\n<li>填充树的样本数量会增加树的每个附加级别。使用 <code>max_depth</code> 来控制树的大小防止过拟合。</li>\n<li>通过使用 <code>min_samples_split</code> 和 <code>min_samples_leaf</code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。</li>\n</ul>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-决策树简介\"><a href=\"#1-决策树简介\" class=\"headerlink\" title=\"1.决策树简介\"></a>1.决策树简介</h3><p>我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。</p>\n<table>\n<thead>\n<tr>\n<th>序号</th>\n<th style=\"text-align:center\">天气</th>\n<th style=\"text-align:center\">温度</th>\n<th style=\"text-align:center\">湿度</th>\n<th style=\"text-align:center\">风速</th>\n<th style=\"text-align:center\">高尔夫</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>2</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>3</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>4</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>5</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>6</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>7</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>8</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>9</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">寒冷</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>10</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>11</td>\n<td style=\"text-align:center\">晴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>12</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">进行</td>\n</tr>\n<tr>\n<td>13</td>\n<td style=\"text-align:center\">阴</td>\n<td style=\"text-align:center\">炎热</td>\n<td style=\"text-align:center\">正常</td>\n<td style=\"text-align:center\">弱</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n<tr>\n<td>14</td>\n<td style=\"text-align:center\">雨</td>\n<td style=\"text-align:center\">适中</td>\n<td style=\"text-align:center\">高</td>\n<td style=\"text-align:center\">强</td>\n<td style=\"text-align:center\">取消</td>\n</tr>\n</tbody>\n</table>\n<p>正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。</p>\n<p><img src=\"机器学习之决策树-C4-5算法/机器学习之决策树图片01.png\" alt=\"机器学习之决策树图片01\"></p>\n<h3 id=\"2-C4-5算法\"><a href=\"#2-C4-5算法\" class=\"headerlink\" title=\"2.C4.5算法\"></a>2.C4.5算法</h3><p><strong>上古之神赐予你智慧</strong>：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。</p>\n<p>C4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设</p>\n<ul>\n<li>类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。</li>\n<li>类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。</li>\n<li>属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。</li>\n<li>属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。</li>\n<li>$p_i$表示类别$i$的概率，比如$p(进行)=\\frac{9}{14}$。</li>\n</ul>\n<h4 id=\"2-1信息增益\"><a href=\"#2-1信息增益\" class=\"headerlink\" title=\"2.1信息增益\"></a>2.1信息增益</h4><p>信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。</p>\n<p><strong>计算类别信息熵</strong>:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息<br>$$<br>Info(D)=-\\sum_{i=1}^{m}p_ilog_2(p_i)<br>$$</p>\n<p>$$<br>Info(D)=-\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14}=0.940<br>$$</p>\n<p><strong>计算每个属性的信息熵</strong>:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。<br>$$<br>Info_A(D)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*Info(D_j)<br>$$</p>\n<p>$$<br>Info(天气)=\\frac{5}{14}<em>[-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5}]+\\frac{4}{14}</em>[-\\frac{4}{4}log_2\\frac{4}{4}]+\\frac{5}{14}*[-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}]=0.694<br>$$</p>\n<p>$$<br>Info(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892<br>$$</p>\n<p><strong>计算信息增益</strong>:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。<br>$$<br>Gain(A)=Info(D)-Info_A(D)<br>$$</p>\n<p>$$<br>Gain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246<br>$$</p>\n<p>$$<br>Gain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048<br>$$</p>\n<p>但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。</p>\n<h4 id=\"2-2信息增益率\"><a href=\"#2-2信息增益率\" class=\"headerlink\" title=\"2.2信息增益率\"></a>2.2信息增益率</h4><p><strong>计算属性分裂信息度量</strong>:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用<strong>信息增益 / 内在信息</strong>表示，信息增益率会导致属性的重要性随着内在信息的增大而减小<strong>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</strong>，这样算是对单纯用信息增益有所补偿。信息增益率定义如下<br>$$<br>SplitInfo_A(D)=-\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}*log_2\\frac{|D_j|}{|D|}<br>$$</p>\n<p>$$<br>SplitInfo(天气)=-\\frac{5}{14}<em>log_2\\frac{5}{14}-\\frac{5}{14}</em>log_2\\frac{5}{14}-\\frac{4}{14}*log_2\\frac{4}{14}=1.577<br>$$</p>\n<p>$$<br>SplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985<br>$$</p>\n<p>$$<br>GainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}<br>$$</p>\n<p>$$<br>GainRatio(天气)=\\frac{Gain(天气)}{SplitInfo(天气)}=\\frac{0.246}{1.577}=0.155<br>$$</p>\n<p>$$<br>GainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048<br>$$</p>\n<p>天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。</p>\n<h3 id=\"3-树剪枝\"><a href=\"#3-树剪枝\" class=\"headerlink\" title=\"3.树剪枝\"></a>3.树剪枝</h3><p>决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。</p>\n<h4 id=\"3-1先剪枝\"><a href=\"#3-1先剪枝\" class=\"headerlink\" title=\"3.1先剪枝\"></a>3.1先剪枝</h4><p>先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法</p>\n<ul>\n<li>当决策树达到一定的高度就停止决策树的生长。</li>\n<li>到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。</li>\n<li>计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。</li>\n</ul>\n<h4 id=\"3-2后剪枝\"><a href=\"#3-2后剪枝\" class=\"headerlink\" title=\"3.2后剪枝\"></a>3.2后剪枝</h4><p>后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。</p>\n<p>把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为<br>$$<br>\\frac{\\sum (E_i+0.5*L)}{\\sum N_i}<br>$$<br>这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。</p>\n<p>假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(<strong>e为分布的固有属性，可以统计出来</strong>)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。<br>$$<br>E(subtree_err_count)=N*e<br>$$</p>\n<p>$$<br>var(subtree_err_count)=\\sqrt{N<em>e</em>(1-e)}<br>$$</p>\n<p>把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为<br>$$<br>E(leaf_err_count)=N*e<br>$$<br>使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝<br>$$<br>E(subtree_err_count)-var(subtree_err_count)&gt;E(leaf_err_count)<br>$$<br>上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。</p>\n<h3 id=\"4-Sklearn实现决策树\"><a href=\"#4-Sklearn实现决策树\" class=\"headerlink\" title=\"4.Sklearn实现决策树\"></a>4.Sklearn实现决策树</h3><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入数据</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练数据和模型,采用ID3或C4.5训练</span></span><br><span class=\"line\">clf=tree.DecisionTreeClassifier(criterion=<span class=\"string\">'entropy'</span>)</span><br><span class=\"line\">clf=clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#引入graphviz模块用来导出图,结果图如下所示</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> graphviz</span><br><span class=\"line\">dot_data=tree.export_graphviz(clf,out_file=<span class=\"keyword\">None</span>,</span><br><span class=\"line\">                              feature_names=iris.feature_names,</span><br><span class=\"line\">                              class_names=iris.target_names,</span><br><span class=\"line\">                              filled=<span class=\"keyword\">True</span>,rounded=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                              special_characters=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph=graphviz.Source(dot_data)</span><br><span class=\"line\">graph.view()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之决策树-C4-5算法/机器学习之决策树图片02.png\" alt=\"机器学习之决策树图片02\"></p>\n<h3 id=\"5-实际使用技巧\"><a href=\"#5-实际使用技巧\" class=\"headerlink\" title=\"5.实际使用技巧\"></a>5.实际使用技巧</h3><ul>\n<li>对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。</li>\n<li>训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code>sample_weight</code>) 的和归一化为相同的值。</li>\n<li>考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。</li>\n<li>通过 <code>export</code> 功能可以可视化您的决策树。使用 <code>max_depth=3</code>作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。</li>\n<li>填充树的样本数量会增加树的每个附加级别。使用 <code>max_depth</code> 来控制树的大小防止过拟合。</li>\n<li>通过使用 <code>min_samples_split</code> 和 <code>min_samples_leaf</code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。</li>\n</ul>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"机器学习之朴素贝叶斯算法","date":"2018-05-14T03:22:17.000Z","mathjax":true,"comments":1,"_content":"\n### 1.朴素贝叶斯简介\n\n**朴素贝叶斯(Naive Bayesian)**算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布$P(X,Y)$，然后用$P(Y|X)=\\frac{P(X,Y)}{P(X)}$得出。\n\n朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。\n\n### 2.朴素贝叶斯算法模型\n\n#### 2.1统计知识回顾\n\n深入算法原理之前，我们先来回顾下统计学的相关知识。\n\n+ **条件概率公式**\n\n$$\nP(X,Y)=P(X)P(Y)\\ X、Y相互独立\n$$\n\n+ **条件概率公式**\n\n$$\nP(Y|X)=\\frac{P(X,Y)}{P(X)}\n\\\\ P(X|Y)=\\frac{P(X,Y)}{P(Y)}\n$$\n\n+ **全概率公式**\n\n$$\nP(X)=\\sum_{j}P(X|Y=Y_j)P(Y_j)\\ 其中\\sum _{j}P(Y_j)=1\n$$\n\n经过上面统计学知识，我们能够得出贝叶斯公式。\n$$\nP(Y_k|X)=\\frac{P(X,Y_k)}{P(X)}=\\frac{P(X|Y_k)P(Y_k)}{\\sum_{k=1}^{K}P(X|Y=Y_k)P(Y_k)}\n$$\n\n#### 2.2朴素贝叶斯模型\n\n假设我们已经有部分数据，并且能从数据中得到先验概率，那么如何得到后验概率$P(Y_k|X)$呢？下面我们通过构建朴素贝叶斯分类模型来解决后验概率问题，假设分类模型数据有m个样本，每个样本有n个特征，特征输出有K个类别，定义为${C1,C2,…,Ck}$。\n$$\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}\n$$\n从样本中我们能够学习得到朴素贝叶斯的先验分布概率\n$$\nP(Y=C_k)(k=1,2,…,K)\n$$\n然后学习得到条件概率分布\n$$\nP(X|Y=C_k)=P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\n$$\n最后结合贝叶斯公式便可得到X和Y的联合分布$P(X,Y=C_k)$\n$$\nP(X,Y=C_k)=P(Y=C_k)P(X|Y=C_k)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)\n\\\\ =P(Y=C_k)P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\\ \\ (2)\n$$\n上式中$P(Y=C_k)$可以通过极大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)$很难求出，因此朴素贝叶斯模型做一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出\n$$\nP(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\n\\\\ =P(X_1=x^{(1)}|Y=C_k)P(X_2=x^{(2)}|Y=C_k),...,P(X_n=x^{(n)}|Y=C_k)\n$$\n这样我们便得到X和Y的联合分布，最后的问题是给定测试集$(x_1^{(test)},x_2^{(test)},…,x_n^{(test)})$，如何判断它属于哪个类型？\n\n#### 2.3朴素贝叶斯推断\n\n假如我们预测的类别结果为$C_{result}$，其中$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大的类别，数学表达式为\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k|X=X^{(test)})\n\\\\= \\underset{C_k}{\\underbrace{\\arg\\max}} \\frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}\n$$\n由于对所有类别计算$P(Y=C_k|X=X^{(test)})$时，分母都是$P(X=X^{(test)})$，因此我们的预测公式可以简化为\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(X=X^{(test)}|Y=C_k)P(Y=C_k)\n$$\n然后利用朴素贝叶斯的独立性假设，就可以得到朴素贝叶斯推断公式\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n#### 2.4朴素贝叶斯参数估计\n\n对于**2.3**中朴素贝叶斯推断公式，我们只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$，就可以得到预测结果。对于$P(Y=C_k)$比较简单，通过极大似然估计能够得到$C_k$出现的概率，也就是样本类别$C_k$出现的次数$m_k$除以样本总数m。而对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$则取决于我们的先验条件。\n\n+ 如果$X_j$是**离散值**，那么可以假设$X_j$服从多项式分布，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别中$X_j^{test}$出现的频率，公式如下所示。其中$m_k$为样本类别$C_k$出现的次数，而$m_{kj}^{(test)}$为类别是$C_k$的样本中，第j维特征$X_j^{(test)}$出现的次数。\n  $$\n  P(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}}{m_k}\n  $$\n  某些时候可能一些类别在样本中从来没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，因此会影响后验概率的估计。为了解决此类情况，我们引入拉普拉斯平滑，公式如下所示。其中$\\lambda$为大于0的常数，常取为1，$O_j$为第j个特征的取值个数。\n\n$$\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}+\\lambda}{m_k+O_j \\lambda}\n$$\n\n+ 如果$X_j$是**稀疏的离散值**，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中$X_j^{(test)}$出现的频率，公式如下所示。其中$X_j^{(test)}$取值为0和1。\n\n$$\nP(X_j=X_j^{(test)}|Y=C_k)=P(X_j|Y=C_k)X_j^{(test)}+(1-P(X_j|Y=C_k))(1-X_j^{(test)})\n$$\n\n+ 如果$X_j$是**连续值**，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布公式如下所示。其中$\\mu_k$和$\\sigma_{k}^2$是正态分布的期望和方差，$\\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\\mu_k$和$\\sigma _{k}^2$可以通过极大似然估计求得。\n  $$\n  P(X_j=X_j^{(test)}|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp(-\\frac{(X_j^{(test)}- \\mu_k)^2}{2\\sigma_{k}^{2}})\n  $$\n\n\n\n### 3.朴素贝叶斯算法流程\n\n我们总结下朴素贝叶斯算法流程，假设训练集有m个样本n个维度，训练样本有K个特征输出类别，分别为$C1,C2,…,C_K$，每个特征输出类别的样本个数为$m_1,m_2,…,m_K$。第K个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$，其中$l$的取值为$1,2,…,S_j$，$S_j$为特征$j$不同的取值数。\n$$\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}\n$$\n预测结果为$X^{(test)}​$的分类，算法流程如下所示\n\n+ 如果没有Y的先验概率，则计算Y的K个先验概率$P(Y=C_k)=\\frac{m_k}{m}$。\n\n+ 分别计算第k个类别的第j为特征的第l个取值条件概率$P(X_j=x_{jl}|Y=C_k)$。\n\n  + 如果是离散值，则公式如下所示，其中$\\lambda$为大于0的常数，常常取为1，$O_j$为第j个特征的取值个数。\n\n  $$\n  P(X_j=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+O_j \\lambda}\n  $$\n\n  + 如果是稀疏离散值，则公式如下所示，此时$l$取值为0或1。\n\n  $$\n  P(X_j=x_{jl}|Y=C_k)=P(X_j|Y=C_k)x_{jl}+(1-P(X_j|Y=C_k))(1-x_{jl})\n  $$\n\n  + 如果是连续值，则不需要计算各个l个取值概率，直接求正态分布的参数，公式如下所示。$\\mu_k$为样本类别$C_k$中所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中所有$X_j$的方差。\n\n  $$\n  P(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}exp(-\\frac{(x_j- \\mu_k)^2}{2\\sigma_{k}^{2}})\n  $$\n\n+ 对于实例$X^{(test)}$，分别计算\n\n$$\nP(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n+ 确定实例$X^{(test)}$的分类$C_{result}$\n\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。\n\n### 4.Sklearn实现朴素贝叶斯\n\n利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.933333333333。更多关于sklearn.naive_bayes的使用技巧可以访问[官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)。\n\n```python\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n#load data\niris=load_iris()\nX=iris.data\ny=iris.target\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nmnb=GaussianNB()\nmnb.fit(X_train,y_train)\n\nprint(mnb.predict(X_test))\n# [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 1 2 1]\nprint(y_test)\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 2 2 1]\nprint(mnb.score(X_test,y_test))\n# 0.933333333333\n```\n\n### 5.朴素贝叶斯优缺点\n\n#### 5.1优点\n\n+ 具有稳定的分类效率。\n+ 对缺失数据不敏感，算法也比较简单。\n+ 对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。\n\n#### 5.2缺点\n\n+ 对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。\n+ 由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。\n+ 假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n> [刘建平_Pinard-朴素贝叶斯算法原理小结](https://www.cnblogs.com/pinard/p/6069267.html)","source":"_posts/机器学习之朴素贝叶斯算法.md","raw":"---\ntitle: 机器学习之朴素贝叶斯算法\ndate: 2018-05-14 11:22:17\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.朴素贝叶斯简介\n\n**朴素贝叶斯(Naive Bayesian)**算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布$P(X,Y)$，然后用$P(Y|X)=\\frac{P(X,Y)}{P(X)}$得出。\n\n朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。\n\n### 2.朴素贝叶斯算法模型\n\n#### 2.1统计知识回顾\n\n深入算法原理之前，我们先来回顾下统计学的相关知识。\n\n+ **条件概率公式**\n\n$$\nP(X,Y)=P(X)P(Y)\\ X、Y相互独立\n$$\n\n+ **条件概率公式**\n\n$$\nP(Y|X)=\\frac{P(X,Y)}{P(X)}\n\\\\ P(X|Y)=\\frac{P(X,Y)}{P(Y)}\n$$\n\n+ **全概率公式**\n\n$$\nP(X)=\\sum_{j}P(X|Y=Y_j)P(Y_j)\\ 其中\\sum _{j}P(Y_j)=1\n$$\n\n经过上面统计学知识，我们能够得出贝叶斯公式。\n$$\nP(Y_k|X)=\\frac{P(X,Y_k)}{P(X)}=\\frac{P(X|Y_k)P(Y_k)}{\\sum_{k=1}^{K}P(X|Y=Y_k)P(Y_k)}\n$$\n\n#### 2.2朴素贝叶斯模型\n\n假设我们已经有部分数据，并且能从数据中得到先验概率，那么如何得到后验概率$P(Y_k|X)$呢？下面我们通过构建朴素贝叶斯分类模型来解决后验概率问题，假设分类模型数据有m个样本，每个样本有n个特征，特征输出有K个类别，定义为${C1,C2,…,Ck}$。\n$$\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}\n$$\n从样本中我们能够学习得到朴素贝叶斯的先验分布概率\n$$\nP(Y=C_k)(k=1,2,…,K)\n$$\n然后学习得到条件概率分布\n$$\nP(X|Y=C_k)=P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\n$$\n最后结合贝叶斯公式便可得到X和Y的联合分布$P(X,Y=C_k)$\n$$\nP(X,Y=C_k)=P(Y=C_k)P(X|Y=C_k)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)\n\\\\ =P(Y=C_k)P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\\ \\ (2)\n$$\n上式中$P(Y=C_k)$可以通过极大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)$很难求出，因此朴素贝叶斯模型做一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出\n$$\nP(X_1=x^{(1)},X_2=x^{(2)},...,X_n=x^{(n)}|Y=C_k)\n\\\\ =P(X_1=x^{(1)}|Y=C_k)P(X_2=x^{(2)}|Y=C_k),...,P(X_n=x^{(n)}|Y=C_k)\n$$\n这样我们便得到X和Y的联合分布，最后的问题是给定测试集$(x_1^{(test)},x_2^{(test)},…,x_n^{(test)})$，如何判断它属于哪个类型？\n\n#### 2.3朴素贝叶斯推断\n\n假如我们预测的类别结果为$C_{result}$，其中$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大的类别，数学表达式为\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k|X=X^{(test)})\n\\\\= \\underset{C_k}{\\underbrace{\\arg\\max}} \\frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}\n$$\n由于对所有类别计算$P(Y=C_k|X=X^{(test)})$时，分母都是$P(X=X^{(test)})$，因此我们的预测公式可以简化为\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(X=X^{(test)}|Y=C_k)P(Y=C_k)\n$$\n然后利用朴素贝叶斯的独立性假设，就可以得到朴素贝叶斯推断公式\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n#### 2.4朴素贝叶斯参数估计\n\n对于**2.3**中朴素贝叶斯推断公式，我们只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$，就可以得到预测结果。对于$P(Y=C_k)$比较简单，通过极大似然估计能够得到$C_k$出现的概率，也就是样本类别$C_k$出现的次数$m_k$除以样本总数m。而对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$则取决于我们的先验条件。\n\n+ 如果$X_j$是**离散值**，那么可以假设$X_j$服从多项式分布，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别中$X_j^{test}$出现的频率，公式如下所示。其中$m_k$为样本类别$C_k$出现的次数，而$m_{kj}^{(test)}$为类别是$C_k$的样本中，第j维特征$X_j^{(test)}$出现的次数。\n  $$\n  P(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}}{m_k}\n  $$\n  某些时候可能一些类别在样本中从来没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，因此会影响后验概率的估计。为了解决此类情况，我们引入拉普拉斯平滑，公式如下所示。其中$\\lambda$为大于0的常数，常取为1，$O_j$为第j个特征的取值个数。\n\n$$\nP(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}+\\lambda}{m_k+O_j \\lambda}\n$$\n\n+ 如果$X_j$是**稀疏的离散值**，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中$X_j^{(test)}$出现的频率，公式如下所示。其中$X_j^{(test)}$取值为0和1。\n\n$$\nP(X_j=X_j^{(test)}|Y=C_k)=P(X_j|Y=C_k)X_j^{(test)}+(1-P(X_j|Y=C_k))(1-X_j^{(test)})\n$$\n\n+ 如果$X_j$是**连续值**，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布公式如下所示。其中$\\mu_k$和$\\sigma_{k}^2$是正态分布的期望和方差，$\\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\\mu_k$和$\\sigma _{k}^2$可以通过极大似然估计求得。\n  $$\n  P(X_j=X_j^{(test)}|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp(-\\frac{(X_j^{(test)}- \\mu_k)^2}{2\\sigma_{k}^{2}})\n  $$\n\n\n\n### 3.朴素贝叶斯算法流程\n\n我们总结下朴素贝叶斯算法流程，假设训练集有m个样本n个维度，训练样本有K个特征输出类别，分别为$C1,C2,…,C_K$，每个特征输出类别的样本个数为$m_1,m_2,…,m_K$。第K个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$，其中$l$的取值为$1,2,…,S_j$，$S_j$为特征$j$不同的取值数。\n$$\n\\{ (x_{1}^{(1)},x_{2}^{(1)},...,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},...,x_{n}^{(2)},y_2),...,(x_{1}^{(m)},x_{2}^{(m)},...,x_{n}^{(m)},y_m)\\}\n$$\n预测结果为$X^{(test)}​$的分类，算法流程如下所示\n\n+ 如果没有Y的先验概率，则计算Y的K个先验概率$P(Y=C_k)=\\frac{m_k}{m}$。\n\n+ 分别计算第k个类别的第j为特征的第l个取值条件概率$P(X_j=x_{jl}|Y=C_k)$。\n\n  + 如果是离散值，则公式如下所示，其中$\\lambda$为大于0的常数，常常取为1，$O_j$为第j个特征的取值个数。\n\n  $$\n  P(X_j=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+O_j \\lambda}\n  $$\n\n  + 如果是稀疏离散值，则公式如下所示，此时$l$取值为0或1。\n\n  $$\n  P(X_j=x_{jl}|Y=C_k)=P(X_j|Y=C_k)x_{jl}+(1-P(X_j|Y=C_k))(1-x_{jl})\n  $$\n\n  + 如果是连续值，则不需要计算各个l个取值概率，直接求正态分布的参数，公式如下所示。$\\mu_k$为样本类别$C_k$中所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中所有$X_j$的方差。\n\n  $$\n  P(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}exp(-\\frac{(x_j- \\mu_k)^2}{2\\sigma_{k}^{2}})\n  $$\n\n+ 对于实例$X^{(test)}$，分别计算\n\n$$\nP(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n+ 确定实例$X^{(test)}$的分类$C_{result}$\n\n$$\nC_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)\n$$\n\n从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。\n\n### 4.Sklearn实现朴素贝叶斯\n\n利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.933333333333。更多关于sklearn.naive_bayes的使用技巧可以访问[官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)。\n\n```python\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n#load data\niris=load_iris()\nX=iris.data\ny=iris.target\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nmnb=GaussianNB()\nmnb.fit(X_train,y_train)\n\nprint(mnb.predict(X_test))\n# [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 1 2 1]\nprint(y_test)\n# [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n#  0 1 2 2 0 2 2 1]\nprint(mnb.score(X_test,y_test))\n# 0.933333333333\n```\n\n### 5.朴素贝叶斯优缺点\n\n#### 5.1优点\n\n+ 具有稳定的分类效率。\n+ 对缺失数据不敏感，算法也比较简单。\n+ 对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。\n\n#### 5.2缺点\n\n+ 对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。\n+ 由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。\n+ 假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n> [刘建平_Pinard-朴素贝叶斯算法原理小结](https://www.cnblogs.com/pinard/p/6069267.html)","slug":"机器学习之朴素贝叶斯算法","published":1,"updated":"2018-05-20T04:02:07.918Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdy00102e013zkegnvw","content":"<h3 id=\"1-朴素贝叶斯简介\"><a href=\"#1-朴素贝叶斯简介\" class=\"headerlink\" title=\"1.朴素贝叶斯简介\"></a>1.朴素贝叶斯简介</h3><p><strong>朴素贝叶斯(Naive Bayesian)</strong>算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布$P(X,Y)$，然后用$P(Y|X)=\\frac{P(X,Y)}{P(X)}$得出。</p>\n<p>朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。</p>\n<h3 id=\"2-朴素贝叶斯算法模型\"><a href=\"#2-朴素贝叶斯算法模型\" class=\"headerlink\" title=\"2.朴素贝叶斯算法模型\"></a>2.朴素贝叶斯算法模型</h3><h4 id=\"2-1统计知识回顾\"><a href=\"#2-1统计知识回顾\" class=\"headerlink\" title=\"2.1统计知识回顾\"></a>2.1统计知识回顾</h4><p>深入算法原理之前，我们先来回顾下统计学的相关知识。</p>\n<ul>\n<li><strong>条件概率公式</strong></li>\n</ul>\n<p>$$<br>P(X,Y)=P(X)P(Y)\\ X、Y相互独立<br>$$</p>\n<ul>\n<li><strong>条件概率公式</strong></li>\n</ul>\n<p>$$<br>P(Y|X)=\\frac{P(X,Y)}{P(X)}<br>\\ P(X|Y)=\\frac{P(X,Y)}{P(Y)}<br>$$</p>\n<ul>\n<li><strong>全概率公式</strong></li>\n</ul>\n<p>$$<br>P(X)=\\sum_{j}P(X|Y=Y_j)P(Y_j)\\ 其中\\sum _{j}P(Y_j)=1<br>$$</p>\n<p>经过上面统计学知识，我们能够得出贝叶斯公式。<br>$$<br>P(Y_k|X)=\\frac{P(X,Y_k)}{P(X)}=\\frac{P(X|Y_k)P(Y_k)}{\\sum_{k=1}^{K}P(X|Y=Y_k)P(Y_k)}<br>$$</p>\n<h4 id=\"2-2朴素贝叶斯模型\"><a href=\"#2-2朴素贝叶斯模型\" class=\"headerlink\" title=\"2.2朴素贝叶斯模型\"></a>2.2朴素贝叶斯模型</h4><p>假设我们已经有部分数据，并且能从数据中得到先验概率，那么如何得到后验概率$P(Y_k|X)$呢？下面我们通过构建朴素贝叶斯分类模型来解决后验概率问题，假设分类模型数据有m个样本，每个样本有n个特征，特征输出有K个类别，定义为${C1,C2,…,Ck}$。<br>$$<br>{ (x_{1}^{(1)},x_{2}^{(1)},…,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},…,x_{n}^{(2)},y_2),…,(x_{1}^{(m)},x_{2}^{(m)},…,x_{n}^{(m)},y_m)}<br>$$<br>从样本中我们能够学习得到朴素贝叶斯的先验分布概率<br>$$<br>P(Y=C_k)(k=1,2,…,K)<br>$$<br>然后学习得到条件概率分布<br>$$<br>P(X|Y=C_k)=P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)<br>$$<br>最后结合贝叶斯公式便可得到X和Y的联合分布$P(X,Y=C_k)$<br>$$<br>P(X,Y=C_k)=P(Y=C_k)P(X|Y=C_k)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)<br>\\ =P(Y=C_k)P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)\\ \\ (2)<br>$$<br>上式中$P(Y=C_k)$可以通过极大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)$很难求出，因此朴素贝叶斯模型做一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出<br>$$<br>P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)<br>\\ =P(X_1=x^{(1)}|Y=C_k)P(X_2=x^{(2)}|Y=C_k),…,P(X_n=x^{(n)}|Y=C_k)<br>$$<br>这样我们便得到X和Y的联合分布，最后的问题是给定测试集$(x_1^{(test)},x_2^{(test)},…,x_n^{(test)})$，如何判断它属于哪个类型？</p>\n<h4 id=\"2-3朴素贝叶斯推断\"><a href=\"#2-3朴素贝叶斯推断\" class=\"headerlink\" title=\"2.3朴素贝叶斯推断\"></a>2.3朴素贝叶斯推断</h4><p>假如我们预测的类别结果为$C_{result}$，其中$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大的类别，数学表达式为<br>$$<br>C_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k|X=X^{(test)})<br>\\= \\underset{C_k}{\\underbrace{\\arg\\max}} \\frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}<br>$$<br>由于对所有类别计算$P(Y=C_k|X=X^{(test)})$时，分母都是$P(X=X^{(test)})$，因此我们的预测公式可以简化为<br>$$<br>C_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(X=X^{(test)}|Y=C_k)P(Y=C_k)<br>$$<br>然后利用朴素贝叶斯的独立性假设，就可以得到朴素贝叶斯推断公式<br>$$<br>C_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)<br>$$</p>\n<h4 id=\"2-4朴素贝叶斯参数估计\"><a href=\"#2-4朴素贝叶斯参数估计\" class=\"headerlink\" title=\"2.4朴素贝叶斯参数估计\"></a>2.4朴素贝叶斯参数估计</h4><p>对于<strong>2.3</strong>中朴素贝叶斯推断公式，我们只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$，就可以得到预测结果。对于$P(Y=C_k)$比较简单，通过极大似然估计能够得到$C_k$出现的概率，也就是样本类别$C_k$出现的次数$m_k$除以样本总数m。而对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$则取决于我们的先验条件。</p>\n<ul>\n<li>如果$X_j$是<strong>离散值</strong>，那么可以假设$X_j$服从多项式分布，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别中$X_j^{test}$出现的频率，公式如下所示。其中$m_k$为样本类别$C_k$出现的次数，而$m_{kj}^{(test)}$为类别是$C_k$的样本中，第j维特征$X_j^{(test)}$出现的次数。<br>$$<br>P(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}}{m_k}<br>$$<br>某些时候可能一些类别在样本中从来没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，因此会影响后验概率的估计。为了解决此类情况，我们引入拉普拉斯平滑，公式如下所示。其中$\\lambda$为大于0的常数，常取为1，$O_j$为第j个特征的取值个数。</li>\n</ul>\n<p>$$<br>P(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}+\\lambda}{m_k+O_j \\lambda}<br>$$</p>\n<ul>\n<li>如果$X_j$是<strong>稀疏的离散值</strong>，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中$X_j^{(test)}$出现的频率，公式如下所示。其中$X_j^{(test)}$取值为0和1。</li>\n</ul>\n<p>$$<br>P(X_j=X_j^{(test)}|Y=C_k)=P(X_j|Y=C_k)X_j^{(test)}+(1-P(X_j|Y=C_k))(1-X_j^{(test)})<br>$$</p>\n<ul>\n<li>如果$X_j$是<strong>连续值</strong>，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布公式如下所示。其中$\\mu_k$和$\\sigma_{k}^2$是正态分布的期望和方差，$\\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\\mu_k$和$\\sigma _{k}^2$可以通过极大似然估计求得。<br>$$<br>P(X_j=X_j^{(test)}|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp(-\\frac{(X_j^{(test)}- \\mu_k)^2}{2\\sigma_{k}^{2}})<br>$$</li>\n</ul>\n<h3 id=\"3-朴素贝叶斯算法流程\"><a href=\"#3-朴素贝叶斯算法流程\" class=\"headerlink\" title=\"3.朴素贝叶斯算法流程\"></a>3.朴素贝叶斯算法流程</h3><p>我们总结下朴素贝叶斯算法流程，假设训练集有m个样本n个维度，训练样本有K个特征输出类别，分别为$C1,C2,…,C_K$，每个特征输出类别的样本个数为$m_1,m_2,…,m_K$。第K个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$，其中$l$的取值为$1,2,…,S_j$，$S_j$为特征$j$不同的取值数。<br>$$<br>{ (x_{1}^{(1)},x_{2}^{(1)},…,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},…,x_{n}^{(2)},y_2),…,(x_{1}^{(m)},x_{2}^{(m)},…,x_{n}^{(m)},y_m)}<br>$$<br>预测结果为$X^{(test)}​$的分类，算法流程如下所示</p>\n<ul>\n<li><p>如果没有Y的先验概率，则计算Y的K个先验概率$P(Y=C_k)=\\frac{m_k}{m}$。</p>\n</li>\n<li><p>分别计算第k个类别的第j为特征的第l个取值条件概率$P(X_j=x_{jl}|Y=C_k)$。</p>\n<ul>\n<li>如果是离散值，则公式如下所示，其中$\\lambda$为大于0的常数，常常取为1，$O_j$为第j个特征的取值个数。</li>\n</ul>\n<p>$$<br>P(X_j=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+O_j \\lambda}<br>$$</p>\n<ul>\n<li>如果是稀疏离散值，则公式如下所示，此时$l$取值为0或1。</li>\n</ul>\n<p>$$<br>P(X_j=x_{jl}|Y=C_k)=P(X_j|Y=C_k)x_{jl}+(1-P(X_j|Y=C_k))(1-x_{jl})<br>$$</p>\n<ul>\n<li>如果是连续值，则不需要计算各个l个取值概率，直接求正态分布的参数，公式如下所示。$\\mu_k$为样本类别$C_k$中所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中所有$X_j$的方差。</li>\n</ul>\n<p>$$<br>P(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}exp(-\\frac{(x_j- \\mu_k)^2}{2\\sigma_{k}^{2}})<br>$$</p>\n</li>\n<li><p>对于实例$X^{(test)}$，分别计算</p>\n</li>\n</ul>\n<p>$$<br>P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)<br>$$</p>\n<ul>\n<li>确定实例$X^{(test)}$的分类$C_{result}$</li>\n</ul>\n<p>$$<br>C_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)<br>$$</p>\n<p>从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。</p>\n<h3 id=\"4-Sklearn实现朴素贝叶斯\"><a href=\"#4-Sklearn实现朴素贝叶斯\" class=\"headerlink\" title=\"4.Sklearn实现朴素贝叶斯\"></a>4.Sklearn实现朴素贝叶斯</h3><p>利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.933333333333。更多关于sklearn.naive_bayes的使用技巧可以访问<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\" target=\"_blank\" rel=\"noopener\">官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">mnb=GaussianNB()</span><br><span class=\"line\">mnb.fit(X_train,y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">print(mnb.predict(X_test))</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 1 2 1]</span></span><br><span class=\"line\">print(y_test)</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 2 2 1]</span></span><br><span class=\"line\">print(mnb.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.933333333333</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"5-朴素贝叶斯优缺点\"><a href=\"#5-朴素贝叶斯优缺点\" class=\"headerlink\" title=\"5.朴素贝叶斯优缺点\"></a>5.朴素贝叶斯优缺点</h3><h4 id=\"5-1优点\"><a href=\"#5-1优点\" class=\"headerlink\" title=\"5.1优点\"></a>5.1优点</h4><ul>\n<li>具有稳定的分类效率。</li>\n<li>对缺失数据不敏感，算法也比较简单。</li>\n<li>对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。</li>\n</ul>\n<h4 id=\"5-2缺点\"><a href=\"#5-2缺点\" class=\"headerlink\" title=\"5.2缺点\"></a>5.2缺点</h4><ul>\n<li>对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。</li>\n<li>由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。</li>\n<li>假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。</li>\n</ul>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6069267.html\" target=\"_blank\" rel=\"noopener\">刘建平_Pinard-朴素贝叶斯算法原理小结</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-朴素贝叶斯简介\"><a href=\"#1-朴素贝叶斯简介\" class=\"headerlink\" title=\"1.朴素贝叶斯简介\"></a>1.朴素贝叶斯简介</h3><p><strong>朴素贝叶斯(Naive Bayesian)</strong>算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布$P(X,Y)$，然后用$P(Y|X)=\\frac{P(X,Y)}{P(X)}$得出。</p>\n<p>朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。</p>\n<h3 id=\"2-朴素贝叶斯算法模型\"><a href=\"#2-朴素贝叶斯算法模型\" class=\"headerlink\" title=\"2.朴素贝叶斯算法模型\"></a>2.朴素贝叶斯算法模型</h3><h4 id=\"2-1统计知识回顾\"><a href=\"#2-1统计知识回顾\" class=\"headerlink\" title=\"2.1统计知识回顾\"></a>2.1统计知识回顾</h4><p>深入算法原理之前，我们先来回顾下统计学的相关知识。</p>\n<ul>\n<li><strong>条件概率公式</strong></li>\n</ul>\n<p>$$<br>P(X,Y)=P(X)P(Y)\\ X、Y相互独立<br>$$</p>\n<ul>\n<li><strong>条件概率公式</strong></li>\n</ul>\n<p>$$<br>P(Y|X)=\\frac{P(X,Y)}{P(X)}<br>\\ P(X|Y)=\\frac{P(X,Y)}{P(Y)}<br>$$</p>\n<ul>\n<li><strong>全概率公式</strong></li>\n</ul>\n<p>$$<br>P(X)=\\sum_{j}P(X|Y=Y_j)P(Y_j)\\ 其中\\sum _{j}P(Y_j)=1<br>$$</p>\n<p>经过上面统计学知识，我们能够得出贝叶斯公式。<br>$$<br>P(Y_k|X)=\\frac{P(X,Y_k)}{P(X)}=\\frac{P(X|Y_k)P(Y_k)}{\\sum_{k=1}^{K}P(X|Y=Y_k)P(Y_k)}<br>$$</p>\n<h4 id=\"2-2朴素贝叶斯模型\"><a href=\"#2-2朴素贝叶斯模型\" class=\"headerlink\" title=\"2.2朴素贝叶斯模型\"></a>2.2朴素贝叶斯模型</h4><p>假设我们已经有部分数据，并且能从数据中得到先验概率，那么如何得到后验概率$P(Y_k|X)$呢？下面我们通过构建朴素贝叶斯分类模型来解决后验概率问题，假设分类模型数据有m个样本，每个样本有n个特征，特征输出有K个类别，定义为${C1,C2,…,Ck}$。<br>$$<br>{ (x_{1}^{(1)},x_{2}^{(1)},…,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},…,x_{n}^{(2)},y_2),…,(x_{1}^{(m)},x_{2}^{(m)},…,x_{n}^{(m)},y_m)}<br>$$<br>从样本中我们能够学习得到朴素贝叶斯的先验分布概率<br>$$<br>P(Y=C_k)(k=1,2,…,K)<br>$$<br>然后学习得到条件概率分布<br>$$<br>P(X|Y=C_k)=P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)<br>$$<br>最后结合贝叶斯公式便可得到X和Y的联合分布$P(X,Y=C_k)$<br>$$<br>P(X,Y=C_k)=P(Y=C_k)P(X|Y=C_k)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)<br>\\ =P(Y=C_k)P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)\\ \\ (2)<br>$$<br>上式中$P(Y=C_k)$可以通过极大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)$很难求出，因此朴素贝叶斯模型做一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出<br>$$<br>P(X_1=x^{(1)},X_2=x^{(2)},…,X_n=x^{(n)}|Y=C_k)<br>\\ =P(X_1=x^{(1)}|Y=C_k)P(X_2=x^{(2)}|Y=C_k),…,P(X_n=x^{(n)}|Y=C_k)<br>$$<br>这样我们便得到X和Y的联合分布，最后的问题是给定测试集$(x_1^{(test)},x_2^{(test)},…,x_n^{(test)})$，如何判断它属于哪个类型？</p>\n<h4 id=\"2-3朴素贝叶斯推断\"><a href=\"#2-3朴素贝叶斯推断\" class=\"headerlink\" title=\"2.3朴素贝叶斯推断\"></a>2.3朴素贝叶斯推断</h4><p>假如我们预测的类别结果为$C_{result}$，其中$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大的类别，数学表达式为<br>$$<br>C_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k|X=X^{(test)})<br>\\= \\underset{C_k}{\\underbrace{\\arg\\max}} \\frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}<br>$$<br>由于对所有类别计算$P(Y=C_k|X=X^{(test)})$时，分母都是$P(X=X^{(test)})$，因此我们的预测公式可以简化为<br>$$<br>C_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(X=X^{(test)}|Y=C_k)P(Y=C_k)<br>$$<br>然后利用朴素贝叶斯的独立性假设，就可以得到朴素贝叶斯推断公式<br>$$<br>C_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)<br>$$</p>\n<h4 id=\"2-4朴素贝叶斯参数估计\"><a href=\"#2-4朴素贝叶斯参数估计\" class=\"headerlink\" title=\"2.4朴素贝叶斯参数估计\"></a>2.4朴素贝叶斯参数估计</h4><p>对于<strong>2.3</strong>中朴素贝叶斯推断公式，我们只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$，就可以得到预测结果。对于$P(Y=C_k)$比较简单，通过极大似然估计能够得到$C_k$出现的概率，也就是样本类别$C_k$出现的次数$m_k$除以样本总数m。而对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,…,n)$则取决于我们的先验条件。</p>\n<ul>\n<li>如果$X_j$是<strong>离散值</strong>，那么可以假设$X_j$服从多项式分布，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别中$X_j^{test}$出现的频率，公式如下所示。其中$m_k$为样本类别$C_k$出现的次数，而$m_{kj}^{(test)}$为类别是$C_k$的样本中，第j维特征$X_j^{(test)}$出现的次数。<br>$$<br>P(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}}{m_k}<br>$$<br>某些时候可能一些类别在样本中从来没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，因此会影响后验概率的估计。为了解决此类情况，我们引入拉普拉斯平滑，公式如下所示。其中$\\lambda$为大于0的常数，常取为1，$O_j$为第j个特征的取值个数。</li>\n</ul>\n<p>$$<br>P(X_j=X_j^{(test)}|Y=C_k)=\\frac{m_{kj}^{(test)}+\\lambda}{m_k+O_j \\lambda}<br>$$</p>\n<ul>\n<li>如果$X_j$是<strong>稀疏的离散值</strong>，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中$X_j^{(test)}$出现的频率，公式如下所示。其中$X_j^{(test)}$取值为0和1。</li>\n</ul>\n<p>$$<br>P(X_j=X_j^{(test)}|Y=C_k)=P(X_j|Y=C_k)X_j^{(test)}+(1-P(X_j|Y=C_k))(1-X_j^{(test)})<br>$$</p>\n<ul>\n<li>如果$X_j$是<strong>连续值</strong>，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布公式如下所示。其中$\\mu_k$和$\\sigma_{k}^2$是正态分布的期望和方差，$\\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\\mu_k$和$\\sigma _{k}^2$可以通过极大似然估计求得。<br>$$<br>P(X_j=X_j^{(test)}|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp(-\\frac{(X_j^{(test)}- \\mu_k)^2}{2\\sigma_{k}^{2}})<br>$$</li>\n</ul>\n<h3 id=\"3-朴素贝叶斯算法流程\"><a href=\"#3-朴素贝叶斯算法流程\" class=\"headerlink\" title=\"3.朴素贝叶斯算法流程\"></a>3.朴素贝叶斯算法流程</h3><p>我们总结下朴素贝叶斯算法流程，假设训练集有m个样本n个维度，训练样本有K个特征输出类别，分别为$C1,C2,…,C_K$，每个特征输出类别的样本个数为$m_1,m_2,…,m_K$。第K个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$，其中$l$的取值为$1,2,…,S_j$，$S_j$为特征$j$不同的取值数。<br>$$<br>{ (x_{1}^{(1)},x_{2}^{(1)},…,x_{n}^{(1)},y_1),(x_{1}^{(2)},x_{2}^{(2)},…,x_{n}^{(2)},y_2),…,(x_{1}^{(m)},x_{2}^{(m)},…,x_{n}^{(m)},y_m)}<br>$$<br>预测结果为$X^{(test)}​$的分类，算法流程如下所示</p>\n<ul>\n<li><p>如果没有Y的先验概率，则计算Y的K个先验概率$P(Y=C_k)=\\frac{m_k}{m}$。</p>\n</li>\n<li><p>分别计算第k个类别的第j为特征的第l个取值条件概率$P(X_j=x_{jl}|Y=C_k)$。</p>\n<ul>\n<li>如果是离散值，则公式如下所示，其中$\\lambda$为大于0的常数，常常取为1，$O_j$为第j个特征的取值个数。</li>\n</ul>\n<p>$$<br>P(X_j=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+O_j \\lambda}<br>$$</p>\n<ul>\n<li>如果是稀疏离散值，则公式如下所示，此时$l$取值为0或1。</li>\n</ul>\n<p>$$<br>P(X_j=x_{jl}|Y=C_k)=P(X_j|Y=C_k)x_{jl}+(1-P(X_j|Y=C_k))(1-x_{jl})<br>$$</p>\n<ul>\n<li>如果是连续值，则不需要计算各个l个取值概率，直接求正态分布的参数，公式如下所示。$\\mu_k$为样本类别$C_k$中所有$X_j$的平均值，$\\sigma_{k}^2$为样本类别$C_k$中所有$X_j$的方差。</li>\n</ul>\n<p>$$<br>P(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}exp(-\\frac{(x_j- \\mu_k)^2}{2\\sigma_{k}^{2}})<br>$$</p>\n</li>\n<li><p>对于实例$X^{(test)}$，分别计算</p>\n</li>\n</ul>\n<p>$$<br>P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)<br>$$</p>\n<ul>\n<li>确定实例$X^{(test)}$的分类$C_{result}$</li>\n</ul>\n<p>$$<br>C_{result}=\\underset{C_k}{\\underbrace{\\arg\\max}} P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)<br>$$</p>\n<p>从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。</p>\n<h3 id=\"4-Sklearn实现朴素贝叶斯\"><a href=\"#4-Sklearn实现朴素贝叶斯\" class=\"headerlink\" title=\"4.Sklearn实现朴素贝叶斯\"></a>4.Sklearn实现朴素贝叶斯</h3><p>利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.933333333333。更多关于sklearn.naive_bayes的使用技巧可以访问<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\" target=\"_blank\" rel=\"noopener\">官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class=\"number\">0.3</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">mnb=GaussianNB()</span><br><span class=\"line\">mnb.fit(X_train,y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">print(mnb.predict(X_test))</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 1 2 1]</span></span><br><span class=\"line\">print(y_test)</span><br><span class=\"line\"><span class=\"comment\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span></span><br><span class=\"line\"><span class=\"comment\">#  0 1 2 2 0 2 2 1]</span></span><br><span class=\"line\">print(mnb.score(X_test,y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.933333333333</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"5-朴素贝叶斯优缺点\"><a href=\"#5-朴素贝叶斯优缺点\" class=\"headerlink\" title=\"5.朴素贝叶斯优缺点\"></a>5.朴素贝叶斯优缺点</h3><h4 id=\"5-1优点\"><a href=\"#5-1优点\" class=\"headerlink\" title=\"5.1优点\"></a>5.1优点</h4><ul>\n<li>具有稳定的分类效率。</li>\n<li>对缺失数据不敏感，算法也比较简单。</li>\n<li>对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。</li>\n</ul>\n<h4 id=\"5-2缺点\"><a href=\"#5-2缺点\" class=\"headerlink\" title=\"5.2缺点\"></a>5.2缺点</h4><ul>\n<li>对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。</li>\n<li>由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。</li>\n<li>假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。</li>\n</ul>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6069267.html\" target=\"_blank\" rel=\"noopener\">刘建平_Pinard-朴素贝叶斯算法原理小结</a></p>\n</blockquote>\n"},{"title":"机器学习之最大期望(EM)算法","date":"2018-05-09T02:20:41.000Z","mathjax":true,"comments":1,"_content":"\n### 1.EM算法简介\n\n**最大期望(Expectation Maximum)算法**是一种迭代优化算法，其计算方法是每次迭代分为**期望(E)步**和**最大(M)步**。我们先看下最大期望算法能够解决什么样的问题。\n\n假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。\n$$\nN_1(\\mu_1,\\sigma_1^2)\\ ; N_2(\\mu_2,\\sigma_2^2)\n$$\n但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ \n\nEM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。\n\n### 2.EM算法实例\n\n假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。\n\n| 硬币 |    结果    |  统计   |\n| :--: | :--------: | :-----: |\n|  1   | 正正反正反 | 3正-2反 |\n|  2   | 反反正正反 | 2正-3反 |\n|  1   | 正反反反反 | 1正-4反 |\n|  2   | 正反反正正 | 3正-2反 |\n|  1   | 反正正反反 | 2正-3反 |\n\n我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。\n$$\nP1=\\frac{3+1+2}{15}=0.4\n$$\n\n$$\nP2=\\frac{2+3}{10}=0.5\n$$\n\n下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?\n\n|  硬币   |    结果    |  统计   |\n| :-----: | :--------: | :-----: |\n| Unknown | 正正反正反 | 3正-2反 |\n| Unknown | 反反正正反 | 2正-3反 |\n| Unknown | 正反反反反 | 1正-4反 |\n| Unknown | 正反反正正 | 3正-2反 |\n| Unknown | 反正正反反 | 2正-3反 |\n\n此时我们加入**隐含变量z**，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。\n\n我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。\n\n例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2\\*0.2\\*0.2\\*0.8\\*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7\\*0.7\\*0.7\\*0.3\\*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。\n\n| 轮数 | 若是硬币1 | 若是硬币2 | 最有可能硬币 |\n| :--: | :-------: | :-------: | :----------: |\n|  1   |  0.00512  |  0.03087  |    硬币2     |\n|  2   |  0.02048  |  0.01323  |    硬币1     |\n|  3   |  0.08192  |  0.00567  |    硬币1     |\n|  4   |  0.00512  |  0.03087  |    硬币2     |\n|  5   |  0.02048  |  0.01323  |    硬币1     |\n\n我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到\n$$\nP1=\\frac{2+1+2}{15}=0.33\n$$\n\n$$\nP2=\\frac{3+3}{10}=0.6\n$$\n\nP1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。\n\n| 初始化的P1 | 估计的P1 | 真实的P1 | 初始化的P2 | 估计的P2 | 真实的P2 |\n| :--------: | :------: | :------: | :--------: | :------: | :------: |\n|    0.2     |   0.33   |   0.4    |    0.7     |   0.6    |   0.5    |\n\n可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。\n\n上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?\n\n但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。\n\n| 轮数 | 若是硬币1 | 若是硬币2 |\n| :--: | :-------: | :-------: |\n|  1   |  0.00512  |  0.03087  |\n|  2   |  0.02048  |  0.01323  |\n|  3   |  0.08192  |  0.00567  |\n|  4   |  0.00512  |  0.03087  |\n|  5   |  0.02048  |  0.01323  |\n\n利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率\n$$\nz_1=\\frac{0.00512}{0.00512+0.03087}=0.14\n$$\n相应的算出其他4轮的概率。\n\n| 轮数 | z_i=硬币1 | z_i=硬币2 |\n| :--: | :-------: | :-------: |\n|  1   |   0.14    |   0.86    |\n|  2   |   0.61    |   0.39    |\n|  3   |   0.94    |   0.06    |\n|  4   |   0.14    |   0.86    |\n|  5   |   0.61    |   0.39    |\n\n上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为**E步**。\n\n按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14\\*3=0.42的概率为正，有0.14\\*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为**M步**。\n\n| 轮数 | 正面 | 反面 |\n| :--: | :--: | :--: |\n|  1   | 0.42 | 0.28 |\n|  2   | 1.22 | 1.83 |\n|  3   | 0.94 | 3.76 |\n|  4   | 0.42 | 0.28 |\n|  5   | 1.22 | 1.93 |\n| 总计 | 4.22 | 7.98 |\n\n$$\nP1=\\frac{4.22}{4.22+7.98}=0.35\n$$\n上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。\n\n### 3.EM算法推导\n\n对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数$\\theta$，极大化模型分布的对数似然函数如下所示\n$$\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)\n$$\n如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下\n$$\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)=\\arg \\max_{\\theta} \\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)\n$$\n上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入**Jensen不等式**。\n\n> 设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。\n>\n> **Jensen不等式定义：**如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。\n\n我们再回到上述推导过程，得到如下方程式。\n$$\n\\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta) \n\\\\=\\sum _{i=1}^{m}log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\\ \\ \\ \\ \\ \\ \\ (1)\n\\\\ \\ge \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})} \\ \\ \\ \\ \\ \\ \\ (2)\n$$\n\n我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。\n\n首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$是$\\frac{P(x^{(i)},z^{(i);\\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$便相当于f(E(X))，同时$\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。\n\n如果要满足Jensen不等式的等号，那么需要满足X为常量，即为\n$$\n\\frac{P(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c,\\ c为常量\n$$\n那么稍加改变能够得到\n$$\nc Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\\theta)\\ ,c为常量\n$$\n\n$$\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta),\\ c为常量\n$$\n\n其中c是不依赖于$z^{(i)}$的常量，由于$Q_i(z^{(i)})$是一个分布，所以满足\n$$\n\\sum_{z}Q_i(z^{(i)})=1\n$$\n\n$$\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta)= c\n$$\n\n因此得到下列方程，其中方程(3)利用到条件概率公式。\n$$\nQ_i(z^{(i)})=\\frac{P(x^{(i)},z^{(i)};\\theta)}{c}=\\frac{P(x^{(i)},z^{(i)};\\theta)}{\\sum _z P(x^{(i)},z^{(i)};\\theta)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n\\\\=\\frac{P(x^{(i)},z^{(i)};\\theta)}{ P(x^{(i)};\\theta)}=P(z^{(i)}|x^{(i)};\\theta)  \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\   (3)\n$$\n如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)$，那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式\n$$\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\n$$\n去掉上式中常数部分，则我们需要极大化的对数似然下界为\n$$\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\\theta)-log {Q_i(z^{(i)})}]\n\\\\=\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)\n$$\n注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)$可以理解为$logP(x^{(i)},z^{(i)};\\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中**E步**。极大化方程式(4)也就是我们EM算法中的**M步**。\n\n### 4.EM算法流程\n\n现在我们总结下EM算法流程。\n\n输入：观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，联合分布$P(x^{(i)},z^{(i)};\\theta)$，条件分布$P(z^{(i)}|x^{(i)};\\theta)$，最大迭代次数$J$。\n\n+ 随机初始化模型参数$\\theta$的初始值$\\theta^0$。\n\n+ $for\\  j\\  from \\ 1\\  to \\ J$开始EM算法迭代。\n\n  + E步:计算联合分布的条件概率期望\n\n  $$\n  Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)\n  $$\n\n  $$\n  L(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\n  $$\n\n  + M步:极大化$L(\\theta,\\theta^j)$，得到$\\theta^{j+1}$\n\n  $$\n  \\theta^{j+1}=\\arg \\max _{x} L(\\theta,\\theta^j)\n  $$\n\n  + 如果$\\theta^{j+1}$收敛，则算法结束，否则继续迭代。\n\n+ 输出模型参数$\\theta$。\n\n### 5.EM算法的收敛性\n\n我们现在来解答下**2.EM算法实例**中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？\n\n首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即\n$$\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})\\ge \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j})\n$$\n由于\n$$\nL(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(x^{(i)},z^{(i)};\\theta)\n$$\n令\n$$\nH(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(z^{(i)}|x^{(i)};\\theta)\n$$\n上两式相减得到\n$$\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta)=L(\\theta,\\theta^j)-H(\\theta,\\theta^j)\n$$\n在上式中分别取$\\theta$为$\\theta^j$和$\\theta^{j+1}$，并相减得到\n$$\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta^{j+1})-\\sum_{i=1}^{m}log P(x^{(i)};\\theta^j)=[L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)]-[H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)]\n$$\n要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。由于$\\theta^{j+1}$使得$L(\\theta,\\theta^j)$极大，因此有\n$$\nL(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)\\ge 0\n$$\n而对于第二部分，我们有\n$$\nH(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log \\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\ \\ \\ \\ \\ \\ \\ \\ \\ (5)\n\\\\ \\le \\sum _{i=1}^{m}log \\{ \\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)\\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\}\\ \\ \\ \\ \\ \\ \\ \\ \\ (6)\n\\\\ = \\sum _{i=1}^{m}log (\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{j+1}))=0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7) \n$$\n其中第(6)式用到了Jensen不等式，只不过和第**3.EM算法推导**中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到\n$$\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})- \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j}) \\ge 0\n$$\n证明了EM算法的收敛性。\n\n从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。\n\n### 6.Sklearn实现EM算法\n\n高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去[这儿](https://mp.weixin.qq.com/s?src=11&timestamp=1525932817&ver=867&signature=U4UxviydVELD71Ju*bRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm*budZlpZdMQMLOcXB-FnvKKlkV2H&new=1)。下列代码来自于[Sklearn官网GMM模块](http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py)，利用高斯混合模型确定iris聚类。\n\n```python\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import StratifiedKFold\n\ncolors = ['navy', 'turquoise', 'darkorange']\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate(colors):\n        if gmm.covariance_type == 'full':\n            covariances = gmm.covariances_[n][:2, :2]\n        elif gmm.covariance_type == 'tied':\n            covariances = gmm.covariances_[:2, :2]\n        elif gmm.covariance_type == 'diag':\n            covariances = np.diag(gmm.covariances_[n][:2])\n        elif gmm.covariance_type == 'spherical':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n                                  180 + angle, color=color)\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n\niris = datasets.load_iris()\n\n# Break up the dataset into non-overlapping training (75%)\n# and testing (25%) sets.\nskf = StratifiedKFold(n_splits=4)\n# Only take the first fold.\ntrain_index, test_index = next(iter(skf.split(iris.data, iris.target)))\n\n\nX_train = iris.data[train_index]\ny_train = iris.target[train_index]\nX_test = iris.data[test_index]\ny_test = iris.target[test_index]\n\nn_classes = len(np.unique(y_train))\n\n# Try GMMs using different types of covariances.\nestimators = dict((cov_type, GaussianMixture(n_components=n_classes,\n                   covariance_type=cov_type, max_iter=20, random_state=0))\n                  for cov_type in ['spherical', 'diag', 'tied', 'full'])\n\nn_estimators = len(estimators)\n\nplt.figure(figsize=(3 * n_estimators // 2, 6))\nplt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n                    left=.01, right=.99)\n\n\nfor index, (name, estimator) in enumerate(estimators.items()):\n    # Since we have class labels for the training data, we can\n    # initialize the GMM parameters in a supervised manner.\n    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n                                    for i in range(n_classes)])\n\n    # Train the other parameters using the EM algorithm.\n    estimator.fit(X_train)\n\n    h = plt.subplot(2, n_estimators // 2, index + 1)\n    make_ellipses(estimator, h)\n\n    for n, color in enumerate(colors):\n        data = iris.data[iris.target == n]\n        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n                    label=iris.target_names[n])\n    # Plot the test data with crosses\n    for n, color in enumerate(colors):\n        data = X_test[y_test == n]\n        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n\n    y_train_pred = estimator.predict(X_train)\n    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n             transform=h.transAxes)\n\n    y_test_pred = estimator.predict(X_test)\n    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n             transform=h.transAxes)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(name)\n\nplt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\nplt.show()\n```\n\n![机器学习之最大期望算法图片01](机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png)\n\n### 7.EM算法优缺点\n\n#### 7.1优点\n\n+ 聚类。\n\n+ 算法计算结果稳定、准确。\n\n+ EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。\n\n#### 7.2缺点\n\n+ 对初始化数据敏感。\n\n+ EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。\n\n+ 当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n+ [milter_如何感性地理解EM算法](https://www.jianshu.com/p/1121509ac1dc)\n+ [刘建平Pinard_EM算法原理总结](https://www.cnblogs.com/pinard/p/6912636.html)\n+ [Gaussian mixture models](http://scikit-learn.org/stable/modules/mixture.html)\n\n","source":"_posts/机器学习之最大期望-EM-算法.md","raw":"---\ntitle: 机器学习之最大期望(EM)算法\ndate: 2018-05-09 10:20:41\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.EM算法简介\n\n**最大期望(Expectation Maximum)算法**是一种迭代优化算法，其计算方法是每次迭代分为**期望(E)步**和**最大(M)步**。我们先看下最大期望算法能够解决什么样的问题。\n\n假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。\n$$\nN_1(\\mu_1,\\sigma_1^2)\\ ; N_2(\\mu_2,\\sigma_2^2)\n$$\n但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ \n\nEM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。\n\n### 2.EM算法实例\n\n假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。\n\n| 硬币 |    结果    |  统计   |\n| :--: | :--------: | :-----: |\n|  1   | 正正反正反 | 3正-2反 |\n|  2   | 反反正正反 | 2正-3反 |\n|  1   | 正反反反反 | 1正-4反 |\n|  2   | 正反反正正 | 3正-2反 |\n|  1   | 反正正反反 | 2正-3反 |\n\n我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。\n$$\nP1=\\frac{3+1+2}{15}=0.4\n$$\n\n$$\nP2=\\frac{2+3}{10}=0.5\n$$\n\n下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?\n\n|  硬币   |    结果    |  统计   |\n| :-----: | :--------: | :-----: |\n| Unknown | 正正反正反 | 3正-2反 |\n| Unknown | 反反正正反 | 2正-3反 |\n| Unknown | 正反反反反 | 1正-4反 |\n| Unknown | 正反反正正 | 3正-2反 |\n| Unknown | 反正正反反 | 2正-3反 |\n\n此时我们加入**隐含变量z**，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。\n\n我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。\n\n例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2\\*0.2\\*0.2\\*0.8\\*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7\\*0.7\\*0.7\\*0.3\\*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。\n\n| 轮数 | 若是硬币1 | 若是硬币2 | 最有可能硬币 |\n| :--: | :-------: | :-------: | :----------: |\n|  1   |  0.00512  |  0.03087  |    硬币2     |\n|  2   |  0.02048  |  0.01323  |    硬币1     |\n|  3   |  0.08192  |  0.00567  |    硬币1     |\n|  4   |  0.00512  |  0.03087  |    硬币2     |\n|  5   |  0.02048  |  0.01323  |    硬币1     |\n\n我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到\n$$\nP1=\\frac{2+1+2}{15}=0.33\n$$\n\n$$\nP2=\\frac{3+3}{10}=0.6\n$$\n\nP1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。\n\n| 初始化的P1 | 估计的P1 | 真实的P1 | 初始化的P2 | 估计的P2 | 真实的P2 |\n| :--------: | :------: | :------: | :--------: | :------: | :------: |\n|    0.2     |   0.33   |   0.4    |    0.7     |   0.6    |   0.5    |\n\n可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。\n\n上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?\n\n但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。\n\n| 轮数 | 若是硬币1 | 若是硬币2 |\n| :--: | :-------: | :-------: |\n|  1   |  0.00512  |  0.03087  |\n|  2   |  0.02048  |  0.01323  |\n|  3   |  0.08192  |  0.00567  |\n|  4   |  0.00512  |  0.03087  |\n|  5   |  0.02048  |  0.01323  |\n\n利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率\n$$\nz_1=\\frac{0.00512}{0.00512+0.03087}=0.14\n$$\n相应的算出其他4轮的概率。\n\n| 轮数 | z_i=硬币1 | z_i=硬币2 |\n| :--: | :-------: | :-------: |\n|  1   |   0.14    |   0.86    |\n|  2   |   0.61    |   0.39    |\n|  3   |   0.94    |   0.06    |\n|  4   |   0.14    |   0.86    |\n|  5   |   0.61    |   0.39    |\n\n上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为**E步**。\n\n按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14\\*3=0.42的概率为正，有0.14\\*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为**M步**。\n\n| 轮数 | 正面 | 反面 |\n| :--: | :--: | :--: |\n|  1   | 0.42 | 0.28 |\n|  2   | 1.22 | 1.83 |\n|  3   | 0.94 | 3.76 |\n|  4   | 0.42 | 0.28 |\n|  5   | 1.22 | 1.93 |\n| 总计 | 4.22 | 7.98 |\n\n$$\nP1=\\frac{4.22}{4.22+7.98}=0.35\n$$\n上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。\n\n### 3.EM算法推导\n\n对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数$\\theta$，极大化模型分布的对数似然函数如下所示\n$$\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)\n$$\n如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下\n$$\n\\theta =\\arg \\max_{\\theta} \\sum _{i=1}^{m}logP(x^{(i)};\\theta)=\\arg \\max_{\\theta} \\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)\n$$\n上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入**Jensen不等式**。\n\n> 设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。\n>\n> **Jensen不等式定义：**如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。\n\n我们再回到上述推导过程，得到如下方程式。\n$$\n\\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta) \n\\\\=\\sum _{i=1}^{m}log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\\ \\ \\ \\ \\ \\ \\ (1)\n\\\\ \\ge \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})} \\ \\ \\ \\ \\ \\ \\ (2)\n$$\n\n我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。\n\n首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$是$\\frac{P(x^{(i)},z^{(i);\\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$便相当于f(E(X))，同时$\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。\n\n如果要满足Jensen不等式的等号，那么需要满足X为常量，即为\n$$\n\\frac{P(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c,\\ c为常量\n$$\n那么稍加改变能够得到\n$$\nc Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\\theta)\\ ,c为常量\n$$\n\n$$\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta),\\ c为常量\n$$\n\n其中c是不依赖于$z^{(i)}$的常量，由于$Q_i(z^{(i)})$是一个分布，所以满足\n$$\n\\sum_{z}Q_i(z^{(i)})=1\n$$\n\n$$\n\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta)= c\n$$\n\n因此得到下列方程，其中方程(3)利用到条件概率公式。\n$$\nQ_i(z^{(i)})=\\frac{P(x^{(i)},z^{(i)};\\theta)}{c}=\\frac{P(x^{(i)},z^{(i)};\\theta)}{\\sum _z P(x^{(i)},z^{(i)};\\theta)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n\\\\=\\frac{P(x^{(i)},z^{(i)};\\theta)}{ P(x^{(i)};\\theta)}=P(z^{(i)}|x^{(i)};\\theta)  \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\   (3)\n$$\n如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)$，那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式\n$$\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\n$$\n去掉上式中常数部分，则我们需要极大化的对数似然下界为\n$$\n\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\\theta)-log {Q_i(z^{(i)})}]\n\\\\=\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)\n$$\n注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)$可以理解为$logP(x^{(i)},z^{(i)};\\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中**E步**。极大化方程式(4)也就是我们EM算法中的**M步**。\n\n### 4.EM算法流程\n\n现在我们总结下EM算法流程。\n\n输入：观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，联合分布$P(x^{(i)},z^{(i)};\\theta)$，条件分布$P(z^{(i)}|x^{(i)};\\theta)$，最大迭代次数$J$。\n\n+ 随机初始化模型参数$\\theta$的初始值$\\theta^0$。\n\n+ $for\\  j\\  from \\ 1\\  to \\ J$开始EM算法迭代。\n\n  + E步:计算联合分布的条件概率期望\n\n  $$\n  Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)\n  $$\n\n  $$\n  L(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\n  $$\n\n  + M步:极大化$L(\\theta,\\theta^j)$，得到$\\theta^{j+1}$\n\n  $$\n  \\theta^{j+1}=\\arg \\max _{x} L(\\theta,\\theta^j)\n  $$\n\n  + 如果$\\theta^{j+1}$收敛，则算法结束，否则继续迭代。\n\n+ 输出模型参数$\\theta$。\n\n### 5.EM算法的收敛性\n\n我们现在来解答下**2.EM算法实例**中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？\n\n首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即\n$$\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})\\ge \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j})\n$$\n由于\n$$\nL(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(x^{(i)},z^{(i)};\\theta)\n$$\n令\n$$\nH(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(z^{(i)}|x^{(i)};\\theta)\n$$\n上两式相减得到\n$$\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta)=L(\\theta,\\theta^j)-H(\\theta,\\theta^j)\n$$\n在上式中分别取$\\theta$为$\\theta^j$和$\\theta^{j+1}$，并相减得到\n$$\n\\sum_{i=1}^{m}log P(x^{(i)};\\theta^{j+1})-\\sum_{i=1}^{m}log P(x^{(i)};\\theta^j)=[L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)]-[H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)]\n$$\n要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。由于$\\theta^{j+1}$使得$L(\\theta,\\theta^j)$极大，因此有\n$$\nL(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)\\ge 0\n$$\n而对于第二部分，我们有\n$$\nH(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log \\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\ \\ \\ \\ \\ \\ \\ \\ \\ (5)\n\\\\ \\le \\sum _{i=1}^{m}log \\{ \\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)\\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\}\\ \\ \\ \\ \\ \\ \\ \\ \\ (6)\n\\\\ = \\sum _{i=1}^{m}log (\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{j+1}))=0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7) \n$$\n其中第(6)式用到了Jensen不等式，只不过和第**3.EM算法推导**中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到\n$$\n\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})- \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j}) \\ge 0\n$$\n证明了EM算法的收敛性。\n\n从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。\n\n### 6.Sklearn实现EM算法\n\n高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去[这儿](https://mp.weixin.qq.com/s?src=11&timestamp=1525932817&ver=867&signature=U4UxviydVELD71Ju*bRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm*budZlpZdMQMLOcXB-FnvKKlkV2H&new=1)。下列代码来自于[Sklearn官网GMM模块](http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py)，利用高斯混合模型确定iris聚类。\n\n```python\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import StratifiedKFold\n\ncolors = ['navy', 'turquoise', 'darkorange']\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate(colors):\n        if gmm.covariance_type == 'full':\n            covariances = gmm.covariances_[n][:2, :2]\n        elif gmm.covariance_type == 'tied':\n            covariances = gmm.covariances_[:2, :2]\n        elif gmm.covariance_type == 'diag':\n            covariances = np.diag(gmm.covariances_[n][:2])\n        elif gmm.covariance_type == 'spherical':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n                                  180 + angle, color=color)\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n\niris = datasets.load_iris()\n\n# Break up the dataset into non-overlapping training (75%)\n# and testing (25%) sets.\nskf = StratifiedKFold(n_splits=4)\n# Only take the first fold.\ntrain_index, test_index = next(iter(skf.split(iris.data, iris.target)))\n\n\nX_train = iris.data[train_index]\ny_train = iris.target[train_index]\nX_test = iris.data[test_index]\ny_test = iris.target[test_index]\n\nn_classes = len(np.unique(y_train))\n\n# Try GMMs using different types of covariances.\nestimators = dict((cov_type, GaussianMixture(n_components=n_classes,\n                   covariance_type=cov_type, max_iter=20, random_state=0))\n                  for cov_type in ['spherical', 'diag', 'tied', 'full'])\n\nn_estimators = len(estimators)\n\nplt.figure(figsize=(3 * n_estimators // 2, 6))\nplt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n                    left=.01, right=.99)\n\n\nfor index, (name, estimator) in enumerate(estimators.items()):\n    # Since we have class labels for the training data, we can\n    # initialize the GMM parameters in a supervised manner.\n    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n                                    for i in range(n_classes)])\n\n    # Train the other parameters using the EM algorithm.\n    estimator.fit(X_train)\n\n    h = plt.subplot(2, n_estimators // 2, index + 1)\n    make_ellipses(estimator, h)\n\n    for n, color in enumerate(colors):\n        data = iris.data[iris.target == n]\n        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n                    label=iris.target_names[n])\n    # Plot the test data with crosses\n    for n, color in enumerate(colors):\n        data = X_test[y_test == n]\n        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n\n    y_train_pred = estimator.predict(X_train)\n    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n             transform=h.transAxes)\n\n    y_test_pred = estimator.predict(X_test)\n    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n             transform=h.transAxes)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(name)\n\nplt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\nplt.show()\n```\n\n![机器学习之最大期望算法图片01](机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png)\n\n### 7.EM算法优缺点\n\n#### 7.1优点\n\n+ 聚类。\n\n+ 算法计算结果稳定、准确。\n\n+ EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。\n\n#### 7.2缺点\n\n+ 对初始化数据敏感。\n\n+ EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。\n\n+ 当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n参考\n\n+ [milter_如何感性地理解EM算法](https://www.jianshu.com/p/1121509ac1dc)\n+ [刘建平Pinard_EM算法原理总结](https://www.cnblogs.com/pinard/p/6912636.html)\n+ [Gaussian mixture models](http://scikit-learn.org/stable/modules/mixture.html)\n\n","slug":"机器学习之最大期望-EM-算法","published":1,"updated":"2018-05-20T04:02:29.484Z","layout":"post","photos":[],"link":"","_id":"cji4rdzdz00142e01xw2dy74c","content":"<h3 id=\"1-EM算法简介\"><a href=\"#1-EM算法简介\" class=\"headerlink\" title=\"1.EM算法简介\"></a>1.EM算法简介</h3><p><strong>最大期望(Expectation Maximum)算法</strong>是一种迭代优化算法，其计算方法是每次迭代分为<strong>期望(E)步</strong>和<strong>最大(M)步</strong>。我们先看下最大期望算法能够解决什么样的问题。</p>\n<p>假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。<br>$$<br>N_1(\\mu_1,\\sigma_1^2)\\ ; N_2(\\mu_2,\\sigma_2^2)<br>$$<br>但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ </p>\n<p>EM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。</p>\n<h3 id=\"2-EM算法实例\"><a href=\"#2-EM算法实例\" class=\"headerlink\" title=\"2.EM算法实例\"></a>2.EM算法实例</h3><p>假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">硬币</th>\n<th style=\"text-align:center\">结果</th>\n<th style=\"text-align:center\">统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正正反正反</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">反反正正反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正反反反反</td>\n<td style=\"text-align:center\">1正-4反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">正反反正正</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">反正正反反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n</tbody>\n</table>\n<p>我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。<br>$$<br>P1=\\frac{3+1+2}{15}=0.4<br>$$</p>\n<p>$$<br>P2=\\frac{2+3}{10}=0.5<br>$$</p>\n<p>下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">硬币</th>\n<th style=\"text-align:center\">结果</th>\n<th style=\"text-align:center\">统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正正反正反</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">反反正正反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正反反反反</td>\n<td style=\"text-align:center\">1正-4反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正反反正正</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">反正正反反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n</tbody>\n</table>\n<p>此时我们加入<strong>隐含变量z</strong>，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。</p>\n<p>我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。</p>\n<p>例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2*0.2*0.2*0.8*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7*0.7*0.7*0.3*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">若是硬币1</th>\n<th style=\"text-align:center\">若是硬币2</th>\n<th style=\"text-align:center\">最有可能硬币</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n<td style=\"text-align:center\">硬币2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.08192</td>\n<td style=\"text-align:center\">0.00567</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n<td style=\"text-align:center\">硬币2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n</tbody>\n</table>\n<p>我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到<br>$$<br>P1=\\frac{2+1+2}{15}=0.33<br>$$</p>\n<p>$$<br>P2=\\frac{3+3}{10}=0.6<br>$$</p>\n<p>P1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">初始化的P1</th>\n<th style=\"text-align:center\">估计的P1</th>\n<th style=\"text-align:center\">真实的P1</th>\n<th style=\"text-align:center\">初始化的P2</th>\n<th style=\"text-align:center\">估计的P2</th>\n<th style=\"text-align:center\">真实的P2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.33</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">0.5</td>\n</tr>\n</tbody>\n</table>\n<p>可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。</p>\n<p>上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?</p>\n<p>但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">若是硬币1</th>\n<th style=\"text-align:center\">若是硬币2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.08192</td>\n<td style=\"text-align:center\">0.00567</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n</tr>\n</tbody>\n</table>\n<p>利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率<br>$$<br>z_1=\\frac{0.00512}{0.00512+0.03087}=0.14<br>$$<br>相应的算出其他4轮的概率。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">z_i=硬币1</th>\n<th style=\"text-align:center\">z_i=硬币2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.14</td>\n<td style=\"text-align:center\">0.86</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.61</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.94</td>\n<td style=\"text-align:center\">0.06</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.14</td>\n<td style=\"text-align:center\">0.86</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.61</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n</tbody>\n</table>\n<p>上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为<strong>E步</strong>。</p>\n<p>按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14*3=0.42的概率为正，有0.14*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为<strong>M步</strong>。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">正面</th>\n<th style=\"text-align:center\">反面</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.42</td>\n<td style=\"text-align:center\">0.28</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1.22</td>\n<td style=\"text-align:center\">1.83</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.94</td>\n<td style=\"text-align:center\">3.76</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.42</td>\n<td style=\"text-align:center\">0.28</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">1.22</td>\n<td style=\"text-align:center\">1.93</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">总计</td>\n<td style=\"text-align:center\">4.22</td>\n<td style=\"text-align:center\">7.98</td>\n</tr>\n</tbody>\n</table>\n<p>$$<br>P1=\\frac{4.22}{4.22+7.98}=0.35<br>$$<br>上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。</p>\n<h3 id=\"3-EM算法推导\"><a href=\"#3-EM算法推导\" class=\"headerlink\" title=\"3.EM算法推导\"></a>3.EM算法推导</h3><p>对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数$\\theta$，极大化模型分布的对数似然函数如下所示<br>$$<br>\\theta =\\arg \\max_{\\theta} \\sum <em>{i=1}^{m}logP(x^{(i)};\\theta)<br>$$<br>如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下<br>$$<br>\\theta =\\arg \\max</em>{\\theta} \\sum <em>{i=1}^{m}logP(x^{(i)};\\theta)=\\arg \\max</em>{\\theta} \\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)<br>$$<br>上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入<strong>Jensen不等式</strong>。</p>\n<blockquote>\n<p>设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。</p>\n<p><strong>Jensen不等式定义：</strong>如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。</p>\n</blockquote>\n<p>我们再回到上述推导过程，得到如下方程式。<br>$$<br>\\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)<br>\\=\\sum _{i=1}^{m}log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\\ \\ \\ \\ \\ \\ \\ (1)<br>\\ \\ge \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})} \\ \\ \\ \\ \\ \\ \\ (2)<br>$$</p>\n<p>我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。</p>\n<p>首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$是$\\frac{P(x^{(i)},z^{(i);\\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$便相当于f(E(X))，同时$\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。</p>\n<p>如果要满足Jensen不等式的等号，那么需要满足X为常量，即为<br>$$<br>\\frac{P(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c,\\ c为常量<br>$$<br>那么稍加改变能够得到<br>$$<br>c Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\\theta)\\ ,c为常量<br>$$</p>\n<p>$$<br>\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta),\\ c为常量<br>$$</p>\n<p>其中c是不依赖于$z^{(i)}$的常量，由于$Q_i(z^{(i)})$是一个分布，所以满足<br>$$<br>\\sum_{z}Q_i(z^{(i)})=1<br>$$</p>\n<p>$$<br>\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta)= c<br>$$</p>\n<p>因此得到下列方程，其中方程(3)利用到条件概率公式。<br>$$<br>Q_i(z^{(i)})=\\frac{P(x^{(i)},z^{(i)};\\theta)}{c}=\\frac{P(x^{(i)},z^{(i)};\\theta)}{\\sum _z P(x^{(i)},z^{(i)};\\theta)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\<br>\\=\\frac{P(x^{(i)},z^{(i)};\\theta)}{ P(x^{(i)};\\theta)}=P(z^{(i)}|x^{(i)};\\theta)  \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\   (3)<br>$$<br>如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)$，那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式<br>$$<br>\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}<br>$$<br>去掉上式中常数部分，则我们需要极大化的对数似然下界为<br>$$<br>\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\\theta)-log {Q_i(z^{(i)})}]<br>\\=\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)<br>$$<br>注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)$可以理解为$logP(x^{(i)},z^{(i)};\\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中<strong>E步</strong>。极大化方程式(4)也就是我们EM算法中的<strong>M步</strong>。</p>\n<h3 id=\"4-EM算法流程\"><a href=\"#4-EM算法流程\" class=\"headerlink\" title=\"4.EM算法流程\"></a>4.EM算法流程</h3><p>现在我们总结下EM算法流程。</p>\n<p>输入：观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，联合分布$P(x^{(i)},z^{(i)};\\theta)$，条件分布$P(z^{(i)}|x^{(i)};\\theta)$，最大迭代次数$J$。</p>\n<ul>\n<li><p>随机初始化模型参数$\\theta$的初始值$\\theta^0$。</p>\n</li>\n<li><p>$for\\  j\\  from \\ 1\\  to \\ J$开始EM算法迭代。</p>\n<ul>\n<li>E步:计算联合分布的条件概率期望</li>\n</ul>\n<p>$$<br>Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)<br>$$</p>\n<p>$$<br>L(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)<br>$$</p>\n<ul>\n<li>M步:极大化$L(\\theta,\\theta^j)$，得到$\\theta^{j+1}$</li>\n</ul>\n<p>$$<br>\\theta^{j+1}=\\arg \\max _{x} L(\\theta,\\theta^j)<br>$$</p>\n<ul>\n<li>如果$\\theta^{j+1}$收敛，则算法结束，否则继续迭代。</li>\n</ul>\n</li>\n<li><p>输出模型参数$\\theta$。</p>\n</li>\n</ul>\n<h3 id=\"5-EM算法的收敛性\"><a href=\"#5-EM算法的收敛性\" class=\"headerlink\" title=\"5.EM算法的收敛性\"></a>5.EM算法的收敛性</h3><p>我们现在来解答下<strong>2.EM算法实例</strong>中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？</p>\n<p>首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即<br>$$<br>\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})\\ge \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j})<br>$$<br>由于<br>$$<br>L(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(x^{(i)},z^{(i)};\\theta)<br>$$<br>令<br>$$<br>H(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum <em>{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(z^{(i)}|x^{(i)};\\theta)<br>$$<br>上两式相减得到<br>$$<br>\\sum</em>{i=1}^{m}log P(x^{(i)};\\theta)=L(\\theta,\\theta^j)-H(\\theta,\\theta^j)<br>$$<br>在上式中分别取$\\theta$为$\\theta^j$和$\\theta^{j+1}$，并相减得到<br>$$<br>\\sum_{i=1}^{m}log P(x^{(i)};\\theta^{j+1})-\\sum_{i=1}^{m}log P(x^{(i)};\\theta^j)=[L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)]-[H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)]<br>$$<br>要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。由于$\\theta^{j+1}$使得$L(\\theta,\\theta^j)$极大，因此有<br>$$<br>L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)\\ge 0<br>$$<br>而对于第二部分，我们有<br>$$<br>H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log \\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\ \\ \\ \\ \\ \\ \\ \\ \\ (5)<br>\\ \\le \\sum _{i=1}^{m}log { \\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)\\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} }\\ \\ \\ \\ \\ \\ \\ \\ \\ (6)<br>\\ = \\sum _{i=1}^{m}log (\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{j+1}))=0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7)<br>$$<br>其中第(6)式用到了Jensen不等式，只不过和第<strong>3.EM算法推导</strong>中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到<br>$$<br>\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})- \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j}) \\ge 0<br>$$<br>证明了EM算法的收敛性。</p>\n<p>从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。</p>\n<h3 id=\"6-Sklearn实现EM算法\"><a href=\"#6-Sklearn实现EM算法\" class=\"headerlink\" title=\"6.Sklearn实现EM算法\"></a>6.Sklearn实现EM算法</h3><p>高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去<a href=\"https://mp.weixin.qq.com/s?src=11&amp;timestamp=1525932817&amp;ver=867&amp;signature=U4UxviydVELD71Ju*bRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm*budZlpZdMQMLOcXB-FnvKKlkV2H&amp;new=1\" target=\"_blank\" rel=\"noopener\">这儿</a>。下列代码来自于<a href=\"http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py\" target=\"_blank\" rel=\"noopener\">Sklearn官网GMM模块</a>，利用高斯混合模型确定iris聚类。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib <span class=\"keyword\">as</span> mpl</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.mixture <span class=\"keyword\">import</span> GaussianMixture</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> StratifiedKFold</span><br><span class=\"line\"></span><br><span class=\"line\">colors = [<span class=\"string\">'navy'</span>, <span class=\"string\">'turquoise'</span>, <span class=\"string\">'darkorange'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">make_ellipses</span><span class=\"params\">(gmm, ax)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gmm.covariance_type == <span class=\"string\">'full'</span>:</span><br><span class=\"line\">            covariances = gmm.covariances_[n][:<span class=\"number\">2</span>, :<span class=\"number\">2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'tied'</span>:</span><br><span class=\"line\">            covariances = gmm.covariances_[:<span class=\"number\">2</span>, :<span class=\"number\">2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'diag'</span>:</span><br><span class=\"line\">            covariances = np.diag(gmm.covariances_[n][:<span class=\"number\">2</span>])</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'spherical'</span>:</span><br><span class=\"line\">            covariances = np.eye(gmm.means_.shape[<span class=\"number\">1</span>]) * gmm.covariances_[n]</span><br><span class=\"line\">        v, w = np.linalg.eigh(covariances)</span><br><span class=\"line\">        u = w[<span class=\"number\">0</span>] / np.linalg.norm(w[<span class=\"number\">0</span>])</span><br><span class=\"line\">        angle = np.arctan2(u[<span class=\"number\">1</span>], u[<span class=\"number\">0</span>])</span><br><span class=\"line\">        angle = <span class=\"number\">180</span> * angle / np.pi  <span class=\"comment\"># convert to degrees</span></span><br><span class=\"line\">        v = <span class=\"number\">2.</span> * np.sqrt(<span class=\"number\">2.</span>) * np.sqrt(v)</span><br><span class=\"line\">        ell = mpl.patches.Ellipse(gmm.means_[n, :<span class=\"number\">2</span>], v[<span class=\"number\">0</span>], v[<span class=\"number\">1</span>],</span><br><span class=\"line\">                                  <span class=\"number\">180</span> + angle, color=color)</span><br><span class=\"line\">        ell.set_clip_box(ax.bbox)</span><br><span class=\"line\">        ell.set_alpha(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">        ax.add_artist(ell)</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Break up the dataset into non-overlapping training (75%)</span></span><br><span class=\"line\"><span class=\"comment\"># and testing (25%) sets.</span></span><br><span class=\"line\">skf = StratifiedKFold(n_splits=<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"comment\"># Only take the first fold.</span></span><br><span class=\"line\">train_index, test_index = next(iter(skf.split(iris.data, iris.target)))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X_train = iris.data[train_index]</span><br><span class=\"line\">y_train = iris.target[train_index]</span><br><span class=\"line\">X_test = iris.data[test_index]</span><br><span class=\"line\">y_test = iris.target[test_index]</span><br><span class=\"line\"></span><br><span class=\"line\">n_classes = len(np.unique(y_train))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Try GMMs using different types of covariances.</span></span><br><span class=\"line\">estimators = dict((cov_type, GaussianMixture(n_components=n_classes,</span><br><span class=\"line\">                   covariance_type=cov_type, max_iter=<span class=\"number\">20</span>, random_state=<span class=\"number\">0</span>))</span><br><span class=\"line\">                  <span class=\"keyword\">for</span> cov_type <span class=\"keyword\">in</span> [<span class=\"string\">'spherical'</span>, <span class=\"string\">'diag'</span>, <span class=\"string\">'tied'</span>, <span class=\"string\">'full'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">n_estimators = len(estimators)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">3</span> * n_estimators // <span class=\"number\">2</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">plt.subplots_adjust(bottom=<span class=\"number\">.01</span>, top=<span class=\"number\">0.95</span>, hspace=<span class=\"number\">.15</span>, wspace=<span class=\"number\">.05</span>,</span><br><span class=\"line\">                    left=<span class=\"number\">.01</span>, right=<span class=\"number\">.99</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> index, (name, estimator) <span class=\"keyword\">in</span> enumerate(estimators.items()):</span><br><span class=\"line\">    <span class=\"comment\"># Since we have class labels for the training data, we can</span></span><br><span class=\"line\">    <span class=\"comment\"># initialize the GMM parameters in a supervised manner.</span></span><br><span class=\"line\">    estimator.means_init = np.array([X_train[y_train == i].mean(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">                                    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n_classes)])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Train the other parameters using the EM algorithm.</span></span><br><span class=\"line\">    estimator.fit(X_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    h = plt.subplot(<span class=\"number\">2</span>, n_estimators // <span class=\"number\">2</span>, index + <span class=\"number\">1</span>)</span><br><span class=\"line\">    make_ellipses(estimator, h)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        data = iris.data[iris.target == n]</span><br><span class=\"line\">        plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], s=<span class=\"number\">0.8</span>, color=color,</span><br><span class=\"line\">                    label=iris.target_names[n])</span><br><span class=\"line\">    <span class=\"comment\"># Plot the test data with crosses</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        data = X_test[y_test == n]</span><br><span class=\"line\">        plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'x'</span>, color=color)</span><br><span class=\"line\"></span><br><span class=\"line\">    y_train_pred = estimator.predict(X_train)</span><br><span class=\"line\">    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * <span class=\"number\">100</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">0.05</span>, <span class=\"number\">0.9</span>, <span class=\"string\">'Train accuracy: %.1f'</span> % train_accuracy,</span><br><span class=\"line\">             transform=h.transAxes)</span><br><span class=\"line\"></span><br><span class=\"line\">    y_test_pred = estimator.predict(X_test)</span><br><span class=\"line\">    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * <span class=\"number\">100</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">0.05</span>, <span class=\"number\">0.8</span>, <span class=\"string\">'Test accuracy: %.1f'</span> % test_accuracy,</span><br><span class=\"line\">             transform=h.transAxes)</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.xticks(())</span><br><span class=\"line\">    plt.yticks(())</span><br><span class=\"line\">    plt.title(name)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.legend(scatterpoints=<span class=\"number\">1</span>, loc=<span class=\"string\">'lower right'</span>, prop=dict(size=<span class=\"number\">12</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png\" alt=\"机器学习之最大期望算法图片01\"></p>\n<h3 id=\"7-EM算法优缺点\"><a href=\"#7-EM算法优缺点\" class=\"headerlink\" title=\"7.EM算法优缺点\"></a>7.EM算法优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li><p>聚类。</p>\n</li>\n<li><p>算法计算结果稳定、准确。</p>\n</li>\n<li><p>EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。</p>\n</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li><p>对初始化数据敏感。</p>\n</li>\n<li><p>EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。</p>\n</li>\n<li><p>当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。</p>\n</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<ul>\n<li><a href=\"https://www.jianshu.com/p/1121509ac1dc\" target=\"_blank\" rel=\"noopener\">milter_如何感性地理解EM算法</a></li>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6912636.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_EM算法原理总结</a></li>\n<li><a href=\"http://scikit-learn.org/stable/modules/mixture.html\" target=\"_blank\" rel=\"noopener\">Gaussian mixture models</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-EM算法简介\"><a href=\"#1-EM算法简介\" class=\"headerlink\" title=\"1.EM算法简介\"></a>1.EM算法简介</h3><p><strong>最大期望(Expectation Maximum)算法</strong>是一种迭代优化算法，其计算方法是每次迭代分为<strong>期望(E)步</strong>和<strong>最大(M)步</strong>。我们先看下最大期望算法能够解决什么样的问题。</p>\n<p>假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。<br>$$<br>N_1(\\mu_1,\\sigma_1^2)\\ ; N_2(\\mu_2,\\sigma_2^2)<br>$$<br>但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ </p>\n<p>EM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。</p>\n<h3 id=\"2-EM算法实例\"><a href=\"#2-EM算法实例\" class=\"headerlink\" title=\"2.EM算法实例\"></a>2.EM算法实例</h3><p>假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">硬币</th>\n<th style=\"text-align:center\">结果</th>\n<th style=\"text-align:center\">统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正正反正反</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">反反正正反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">正反反反反</td>\n<td style=\"text-align:center\">1正-4反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">正反反正正</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">反正正反反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n</tbody>\n</table>\n<p>我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。<br>$$<br>P1=\\frac{3+1+2}{15}=0.4<br>$$</p>\n<p>$$<br>P2=\\frac{2+3}{10}=0.5<br>$$</p>\n<p>下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">硬币</th>\n<th style=\"text-align:center\">结果</th>\n<th style=\"text-align:center\">统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正正反正反</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">反反正正反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正反反反反</td>\n<td style=\"text-align:center\">1正-4反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">正反反正正</td>\n<td style=\"text-align:center\">3正-2反</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Unknown</td>\n<td style=\"text-align:center\">反正正反反</td>\n<td style=\"text-align:center\">2正-3反</td>\n</tr>\n</tbody>\n</table>\n<p>此时我们加入<strong>隐含变量z</strong>，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。</p>\n<p>我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。</p>\n<p>例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2*0.2*0.2*0.8*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7*0.7*0.7*0.3*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">若是硬币1</th>\n<th style=\"text-align:center\">若是硬币2</th>\n<th style=\"text-align:center\">最有可能硬币</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n<td style=\"text-align:center\">硬币2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.08192</td>\n<td style=\"text-align:center\">0.00567</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n<td style=\"text-align:center\">硬币2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n<td style=\"text-align:center\">硬币1</td>\n</tr>\n</tbody>\n</table>\n<p>我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到<br>$$<br>P1=\\frac{2+1+2}{15}=0.33<br>$$</p>\n<p>$$<br>P2=\\frac{3+3}{10}=0.6<br>$$</p>\n<p>P1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">初始化的P1</th>\n<th style=\"text-align:center\">估计的P1</th>\n<th style=\"text-align:center\">真实的P1</th>\n<th style=\"text-align:center\">初始化的P2</th>\n<th style=\"text-align:center\">估计的P2</th>\n<th style=\"text-align:center\">真实的P2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.33</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">0.5</td>\n</tr>\n</tbody>\n</table>\n<p>可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。</p>\n<p>上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?</p>\n<p>但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">若是硬币1</th>\n<th style=\"text-align:center\">若是硬币2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.08192</td>\n<td style=\"text-align:center\">0.00567</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.00512</td>\n<td style=\"text-align:center\">0.03087</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.02048</td>\n<td style=\"text-align:center\">0.01323</td>\n</tr>\n</tbody>\n</table>\n<p>利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率<br>$$<br>z_1=\\frac{0.00512}{0.00512+0.03087}=0.14<br>$$<br>相应的算出其他4轮的概率。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">z_i=硬币1</th>\n<th style=\"text-align:center\">z_i=硬币2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.14</td>\n<td style=\"text-align:center\">0.86</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.61</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.94</td>\n<td style=\"text-align:center\">0.06</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.14</td>\n<td style=\"text-align:center\">0.86</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.61</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n</tbody>\n</table>\n<p>上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为<strong>E步</strong>。</p>\n<p>按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14*3=0.42的概率为正，有0.14*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为<strong>M步</strong>。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">轮数</th>\n<th style=\"text-align:center\">正面</th>\n<th style=\"text-align:center\">反面</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0.42</td>\n<td style=\"text-align:center\">0.28</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1.22</td>\n<td style=\"text-align:center\">1.83</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.94</td>\n<td style=\"text-align:center\">3.76</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.42</td>\n<td style=\"text-align:center\">0.28</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">1.22</td>\n<td style=\"text-align:center\">1.93</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">总计</td>\n<td style=\"text-align:center\">4.22</td>\n<td style=\"text-align:center\">7.98</td>\n</tr>\n</tbody>\n</table>\n<p>$$<br>P1=\\frac{4.22}{4.22+7.98}=0.35<br>$$<br>上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。</p>\n<h3 id=\"3-EM算法推导\"><a href=\"#3-EM算法推导\" class=\"headerlink\" title=\"3.EM算法推导\"></a>3.EM算法推导</h3><p>对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数$\\theta$，极大化模型分布的对数似然函数如下所示<br>$$<br>\\theta =\\arg \\max_{\\theta} \\sum <em>{i=1}^{m}logP(x^{(i)};\\theta)<br>$$<br>如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下<br>$$<br>\\theta =\\arg \\max</em>{\\theta} \\sum <em>{i=1}^{m}logP(x^{(i)};\\theta)=\\arg \\max</em>{\\theta} \\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)<br>$$<br>上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入<strong>Jensen不等式</strong>。</p>\n<blockquote>\n<p>设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。</p>\n<p><strong>Jensen不等式定义：</strong>如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。</p>\n</blockquote>\n<p>我们再回到上述推导过程，得到如下方程式。<br>$$<br>\\sum _{i=1}^{m}log\\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\\theta)<br>\\=\\sum _{i=1}^{m}log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}\\ \\ \\ \\ \\ \\ \\ (1)<br>\\ \\ge \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})} \\ \\ \\ \\ \\ \\ \\ (2)<br>$$</p>\n<p>我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。</p>\n<p>首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$是$\\frac{P(x^{(i)},z^{(i);\\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$便相当于f(E(X))，同时$\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。</p>\n<p>如果要满足Jensen不等式的等号，那么需要满足X为常量，即为<br>$$<br>\\frac{P(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}=c,\\ c为常量<br>$$<br>那么稍加改变能够得到<br>$$<br>c Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\\theta)\\ ,c为常量<br>$$</p>\n<p>$$<br>\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta),\\ c为常量<br>$$</p>\n<p>其中c是不依赖于$z^{(i)}$的常量，由于$Q_i(z^{(i)})$是一个分布，所以满足<br>$$<br>\\sum_{z}Q_i(z^{(i)})=1<br>$$</p>\n<p>$$<br>\\sum_z c\\  Q_i(z^{(i)})= \\sum _z P(x^{(i)},z^{(i)};\\theta)= c<br>$$</p>\n<p>因此得到下列方程，其中方程(3)利用到条件概率公式。<br>$$<br>Q_i(z^{(i)})=\\frac{P(x^{(i)},z^{(i)};\\theta)}{c}=\\frac{P(x^{(i)},z^{(i)};\\theta)}{\\sum _z P(x^{(i)},z^{(i)};\\theta)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\<br>\\=\\frac{P(x^{(i)},z^{(i)};\\theta)}{ P(x^{(i)};\\theta)}=P(z^{(i)}|x^{(i)};\\theta)  \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\   (3)<br>$$<br>如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)$，那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式<br>$$<br>\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)};\\theta) }{Q_i(z^{(i)})}<br>$$<br>去掉上式中常数部分，则我们需要极大化的对数似然下界为<br>$$<br>\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\\theta)-log {Q_i(z^{(i)})}]<br>\\=\\arg \\max _{\\theta} \\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)<br>$$<br>注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)$可以理解为$logP(x^{(i)},z^{(i)};\\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中<strong>E步</strong>。极大化方程式(4)也就是我们EM算法中的<strong>M步</strong>。</p>\n<h3 id=\"4-EM算法流程\"><a href=\"#4-EM算法流程\" class=\"headerlink\" title=\"4.EM算法流程\"></a>4.EM算法流程</h3><p>现在我们总结下EM算法流程。</p>\n<p>输入：观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，联合分布$P(x^{(i)},z^{(i)};\\theta)$，条件分布$P(z^{(i)}|x^{(i)};\\theta)$，最大迭代次数$J$。</p>\n<ul>\n<li><p>随机初始化模型参数$\\theta$的初始值$\\theta^0$。</p>\n</li>\n<li><p>$for\\  j\\  from \\ 1\\  to \\ J$开始EM算法迭代。</p>\n<ul>\n<li>E步:计算联合分布的条件概率期望</li>\n</ul>\n<p>$$<br>Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta)<br>$$</p>\n<p>$$<br>L(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\\theta)<br>$$</p>\n<ul>\n<li>M步:极大化$L(\\theta,\\theta^j)$，得到$\\theta^{j+1}$</li>\n</ul>\n<p>$$<br>\\theta^{j+1}=\\arg \\max _{x} L(\\theta,\\theta^j)<br>$$</p>\n<ul>\n<li>如果$\\theta^{j+1}$收敛，则算法结束，否则继续迭代。</li>\n</ul>\n</li>\n<li><p>输出模型参数$\\theta$。</p>\n</li>\n</ul>\n<h3 id=\"5-EM算法的收敛性\"><a href=\"#5-EM算法的收敛性\" class=\"headerlink\" title=\"5.EM算法的收敛性\"></a>5.EM算法的收敛性</h3><p>我们现在来解答下<strong>2.EM算法实例</strong>中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？</p>\n<p>首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即<br>$$<br>\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})\\ge \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j})<br>$$<br>由于<br>$$<br>L(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(x^{(i)},z^{(i)};\\theta)<br>$$<br>令<br>$$<br>H(\\theta,\\theta^j)=\\sum _{i=1}^{m}\\sum <em>{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log P(z^{(i)}|x^{(i)};\\theta)<br>$$<br>上两式相减得到<br>$$<br>\\sum</em>{i=1}^{m}log P(x^{(i)};\\theta)=L(\\theta,\\theta^j)-H(\\theta,\\theta^j)<br>$$<br>在上式中分别取$\\theta$为$\\theta^j$和$\\theta^{j+1}$，并相减得到<br>$$<br>\\sum_{i=1}^{m}log P(x^{(i)};\\theta^{j+1})-\\sum_{i=1}^{m}log P(x^{(i)};\\theta^j)=[L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)]-[H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)]<br>$$<br>要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。由于$\\theta^{j+1}$使得$L(\\theta,\\theta^j)$极大，因此有<br>$$<br>L(\\theta^{j+1},\\theta^j)-L(\\theta^j,\\theta^j)\\ge 0<br>$$<br>而对于第二部分，我们有<br>$$<br>H(\\theta^{j+1},\\theta^j)-H(\\theta^j,\\theta^j)=\\sum _{i=1}^{m}\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)log \\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} \\ \\ \\ \\ \\ \\ \\ \\ \\ (5)<br>\\ \\le \\sum _{i=1}^{m}log { \\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^j)\\frac{P(z^{(i)}|x^{(i)};\\theta^{j+1})}{P(z^{(i)}|x^{(i)};\\theta^{j})} }\\ \\ \\ \\ \\ \\ \\ \\ \\ (6)<br>\\ = \\sum _{i=1}^{m}log (\\sum _{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{j+1}))=0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7)<br>$$<br>其中第(6)式用到了Jensen不等式，只不过和第<strong>3.EM算法推导</strong>中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到<br>$$<br>\\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j+1})- \\sum _{i=1}^{m}log P(x^{(i)};\\theta^{j}) \\ge 0<br>$$<br>证明了EM算法的收敛性。</p>\n<p>从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。</p>\n<h3 id=\"6-Sklearn实现EM算法\"><a href=\"#6-Sklearn实现EM算法\" class=\"headerlink\" title=\"6.Sklearn实现EM算法\"></a>6.Sklearn实现EM算法</h3><p>高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去<a href=\"https://mp.weixin.qq.com/s?src=11&amp;timestamp=1525932817&amp;ver=867&amp;signature=U4UxviydVELD71Ju*bRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm*budZlpZdMQMLOcXB-FnvKKlkV2H&amp;new=1\" target=\"_blank\" rel=\"noopener\">这儿</a>。下列代码来自于<a href=\"http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py\" target=\"_blank\" rel=\"noopener\">Sklearn官网GMM模块</a>，利用高斯混合模型确定iris聚类。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib <span class=\"keyword\">as</span> mpl</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.mixture <span class=\"keyword\">import</span> GaussianMixture</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> StratifiedKFold</span><br><span class=\"line\"></span><br><span class=\"line\">colors = [<span class=\"string\">'navy'</span>, <span class=\"string\">'turquoise'</span>, <span class=\"string\">'darkorange'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">make_ellipses</span><span class=\"params\">(gmm, ax)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gmm.covariance_type == <span class=\"string\">'full'</span>:</span><br><span class=\"line\">            covariances = gmm.covariances_[n][:<span class=\"number\">2</span>, :<span class=\"number\">2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'tied'</span>:</span><br><span class=\"line\">            covariances = gmm.covariances_[:<span class=\"number\">2</span>, :<span class=\"number\">2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'diag'</span>:</span><br><span class=\"line\">            covariances = np.diag(gmm.covariances_[n][:<span class=\"number\">2</span>])</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> gmm.covariance_type == <span class=\"string\">'spherical'</span>:</span><br><span class=\"line\">            covariances = np.eye(gmm.means_.shape[<span class=\"number\">1</span>]) * gmm.covariances_[n]</span><br><span class=\"line\">        v, w = np.linalg.eigh(covariances)</span><br><span class=\"line\">        u = w[<span class=\"number\">0</span>] / np.linalg.norm(w[<span class=\"number\">0</span>])</span><br><span class=\"line\">        angle = np.arctan2(u[<span class=\"number\">1</span>], u[<span class=\"number\">0</span>])</span><br><span class=\"line\">        angle = <span class=\"number\">180</span> * angle / np.pi  <span class=\"comment\"># convert to degrees</span></span><br><span class=\"line\">        v = <span class=\"number\">2.</span> * np.sqrt(<span class=\"number\">2.</span>) * np.sqrt(v)</span><br><span class=\"line\">        ell = mpl.patches.Ellipse(gmm.means_[n, :<span class=\"number\">2</span>], v[<span class=\"number\">0</span>], v[<span class=\"number\">1</span>],</span><br><span class=\"line\">                                  <span class=\"number\">180</span> + angle, color=color)</span><br><span class=\"line\">        ell.set_clip_box(ax.bbox)</span><br><span class=\"line\">        ell.set_alpha(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">        ax.add_artist(ell)</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Break up the dataset into non-overlapping training (75%)</span></span><br><span class=\"line\"><span class=\"comment\"># and testing (25%) sets.</span></span><br><span class=\"line\">skf = StratifiedKFold(n_splits=<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"comment\"># Only take the first fold.</span></span><br><span class=\"line\">train_index, test_index = next(iter(skf.split(iris.data, iris.target)))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X_train = iris.data[train_index]</span><br><span class=\"line\">y_train = iris.target[train_index]</span><br><span class=\"line\">X_test = iris.data[test_index]</span><br><span class=\"line\">y_test = iris.target[test_index]</span><br><span class=\"line\"></span><br><span class=\"line\">n_classes = len(np.unique(y_train))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Try GMMs using different types of covariances.</span></span><br><span class=\"line\">estimators = dict((cov_type, GaussianMixture(n_components=n_classes,</span><br><span class=\"line\">                   covariance_type=cov_type, max_iter=<span class=\"number\">20</span>, random_state=<span class=\"number\">0</span>))</span><br><span class=\"line\">                  <span class=\"keyword\">for</span> cov_type <span class=\"keyword\">in</span> [<span class=\"string\">'spherical'</span>, <span class=\"string\">'diag'</span>, <span class=\"string\">'tied'</span>, <span class=\"string\">'full'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">n_estimators = len(estimators)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">3</span> * n_estimators // <span class=\"number\">2</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">plt.subplots_adjust(bottom=<span class=\"number\">.01</span>, top=<span class=\"number\">0.95</span>, hspace=<span class=\"number\">.15</span>, wspace=<span class=\"number\">.05</span>,</span><br><span class=\"line\">                    left=<span class=\"number\">.01</span>, right=<span class=\"number\">.99</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> index, (name, estimator) <span class=\"keyword\">in</span> enumerate(estimators.items()):</span><br><span class=\"line\">    <span class=\"comment\"># Since we have class labels for the training data, we can</span></span><br><span class=\"line\">    <span class=\"comment\"># initialize the GMM parameters in a supervised manner.</span></span><br><span class=\"line\">    estimator.means_init = np.array([X_train[y_train == i].mean(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">                                    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n_classes)])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Train the other parameters using the EM algorithm.</span></span><br><span class=\"line\">    estimator.fit(X_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    h = plt.subplot(<span class=\"number\">2</span>, n_estimators // <span class=\"number\">2</span>, index + <span class=\"number\">1</span>)</span><br><span class=\"line\">    make_ellipses(estimator, h)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        data = iris.data[iris.target == n]</span><br><span class=\"line\">        plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], s=<span class=\"number\">0.8</span>, color=color,</span><br><span class=\"line\">                    label=iris.target_names[n])</span><br><span class=\"line\">    <span class=\"comment\"># Plot the test data with crosses</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n, color <span class=\"keyword\">in</span> enumerate(colors):</span><br><span class=\"line\">        data = X_test[y_test == n]</span><br><span class=\"line\">        plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'x'</span>, color=color)</span><br><span class=\"line\"></span><br><span class=\"line\">    y_train_pred = estimator.predict(X_train)</span><br><span class=\"line\">    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * <span class=\"number\">100</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">0.05</span>, <span class=\"number\">0.9</span>, <span class=\"string\">'Train accuracy: %.1f'</span> % train_accuracy,</span><br><span class=\"line\">             transform=h.transAxes)</span><br><span class=\"line\"></span><br><span class=\"line\">    y_test_pred = estimator.predict(X_test)</span><br><span class=\"line\">    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * <span class=\"number\">100</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">0.05</span>, <span class=\"number\">0.8</span>, <span class=\"string\">'Test accuracy: %.1f'</span> % test_accuracy,</span><br><span class=\"line\">             transform=h.transAxes)</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.xticks(())</span><br><span class=\"line\">    plt.yticks(())</span><br><span class=\"line\">    plt.title(name)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.legend(scatterpoints=<span class=\"number\">1</span>, loc=<span class=\"string\">'lower right'</span>, prop=dict(size=<span class=\"number\">12</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png\" alt=\"机器学习之最大期望算法图片01\"></p>\n<h3 id=\"7-EM算法优缺点\"><a href=\"#7-EM算法优缺点\" class=\"headerlink\" title=\"7.EM算法优缺点\"></a>7.EM算法优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li><p>聚类。</p>\n</li>\n<li><p>算法计算结果稳定、准确。</p>\n</li>\n<li><p>EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。</p>\n</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li><p>对初始化数据敏感。</p>\n</li>\n<li><p>EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。</p>\n</li>\n<li><p>当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。</p>\n</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>参考</p>\n<ul>\n<li><a href=\"https://www.jianshu.com/p/1121509ac1dc\" target=\"_blank\" rel=\"noopener\">milter_如何感性地理解EM算法</a></li>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6912636.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_EM算法原理总结</a></li>\n<li><a href=\"http://scikit-learn.org/stable/modules/mixture.html\" target=\"_blank\" rel=\"noopener\">Gaussian mixture models</a></li>\n</ul>\n"},{"title":"机器学习之线性回归","date":"2018-03-24T15:27:53.000Z","comments":1,"mathjax":true,"_content":"### 1.线性回归分析（ Linear Regression Analysis）\n**线性回归分析（Regression Analysis）**：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。\n通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于**监督学习**。\n![图片01](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8701.png)\n上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。\n\n### 2.模型表达\n\n建立数学模型之前，我们先定义如下变量。\n\n+ $x_i$表示输入数据（Feature）\n+ $y_i$表示输出数据（Target）\n+ $(x_i,y_i)$表示一组训练数据（Training example）\n+ m表示训练数据的个数\n+ n表示特征数量\n\n监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)->y$。针对线性回归而言，函数$h(x)$表达式为\n$$\nh(x)=\\theta_0+\\theta_1*x_i+\\theta_2*x_2+...+\\theta_n*x_n\n$$\n为方便我们使用矩阵来表达，$h(x)=\\theta^T*x$，其中$\\theta^T$为$\\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。\n\n### 3.梯度下降算法\n#### 3.1梯度下降算法简述\n实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。\n\n![图片02](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8702.png)\n从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。\n#### 3.2梯度下降算法相关概念\n\n求解梯度下降算法之前，我们先了解相关概念。\n\n+ **步长（Learning Rate）**：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。\n+ **特征（Feature）**：即上述描述的$x_i,y_i$\n+ **假设函数（Hypothesis Function）**：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$\n+ **损失函数（Loss Function）**：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数越小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为\n\n$$\nJ(\\theta)=\\frac{1}{2m}*\\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})\n$$\n\n我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。\n\n#### 3.3梯度下降算法过程\n\n- 随机选取一组$\\theta$。\n- 不断变化$\\theta$，让$J(\\theta)$变小。\n    $$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$\n$j=0,1,2...n$，$\\theta_j$是n+1个值同时变化。$\\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\\theta$前为负号。$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导。\n- 直到$J(\\theta)​$得到最小值。\n\n$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导求解过程如下：\n$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}(h_\\theta(x)-y)^2$$\n$$=2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$\n$$=(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$\n$$=(h_\\theta(x)-y)\\cdot x_j$$\n因此梯度下降算法的最终表述为\n\nRepeat Until Convergence{\n$$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{n}((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j) $$ \n\nfor every  $j$\n\n}\n\n梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到线性回归函数。\n\n### 4.线性回归算法实现\n\n为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址在[这儿](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv) ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\n\n#read_csv\nreaddata=pd.read_csv('data/Advertising.csv')\ndata=np.array(readdata.values)\n\n#训练数据\nX_train=data[0:150,1:3]\nY_train=data[0:150,3]\n\n#测试数据\nX_test=data[150:200,1:3]\nY_test=data[150:200,3]\n\n#回归分析\nregr = linear_model.LinearRegression()\n#进行training set和test set的fit，即是训练的过程\nregr.fit(X_train, Y_train)\n\n# 打印出相关系数和截距等信息\nprint('Coefficients: \\n', regr.coef_)\nprint('Intercept: ', regr.intercept_)\n# The mean square error\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - Y_test) ** 2))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(X_test, Y_test))\n\n#得出回归函数 并自定义数据\nX_line=np.linspace(0,300)\nY_line=np.linspace(0,50)\nZ_line=0.04699836*X_line+0.17913965*Y_line+3.00431061176\n\n#画图\nfig=plt.figure()\nax = plt.subplot(111, projection='3d')  # 创建一个三维的绘图工程\nax.scatter(data[:,1],data[:,2],data[:,3],c='red',)  # 绘制数据点\nax.plot(X_line,Y_line,Z_line,c='blue')#绘制回归曲线\nplt.show()\n```\n\n![图片](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8703.png)\n其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。\n\n------\n\n### 5.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8E%A8%E5%B9%BF.png)\n\n\n\n","source":"_posts/机器学习之线性回归.md","raw":"---\ntitle: 机器学习之线性回归\ndate: 2018-03-24 23:27:53\ntags: [机器学习,算法]\ncategories: 机器学习\ncomments: true\nmathjax: true\n---\n### 1.线性回归分析（ Linear Regression Analysis）\n**线性回归分析（Regression Analysis）**：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。\n通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于**监督学习**。\n![图片01](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8701.png)\n上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。\n\n### 2.模型表达\n\n建立数学模型之前，我们先定义如下变量。\n\n+ $x_i$表示输入数据（Feature）\n+ $y_i$表示输出数据（Target）\n+ $(x_i,y_i)$表示一组训练数据（Training example）\n+ m表示训练数据的个数\n+ n表示特征数量\n\n监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)->y$。针对线性回归而言，函数$h(x)$表达式为\n$$\nh(x)=\\theta_0+\\theta_1*x_i+\\theta_2*x_2+...+\\theta_n*x_n\n$$\n为方便我们使用矩阵来表达，$h(x)=\\theta^T*x$，其中$\\theta^T$为$\\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。\n\n### 3.梯度下降算法\n#### 3.1梯度下降算法简述\n实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。\n\n![图片02](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8702.png)\n从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。\n#### 3.2梯度下降算法相关概念\n\n求解梯度下降算法之前，我们先了解相关概念。\n\n+ **步长（Learning Rate）**：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。\n+ **特征（Feature）**：即上述描述的$x_i,y_i$\n+ **假设函数（Hypothesis Function）**：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$\n+ **损失函数（Loss Function）**：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数越小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为\n\n$$\nJ(\\theta)=\\frac{1}{2m}*\\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})\n$$\n\n我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。\n\n#### 3.3梯度下降算法过程\n\n- 随机选取一组$\\theta$。\n- 不断变化$\\theta$，让$J(\\theta)$变小。\n    $$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$\n$j=0,1,2...n$，$\\theta_j$是n+1个值同时变化。$\\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\\theta$前为负号。$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导。\n- 直到$J(\\theta)​$得到最小值。\n\n$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导求解过程如下：\n$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}(h_\\theta(x)-y)^2$$\n$$=2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$\n$$=(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$\n$$=(h_\\theta(x)-y)\\cdot x_j$$\n因此梯度下降算法的最终表述为\n\nRepeat Until Convergence{\n$$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{n}((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j) $$ \n\nfor every  $j$\n\n}\n\n梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到线性回归函数。\n\n### 4.线性回归算法实现\n\n为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址在[这儿](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv) ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\n\n#read_csv\nreaddata=pd.read_csv('data/Advertising.csv')\ndata=np.array(readdata.values)\n\n#训练数据\nX_train=data[0:150,1:3]\nY_train=data[0:150,3]\n\n#测试数据\nX_test=data[150:200,1:3]\nY_test=data[150:200,3]\n\n#回归分析\nregr = linear_model.LinearRegression()\n#进行training set和test set的fit，即是训练的过程\nregr.fit(X_train, Y_train)\n\n# 打印出相关系数和截距等信息\nprint('Coefficients: \\n', regr.coef_)\nprint('Intercept: ', regr.intercept_)\n# The mean square error\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - Y_test) ** 2))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(X_test, Y_test))\n\n#得出回归函数 并自定义数据\nX_line=np.linspace(0,300)\nY_line=np.linspace(0,50)\nZ_line=0.04699836*X_line+0.17913965*Y_line+3.00431061176\n\n#画图\nfig=plt.figure()\nax = plt.subplot(111, projection='3d')  # 创建一个三维的绘图工程\nax.scatter(data[:,1],data[:,2],data[:,3],c='red',)  # 绘制数据点\nax.plot(X_line,Y_line,Z_line,c='blue')#绘制回归曲线\nplt.show()\n```\n\n![图片](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8703.png)\n其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。\n\n------\n\n### 5.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8E%A8%E5%B9%BF.png)\n\n\n\n","slug":"机器学习之线性回归","published":1,"updated":"2018-05-21T06:26:08.793Z","layout":"post","photos":[],"link":"","_id":"cji4rdze100172e019x7ml08w","content":"<h3 id=\"1-线性回归分析（-Linear-Regression-Analysis）\"><a href=\"#1-线性回归分析（-Linear-Regression-Analysis）\" class=\"headerlink\" title=\"1.线性回归分析（ Linear Regression Analysis）\"></a>1.线性回归分析（ Linear Regression Analysis）</h3><p><strong>线性回归分析（Regression Analysis）</strong>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。<br>通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于<strong>监督学习</strong>。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8701.png\" alt=\"图片01\"><br>上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。</p>\n<h3 id=\"2-模型表达\"><a href=\"#2-模型表达\" class=\"headerlink\" title=\"2.模型表达\"></a>2.模型表达</h3><p>建立数学模型之前，我们先定义如下变量。</p>\n<ul>\n<li>$x_i$表示输入数据（Feature）</li>\n<li>$y_i$表示输出数据（Target）</li>\n<li>$(x_i,y_i)$表示一组训练数据（Training example）</li>\n<li>m表示训练数据的个数</li>\n<li>n表示特征数量</li>\n</ul>\n<p>监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)-&gt;y$。针对线性回归而言，函数$h(x)$表达式为<br>$$<br>h(x)=\\theta_0+\\theta_1<em>x_i+\\theta_2</em>x_2+…+\\theta_n<em>x_n<br>$$<br>为方便我们使用矩阵来表达，$h(x)=\\theta^T</em>x$，其中$\\theta^T$为$\\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。</p>\n<h3 id=\"3-梯度下降算法\"><a href=\"#3-梯度下降算法\" class=\"headerlink\" title=\"3.梯度下降算法\"></a>3.梯度下降算法</h3><h4 id=\"3-1梯度下降算法简述\"><a href=\"#3-1梯度下降算法简述\" class=\"headerlink\" title=\"3.1梯度下降算法简述\"></a>3.1梯度下降算法简述</h4><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8702.png\" alt=\"图片02\"><br>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p>\n<h4 id=\"3-2梯度下降算法相关概念\"><a href=\"#3-2梯度下降算法相关概念\" class=\"headerlink\" title=\"3.2梯度下降算法相关概念\"></a>3.2梯度下降算法相关概念</h4><p>求解梯度下降算法之前，我们先了解相关概念。</p>\n<ul>\n<li><strong>步长（Learning Rate）</strong>：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。</li>\n<li><strong>特征（Feature）</strong>：即上述描述的$x_i,y_i$</li>\n<li><strong>假设函数（Hypothesis Function）</strong>：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$</li>\n<li><strong>损失函数（Loss Function）</strong>：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数越小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为</li>\n</ul>\n<p>$$<br>J(\\theta)=\\frac{1}{2m}*\\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})<br>$$</p>\n<p>我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。</p>\n<h4 id=\"3-3梯度下降算法过程\"><a href=\"#3-3梯度下降算法过程\" class=\"headerlink\" title=\"3.3梯度下降算法过程\"></a>3.3梯度下降算法过程</h4><ul>\n<li>随机选取一组$\\theta$。</li>\n<li>不断变化$\\theta$，让$J(\\theta)$变小。<br>  $$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$<br>$j=0,1,2…n$，$\\theta_j$是n+1个值同时变化。$\\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\\theta$前为负号。$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导。</li>\n<li>直到$J(\\theta)​$得到最小值。</li>\n</ul>\n<p>$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导求解过程如下：<br>$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}(h_\\theta(x)-y)^2$$<br>$$=2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$<br>$$=(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$<br>$$=(h_\\theta(x)-y)\\cdot x_j$$<br>因此梯度下降算法的最终表述为</p>\n<p>Repeat Until Convergence{<br>$$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{n}((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j) $$ </p>\n<p>for every  $j$</p>\n<p>}</p>\n<p>梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到线性回归函数。</p>\n<h3 id=\"4-线性回归算法实现\"><a href=\"#4-线性回归算法实现\" class=\"headerlink\" title=\"4.线性回归算法实现\"></a>4.线性回归算法实现</h3><p>为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址在<a href=\"http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\" target=\"_blank\" rel=\"noopener\">这儿</a> ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> linear_model</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d <span class=\"keyword\">import</span> axes3d</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#read_csv</span></span><br><span class=\"line\">readdata=pd.read_csv(<span class=\"string\">'data/Advertising.csv'</span>)</span><br><span class=\"line\">data=np.array(readdata.values)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练数据</span></span><br><span class=\"line\">X_train=data[<span class=\"number\">0</span>:<span class=\"number\">150</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\">Y_train=data[<span class=\"number\">0</span>:<span class=\"number\">150</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#测试数据</span></span><br><span class=\"line\">X_test=data[<span class=\"number\">150</span>:<span class=\"number\">200</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\">Y_test=data[<span class=\"number\">150</span>:<span class=\"number\">200</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#回归分析</span></span><br><span class=\"line\">regr = linear_model.LinearRegression()</span><br><span class=\"line\"><span class=\"comment\">#进行training set和test set的fit，即是训练的过程</span></span><br><span class=\"line\">regr.fit(X_train, Y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印出相关系数和截距等信息</span></span><br><span class=\"line\">print(<span class=\"string\">'Coefficients: \\n'</span>, regr.coef_)</span><br><span class=\"line\">print(<span class=\"string\">'Intercept: '</span>, regr.intercept_)</span><br><span class=\"line\"><span class=\"comment\"># The mean square error</span></span><br><span class=\"line\">print(<span class=\"string\">\"Residual sum of squares: %.2f\"</span></span><br><span class=\"line\">      % np.mean((regr.predict(X_test) - Y_test) ** <span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"comment\"># Explained variance score: 1 is perfect prediction</span></span><br><span class=\"line\">print(<span class=\"string\">'Variance score: %.2f'</span> % regr.score(X_test, Y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#得出回归函数 并自定义数据</span></span><br><span class=\"line\">X_line=np.linspace(<span class=\"number\">0</span>,<span class=\"number\">300</span>)</span><br><span class=\"line\">Y_line=np.linspace(<span class=\"number\">0</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">Z_line=<span class=\"number\">0.04699836</span>*X_line+<span class=\"number\">0.17913965</span>*Y_line+<span class=\"number\">3.00431061176</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">fig=plt.figure()</span><br><span class=\"line\">ax = plt.subplot(<span class=\"number\">111</span>, projection=<span class=\"string\">'3d'</span>)  <span class=\"comment\"># 创建一个三维的绘图工程</span></span><br><span class=\"line\">ax.scatter(data[:,<span class=\"number\">1</span>],data[:,<span class=\"number\">2</span>],data[:,<span class=\"number\">3</span>],c=<span class=\"string\">'red'</span>,)  <span class=\"comment\"># 绘制数据点</span></span><br><span class=\"line\">ax.plot(X_line,Y_line,Z_line,c=<span class=\"string\">'blue'</span>)<span class=\"comment\">#绘制回归曲线</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8703.png\" alt=\"图片\"><br>其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。</p>\n<hr>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-线性回归分析（-Linear-Regression-Analysis）\"><a href=\"#1-线性回归分析（-Linear-Regression-Analysis）\" class=\"headerlink\" title=\"1.线性回归分析（ Linear Regression Analysis）\"></a>1.线性回归分析（ Linear Regression Analysis）</h3><p><strong>线性回归分析（Regression Analysis）</strong>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。<br>通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于<strong>监督学习</strong>。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8701.png\" alt=\"图片01\"><br>上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。</p>\n<h3 id=\"2-模型表达\"><a href=\"#2-模型表达\" class=\"headerlink\" title=\"2.模型表达\"></a>2.模型表达</h3><p>建立数学模型之前，我们先定义如下变量。</p>\n<ul>\n<li>$x_i$表示输入数据（Feature）</li>\n<li>$y_i$表示输出数据（Target）</li>\n<li>$(x_i,y_i)$表示一组训练数据（Training example）</li>\n<li>m表示训练数据的个数</li>\n<li>n表示特征数量</li>\n</ul>\n<p>监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)-&gt;y$。针对线性回归而言，函数$h(x)$表达式为<br>$$<br>h(x)=\\theta_0+\\theta_1<em>x_i+\\theta_2</em>x_2+…+\\theta_n<em>x_n<br>$$<br>为方便我们使用矩阵来表达，$h(x)=\\theta^T</em>x$，其中$\\theta^T$为$\\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。</p>\n<h3 id=\"3-梯度下降算法\"><a href=\"#3-梯度下降算法\" class=\"headerlink\" title=\"3.梯度下降算法\"></a>3.梯度下降算法</h3><h4 id=\"3-1梯度下降算法简述\"><a href=\"#3-1梯度下降算法简述\" class=\"headerlink\" title=\"3.1梯度下降算法简述\"></a>3.1梯度下降算法简述</h4><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8702.png\" alt=\"图片02\"><br>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p>\n<h4 id=\"3-2梯度下降算法相关概念\"><a href=\"#3-2梯度下降算法相关概念\" class=\"headerlink\" title=\"3.2梯度下降算法相关概念\"></a>3.2梯度下降算法相关概念</h4><p>求解梯度下降算法之前，我们先了解相关概念。</p>\n<ul>\n<li><strong>步长（Learning Rate）</strong>：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。</li>\n<li><strong>特征（Feature）</strong>：即上述描述的$x_i,y_i$</li>\n<li><strong>假设函数（Hypothesis Function）</strong>：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$</li>\n<li><strong>损失函数（Loss Function）</strong>：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数越小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为</li>\n</ul>\n<p>$$<br>J(\\theta)=\\frac{1}{2m}*\\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})<br>$$</p>\n<p>我们利用梯度下降算法，目标便是找到一组$\\theta$使得$J(\\theta)$达到最小。</p>\n<h4 id=\"3-3梯度下降算法过程\"><a href=\"#3-3梯度下降算法过程\" class=\"headerlink\" title=\"3.3梯度下降算法过程\"></a>3.3梯度下降算法过程</h4><ul>\n<li>随机选取一组$\\theta$。</li>\n<li>不断变化$\\theta$，让$J(\\theta)$变小。<br>  $$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$<br>$j=0,1,2…n$，$\\theta_j$是n+1个值同时变化。$\\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\\theta$前为负号。$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导。</li>\n<li>直到$J(\\theta)​$得到最小值。</li>\n</ul>\n<p>$\\alpha\\frac{\\partial}{\\partial\\theta_j}$是对$J(\\theta)$的偏导求解过程如下：<br>$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}(h_\\theta(x)-y)^2$$<br>$$=2\\cdot\\frac{1}{2}(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$<br>$$=(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(x)-y)$$<br>$$=(h_\\theta(x)-y)\\cdot x_j$$<br>因此梯度下降算法的最终表述为</p>\n<p>Repeat Until Convergence{<br>$$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{n}((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j) $$ </p>\n<p>for every  $j$</p>\n<p>}</p>\n<p>梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\\theta$时我们便能得到线性回归函数。</p>\n<h3 id=\"4-线性回归算法实现\"><a href=\"#4-线性回归算法实现\" class=\"headerlink\" title=\"4.线性回归算法实现\"></a>4.线性回归算法实现</h3><p>为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址在<a href=\"http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\" target=\"_blank\" rel=\"noopener\">这儿</a> ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> linear_model</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d <span class=\"keyword\">import</span> axes3d</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#read_csv</span></span><br><span class=\"line\">readdata=pd.read_csv(<span class=\"string\">'data/Advertising.csv'</span>)</span><br><span class=\"line\">data=np.array(readdata.values)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练数据</span></span><br><span class=\"line\">X_train=data[<span class=\"number\">0</span>:<span class=\"number\">150</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\">Y_train=data[<span class=\"number\">0</span>:<span class=\"number\">150</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#测试数据</span></span><br><span class=\"line\">X_test=data[<span class=\"number\">150</span>:<span class=\"number\">200</span>,<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\">Y_test=data[<span class=\"number\">150</span>:<span class=\"number\">200</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#回归分析</span></span><br><span class=\"line\">regr = linear_model.LinearRegression()</span><br><span class=\"line\"><span class=\"comment\">#进行training set和test set的fit，即是训练的过程</span></span><br><span class=\"line\">regr.fit(X_train, Y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印出相关系数和截距等信息</span></span><br><span class=\"line\">print(<span class=\"string\">'Coefficients: \\n'</span>, regr.coef_)</span><br><span class=\"line\">print(<span class=\"string\">'Intercept: '</span>, regr.intercept_)</span><br><span class=\"line\"><span class=\"comment\"># The mean square error</span></span><br><span class=\"line\">print(<span class=\"string\">\"Residual sum of squares: %.2f\"</span></span><br><span class=\"line\">      % np.mean((regr.predict(X_test) - Y_test) ** <span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"comment\"># Explained variance score: 1 is perfect prediction</span></span><br><span class=\"line\">print(<span class=\"string\">'Variance score: %.2f'</span> % regr.score(X_test, Y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#得出回归函数 并自定义数据</span></span><br><span class=\"line\">X_line=np.linspace(<span class=\"number\">0</span>,<span class=\"number\">300</span>)</span><br><span class=\"line\">Y_line=np.linspace(<span class=\"number\">0</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">Z_line=<span class=\"number\">0.04699836</span>*X_line+<span class=\"number\">0.17913965</span>*Y_line+<span class=\"number\">3.00431061176</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">fig=plt.figure()</span><br><span class=\"line\">ax = plt.subplot(<span class=\"number\">111</span>, projection=<span class=\"string\">'3d'</span>)  <span class=\"comment\"># 创建一个三维的绘图工程</span></span><br><span class=\"line\">ax.scatter(data[:,<span class=\"number\">1</span>],data[:,<span class=\"number\">2</span>],data[:,<span class=\"number\">3</span>],c=<span class=\"string\">'red'</span>,)  <span class=\"comment\"># 绘制数据点</span></span><br><span class=\"line\">ax.plot(X_line,Y_line,Z_line,c=<span class=\"string\">'blue'</span>)<span class=\"comment\">#绘制回归曲线</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%BE%E7%89%8703.png\" alt=\"图片\"><br>其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。</p>\n<hr>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。<br><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"机器学习之自适应增强(Adaboost)","date":"2018-05-03T12:38:57.000Z","mathjax":true,"comments":1,"_content":"\n### 1.Adaboost简介\n\n**Adaptive boosting(自适应增强)**是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下**Boost(增强)**和**Adaptive(自适应)**的概念。\n\n#### 1.1集成学习之Boosting\n\n集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。\n\n![机器学习之Adaboost自适应增强图片01](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png)\n\n从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。\n\n#### 1.2Adaptive自适应\n\n**Adaptive(自适应)**体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。\n\n#### 1.3Adaboost流程\n\n结合**Adaptive(自适应)**和**Boost(增强)**概念，我们来具体介绍下**Adaboost**迭代算法流程。\n\n+ 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。\n+ 训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。\n+ 多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。\n\n那么我们便要思考，**如何计算学习误差率e？**,**如何得到弱学习器权重系数α?** ,**如何更新样本权重D？**,**使用哪种结合策略？**我们将在Adaboost分类和回归算法中给出详细解答。\n\n### 2.Adaboost分类算法\n\n假设我们的训练样本为\n$$\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}\n$$\n训练集在第k个弱学习器的输出权重为\n$$\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m\n$$\nAdaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的**学习误差率**为\n$$\ne_k=P(G_k(x_i)\\neq y_i)=\\sum_{i=1}^{m}w_{ki},I(G_k(x_i)\\neq y_i)\n$$\n对于二元分类问题，第k个**弱分类器Gk(x)的权重系数**为\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\n$$\n从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。\n\n假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则**更新后的第k+1个弱分类器的样本集权重系数**如下所示，此处Zk是规范化因子。\n$$\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}exp(-\\alpha_k y_i G_k(x_i))\n$$\n\n$$\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))\n$$\n\n从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)<0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述**权重系数**和**样本权重更新**公式，我们在下面讲**Adaboost损失函数**时会详细介绍。\n\n最后Adaboost分类问题采用**加权平均法结合策略**，最终的强分类器为\n$$\nf(x)=sign(\\sum_{k=1}^{K}\\alpha_k G_k(x))\n$$\n对于**Adaboost多元分类算法**，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}+log(R-1)\n$$\n\n### 3.Adaboost回归算法\n\n假设我们的训练样本为\n$$\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}\n$$\n训练集在第k个弱学习器的输出权重为\n$$\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m\n$$\n我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差\n$$\nE_k=max|y_i-G_k(x_i)|\\ i=1,2,3,...,m\n$$\n然后计算每个样本的相对误差\n$$\ne_{ki}=\\frac{|y_i-G_k(x_i)|}{E_k}\n$$\n上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。\n$$\ne_{ki}=\\frac{(y_i-G_k(x_i))^2}{E_k^2}\n$$\n\n$$\ne_{ki}=1-exp(\\frac{-y_i+G_k(x_i)}{E_k})\n$$\n\n最终得到第k个弱学习器的**学习误差率**\n$$\ne_k=\\sum_{i=1}^{m}w_{ki}e_{ki}\n$$\n那么**弱学习器的权重系数**为\n$$\n\\alpha_k=\\frac{e_k}{1-e_k}\n$$\n然后**更新样本权重D**，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。\n$$\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}\\alpha _k^{1-e_{ki}}\n$$\n\n$$\nZ_k=\\sum _{i=1}^{m}w_{ki}\\alpha _k^{1-e_{ki}}\n$$\n\n最后Adaboost回归问题采用**加权平均法结合策略**，最终的强回归器为\n$$\nf(x)=\\sum_{k=1}^{K}(ln\\frac{1}{\\alpha_k})G_k(x)\n$$\n\n### 4.Adaboost损失函数\n\n上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是**加法模型**，学习算法为**前向分布学习算法**，损失函数为指数函数。\n\n+ **加法模型：**最终强分类器是若干个弱分类器加权平均得到。\n+ **前向分布算法：**算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。\n\n假设第k-1轮和第k轮强学习器为\n$$\nf_{k-1}(x)=\\sum_{i=1}^{k-1}\\alpha_i G_i(x)\n$$\n\n$$\nf_k(x)=\\sum_{i=1}^{k}\\alpha_i G_i(x)\n$$\n\n因此我们可以得到\n$$\nf_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)\n$$\n可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为\n$$\n\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp(-y_if_k(x))\n$$\n利用前向分布学习算法的关系可以得到损失函数为\n$$\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp[(-y_i)(f_{k-1}(x)+\\alpha_k G_k(x))]\n$$\n令${w}'_{ki}=exp(-y_if_{k-1}(x))$，它的值不依赖于$\\alpha,G$而改变，仅仅依赖于$f_{k-1}(x)$，因此${w}'_{ki}$与最小化无关。将此式代入损失函数，损失函数转化为\n\n$$\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki}exp[-y_i\\alpha_k G_k(x))]\n$$\n首先我们求Gk(x)可以得到\n$$\nG_k(x)=\\underset{G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))\n$$\n将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\n$$\n其中ek为我们前面介绍的**分类误差率**\n$$\ne_k=\\frac{\\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))}{\\sum_{i=1}^{m}{w}'_{ki}}=\\sum_{i=1}^{m}w_{ki},I(y_i\\neq G(x_i))\n$$\n最后利用$f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)$、${w}'_{k,i}=exp(-y_if_{k-1}(x))$、${w}'_{k+1,i}=exp(-y_if_{k}(x))$，我们即可得到**样本权重的更新**。\n$$\n{w}'_{k+1,i}={w}'_{k,i}exp[-y_i\\alpha_kG_k(x_i)]\n$$\n\n$$\n{w}'_{k+1,i}=\\frac{w_{k,i}}{Z_k}exp[-y_i\\alpha_kG_k(x_i)]\n$$\n\n$$\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))\n$$\n\n### 5.Adaboost算法正则化\n\n为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代\n$$\nf_k(x)=f_{k-1}(x)+\\alpha_kG_k(x)\n$$\n如果我们加上正则化项，则有\n$$\nf_k(x)=f_{k-1}(x)+v\\alpha_kG_k(x)\n$$\nv的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n\n### 6.Sklearn实现Adaboost算法\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)。\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_gaussian_quantiles\n\n#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2\nX1,y1=make_gaussian_quantiles(cov=2.0,n_samples=500,\n                              n_features=2,n_classes=2,\n                              random_state=1)\n#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5\nX2,y2=make_gaussian_quantiles(mean=(3,3),cov=1.5,\n                              n_samples=400,n_features=2,\n                              n_classes=2,random_state=1)\n#将两组数据合为一组\nX=np.concatenate((X1,X2))\ny=np.concatenate((y1,-y2+1))\n\n#绘画生成的数据点\nplt.figure()\nplt.scatter(X[:,0],X[:,1],marker='o',c=y)\nplt.show()\n```\n\n![机器学习之Adaboost自适应增强图片02](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png)\n\n```python\n# 训练数据\nclf=AdaBoostClassifier(DecisionTreeClassifier(\n                       max_depth=2,min_samples_split=20,\n                       min_samples_leaf=5),algorithm=\"SAMME\",\n                       n_estimators=200,learning_rate=0.8)\nclf.fit(X,y)\n\n#将训练结果绘画出来\nplt.figure()\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y)\nplt.show()\n\n#训练模型的得分\nprint(clf.score(X,y))\n#0.913333333333\n```\n\n![机器学习之Adaboost自适应增强图片03](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png)\n\n### 7.Adaboost算法优缺点\n\n#### 7.1Adaboost优点\n\n+ 不容易发生过拟合。\n+ Adaboost是一种有很高精度的分类器。\n+ 当使用简单分类器时，计算出的结果是可理解的。\n+ 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。\n\n\n#### 7.2Adaboost缺点\n\n+ 训练时间过长。\n+ 执行效果依赖于弱分类器的选择。\n+ 对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n文章参考\n\n- [集成学习之Adaboost算法原理小结](https://www.cnblogs.com/pinard/p/6133937.html)","source":"_posts/机器学习之自适应增强-Adaboost.md","raw":"---\ntitle: 机器学习之自适应增强(Adaboost)\ndate: 2018-05-03 20:38:57\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.Adaboost简介\n\n**Adaptive boosting(自适应增强)**是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下**Boost(增强)**和**Adaptive(自适应)**的概念。\n\n#### 1.1集成学习之Boosting\n\n集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。\n\n![机器学习之Adaboost自适应增强图片01](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png)\n\n从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。\n\n#### 1.2Adaptive自适应\n\n**Adaptive(自适应)**体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。\n\n#### 1.3Adaboost流程\n\n结合**Adaptive(自适应)**和**Boost(增强)**概念，我们来具体介绍下**Adaboost**迭代算法流程。\n\n+ 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。\n+ 训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。\n+ 多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。\n\n那么我们便要思考，**如何计算学习误差率e？**,**如何得到弱学习器权重系数α?** ,**如何更新样本权重D？**,**使用哪种结合策略？**我们将在Adaboost分类和回归算法中给出详细解答。\n\n### 2.Adaboost分类算法\n\n假设我们的训练样本为\n$$\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}\n$$\n训练集在第k个弱学习器的输出权重为\n$$\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m\n$$\nAdaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的**学习误差率**为\n$$\ne_k=P(G_k(x_i)\\neq y_i)=\\sum_{i=1}^{m}w_{ki},I(G_k(x_i)\\neq y_i)\n$$\n对于二元分类问题，第k个**弱分类器Gk(x)的权重系数**为\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\n$$\n从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。\n\n假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则**更新后的第k+1个弱分类器的样本集权重系数**如下所示，此处Zk是规范化因子。\n$$\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}exp(-\\alpha_k y_i G_k(x_i))\n$$\n\n$$\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))\n$$\n\n从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)<0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述**权重系数**和**样本权重更新**公式，我们在下面讲**Adaboost损失函数**时会详细介绍。\n\n最后Adaboost分类问题采用**加权平均法结合策略**，最终的强分类器为\n$$\nf(x)=sign(\\sum_{k=1}^{K}\\alpha_k G_k(x))\n$$\n对于**Adaboost多元分类算法**，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}+log(R-1)\n$$\n\n### 3.Adaboost回归算法\n\n假设我们的训练样本为\n$$\nT=\\{ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)\\}\n$$\n训练集在第k个弱学习器的输出权重为\n$$\nD(k)=(w_{k1},w_{k2},w_{k3},...,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,...,m\n$$\n我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差\n$$\nE_k=max|y_i-G_k(x_i)|\\ i=1,2,3,...,m\n$$\n然后计算每个样本的相对误差\n$$\ne_{ki}=\\frac{|y_i-G_k(x_i)|}{E_k}\n$$\n上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。\n$$\ne_{ki}=\\frac{(y_i-G_k(x_i))^2}{E_k^2}\n$$\n\n$$\ne_{ki}=1-exp(\\frac{-y_i+G_k(x_i)}{E_k})\n$$\n\n最终得到第k个弱学习器的**学习误差率**\n$$\ne_k=\\sum_{i=1}^{m}w_{ki}e_{ki}\n$$\n那么**弱学习器的权重系数**为\n$$\n\\alpha_k=\\frac{e_k}{1-e_k}\n$$\n然后**更新样本权重D**，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。\n$$\nw_{k+1,i}=\\frac{w_{ki}}{Z_k}\\alpha _k^{1-e_{ki}}\n$$\n\n$$\nZ_k=\\sum _{i=1}^{m}w_{ki}\\alpha _k^{1-e_{ki}}\n$$\n\n最后Adaboost回归问题采用**加权平均法结合策略**，最终的强回归器为\n$$\nf(x)=\\sum_{k=1}^{K}(ln\\frac{1}{\\alpha_k})G_k(x)\n$$\n\n### 4.Adaboost损失函数\n\n上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是**加法模型**，学习算法为**前向分布学习算法**，损失函数为指数函数。\n\n+ **加法模型：**最终强分类器是若干个弱分类器加权平均得到。\n+ **前向分布算法：**算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。\n\n假设第k-1轮和第k轮强学习器为\n$$\nf_{k-1}(x)=\\sum_{i=1}^{k-1}\\alpha_i G_i(x)\n$$\n\n$$\nf_k(x)=\\sum_{i=1}^{k}\\alpha_i G_i(x)\n$$\n\n因此我们可以得到\n$$\nf_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)\n$$\n可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为\n$$\n\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp(-y_if_k(x))\n$$\n利用前向分布学习算法的关系可以得到损失函数为\n$$\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp[(-y_i)(f_{k-1}(x)+\\alpha_k G_k(x))]\n$$\n令${w}'_{ki}=exp(-y_if_{k-1}(x))$，它的值不依赖于$\\alpha,G$而改变，仅仅依赖于$f_{k-1}(x)$，因此${w}'_{ki}$与最小化无关。将此式代入损失函数，损失函数转化为\n\n$$\n(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki}exp[-y_i\\alpha_k G_k(x))]\n$$\n首先我们求Gk(x)可以得到\n$$\nG_k(x)=\\underset{G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))\n$$\n将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到\n$$\n\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}\n$$\n其中ek为我们前面介绍的**分类误差率**\n$$\ne_k=\\frac{\\sum_{i=1}^{m}{w}'_{ki},I(y_i\\neq G(x_i))}{\\sum_{i=1}^{m}{w}'_{ki}}=\\sum_{i=1}^{m}w_{ki},I(y_i\\neq G(x_i))\n$$\n最后利用$f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)$、${w}'_{k,i}=exp(-y_if_{k-1}(x))$、${w}'_{k+1,i}=exp(-y_if_{k}(x))$，我们即可得到**样本权重的更新**。\n$$\n{w}'_{k+1,i}={w}'_{k,i}exp[-y_i\\alpha_kG_k(x_i)]\n$$\n\n$$\n{w}'_{k+1,i}=\\frac{w_{k,i}}{Z_k}exp[-y_i\\alpha_kG_k(x_i)]\n$$\n\n$$\nZ_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))\n$$\n\n### 5.Adaboost算法正则化\n\n为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代\n$$\nf_k(x)=f_{k-1}(x)+\\alpha_kG_k(x)\n$$\n如果我们加上正则化项，则有\n$$\nf_k(x)=f_{k-1}(x)+v\\alpha_kG_k(x)\n$$\nv的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n\n### 6.Sklearn实现Adaboost算法\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)。\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_gaussian_quantiles\n\n#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2\nX1,y1=make_gaussian_quantiles(cov=2.0,n_samples=500,\n                              n_features=2,n_classes=2,\n                              random_state=1)\n#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5\nX2,y2=make_gaussian_quantiles(mean=(3,3),cov=1.5,\n                              n_samples=400,n_features=2,\n                              n_classes=2,random_state=1)\n#将两组数据合为一组\nX=np.concatenate((X1,X2))\ny=np.concatenate((y1,-y2+1))\n\n#绘画生成的数据点\nplt.figure()\nplt.scatter(X[:,0],X[:,1],marker='o',c=y)\nplt.show()\n```\n\n![机器学习之Adaboost自适应增强图片02](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png)\n\n```python\n# 训练数据\nclf=AdaBoostClassifier(DecisionTreeClassifier(\n                       max_depth=2,min_samples_split=20,\n                       min_samples_leaf=5),algorithm=\"SAMME\",\n                       n_estimators=200,learning_rate=0.8)\nclf.fit(X,y)\n\n#将训练结果绘画出来\nplt.figure()\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y)\nplt.show()\n\n#训练模型的得分\nprint(clf.score(X,y))\n#0.913333333333\n```\n\n![机器学习之Adaboost自适应增强图片03](机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png)\n\n### 7.Adaboost算法优缺点\n\n#### 7.1Adaboost优点\n\n+ 不容易发生过拟合。\n+ Adaboost是一种有很高精度的分类器。\n+ 当使用简单分类器时，计算出的结果是可理解的。\n+ 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。\n\n\n#### 7.2Adaboost缺点\n\n+ 训练时间过长。\n+ 执行效果依赖于弱分类器的选择。\n+ 对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n文章参考\n\n- [集成学习之Adaboost算法原理小结](https://www.cnblogs.com/pinard/p/6133937.html)","slug":"机器学习之自适应增强-Adaboost","published":1,"updated":"2018-05-07T01:45:33.255Z","layout":"post","photos":[],"link":"","_id":"cji4rdze2001a2e01jl6br241","content":"<h3 id=\"1-Adaboost简介\"><a href=\"#1-Adaboost简介\" class=\"headerlink\" title=\"1.Adaboost简介\"></a>1.Adaboost简介</h3><p><strong>Adaptive boosting(自适应增强)</strong>是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下<strong>Boost(增强)</strong>和<strong>Adaptive(自适应)</strong>的概念。</p>\n<h4 id=\"1-1集成学习之Boosting\"><a href=\"#1-1集成学习之Boosting\" class=\"headerlink\" title=\"1.1集成学习之Boosting\"></a>1.1集成学习之Boosting</h4><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p>\n<p><img src=\"机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png\" alt=\"机器学习之Adaboost自适应增强图片01\"></p>\n<p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>\n<h4 id=\"1-2Adaptive自适应\"><a href=\"#1-2Adaptive自适应\" class=\"headerlink\" title=\"1.2Adaptive自适应\"></a>1.2Adaptive自适应</h4><p><strong>Adaptive(自适应)</strong>体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。</p>\n<h4 id=\"1-3Adaboost流程\"><a href=\"#1-3Adaboost流程\" class=\"headerlink\" title=\"1.3Adaboost流程\"></a>1.3Adaboost流程</h4><p>结合<strong>Adaptive(自适应)</strong>和<strong>Boost(增强)</strong>概念，我们来具体介绍下<strong>Adaboost</strong>迭代算法流程。</p>\n<ul>\n<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。</li>\n<li>训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。</li>\n<li>多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li>\n</ul>\n<p>那么我们便要思考，<strong>如何计算学习误差率e？</strong>,<strong>如何得到弱学习器权重系数α?</strong> ,<strong>如何更新样本权重D？</strong>,<strong>使用哪种结合策略？</strong>我们将在Adaboost分类和回归算法中给出详细解答。</p>\n<h3 id=\"2-Adaboost分类算法\"><a href=\"#2-Adaboost分类算法\" class=\"headerlink\" title=\"2.Adaboost分类算法\"></a>2.Adaboost分类算法</h3><p>假设我们的训练样本为<br>$$<br>T={ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)}<br>$$<br>训练集在第k个弱学习器的输出权重为<br>$$<br>D(k)=(w_{k1},w_{k2},w_{k3},…,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,…,m<br>$$<br>Adaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的<strong>学习误差率</strong>为<br>$$<br>e_k=P(G_k(x_i)\\neq y_i)=\\sum_{i=1}^{m}w_{ki},I(G_k(x_i)\\neq y_i)<br>$$<br>对于二元分类问题，第k个<strong>弱分类器Gk(x)的权重系数</strong>为<br>$$<br>\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}<br>$$<br>从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。</p>\n<p>假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则<strong>更新后的第k+1个弱分类器的样本集权重系数</strong>如下所示，此处Zk是规范化因子。<br>$$<br>w_{k+1,i}=\\frac{w_{ki}}{Z_k}exp(-\\alpha_k y_i G_k(x_i))<br>$$</p>\n<p>$$<br>Z_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))<br>$$</p>\n<p>从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)&lt;0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述<strong>权重系数</strong>和<strong>样本权重更新</strong>公式，我们在下面讲<strong>Adaboost损失函数</strong>时会详细介绍。</p>\n<p>最后Adaboost分类问题采用<strong>加权平均法结合策略</strong>，最终的强分类器为<br>$$<br>f(x)=sign(\\sum_{k=1}^{K}\\alpha_k G_k(x))<br>$$<br>对于<strong>Adaboost多元分类算法</strong>，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。<br>$$<br>\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}+log(R-1)<br>$$</p>\n<h3 id=\"3-Adaboost回归算法\"><a href=\"#3-Adaboost回归算法\" class=\"headerlink\" title=\"3.Adaboost回归算法\"></a>3.Adaboost回归算法</h3><p>假设我们的训练样本为<br>$$<br>T={ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)}<br>$$<br>训练集在第k个弱学习器的输出权重为<br>$$<br>D(k)=(w_{k1},w_{k2},w_{k3},…,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,…,m<br>$$<br>我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差<br>$$<br>E_k=max|y_i-G_k(x_i)|\\ i=1,2,3,…,m<br>$$<br>然后计算每个样本的相对误差<br>$$<br>e_{ki}=\\frac{|y_i-G_k(x_i)|}{E_k}<br>$$<br>上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。<br>$$<br>e_{ki}=\\frac{(y_i-G_k(x_i))^2}{E_k^2}<br>$$</p>\n<p>$$<br>e_{ki}=1-exp(\\frac{-y_i+G_k(x_i)}{E_k})<br>$$</p>\n<p>最终得到第k个弱学习器的<strong>学习误差率</strong><br>$$<br>e_k=\\sum_{i=1}^{m}w_{ki}e_{ki}<br>$$<br>那么<strong>弱学习器的权重系数</strong>为<br>$$<br>\\alpha_k=\\frac{e_k}{1-e_k}<br>$$<br>然后<strong>更新样本权重D</strong>，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。<br>$$<br>w_{k+1,i}=\\frac{w_{ki}}{Z_k}\\alpha <em>k^{1-e</em>{ki}}<br>$$</p>\n<p>$$<br>Z_k=\\sum <em>{i=1}^{m}w</em>{ki}\\alpha <em>k^{1-e</em>{ki}}<br>$$</p>\n<p>最后Adaboost回归问题采用<strong>加权平均法结合策略</strong>，最终的强回归器为<br>$$<br>f(x)=\\sum_{k=1}^{K}(ln\\frac{1}{\\alpha_k})G_k(x)<br>$$</p>\n<h3 id=\"4-Adaboost损失函数\"><a href=\"#4-Adaboost损失函数\" class=\"headerlink\" title=\"4.Adaboost损失函数\"></a>4.Adaboost损失函数</h3><p>上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是<strong>加法模型</strong>，学习算法为<strong>前向分布学习算法</strong>，损失函数为指数函数。</p>\n<ul>\n<li><strong>加法模型：</strong>最终强分类器是若干个弱分类器加权平均得到。</li>\n<li><strong>前向分布算法：</strong>算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。</li>\n</ul>\n<p>假设第k-1轮和第k轮强学习器为<br>$$<br>f_{k-1}(x)=\\sum_{i=1}^{k-1}\\alpha_i G_i(x)<br>$$</p>\n<p>$$<br>f_k(x)=\\sum_{i=1}^{k}\\alpha_i G_i(x)<br>$$</p>\n<p>因此我们可以得到<br>$$<br>f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)<br>$$<br>可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为<br>$$<br>\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp(-y_if_k(x))<br>$$<br>利用前向分布学习算法的关系可以得到损失函数为<br>$$<br>(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp[(-y_i)(f_{k-1}(x)+\\alpha_k G_k(x))]<br>$$<br>令${w}’_{ki}=exp(-y_if_{k-1}(x))$，它的值不依赖于$\\alpha,G$而改变，仅仅依赖于$f_{k-1}(x)$，因此${w}’_{ki}$与最小化无关。将此式代入损失函数，损失函数转化为</p>\n<p>$$<br>(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}’_{ki}exp[-y_i\\alpha_k G_k(x))]<br>$$<br>首先我们求Gk(x)可以得到<br>$$<br>G_k(x)=\\underset{G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}’_{ki},I(y_i\\neq G(x_i))<br>$$<br>将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到<br>$$<br>\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}<br>$$<br>其中ek为我们前面介绍的<strong>分类误差率</strong><br>$$<br>e_k=\\frac{\\sum_{i=1}^{m}{w}’_{ki},I(y_i\\neq G(x_i))}{\\sum_{i=1}^{m}{w}’<em>{ki}}=\\sum</em>{i=1}^{m}w_{ki},I(y_i\\neq G(x_i))<br>$$<br>最后利用$f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)$、${w}’_{k,i}=exp(-y_if_{k-1}(x))$、${w}’_{k+1,i}=exp(-y_if_{k}(x))$，我们即可得到<strong>样本权重的更新</strong>。<br>$$<br>{w}’<em>{k+1,i}={w}’</em>{k,i}exp[-y_i\\alpha_kG_k(x_i)]<br>$$</p>\n<p>$$<br>{w}’<em>{k+1,i}=\\frac{w</em>{k,i}}{Z_k}exp[-y_i\\alpha_kG_k(x_i)]<br>$$</p>\n<p>$$<br>Z_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))<br>$$</p>\n<h3 id=\"5-Adaboost算法正则化\"><a href=\"#5-Adaboost算法正则化\" class=\"headerlink\" title=\"5.Adaboost算法正则化\"></a>5.Adaboost算法正则化</h3><p>为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代<br>$$<br>f_k(x)=f_{k-1}(x)+\\alpha_kG_k(x)<br>$$<br>如果我们加上正则化项，则有<br>$$<br>f_k(x)=f_{k-1}(x)+v\\alpha_kG_k(x)<br>$$<br>v的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>\n<h3 id=\"6-Sklearn实现Adaboost算法\"><a href=\"#6-Sklearn实现Adaboost算法\" class=\"headerlink\" title=\"6.Sklearn实现Adaboost算法\"></a>6.Sklearn实现Adaboost算法</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> AdaBoostClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_gaussian_quantiles</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2</span></span><br><span class=\"line\">X1,y1=make_gaussian_quantiles(cov=<span class=\"number\">2.0</span>,n_samples=<span class=\"number\">500</span>,</span><br><span class=\"line\">                              n_features=<span class=\"number\">2</span>,n_classes=<span class=\"number\">2</span>,</span><br><span class=\"line\">                              random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5</span></span><br><span class=\"line\">X2,y2=make_gaussian_quantiles(mean=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),cov=<span class=\"number\">1.5</span>,</span><br><span class=\"line\">                              n_samples=<span class=\"number\">400</span>,n_features=<span class=\"number\">2</span>,</span><br><span class=\"line\">                              n_classes=<span class=\"number\">2</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#将两组数据合为一组</span></span><br><span class=\"line\">X=np.concatenate((X1,X2))</span><br><span class=\"line\">y=np.concatenate((y1,-y2+<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘画生成的数据点</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],marker=<span class=\"string\">'o'</span>,c=y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png\" alt=\"机器学习之Adaboost自适应增强图片02\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 训练数据</span></span><br><span class=\"line\">clf=AdaBoostClassifier(DecisionTreeClassifier(</span><br><span class=\"line\">                       max_depth=<span class=\"number\">2</span>,min_samples_split=<span class=\"number\">20</span>,</span><br><span class=\"line\">                       min_samples_leaf=<span class=\"number\">5</span>),algorithm=<span class=\"string\">\"SAMME\"</span>,</span><br><span class=\"line\">                       n_estimators=<span class=\"number\">200</span>,learning_rate=<span class=\"number\">0.8</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将训练结果绘画出来</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">x_min, x_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">y_min, y_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class=\"number\">0.02</span>),</span><br><span class=\"line\">                     np.arange(y_min, y_max, <span class=\"number\">0.02</span>))</span><br><span class=\"line\">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">Z = Z.reshape(xx.shape)</span><br><span class=\"line\">cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=y)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练模型的得分</span></span><br><span class=\"line\">print(clf.score(X,y))</span><br><span class=\"line\"><span class=\"comment\">#0.913333333333</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png\" alt=\"机器学习之Adaboost自适应增强图片03\"></p>\n<h3 id=\"7-Adaboost算法优缺点\"><a href=\"#7-Adaboost算法优缺点\" class=\"headerlink\" title=\"7.Adaboost算法优缺点\"></a>7.Adaboost算法优缺点</h3><h4 id=\"7-1Adaboost优点\"><a href=\"#7-1Adaboost优点\" class=\"headerlink\" title=\"7.1Adaboost优点\"></a>7.1Adaboost优点</h4><ul>\n<li>不容易发生过拟合。</li>\n<li>Adaboost是一种有很高精度的分类器。</li>\n<li>当使用简单分类器时，计算出的结果是可理解的。</li>\n<li>可以使用各种方法构建子分类器，Adaboost算法提供的是框架。</li>\n</ul>\n<h4 id=\"7-2Adaboost缺点\"><a href=\"#7-2Adaboost缺点\" class=\"headerlink\" title=\"7.2Adaboost缺点\"></a>7.2Adaboost缺点</h4><ul>\n<li>训练时间过长。</li>\n<li>执行效果依赖于弱分类器的选择。</li>\n<li>对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>文章参考</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6133937.html\" target=\"_blank\" rel=\"noopener\">集成学习之Adaboost算法原理小结</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Adaboost简介\"><a href=\"#1-Adaboost简介\" class=\"headerlink\" title=\"1.Adaboost简介\"></a>1.Adaboost简介</h3><p><strong>Adaptive boosting(自适应增强)</strong>是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下<strong>Boost(增强)</strong>和<strong>Adaptive(自适应)</strong>的概念。</p>\n<h4 id=\"1-1集成学习之Boosting\"><a href=\"#1-1集成学习之Boosting\" class=\"headerlink\" title=\"1.1集成学习之Boosting\"></a>1.1集成学习之Boosting</h4><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p>\n<p><img src=\"机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png\" alt=\"机器学习之Adaboost自适应增强图片01\"></p>\n<p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>\n<h4 id=\"1-2Adaptive自适应\"><a href=\"#1-2Adaptive自适应\" class=\"headerlink\" title=\"1.2Adaptive自适应\"></a>1.2Adaptive自适应</h4><p><strong>Adaptive(自适应)</strong>体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。</p>\n<h4 id=\"1-3Adaboost流程\"><a href=\"#1-3Adaboost流程\" class=\"headerlink\" title=\"1.3Adaboost流程\"></a>1.3Adaboost流程</h4><p>结合<strong>Adaptive(自适应)</strong>和<strong>Boost(增强)</strong>概念，我们来具体介绍下<strong>Adaboost</strong>迭代算法流程。</p>\n<ul>\n<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。</li>\n<li>训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。</li>\n<li>多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li>\n</ul>\n<p>那么我们便要思考，<strong>如何计算学习误差率e？</strong>,<strong>如何得到弱学习器权重系数α?</strong> ,<strong>如何更新样本权重D？</strong>,<strong>使用哪种结合策略？</strong>我们将在Adaboost分类和回归算法中给出详细解答。</p>\n<h3 id=\"2-Adaboost分类算法\"><a href=\"#2-Adaboost分类算法\" class=\"headerlink\" title=\"2.Adaboost分类算法\"></a>2.Adaboost分类算法</h3><p>假设我们的训练样本为<br>$$<br>T={ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)}<br>$$<br>训练集在第k个弱学习器的输出权重为<br>$$<br>D(k)=(w_{k1},w_{k2},w_{k3},…,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,…,m<br>$$<br>Adaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的<strong>学习误差率</strong>为<br>$$<br>e_k=P(G_k(x_i)\\neq y_i)=\\sum_{i=1}^{m}w_{ki},I(G_k(x_i)\\neq y_i)<br>$$<br>对于二元分类问题，第k个<strong>弱分类器Gk(x)的权重系数</strong>为<br>$$<br>\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}<br>$$<br>从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。</p>\n<p>假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则<strong>更新后的第k+1个弱分类器的样本集权重系数</strong>如下所示，此处Zk是规范化因子。<br>$$<br>w_{k+1,i}=\\frac{w_{ki}}{Z_k}exp(-\\alpha_k y_i G_k(x_i))<br>$$</p>\n<p>$$<br>Z_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))<br>$$</p>\n<p>从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)&lt;0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述<strong>权重系数</strong>和<strong>样本权重更新</strong>公式，我们在下面讲<strong>Adaboost损失函数</strong>时会详细介绍。</p>\n<p>最后Adaboost分类问题采用<strong>加权平均法结合策略</strong>，最终的强分类器为<br>$$<br>f(x)=sign(\\sum_{k=1}^{K}\\alpha_k G_k(x))<br>$$<br>对于<strong>Adaboost多元分类算法</strong>，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。<br>$$<br>\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}+log(R-1)<br>$$</p>\n<h3 id=\"3-Adaboost回归算法\"><a href=\"#3-Adaboost回归算法\" class=\"headerlink\" title=\"3.Adaboost回归算法\"></a>3.Adaboost回归算法</h3><p>假设我们的训练样本为<br>$$<br>T={ (x_1,y_1) , (x_2,y_2),…, (x_m,y_m)}<br>$$<br>训练集在第k个弱学习器的输出权重为<br>$$<br>D(k)=(w_{k1},w_{k2},w_{k3},…,w_{km});\\ w_{1i}=\\frac{1}{m};\\ i=1,2,3,…,m<br>$$<br>我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差<br>$$<br>E_k=max|y_i-G_k(x_i)|\\ i=1,2,3,…,m<br>$$<br>然后计算每个样本的相对误差<br>$$<br>e_{ki}=\\frac{|y_i-G_k(x_i)|}{E_k}<br>$$<br>上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。<br>$$<br>e_{ki}=\\frac{(y_i-G_k(x_i))^2}{E_k^2}<br>$$</p>\n<p>$$<br>e_{ki}=1-exp(\\frac{-y_i+G_k(x_i)}{E_k})<br>$$</p>\n<p>最终得到第k个弱学习器的<strong>学习误差率</strong><br>$$<br>e_k=\\sum_{i=1}^{m}w_{ki}e_{ki}<br>$$<br>那么<strong>弱学习器的权重系数</strong>为<br>$$<br>\\alpha_k=\\frac{e_k}{1-e_k}<br>$$<br>然后<strong>更新样本权重D</strong>，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。<br>$$<br>w_{k+1,i}=\\frac{w_{ki}}{Z_k}\\alpha <em>k^{1-e</em>{ki}}<br>$$</p>\n<p>$$<br>Z_k=\\sum <em>{i=1}^{m}w</em>{ki}\\alpha <em>k^{1-e</em>{ki}}<br>$$</p>\n<p>最后Adaboost回归问题采用<strong>加权平均法结合策略</strong>，最终的强回归器为<br>$$<br>f(x)=\\sum_{k=1}^{K}(ln\\frac{1}{\\alpha_k})G_k(x)<br>$$</p>\n<h3 id=\"4-Adaboost损失函数\"><a href=\"#4-Adaboost损失函数\" class=\"headerlink\" title=\"4.Adaboost损失函数\"></a>4.Adaboost损失函数</h3><p>上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是<strong>加法模型</strong>，学习算法为<strong>前向分布学习算法</strong>，损失函数为指数函数。</p>\n<ul>\n<li><strong>加法模型：</strong>最终强分类器是若干个弱分类器加权平均得到。</li>\n<li><strong>前向分布算法：</strong>算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。</li>\n</ul>\n<p>假设第k-1轮和第k轮强学习器为<br>$$<br>f_{k-1}(x)=\\sum_{i=1}^{k-1}\\alpha_i G_i(x)<br>$$</p>\n<p>$$<br>f_k(x)=\\sum_{i=1}^{k}\\alpha_i G_i(x)<br>$$</p>\n<p>因此我们可以得到<br>$$<br>f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)<br>$$<br>可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为<br>$$<br>\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp(-y_if_k(x))<br>$$<br>利用前向分布学习算法的关系可以得到损失函数为<br>$$<br>(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}exp[(-y_i)(f_{k-1}(x)+\\alpha_k G_k(x))]<br>$$<br>令${w}’_{ki}=exp(-y_if_{k-1}(x))$，它的值不依赖于$\\alpha,G$而改变，仅仅依赖于$f_{k-1}(x)$，因此${w}’_{ki}$与最小化无关。将此式代入损失函数，损失函数转化为</p>\n<p>$$<br>(\\alpha_k,G_k(x))=\\underset{\\alpha ,G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}’_{ki}exp[-y_i\\alpha_k G_k(x))]<br>$$<br>首先我们求Gk(x)可以得到<br>$$<br>G_k(x)=\\underset{G}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}{w}’_{ki},I(y_i\\neq G(x_i))<br>$$<br>将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到<br>$$<br>\\alpha_k=\\frac{1}{2}log\\frac{1-e_k}{e_k}<br>$$<br>其中ek为我们前面介绍的<strong>分类误差率</strong><br>$$<br>e_k=\\frac{\\sum_{i=1}^{m}{w}’_{ki},I(y_i\\neq G(x_i))}{\\sum_{i=1}^{m}{w}’<em>{ki}}=\\sum</em>{i=1}^{m}w_{ki},I(y_i\\neq G(x_i))<br>$$<br>最后利用$f_k(x)=f_{k-1}(x)+\\alpha_k G_k(x)$、${w}’_{k,i}=exp(-y_if_{k-1}(x))$、${w}’_{k+1,i}=exp(-y_if_{k}(x))$，我们即可得到<strong>样本权重的更新</strong>。<br>$$<br>{w}’<em>{k+1,i}={w}’</em>{k,i}exp[-y_i\\alpha_kG_k(x_i)]<br>$$</p>\n<p>$$<br>{w}’<em>{k+1,i}=\\frac{w</em>{k,i}}{Z_k}exp[-y_i\\alpha_kG_k(x_i)]<br>$$</p>\n<p>$$<br>Z_k=\\sum_{i=1}^{m}w_{ki}exp(-\\alpha_ky_iG_k(x_i))<br>$$</p>\n<h3 id=\"5-Adaboost算法正则化\"><a href=\"#5-Adaboost算法正则化\" class=\"headerlink\" title=\"5.Adaboost算法正则化\"></a>5.Adaboost算法正则化</h3><p>为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代<br>$$<br>f_k(x)=f_{k-1}(x)+\\alpha_kG_k(x)<br>$$<br>如果我们加上正则化项，则有<br>$$<br>f_k(x)=f_{k-1}(x)+v\\alpha_kG_k(x)<br>$$<br>v的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>\n<h3 id=\"6-Sklearn实现Adaboost算法\"><a href=\"#6-Sklearn实现Adaboost算法\" class=\"headerlink\" title=\"6.Sklearn实现Adaboost算法\"></a>6.Sklearn实现Adaboost算法</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> AdaBoostClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_gaussian_quantiles</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2</span></span><br><span class=\"line\">X1,y1=make_gaussian_quantiles(cov=<span class=\"number\">2.0</span>,n_samples=<span class=\"number\">500</span>,</span><br><span class=\"line\">                              n_features=<span class=\"number\">2</span>,n_classes=<span class=\"number\">2</span>,</span><br><span class=\"line\">                              random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5</span></span><br><span class=\"line\">X2,y2=make_gaussian_quantiles(mean=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),cov=<span class=\"number\">1.5</span>,</span><br><span class=\"line\">                              n_samples=<span class=\"number\">400</span>,n_features=<span class=\"number\">2</span>,</span><br><span class=\"line\">                              n_classes=<span class=\"number\">2</span>,random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#将两组数据合为一组</span></span><br><span class=\"line\">X=np.concatenate((X1,X2))</span><br><span class=\"line\">y=np.concatenate((y1,-y2+<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#绘画生成的数据点</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">plt.scatter(X[:,<span class=\"number\">0</span>],X[:,<span class=\"number\">1</span>],marker=<span class=\"string\">'o'</span>,c=y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png\" alt=\"机器学习之Adaboost自适应增强图片02\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 训练数据</span></span><br><span class=\"line\">clf=AdaBoostClassifier(DecisionTreeClassifier(</span><br><span class=\"line\">                       max_depth=<span class=\"number\">2</span>,min_samples_split=<span class=\"number\">20</span>,</span><br><span class=\"line\">                       min_samples_leaf=<span class=\"number\">5</span>),algorithm=<span class=\"string\">\"SAMME\"</span>,</span><br><span class=\"line\">                       n_estimators=<span class=\"number\">200</span>,learning_rate=<span class=\"number\">0.8</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将训练结果绘画出来</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\">x_min, x_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">y_min, y_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">1</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">1</span></span><br><span class=\"line\">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class=\"number\">0.02</span>),</span><br><span class=\"line\">                     np.arange(y_min, y_max, <span class=\"number\">0.02</span>))</span><br><span class=\"line\">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">Z = Z.reshape(xx.shape)</span><br><span class=\"line\">cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=y)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#训练模型的得分</span></span><br><span class=\"line\">print(clf.score(X,y))</span><br><span class=\"line\"><span class=\"comment\">#0.913333333333</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png\" alt=\"机器学习之Adaboost自适应增强图片03\"></p>\n<h3 id=\"7-Adaboost算法优缺点\"><a href=\"#7-Adaboost算法优缺点\" class=\"headerlink\" title=\"7.Adaboost算法优缺点\"></a>7.Adaboost算法优缺点</h3><h4 id=\"7-1Adaboost优点\"><a href=\"#7-1Adaboost优点\" class=\"headerlink\" title=\"7.1Adaboost优点\"></a>7.1Adaboost优点</h4><ul>\n<li>不容易发生过拟合。</li>\n<li>Adaboost是一种有很高精度的分类器。</li>\n<li>当使用简单分类器时，计算出的结果是可理解的。</li>\n<li>可以使用各种方法构建子分类器，Adaboost算法提供的是框架。</li>\n</ul>\n<h4 id=\"7-2Adaboost缺点\"><a href=\"#7-2Adaboost缺点\" class=\"headerlink\" title=\"7.2Adaboost缺点\"></a>7.2Adaboost缺点</h4><ul>\n<li>训练时间过长。</li>\n<li>执行效果依赖于弱分类器的选择。</li>\n<li>对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>文章参考</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6133937.html\" target=\"_blank\" rel=\"noopener\">集成学习之Adaboost算法原理小结</a></li>\n</ul>\n"},{"title":"机器学习之梯度提升决策树(GBDT)","date":"2018-04-30T15:15:14.000Z","mathjax":true,"comments":1,"_content":"\n### 1.GBDT算法简介\n\n**GBDT(Gradient Boosting Decision Tree)**是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(**Gradient Boosting Decision Tree**)来展开推导过程。决策树(**Decision Tree**)我们已经不再陌生，在之前介绍到的[机器学习之决策树(C4.5算法)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483837&idx=1&sn=f73ca53c5d50f7cd090ba3bc0e17c56b&chksm=fcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3#rd)、[机器学习之分类与回归树(CART)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483841&idx=1&sn=b67c59dc4284f0b363b2de881c5da729&chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd)、[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中已经多次接触，在此不再赘述。但**Boosting**和**Gradient**方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。\n\n#### 1.1集成学习之Boosting\n\n集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。\n\n![机器学习之梯度提升决策树图片01](机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png)\n\n从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到**Boosting Decision Tree**。\n\n#### 1.2 Boosting Decision Tree\n\n**提升树(Boosting Decision Tree)**由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在[机器学习之分类与回归树(CART)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483841&idx=1&sn=b67c59dc4284f0b363b2de881c5da729&chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd)中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。\n\n我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。\n\n我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下\n\n![机器学习之梯度提升决策树图片02](机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png)\n\n我们能够直观的看到，预测值等于所有树值的累加，如**A的预测值=树1左节点(15)+树2左节点(-1)=14**。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下\n\n+ 初始化$f_0(x)=0$\n+ 对$t=1,2,3,…,T$\n  + 计算残差$r_{ti}=y_i-f_{t-1}(x_i),i=1,2,3,…,m$。\n  + 拟合残差$r_{ti}$学习得到回归树$h_t(x)$\n  + 更新$f_t(x)=f_{t-1}(x)+h_t(x)$\n+ 得到回归问题提升树$f_T(x)=f_0(x)+\\sum_{t=1}^{T}h_t(x)$\n\n我们介绍了**Boosting Decision Tree**的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到**Gradient Boosting Decision Tree的负梯度拟合**。\n\n#### 1.3GBDT负梯度拟合\n\nBoosting Decision Tree迭代过程中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$，我们本轮迭代的目标是找到一个回归树模型的弱学习器$h_t(x)$，让本轮的损失$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到的决策树，要让样本的损失函数尽量变得更小。\n\n我们利用损失函数$L(y_i,f(x_i))$的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为\n$$\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}\n$$\n利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一棵CART回归树，得到第t棵回归树，其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中$J$为叶子节点的个数。\n\n针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值$c_{tj}$。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。\n$$\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)\n$$\n这样我们便得到本轮的决策树拟合函数\n$$\nh_t(x)=\\sum _{j=1} ^{J} c_{tj},I(x \\in R_{tj})\n$$\n从而本轮最终得到的强学习器表达式如下\n$$\nf_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n$$\n通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。\n\n### 2.GBDT回归算法\n\n通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。\n\n假设训练集样本$T=\\{ (x,y_1),(x,y_2),…,(x,y_m)\\}$，最大迭代次数为$T$，损失函数$L$，输出是强学习器$f(x)$。回归算法过程如下所示\n\n+ 初始化弱学习器，c的均值可设置为样本y的均值。\n\n$$\nf_0(x)=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}L(y_i,c)\n$$\n\n+ 对迭代次数$t=1,2,3,…,T$有\n\n  + 对样本$i=1,2,3,…,m$，计算负梯度\n\n  $$\n  r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}\n  $$\n\n  + 利用$(x_i,r_{ti})i=1,2,3,…,m$，拟合一棵CART回归树，得到第t棵回归树，其对应的叶子节点区域为$R_{tj},j=1,2,3,…,J$。其中$J$为回归树$t$的叶子节点个数。\n  + 对叶子区域$j=1,2,3,…,J$，计算最佳拟合值\n\n  $$\n  c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)\n  $$\n\n  + 更新强学习器\n\n  $$\n  f_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n  $$\n\n+ 得到强学习器$f(x)$表达式\n  $$\n  f(x)=f_T(x)=f_0(x)+\\sum_{t=1}^{T}\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n  $$\n\n\n### 3.GBDT分类算法\n\nGBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。\n\n#### 3.1二元GBDT分类算法\n\n对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为\n$$\nL(y,f(x))=log(1+exp(-yf(x)))\n$$\n其中$y \\in \\{ -1,1\\}$。则此时的负梯度误差为\n$$\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}\n$$\n对于生成的决策树，我们各个叶子节点的最佳残差拟合值为\n$$\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum _{x_i\\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))\n$$\n由于上式比较难优化，我们一般使用近似值代替\n$$\nc_{tj}=\\frac{\\sum _{x_i\\in R_{tj}}r_{ti}}{\\sum _{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}\n$$\n除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。\n\n#### 3.2多元GBDT分类算法\n\n多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为\n$$\nL(y,f(x))=-\\sum_{k=1}^{K}y_k log(p_k(x))\n$$\n其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为\n$$\np_k(x)=\\frac {exp(f_k(x))}{\\sum _{l=1}^{K}exp(f_l(x))}\n$$\n集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为\n$$\nr_{til}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)\n$$\n其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为\n$$\nc_{tjl}=\\underset{cjl}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m} \\sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\\sum _{j=1}^{J}c_{jl},I(x_i\\in R_{tj})\n$$\n由于上式比较难优化，我们用近似值代替\n$$\nc_{tjl}=\\frac{K-1}{K}=\\frac{\\sum_{x_i\\in R_{tjl}}r_{til}}{\\sum _{x_i\\in R_{til}}|r_{til}|(1-|r_{til}|)}\n$$\n除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。\n\n### 4.GBDT损失函数\n\n对于**回归算法**，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。\n\n+ 均方差损失。\n\n$$\nL(y,f(x))=(y-f(x))^2\n$$\n\n+ 绝对损失和对应的负梯度误差。\n\n$$\nL(y,f(x))=|y-f(x)|\n$$\n\n$$\nsign(y_i-f(x_i))\n$$\n\n+ Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下\n\n$$\nL(y,f(x))=\\left\\{\\begin{matrix}\n\\frac{1}{2}(y-f(x))^2 &|y-f(x)|\\le \\delta \\\\ \n \\delta (|y-f(x)|-\\frac{\\delta}{2})&|y-f(x)|> \\delta \n\\end{matrix}\\right.\n$$\n\n$$\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\ny_i-f(x_i) &|y_i-f(x_i)|\\le \\delta \\\\ \n\\delta sign(y_i-f(x_i))&|y_i-f(x_i)|> \\delta \n\\end{matrix}\\right.\n$$\n\n+ 分位数损失和负梯度误差如下所示。其中其中$\\theta$为分位数，需要我们在回归前指定。\n\n$$\nL(y,f(x))=\\sum _{y\\ge f(x)} \\theta|y-f(x)|+\\sum _{y<f(x)}(1-\\theta)|y-f(x)|\n$$\n\n$$\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\n\\theta &y_i\\ge f(x_i) \\\\ \n\\theta -1 &y_i<f(x_i)\n\\end{matrix}\\right.\n$$\n\n对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。\n\n对于分类算法，常用损失函数有指数损失函数和对数损失函数。\n\n+ 对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。\n\n\n+ 指数损失函数\n  $$\n  L(y,f(x))=exp(-yf(x))\n  $$\n\n\n### 5.GBDT正则化\n\n针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。\n\n+ **子采样比例:**通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。\n\n\n+ **定义步长v:**针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n  $$\n  f_k(x)=f_{k-1}(x)+vh_k(x)\n  $$\n\n\n### 6.Sklearn实现GBDT算法\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)。\n\n```python\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.datasets import make_regression\n\nX,y=make_regression(n_samples=1000,n_features=4,\n                    n_informative=2,random_state=0)\n\nprint(X[0:10],y[0:10])\n### X Number\n# [[-0.34323505  0.73129362  0.07077408 -0.78422138]\n#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]\n#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]\n#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]\n#  [-0.97240289  1.49613964  1.34622107 -1.49026539]\n#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]\n#  [ 0.77083696  0.96234174  0.24316822  0.45730965]\n#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]\n#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]\n#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]\n\n### Y Number\n# [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376\n#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]\n\n\nclf=GradientBoostingRegressor(n_estimators=150,learning_rate=0.6,\n                              max_depth=15,random_state=0,loss='ls')\nclf.fit(X,y)\n\nprint(clf.predict([[1,-1,-1,1]]))\n# [ 25.62761791]\nprint(clf.score(X,y))\n# 0.999999999987\n```\n\n### 7.GBDT优缺点\n\n#### 7.1优点\n\n+ 相对少的调参时间情况下可以得到较高的准确率。\n\n\n+ 可灵活处理各种类型数据，包括连续值和离散值，使用范围广。\n+ 可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。\n\n#### 7.2缺点\n\n+ 弱学习器之间存在依赖关系，难以并行训练数据。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n文章参考\n\n+ [刘建平Pinard_梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html#!comments)\n+ [taotick_GBDT梯度提升决策树](https://blog.csdn.net/taoqick/article/details/72822727)","source":"_posts/机器学习之梯度提升决策树-GBDT.md","raw":"---\ntitle: 机器学习之梯度提升决策树(GBDT)\ndate: 2018-04-30 23:15:14\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.GBDT算法简介\n\n**GBDT(Gradient Boosting Decision Tree)**是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(**Gradient Boosting Decision Tree**)来展开推导过程。决策树(**Decision Tree**)我们已经不再陌生，在之前介绍到的[机器学习之决策树(C4.5算法)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483837&idx=1&sn=f73ca53c5d50f7cd090ba3bc0e17c56b&chksm=fcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3#rd)、[机器学习之分类与回归树(CART)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483841&idx=1&sn=b67c59dc4284f0b363b2de881c5da729&chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd)、[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中已经多次接触，在此不再赘述。但**Boosting**和**Gradient**方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。\n\n#### 1.1集成学习之Boosting\n\n集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在[机器学习之随机森林](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483845&idx=1&sn=5484385408d694ba03a8bdc3a03c2263&chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd)中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。\n\n![机器学习之梯度提升决策树图片01](机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png)\n\n从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到**Boosting Decision Tree**。\n\n#### 1.2 Boosting Decision Tree\n\n**提升树(Boosting Decision Tree)**由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在[机器学习之分类与回归树(CART)](https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&mid=2247483841&idx=1&sn=b67c59dc4284f0b363b2de881c5da729&chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd)中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。\n\n我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。\n\n我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下\n\n![机器学习之梯度提升决策树图片02](机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png)\n\n我们能够直观的看到，预测值等于所有树值的累加，如**A的预测值=树1左节点(15)+树2左节点(-1)=14**。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下\n\n+ 初始化$f_0(x)=0$\n+ 对$t=1,2,3,…,T$\n  + 计算残差$r_{ti}=y_i-f_{t-1}(x_i),i=1,2,3,…,m$。\n  + 拟合残差$r_{ti}$学习得到回归树$h_t(x)$\n  + 更新$f_t(x)=f_{t-1}(x)+h_t(x)$\n+ 得到回归问题提升树$f_T(x)=f_0(x)+\\sum_{t=1}^{T}h_t(x)$\n\n我们介绍了**Boosting Decision Tree**的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到**Gradient Boosting Decision Tree的负梯度拟合**。\n\n#### 1.3GBDT负梯度拟合\n\nBoosting Decision Tree迭代过程中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$，我们本轮迭代的目标是找到一个回归树模型的弱学习器$h_t(x)$，让本轮的损失$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到的决策树，要让样本的损失函数尽量变得更小。\n\n我们利用损失函数$L(y_i,f(x_i))$的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为\n$$\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}\n$$\n利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一棵CART回归树，得到第t棵回归树，其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中$J$为叶子节点的个数。\n\n针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值$c_{tj}$。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。\n$$\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)\n$$\n这样我们便得到本轮的决策树拟合函数\n$$\nh_t(x)=\\sum _{j=1} ^{J} c_{tj},I(x \\in R_{tj})\n$$\n从而本轮最终得到的强学习器表达式如下\n$$\nf_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n$$\n通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。\n\n### 2.GBDT回归算法\n\n通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。\n\n假设训练集样本$T=\\{ (x,y_1),(x,y_2),…,(x,y_m)\\}$，最大迭代次数为$T$，损失函数$L$，输出是强学习器$f(x)$。回归算法过程如下所示\n\n+ 初始化弱学习器，c的均值可设置为样本y的均值。\n\n$$\nf_0(x)=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}L(y_i,c)\n$$\n\n+ 对迭代次数$t=1,2,3,…,T$有\n\n  + 对样本$i=1,2,3,…,m$，计算负梯度\n\n  $$\n  r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}\n  $$\n\n  + 利用$(x_i,r_{ti})i=1,2,3,…,m$，拟合一棵CART回归树，得到第t棵回归树，其对应的叶子节点区域为$R_{tj},j=1,2,3,…,J$。其中$J$为回归树$t$的叶子节点个数。\n  + 对叶子区域$j=1,2,3,…,J$，计算最佳拟合值\n\n  $$\n  c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)\n  $$\n\n  + 更新强学习器\n\n  $$\n  f_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n  $$\n\n+ 得到强学习器$f(x)$表达式\n  $$\n  f(x)=f_T(x)=f_0(x)+\\sum_{t=1}^{T}\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})\n  $$\n\n\n### 3.GBDT分类算法\n\nGBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。\n\n#### 3.1二元GBDT分类算法\n\n对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为\n$$\nL(y,f(x))=log(1+exp(-yf(x)))\n$$\n其中$y \\in \\{ -1,1\\}$。则此时的负梯度误差为\n$$\nr_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{t-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}\n$$\n对于生成的决策树，我们各个叶子节点的最佳残差拟合值为\n$$\nc_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum _{x_i\\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))\n$$\n由于上式比较难优化，我们一般使用近似值代替\n$$\nc_{tj}=\\frac{\\sum _{x_i\\in R_{tj}}r_{ti}}{\\sum _{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}\n$$\n除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。\n\n#### 3.2多元GBDT分类算法\n\n多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为\n$$\nL(y,f(x))=-\\sum_{k=1}^{K}y_k log(p_k(x))\n$$\n其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为\n$$\np_k(x)=\\frac {exp(f_k(x))}{\\sum _{l=1}^{K}exp(f_l(x))}\n$$\n集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为\n$$\nr_{til}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)\n$$\n其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为\n$$\nc_{tjl}=\\underset{cjl}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m} \\sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\\sum _{j=1}^{J}c_{jl},I(x_i\\in R_{tj})\n$$\n由于上式比较难优化，我们用近似值代替\n$$\nc_{tjl}=\\frac{K-1}{K}=\\frac{\\sum_{x_i\\in R_{tjl}}r_{til}}{\\sum _{x_i\\in R_{til}}|r_{til}|(1-|r_{til}|)}\n$$\n除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。\n\n### 4.GBDT损失函数\n\n对于**回归算法**，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。\n\n+ 均方差损失。\n\n$$\nL(y,f(x))=(y-f(x))^2\n$$\n\n+ 绝对损失和对应的负梯度误差。\n\n$$\nL(y,f(x))=|y-f(x)|\n$$\n\n$$\nsign(y_i-f(x_i))\n$$\n\n+ Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下\n\n$$\nL(y,f(x))=\\left\\{\\begin{matrix}\n\\frac{1}{2}(y-f(x))^2 &|y-f(x)|\\le \\delta \\\\ \n \\delta (|y-f(x)|-\\frac{\\delta}{2})&|y-f(x)|> \\delta \n\\end{matrix}\\right.\n$$\n\n$$\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\ny_i-f(x_i) &|y_i-f(x_i)|\\le \\delta \\\\ \n\\delta sign(y_i-f(x_i))&|y_i-f(x_i)|> \\delta \n\\end{matrix}\\right.\n$$\n\n+ 分位数损失和负梯度误差如下所示。其中其中$\\theta$为分位数，需要我们在回归前指定。\n\n$$\nL(y,f(x))=\\sum _{y\\ge f(x)} \\theta|y-f(x)|+\\sum _{y<f(x)}(1-\\theta)|y-f(x)|\n$$\n\n$$\nr(y_i,f(x_i))=\\left\\{\\begin{matrix}\n\\theta &y_i\\ge f(x_i) \\\\ \n\\theta -1 &y_i<f(x_i)\n\\end{matrix}\\right.\n$$\n\n对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。\n\n对于分类算法，常用损失函数有指数损失函数和对数损失函数。\n\n+ 对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。\n\n\n+ 指数损失函数\n  $$\n  L(y,f(x))=exp(-yf(x))\n  $$\n\n\n### 5.GBDT正则化\n\n针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。\n\n+ **子采样比例:**通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。\n\n\n+ **定义步长v:**针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n  $$\n  f_k(x)=f_{k-1}(x)+vh_k(x)\n  $$\n\n\n### 6.Sklearn实现GBDT算法\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)。\n\n```python\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.datasets import make_regression\n\nX,y=make_regression(n_samples=1000,n_features=4,\n                    n_informative=2,random_state=0)\n\nprint(X[0:10],y[0:10])\n### X Number\n# [[-0.34323505  0.73129362  0.07077408 -0.78422138]\n#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]\n#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]\n#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]\n#  [-0.97240289  1.49613964  1.34622107 -1.49026539]\n#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]\n#  [ 0.77083696  0.96234174  0.24316822  0.45730965]\n#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]\n#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]\n#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]\n\n### Y Number\n# [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376\n#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]\n\n\nclf=GradientBoostingRegressor(n_estimators=150,learning_rate=0.6,\n                              max_depth=15,random_state=0,loss='ls')\nclf.fit(X,y)\n\nprint(clf.predict([[1,-1,-1,1]]))\n# [ 25.62761791]\nprint(clf.score(X,y))\n# 0.999999999987\n```\n\n### 7.GBDT优缺点\n\n#### 7.1优点\n\n+ 相对少的调参时间情况下可以得到较高的准确率。\n\n\n+ 可灵活处理各种类型数据，包括连续值和离散值，使用范围广。\n+ 可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。\n\n#### 7.2缺点\n\n+ 弱学习器之间存在依赖关系，难以并行训练数据。\n\n### 8.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)\n\n文章参考\n\n+ [刘建平Pinard_梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html#!comments)\n+ [taotick_GBDT梯度提升决策树](https://blog.csdn.net/taoqick/article/details/72822727)","slug":"机器学习之梯度提升决策树-GBDT","published":1,"updated":"2018-05-02T08:10:56.595Z","layout":"post","photos":[],"link":"","_id":"cji4rdze3001c2e012vnymvzm","content":"<h3 id=\"1-GBDT算法简介\"><a href=\"#1-GBDT算法简介\" class=\"headerlink\" title=\"1.GBDT算法简介\"></a>1.GBDT算法简介</h3><p><strong>GBDT(Gradient Boosting Decision Tree)</strong>是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(<strong>Gradient Boosting Decision Tree</strong>)来展开推导过程。决策树(<strong>Decision Tree</strong>)我们已经不再陌生，在之前介绍到的<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483837&amp;idx=1&amp;sn=f73ca53c5d50f7cd090ba3bc0e17c56b&amp;chksm=fcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3#rd\" target=\"_blank\" rel=\"noopener\">机器学习之决策树(C4.5算法)</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483841&amp;idx=1&amp;sn=b67c59dc4284f0b363b2de881c5da729&amp;chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd\" target=\"_blank\" rel=\"noopener\">机器学习之分类与回归树(CART)</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中已经多次接触，在此不再赘述。但<strong>Boosting</strong>和<strong>Gradient</strong>方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。</p>\n<h4 id=\"1-1集成学习之Boosting\"><a href=\"#1-1集成学习之Boosting\" class=\"headerlink\" title=\"1.1集成学习之Boosting\"></a>1.1集成学习之Boosting</h4><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p>\n<p><img src=\"机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png\" alt=\"机器学习之梯度提升决策树图片01\"></p>\n<p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到<strong>Boosting Decision Tree</strong>。</p>\n<h4 id=\"1-2-Boosting-Decision-Tree\"><a href=\"#1-2-Boosting-Decision-Tree\" class=\"headerlink\" title=\"1.2 Boosting Decision Tree\"></a>1.2 Boosting Decision Tree</h4><p><strong>提升树(Boosting Decision Tree)</strong>由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483841&amp;idx=1&amp;sn=b67c59dc4284f0b363b2de881c5da729&amp;chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd\" target=\"_blank\" rel=\"noopener\">机器学习之分类与回归树(CART)</a>中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。</p>\n<p>我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。</p>\n<p>我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下</p>\n<p><img src=\"机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png\" alt=\"机器学习之梯度提升决策树图片02\"></p>\n<p>我们能够直观的看到，预测值等于所有树值的累加，如<strong>A的预测值=树1左节点(15)+树2左节点(-1)=14</strong>。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下</p>\n<ul>\n<li>初始化$f_0(x)=0$</li>\n<li>对$t=1,2,3,…,T$<ul>\n<li>计算残差$r_{ti}=y_i-f_{t-1}(x_i),i=1,2,3,…,m$。</li>\n<li>拟合残差$r_{ti}$学习得到回归树$h_t(x)$</li>\n<li>更新$f_t(x)=f_{t-1}(x)+h_t(x)$</li>\n</ul>\n</li>\n<li>得到回归问题提升树$f_T(x)=f_0(x)+\\sum_{t=1}^{T}h_t(x)$</li>\n</ul>\n<p>我们介绍了<strong>Boosting Decision Tree</strong>的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到<strong>Gradient Boosting Decision Tree的负梯度拟合</strong>。</p>\n<h4 id=\"1-3GBDT负梯度拟合\"><a href=\"#1-3GBDT负梯度拟合\" class=\"headerlink\" title=\"1.3GBDT负梯度拟合\"></a>1.3GBDT负梯度拟合</h4><p>Boosting Decision Tree迭代过程中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$，我们本轮迭代的目标是找到一个回归树模型的弱学习器$h_t(x)$，让本轮的损失$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到的决策树，要让样本的损失函数尽量变得更小。</p>\n<p>我们利用损失函数$L(y_i,f(x_i))$的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为<br>$$<br>r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]<em>{f(x)=f</em>{t-1}(x)}<br>$$<br>利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一棵CART回归树，得到第t棵回归树，其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中$J$为叶子节点的个数。</p>\n<p>针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值$c_{tj}$。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。<br>$$<br>c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)<br>$$<br>这样我们便得到本轮的决策树拟合函数<br>$$<br>h_t(x)=\\sum <em>{j=1} ^{J} c</em>{tj},I(x \\in R_{tj})<br>$$<br>从而本轮最终得到的强学习器表达式如下<br>$$<br>f_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})<br>$$<br>通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。</p>\n<h3 id=\"2-GBDT回归算法\"><a href=\"#2-GBDT回归算法\" class=\"headerlink\" title=\"2.GBDT回归算法\"></a>2.GBDT回归算法</h3><p>通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。</p>\n<p>假设训练集样本$T={ (x,y_1),(x,y_2),…,(x,y_m)}$，最大迭代次数为$T$，损失函数$L$，输出是强学习器$f(x)$。回归算法过程如下所示</p>\n<ul>\n<li>初始化弱学习器，c的均值可设置为样本y的均值。</li>\n</ul>\n<p>$$<br>f_0(x)=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}L(y_i,c)<br>$$</p>\n<ul>\n<li><p>对迭代次数$t=1,2,3,…,T$有</p>\n<ul>\n<li>对样本$i=1,2,3,…,m$，计算负梯度</li>\n</ul>\n<p>$$<br>r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]<em>{f(x)=f</em>{t-1}(x)}<br>$$</p>\n<ul>\n<li>利用$(x_i,r_{ti})i=1,2,3,…,m$，拟合一棵CART回归树，得到第t棵回归树，其对应的叶子节点区域为$R_{tj},j=1,2,3,…,J$。其中$J$为回归树$t$的叶子节点个数。</li>\n<li>对叶子区域$j=1,2,3,…,J$，计算最佳拟合值</li>\n</ul>\n<p>$$<br>c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)<br>$$</p>\n<ul>\n<li>更新强学习器</li>\n</ul>\n<p>$$<br>f_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})<br>$$</p>\n</li>\n<li><p>得到强学习器$f(x)$表达式<br>$$<br>f(x)=f_T(x)=f_0(x)+\\sum_{t=1}^{T}\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})<br>$$</p>\n</li>\n</ul>\n<h3 id=\"3-GBDT分类算法\"><a href=\"#3-GBDT分类算法\" class=\"headerlink\" title=\"3.GBDT分类算法\"></a>3.GBDT分类算法</h3><p>GBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。</p>\n<h4 id=\"3-1二元GBDT分类算法\"><a href=\"#3-1二元GBDT分类算法\" class=\"headerlink\" title=\"3.1二元GBDT分类算法\"></a>3.1二元GBDT分类算法</h4><p>对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为<br>$$<br>L(y,f(x))=log(1+exp(-yf(x)))<br>$$<br>其中$y \\in { -1,1}$。则此时的负梯度误差为<br>$$<br>r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]<em>{f(x)=f</em>{t-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}<br>$$<br>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为<br>$$<br>c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum _{x_i\\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))<br>$$<br>由于上式比较难优化，我们一般使用近似值代替<br>$$<br>c_{tj}=\\frac{\\sum _{x_i\\in R_{tj}}r_{ti}}{\\sum _{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}<br>$$<br>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。</p>\n<h4 id=\"3-2多元GBDT分类算法\"><a href=\"#3-2多元GBDT分类算法\" class=\"headerlink\" title=\"3.2多元GBDT分类算法\"></a>3.2多元GBDT分类算法</h4><p>多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为<br>$$<br>L(y,f(x))=-\\sum_{k=1}^{K}y_k log(p_k(x))<br>$$<br>其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为<br>$$<br>p_k(x)=\\frac {exp(f_k(x))}{\\sum _{l=1}^{K}exp(f_l(x))}<br>$$<br>集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为<br>$$<br>r_{til}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)<br>$$<br>其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为<br>$$<br>c_{tjl}=\\underset{cjl}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m} \\sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\\sum <em>{j=1}^{J}c</em>{jl},I(x_i\\in R_{tj})<br>$$<br>由于上式比较难优化，我们用近似值代替<br>$$<br>c_{tjl}=\\frac{K-1}{K}=\\frac{\\sum_{x_i\\in R_{tjl}}r_{til}}{\\sum _{x_i\\in R_{til}}|r_{til}|(1-|r_{til}|)}<br>$$<br>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p>\n<h3 id=\"4-GBDT损失函数\"><a href=\"#4-GBDT损失函数\" class=\"headerlink\" title=\"4.GBDT损失函数\"></a>4.GBDT损失函数</h3><p>对于<strong>回归算法</strong>，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。</p>\n<ul>\n<li>均方差损失。</li>\n</ul>\n<p>$$<br>L(y,f(x))=(y-f(x))^2<br>$$</p>\n<ul>\n<li>绝对损失和对应的负梯度误差。</li>\n</ul>\n<p>$$<br>L(y,f(x))=|y-f(x)|<br>$$</p>\n<p>$$<br>sign(y_i-f(x_i))<br>$$</p>\n<ul>\n<li>Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下</li>\n</ul>\n<p>$$<br>L(y,f(x))=\\left{\\begin{matrix}<br>\\frac{1}{2}(y-f(x))^2 &amp;|y-f(x)|\\le \\delta \\<br> \\delta (|y-f(x)|-\\frac{\\delta}{2})&amp;|y-f(x)|&gt; \\delta<br>\\end{matrix}\\right.<br>$$</p>\n<p>$$<br>r(y_i,f(x_i))=\\left{\\begin{matrix}<br>y_i-f(x_i) &amp;|y_i-f(x_i)|\\le \\delta \\<br>\\delta sign(y_i-f(x_i))&amp;|y_i-f(x_i)|&gt; \\delta<br>\\end{matrix}\\right.<br>$$</p>\n<ul>\n<li>分位数损失和负梯度误差如下所示。其中其中$\\theta$为分位数，需要我们在回归前指定。</li>\n</ul>\n<p>$$<br>L(y,f(x))=\\sum _{y\\ge f(x)} \\theta|y-f(x)|+\\sum _{y&lt;f(x)}(1-\\theta)|y-f(x)|<br>$$</p>\n<p>$$<br>r(y_i,f(x_i))=\\left{\\begin{matrix}<br>\\theta &amp;y_i\\ge f(x_i) \\<br>\\theta -1 &amp;y_i&lt;f(x_i)<br>\\end{matrix}\\right.<br>$$</p>\n<p>对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p>\n<p>对于分类算法，常用损失函数有指数损失函数和对数损失函数。</p>\n<ul>\n<li>对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。</li>\n</ul>\n<ul>\n<li>指数损失函数<br>$$<br>L(y,f(x))=exp(-yf(x))<br>$$</li>\n</ul>\n<h3 id=\"5-GBDT正则化\"><a href=\"#5-GBDT正则化\" class=\"headerlink\" title=\"5.GBDT正则化\"></a>5.GBDT正则化</h3><p>针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。</p>\n<ul>\n<li><strong>子采样比例:</strong>通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。</li>\n</ul>\n<ul>\n<li><strong>定义步长v:</strong>针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。<br>$$<br>f_k(x)=f_{k-1}(x)+vh_k(x)<br>$$</li>\n</ul>\n<h3 id=\"6-Sklearn实现GBDT算法\"><a href=\"#6-Sklearn实现GBDT算法\" class=\"headerlink\" title=\"6.Sklearn实现GBDT算法\"></a>6.Sklearn实现GBDT算法</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> GradientBoostingRegressor</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_regression</span><br><span class=\"line\"></span><br><span class=\"line\">X,y=make_regression(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                    n_informative=<span class=\"number\">2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(X[<span class=\"number\">0</span>:<span class=\"number\">10</span>],y[<span class=\"number\">0</span>:<span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"comment\">### X Number</span></span><br><span class=\"line\"><span class=\"comment\"># [[-0.34323505  0.73129362  0.07077408 -0.78422138]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.97240289  1.49613964  1.34622107 -1.49026539]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.77083696  0.96234174  0.24316822  0.45730965]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">### Y Number</span></span><br><span class=\"line\"><span class=\"comment\"># [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376</span></span><br><span class=\"line\"><span class=\"comment\">#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">clf=GradientBoostingRegressor(n_estimators=<span class=\"number\">150</span>,learning_rate=<span class=\"number\">0.6</span>,</span><br><span class=\"line\">                              max_depth=<span class=\"number\">15</span>,random_state=<span class=\"number\">0</span>,loss=<span class=\"string\">'ls'</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\">print(clf.predict([[<span class=\"number\">1</span>,<span class=\"number\">-1</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>]]))</span><br><span class=\"line\"><span class=\"comment\"># [ 25.62761791]</span></span><br><span class=\"line\">print(clf.score(X,y))</span><br><span class=\"line\"><span class=\"comment\"># 0.999999999987</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"7-GBDT优缺点\"><a href=\"#7-GBDT优缺点\" class=\"headerlink\" title=\"7.GBDT优缺点\"></a>7.GBDT优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li>相对少的调参时间情况下可以得到较高的准确率。</li>\n</ul>\n<ul>\n<li>可灵活处理各种类型数据，包括连续值和离散值，使用范围广。</li>\n<li>可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li>弱学习器之间存在依赖关系，难以并行训练数据。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>文章参考</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6140514.html#!comments\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_梯度提升树(GBDT)原理小结</a></li>\n<li><a href=\"https://blog.csdn.net/taoqick/article/details/72822727\" target=\"_blank\" rel=\"noopener\">taotick_GBDT梯度提升决策树</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-GBDT算法简介\"><a href=\"#1-GBDT算法简介\" class=\"headerlink\" title=\"1.GBDT算法简介\"></a>1.GBDT算法简介</h3><p><strong>GBDT(Gradient Boosting Decision Tree)</strong>是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(<strong>Gradient Boosting Decision Tree</strong>)来展开推导过程。决策树(<strong>Decision Tree</strong>)我们已经不再陌生，在之前介绍到的<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483837&amp;idx=1&amp;sn=f73ca53c5d50f7cd090ba3bc0e17c56b&amp;chksm=fcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3#rd\" target=\"_blank\" rel=\"noopener\">机器学习之决策树(C4.5算法)</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483841&amp;idx=1&amp;sn=b67c59dc4284f0b363b2de881c5da729&amp;chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd\" target=\"_blank\" rel=\"noopener\">机器学习之分类与回归树(CART)</a>、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中已经多次接触，在此不再赘述。但<strong>Boosting</strong>和<strong>Gradient</strong>方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。</p>\n<h4 id=\"1-1集成学习之Boosting\"><a href=\"#1-1集成学习之Boosting\" class=\"headerlink\" title=\"1.1集成学习之Boosting\"></a>1.1集成学习之Boosting</h4><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483845&amp;idx=1&amp;sn=5484385408d694ba03a8bdc3a03c2263&amp;chksm=fcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66#rd\" target=\"_blank\" rel=\"noopener\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p>\n<p><img src=\"机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png\" alt=\"机器学习之梯度提升决策树图片01\"></p>\n<p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到<strong>Boosting Decision Tree</strong>。</p>\n<h4 id=\"1-2-Boosting-Decision-Tree\"><a href=\"#1-2-Boosting-Decision-Tree\" class=\"headerlink\" title=\"1.2 Boosting Decision Tree\"></a>1.2 Boosting Decision Tree</h4><p><strong>提升树(Boosting Decision Tree)</strong>由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在<a href=\"https://mp.weixin.qq.com/s?__biz=MzU3MjA2NTQzMw==&amp;mid=2247483841&amp;idx=1&amp;sn=b67c59dc4284f0b363b2de881c5da729&amp;chksm=fcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0#rd\" target=\"_blank\" rel=\"noopener\">机器学习之分类与回归树(CART)</a>中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。</p>\n<p>我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。</p>\n<p>我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下</p>\n<p><img src=\"机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png\" alt=\"机器学习之梯度提升决策树图片02\"></p>\n<p>我们能够直观的看到，预测值等于所有树值的累加，如<strong>A的预测值=树1左节点(15)+树2左节点(-1)=14</strong>。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下</p>\n<ul>\n<li>初始化$f_0(x)=0$</li>\n<li>对$t=1,2,3,…,T$<ul>\n<li>计算残差$r_{ti}=y_i-f_{t-1}(x_i),i=1,2,3,…,m$。</li>\n<li>拟合残差$r_{ti}$学习得到回归树$h_t(x)$</li>\n<li>更新$f_t(x)=f_{t-1}(x)+h_t(x)$</li>\n</ul>\n</li>\n<li>得到回归问题提升树$f_T(x)=f_0(x)+\\sum_{t=1}^{T}h_t(x)$</li>\n</ul>\n<p>我们介绍了<strong>Boosting Decision Tree</strong>的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到<strong>Gradient Boosting Decision Tree的负梯度拟合</strong>。</p>\n<h4 id=\"1-3GBDT负梯度拟合\"><a href=\"#1-3GBDT负梯度拟合\" class=\"headerlink\" title=\"1.3GBDT负梯度拟合\"></a>1.3GBDT负梯度拟合</h4><p>Boosting Decision Tree迭代过程中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$，我们本轮迭代的目标是找到一个回归树模型的弱学习器$h_t(x)$，让本轮的损失$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到的决策树，要让样本的损失函数尽量变得更小。</p>\n<p>我们利用损失函数$L(y_i,f(x_i))$的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为<br>$$<br>r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]<em>{f(x)=f</em>{t-1}(x)}<br>$$<br>利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一棵CART回归树，得到第t棵回归树，其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中$J$为叶子节点的个数。</p>\n<p>针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值$c_{tj}$。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。<br>$$<br>c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)<br>$$<br>这样我们便得到本轮的决策树拟合函数<br>$$<br>h_t(x)=\\sum <em>{j=1} ^{J} c</em>{tj},I(x \\in R_{tj})<br>$$<br>从而本轮最终得到的强学习器表达式如下<br>$$<br>f_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})<br>$$<br>通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。</p>\n<h3 id=\"2-GBDT回归算法\"><a href=\"#2-GBDT回归算法\" class=\"headerlink\" title=\"2.GBDT回归算法\"></a>2.GBDT回归算法</h3><p>通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。</p>\n<p>假设训练集样本$T={ (x,y_1),(x,y_2),…,(x,y_m)}$，最大迭代次数为$T$，损失函数$L$，输出是强学习器$f(x)$。回归算法过程如下所示</p>\n<ul>\n<li>初始化弱学习器，c的均值可设置为样本y的均值。</li>\n</ul>\n<p>$$<br>f_0(x)=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m}L(y_i,c)<br>$$</p>\n<ul>\n<li><p>对迭代次数$t=1,2,3,…,T$有</p>\n<ul>\n<li>对样本$i=1,2,3,…,m$，计算负梯度</li>\n</ul>\n<p>$$<br>r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]<em>{f(x)=f</em>{t-1}(x)}<br>$$</p>\n<ul>\n<li>利用$(x_i,r_{ti})i=1,2,3,…,m$，拟合一棵CART回归树，得到第t棵回归树，其对应的叶子节点区域为$R_{tj},j=1,2,3,…,J$。其中$J$为回归树$t$的叶子节点个数。</li>\n<li>对叶子区域$j=1,2,3,…,J$，计算最佳拟合值</li>\n</ul>\n<p>$$<br>c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum_{x_i \\in R_{tj}}L(y_i,f_{t-1}(x_i)+c)<br>$$</p>\n<ul>\n<li>更新强学习器</li>\n</ul>\n<p>$$<br>f_t(x)=f_{t-1}(x)+\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})<br>$$</p>\n</li>\n<li><p>得到强学习器$f(x)$表达式<br>$$<br>f(x)=f_T(x)=f_0(x)+\\sum_{t=1}^{T}\\sum_{j=1}^{J}c_{tj},I(x\\in R_{tj})<br>$$</p>\n</li>\n</ul>\n<h3 id=\"3-GBDT分类算法\"><a href=\"#3-GBDT分类算法\" class=\"headerlink\" title=\"3.GBDT分类算法\"></a>3.GBDT分类算法</h3><p>GBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。</p>\n<h4 id=\"3-1二元GBDT分类算法\"><a href=\"#3-1二元GBDT分类算法\" class=\"headerlink\" title=\"3.1二元GBDT分类算法\"></a>3.1二元GBDT分类算法</h4><p>对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为<br>$$<br>L(y,f(x))=log(1+exp(-yf(x)))<br>$$<br>其中$y \\in { -1,1}$。则此时的负梯度误差为<br>$$<br>r_{ti}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]<em>{f(x)=f</em>{t-1}(x)}=\\frac{y_i}{1+exp(y_if(x_i))}<br>$$<br>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为<br>$$<br>c_{tj}=\\underset{c}{\\underbrace{\\arg\\min}} \\sum _{x_i\\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))<br>$$<br>由于上式比较难优化，我们一般使用近似值代替<br>$$<br>c_{tj}=\\frac{\\sum _{x_i\\in R_{tj}}r_{ti}}{\\sum _{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|)}<br>$$<br>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。</p>\n<h4 id=\"3-2多元GBDT分类算法\"><a href=\"#3-2多元GBDT分类算法\" class=\"headerlink\" title=\"3.2多元GBDT分类算法\"></a>3.2多元GBDT分类算法</h4><p>多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为<br>$$<br>L(y,f(x))=-\\sum_{k=1}^{K}y_k log(p_k(x))<br>$$<br>其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为<br>$$<br>p_k(x)=\\frac {exp(f_k(x))}{\\sum _{l=1}^{K}exp(f_l(x))}<br>$$<br>集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为<br>$$<br>r_{til}=-\\left[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)<br>$$<br>其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为<br>$$<br>c_{tjl}=\\underset{cjl}{\\underbrace{\\arg\\min}} \\sum_{i=1}^{m} \\sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\\sum <em>{j=1}^{J}c</em>{jl},I(x_i\\in R_{tj})<br>$$<br>由于上式比较难优化，我们用近似值代替<br>$$<br>c_{tjl}=\\frac{K-1}{K}=\\frac{\\sum_{x_i\\in R_{tjl}}r_{til}}{\\sum _{x_i\\in R_{til}}|r_{til}|(1-|r_{til}|)}<br>$$<br>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p>\n<h3 id=\"4-GBDT损失函数\"><a href=\"#4-GBDT损失函数\" class=\"headerlink\" title=\"4.GBDT损失函数\"></a>4.GBDT损失函数</h3><p>对于<strong>回归算法</strong>，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。</p>\n<ul>\n<li>均方差损失。</li>\n</ul>\n<p>$$<br>L(y,f(x))=(y-f(x))^2<br>$$</p>\n<ul>\n<li>绝对损失和对应的负梯度误差。</li>\n</ul>\n<p>$$<br>L(y,f(x))=|y-f(x)|<br>$$</p>\n<p>$$<br>sign(y_i-f(x_i))<br>$$</p>\n<ul>\n<li>Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下</li>\n</ul>\n<p>$$<br>L(y,f(x))=\\left{\\begin{matrix}<br>\\frac{1}{2}(y-f(x))^2 &amp;|y-f(x)|\\le \\delta \\<br> \\delta (|y-f(x)|-\\frac{\\delta}{2})&amp;|y-f(x)|&gt; \\delta<br>\\end{matrix}\\right.<br>$$</p>\n<p>$$<br>r(y_i,f(x_i))=\\left{\\begin{matrix}<br>y_i-f(x_i) &amp;|y_i-f(x_i)|\\le \\delta \\<br>\\delta sign(y_i-f(x_i))&amp;|y_i-f(x_i)|&gt; \\delta<br>\\end{matrix}\\right.<br>$$</p>\n<ul>\n<li>分位数损失和负梯度误差如下所示。其中其中$\\theta$为分位数，需要我们在回归前指定。</li>\n</ul>\n<p>$$<br>L(y,f(x))=\\sum _{y\\ge f(x)} \\theta|y-f(x)|+\\sum _{y&lt;f(x)}(1-\\theta)|y-f(x)|<br>$$</p>\n<p>$$<br>r(y_i,f(x_i))=\\left{\\begin{matrix}<br>\\theta &amp;y_i\\ge f(x_i) \\<br>\\theta -1 &amp;y_i&lt;f(x_i)<br>\\end{matrix}\\right.<br>$$</p>\n<p>对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p>\n<p>对于分类算法，常用损失函数有指数损失函数和对数损失函数。</p>\n<ul>\n<li>对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。</li>\n</ul>\n<ul>\n<li>指数损失函数<br>$$<br>L(y,f(x))=exp(-yf(x))<br>$$</li>\n</ul>\n<h3 id=\"5-GBDT正则化\"><a href=\"#5-GBDT正则化\" class=\"headerlink\" title=\"5.GBDT正则化\"></a>5.GBDT正则化</h3><p>针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。</p>\n<ul>\n<li><strong>子采样比例:</strong>通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。</li>\n</ul>\n<ul>\n<li><strong>定义步长v:</strong>针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。<br>$$<br>f_k(x)=f_{k-1}(x)+vh_k(x)<br>$$</li>\n</ul>\n<h3 id=\"6-Sklearn实现GBDT算法\"><a href=\"#6-Sklearn实现GBDT算法\" class=\"headerlink\" title=\"6.Sklearn实现GBDT算法\"></a>6.Sklearn实现GBDT算法</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> GradientBoostingRegressor</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_regression</span><br><span class=\"line\"></span><br><span class=\"line\">X,y=make_regression(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                    n_informative=<span class=\"number\">2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(X[<span class=\"number\">0</span>:<span class=\"number\">10</span>],y[<span class=\"number\">0</span>:<span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"comment\">### X Number</span></span><br><span class=\"line\"><span class=\"comment\"># [[-0.34323505  0.73129362  0.07077408 -0.78422138]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.97240289  1.49613964  1.34622107 -1.49026539]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.77083696  0.96234174  0.24316822  0.45730965]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">### Y Number</span></span><br><span class=\"line\"><span class=\"comment\"># [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376</span></span><br><span class=\"line\"><span class=\"comment\">#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">clf=GradientBoostingRegressor(n_estimators=<span class=\"number\">150</span>,learning_rate=<span class=\"number\">0.6</span>,</span><br><span class=\"line\">                              max_depth=<span class=\"number\">15</span>,random_state=<span class=\"number\">0</span>,loss=<span class=\"string\">'ls'</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\">print(clf.predict([[<span class=\"number\">1</span>,<span class=\"number\">-1</span>,<span class=\"number\">-1</span>,<span class=\"number\">1</span>]]))</span><br><span class=\"line\"><span class=\"comment\"># [ 25.62761791]</span></span><br><span class=\"line\">print(clf.score(X,y))</span><br><span class=\"line\"><span class=\"comment\"># 0.999999999987</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"7-GBDT优缺点\"><a href=\"#7-GBDT优缺点\" class=\"headerlink\" title=\"7.GBDT优缺点\"></a>7.GBDT优缺点</h3><h4 id=\"7-1优点\"><a href=\"#7-1优点\" class=\"headerlink\" title=\"7.1优点\"></a>7.1优点</h4><ul>\n<li>相对少的调参时间情况下可以得到较高的准确率。</li>\n</ul>\n<ul>\n<li>可灵活处理各种类型数据，包括连续值和离散值，使用范围广。</li>\n<li>可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。</li>\n</ul>\n<h4 id=\"7-2缺点\"><a href=\"#7-2缺点\" class=\"headerlink\" title=\"7.2缺点\"></a>7.2缺点</h4><ul>\n<li>弱学习器之间存在依赖关系，难以并行训练数据。</li>\n</ul>\n<h3 id=\"8-推广\"><a href=\"#8-推广\" class=\"headerlink\" title=\"8.推广\"></a>8.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n<p>文章参考</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/6140514.html#!comments\" target=\"_blank\" rel=\"noopener\">刘建平Pinard_梯度提升树(GBDT)原理小结</a></li>\n<li><a href=\"https://blog.csdn.net/taoqick/article/details/72822727\" target=\"_blank\" rel=\"noopener\">taotick_GBDT梯度提升决策树</a></li>\n</ul>\n"},{"title":"机器学习之随机森林","date":"2018-04-30T05:38:54.000Z","mathjax":true,"comments":1,"_content":"\n### 1.随机森林简介\n\n随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于**分类**和**回归**问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中**森林**和**随机**的概念。\n\n#### 1.1集成学习\n\n**集成学习**是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。\n\n单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了**森林**。\n\n#### 1.2随机决策树\n\n我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。\n\n那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n<N)，然后从所有特征中选出k个特征生成单个随机决策树，这便是随机森林中**随机**的概念。\n\n#### 1.3随机森林算法\n\n由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？\n\n好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示\n\n+ 从样本集N中有放回随机采样选出n个样本。\n+ 从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。\n+ 重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。\n+ 对于新数据，经过每棵决策树投票分类。\n\n![机器学习之随机森林图片01](机器学习之随机森林/机器学习之随机森林图片01.png)\n\n### 2.CART算法\n\n随机森林包含众多决策树，能够用于**分类**和**回归**问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。\n\n#### 2.1CART分类树算法推导\n\nCART分类树预测离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为\n$$\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\n$$\n根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。\n$$\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2\n$$\n如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n$$\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)\n$$\n对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。\n$$\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))\n$$\n\n$$\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))\n$$\n\n#### 2.2CART分类树实例详解\n\n|  名称  | 体温 | 胎生 | 水生 | 类标记 |\n| :----: | :--: | :--: | :--: | :----: |\n|   人   | 恒温 |  是  |  否  | 哺乳类 |\n|  巨蟒  | 冷血 |  否  |  否  | 爬行类 |\n|  鲑鱼  | 冷血 |  否  |  是  |  鱼类  |\n|   鲸   | 恒温 |  是  |  是  | 哺乳类 |\n|   蛙   | 冷血 |  否  | 有时 |  鱼类  |\n|  巨蜥  | 冷血 |  否  |  否  | 爬行类 |\n|  蝙蝠  | 恒温 |  是  |  否  | 哺乳类 |\n|   猫   | 恒温 |  是  |  否  | 哺乳类 |\n| 豹纹鲨 | 冷血 |  是  |  是  |  鱼类  |\n|  海龟  | 冷血 |  否  | 有时 | 爬行类 |\n|  豪猪  | 恒温 |  是  |  否  | 哺乳类 |\n|   鳗   | 冷血 |  否  |  是  |  鱼类  |\n|  蝾螈  | 冷血 |  否  | 有时 | 两栖类 |\n\n针对上述离散型数据，按照**体温为恒温和非恒温**进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。\n$$\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}\n$$\n\n$$\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}\n$$\n\n然后计算得到特征**体温**下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。\n$$\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}\n$$\n\n#### 2.3CART回归树算法详解\n\nCART回归树预测连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n$$\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}\n$$\n**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。\n$$\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]\n$$\n**用选定的(j,s)对，划分区域并决定相应的输出值**\n$$\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}\n$$\n\n$$\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i\n$$\n\n$$\nx\\epsilon R_m,m=1,2\n$$\n\n**继续对两个子区域调用上述步骤**，**将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。**\n$$\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)\n$$\n当输入空间划分确定时，可以用**平方误差**来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。\n$$\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2\n$$\n\n#### 2.4CART回归树实例详解\n\n| $x_i$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |\n| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |\n\n考虑如上所示的连续性变量，根据给定的数据点，考虑**1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5**切分点。对各切分点依次求出**R1,R2,c1,c2及m(s)**，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。\n$$\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56\n$$\n\n$$\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50\n$$\n\n$$\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72\n$$\n\n**依次改变(j,s)对，可以得到s及m(s)的计算结果**，如下表所示。\n\n|  $s$   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |\n| :----: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |\n| $m(s)$ | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |\n\n当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。**回归树T1(x)**为\n$$\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}\n$$\n\n$$\nf_1(x)=T_1(x)\n$$\n\n**然后我们利用f1(x)拟合训练数据的残差**，如下表所示\n\n| $x_i$ |   1   |   2   |   3   |  4   |  5   |  6   |   7   |   8   |  9   |  10  |\n| :---: | :---: | :---: | :---: | :--: | :--: | :--: | :---: | :---: | :--: | :--: |\n| $y_i$ | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |\n\n**用f1(x)拟合训练数据得到平方误差**\n$$\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93\n$$\n第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到\n$$\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}\n$$\n\n$$\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}\n$$\n\n用f2(x)拟合训练数据的平方误差\n$$\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79\n$$\n继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示\n$$\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47\n$$\n\n$$\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30\n$$\n\n$$\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23\n$$\n\n$$\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}\n$$\n\n$$\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}\n$$\n\n用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。\n$$\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71\n$$\n\n### 3.Sklearn实现随机森林\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)。\n\n```python \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nX,y=make_classification(n_samples=1000,n_features=4,\n                        n_informative=2,n_redundant=0,\n                        random_state=0,shuffle=0)\nprint(X[:10],y[:10])\n#  X\n# [[-1.66853167 -1.29901346  0.2746472  -0.60362044]\n#  [-2.9728827  -1.08878294  0.70885958  0.42281857]\n#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]\n#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]\n#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]\n#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]\n#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]\n#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]\n#  [-1.13449871 -1.27403448  0.74355352  0.21035937]\n#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]\n\n#  y\n# [0 0 0 0 0 0 0 0 0 0]\n\nclf=RandomForestClassifier(max_depth=2,random_state=0)\nclf.fit(X,y)\nprint(clf.feature_importances_)\n# [ 0.17287856  0.80608704  0.01884792  0.00218648]\nprint(clf.predict([[0,0,0,0]]))\n# [1]\n```\n\n### 4.随机森林优缺点\n\n#### 4.1优点\n\n- 决策树选择部分样本及部分特征，一定程度上避免过拟合。\n- 决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。\n- 能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。\n- 对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。\n- 训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。\n- 随机森林有oob，不需要单独划分交叉验证集。\n\n#### 4.2缺点\n\n+ 可能有很多相似决策树，掩盖真实结果。\n+ 对小数据或低维数据可能不能产生很好分类。\n+ 产生众多决策树，算法较慢。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/机器学习之随机森林.md","raw":"---\ntitle: 机器学习之随机森林\ndate: 2018-04-30 13:38:54\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.随机森林简介\n\n随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于**分类**和**回归**问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中**森林**和**随机**的概念。\n\n#### 1.1集成学习\n\n**集成学习**是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。\n\n单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了**森林**。\n\n#### 1.2随机决策树\n\n我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。\n\n那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n<N)，然后从所有特征中选出k个特征生成单个随机决策树，这便是随机森林中**随机**的概念。\n\n#### 1.3随机森林算法\n\n由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？\n\n好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示\n\n+ 从样本集N中有放回随机采样选出n个样本。\n+ 从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。\n+ 重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。\n+ 对于新数据，经过每棵决策树投票分类。\n\n![机器学习之随机森林图片01](机器学习之随机森林/机器学习之随机森林图片01.png)\n\n### 2.CART算法\n\n随机森林包含众多决策树，能够用于**分类**和**回归**问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。\n\n#### 2.1CART分类树算法推导\n\nCART分类树预测离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为\n$$\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\n$$\n根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。\n$$\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2\n$$\n如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n$$\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)\n$$\n对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。\n$$\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))\n$$\n\n$$\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))\n$$\n\n#### 2.2CART分类树实例详解\n\n|  名称  | 体温 | 胎生 | 水生 | 类标记 |\n| :----: | :--: | :--: | :--: | :----: |\n|   人   | 恒温 |  是  |  否  | 哺乳类 |\n|  巨蟒  | 冷血 |  否  |  否  | 爬行类 |\n|  鲑鱼  | 冷血 |  否  |  是  |  鱼类  |\n|   鲸   | 恒温 |  是  |  是  | 哺乳类 |\n|   蛙   | 冷血 |  否  | 有时 |  鱼类  |\n|  巨蜥  | 冷血 |  否  |  否  | 爬行类 |\n|  蝙蝠  | 恒温 |  是  |  否  | 哺乳类 |\n|   猫   | 恒温 |  是  |  否  | 哺乳类 |\n| 豹纹鲨 | 冷血 |  是  |  是  |  鱼类  |\n|  海龟  | 冷血 |  否  | 有时 | 爬行类 |\n|  豪猪  | 恒温 |  是  |  否  | 哺乳类 |\n|   鳗   | 冷血 |  否  |  是  |  鱼类  |\n|  蝾螈  | 冷血 |  否  | 有时 | 两栖类 |\n\n针对上述离散型数据，按照**体温为恒温和非恒温**进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。\n$$\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}\n$$\n\n$$\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}\n$$\n\n然后计算得到特征**体温**下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。\n$$\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}\n$$\n\n#### 2.3CART回归树算法详解\n\nCART回归树预测连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n$$\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}\n$$\n**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。\n$$\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]\n$$\n**用选定的(j,s)对，划分区域并决定相应的输出值**\n$$\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}\n$$\n\n$$\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i\n$$\n\n$$\nx\\epsilon R_m,m=1,2\n$$\n\n**继续对两个子区域调用上述步骤**，**将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。**\n$$\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)\n$$\n当输入空间划分确定时，可以用**平方误差**来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。\n$$\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2\n$$\n\n#### 2.4CART回归树实例详解\n\n| $x_i$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |\n| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |\n\n考虑如上所示的连续性变量，根据给定的数据点，考虑**1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5**切分点。对各切分点依次求出**R1,R2,c1,c2及m(s)**，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。\n$$\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56\n$$\n\n$$\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50\n$$\n\n$$\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72\n$$\n\n**依次改变(j,s)对，可以得到s及m(s)的计算结果**，如下表所示。\n\n|  $s$   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |\n| :----: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |\n| $m(s)$ | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |\n\n当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。**回归树T1(x)**为\n$$\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}\n$$\n\n$$\nf_1(x)=T_1(x)\n$$\n\n**然后我们利用f1(x)拟合训练数据的残差**，如下表所示\n\n| $x_i$ |   1   |   2   |   3   |  4   |  5   |  6   |   7   |   8   |  9   |  10  |\n| :---: | :---: | :---: | :---: | :--: | :--: | :--: | :---: | :---: | :--: | :--: |\n| $y_i$ | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |\n\n**用f1(x)拟合训练数据得到平方误差**\n$$\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93\n$$\n第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到\n$$\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}\n$$\n\n$$\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}\n$$\n\n用f2(x)拟合训练数据的平方误差\n$$\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79\n$$\n继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示\n$$\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47\n$$\n\n$$\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30\n$$\n\n$$\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23\n$$\n\n$$\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}\n$$\n\n$$\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}\n$$\n\n用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。\n$$\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71\n$$\n\n### 3.Sklearn实现随机森林\n\n我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考[sklearn官方教程](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)。\n\n```python \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nX,y=make_classification(n_samples=1000,n_features=4,\n                        n_informative=2,n_redundant=0,\n                        random_state=0,shuffle=0)\nprint(X[:10],y[:10])\n#  X\n# [[-1.66853167 -1.29901346  0.2746472  -0.60362044]\n#  [-2.9728827  -1.08878294  0.70885958  0.42281857]\n#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]\n#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]\n#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]\n#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]\n#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]\n#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]\n#  [-1.13449871 -1.27403448  0.74355352  0.21035937]\n#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]\n\n#  y\n# [0 0 0 0 0 0 0 0 0 0]\n\nclf=RandomForestClassifier(max_depth=2,random_state=0)\nclf.fit(X,y)\nprint(clf.feature_importances_)\n# [ 0.17287856  0.80608704  0.01884792  0.00218648]\nprint(clf.predict([[0,0,0,0]]))\n# [1]\n```\n\n### 4.随机森林优缺点\n\n#### 4.1优点\n\n- 决策树选择部分样本及部分特征，一定程度上避免过拟合。\n- 决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。\n- 能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。\n- 对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。\n- 训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。\n- 随机森林有oob，不需要单独划分交叉验证集。\n\n#### 4.2缺点\n\n+ 可能有很多相似决策树，掩盖真实结果。\n+ 对小数据或低维数据可能不能产生很好分类。\n+ 产生众多决策树，算法较慢。\n\n### 5.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"机器学习之随机森林","published":1,"updated":"2018-04-30T09:11:49.847Z","layout":"post","photos":[],"link":"","_id":"cji4rdze5001f2e01pgkkqok2","content":"<h3 id=\"1-随机森林简介\"><a href=\"#1-随机森林简介\" class=\"headerlink\" title=\"1.随机森林简介\"></a>1.随机森林简介</h3><p>随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于<strong>分类</strong>和<strong>回归</strong>问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中<strong>森林</strong>和<strong>随机</strong>的概念。</p>\n<h4 id=\"1-1集成学习\"><a href=\"#1-1集成学习\" class=\"headerlink\" title=\"1.1集成学习\"></a>1.1集成学习</h4><p><strong>集成学习</strong>是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。</p>\n<p>单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了<strong>森林</strong>。</p>\n<h4 id=\"1-2随机决策树\"><a href=\"#1-2随机决策树\" class=\"headerlink\" title=\"1.2随机决策树\"></a>1.2随机决策树</h4><p>我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。</p>\n<p>那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n&lt;N)，然后从所有特征中选出k个特征生成单个随机决策树，这便是随机森林中<strong>随机</strong>的概念。</p>\n<h4 id=\"1-3随机森林算法\"><a href=\"#1-3随机森林算法\" class=\"headerlink\" title=\"1.3随机森林算法\"></a>1.3随机森林算法</h4><p>由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？</p>\n<p>好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示</p>\n<ul>\n<li>从样本集N中有放回随机采样选出n个样本。</li>\n<li>从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。</li>\n<li>重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。</li>\n<li>对于新数据，经过每棵决策树投票分类。</li>\n</ul>\n<p><img src=\"机器学习之随机森林/机器学习之随机森林图片01.png\" alt=\"机器学习之随机森林图片01\"></p>\n<h3 id=\"2-CART算法\"><a href=\"#2-CART算法\" class=\"headerlink\" title=\"2.CART算法\"></a>2.CART算法</h3><p>随机森林包含众多决策树，能够用于<strong>分类</strong>和<strong>回归</strong>问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。</p>\n<h4 id=\"2-1CART分类树算法推导\"><a href=\"#2-1CART分类树算法推导\" class=\"headerlink\" title=\"2.1CART分类树算法推导\"></a>2.1CART分类树算法推导</h4><p>CART分类树预测离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为<br>$$<br>Gini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}<br>$$<br>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。<br>$$<br>Gini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2<br>$$<br>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。<br>$$<br>Gain_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)<br>$$<br>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。<br>$$<br>\\min_{i\\epsilon A}(Gain_Gini(D,A))<br>$$</p>\n<p>$$<br>\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain_Gini(D,A)))<br>$$</p>\n<h4 id=\"2-2CART分类树实例详解\"><a href=\"#2-2CART分类树实例详解\" class=\"headerlink\" title=\"2.2CART分类树实例详解\"></a>2.2CART分类树实例详解</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">名称</th>\n<th style=\"text-align:center\">体温</th>\n<th style=\"text-align:center\">胎生</th>\n<th style=\"text-align:center\">水生</th>\n<th style=\"text-align:center\">类标记</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">人</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蟒</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲑鱼</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲸</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蛙</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蜥</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝙蝠</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">猫</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豹纹鲨</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">海龟</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豪猪</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鳗</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝾螈</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">两栖类</td>\n</tr>\n</tbody>\n</table>\n<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。<br>$$<br>Gini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}<br>$$</p>\n<p>$$<br>Gini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}<br>$$</p>\n<p>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。<br>$$<br>Gain_Gini(D,体温)=\\frac{7}{15}<em>\\frac{20}{49}+\\frac{8}{15}</em>\\frac{42}{64}<br>$$</p>\n<h4 id=\"2-3CART回归树算法详解\"><a href=\"#2-3CART回归树算法详解\" class=\"headerlink\" title=\"2.3CART回归树算法详解\"></a>2.3CART回归树算法详解</h4><p>CART回归树预测连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。<br>$$<br>D={(x_1,y_1),(x_2,y_2),(x_3,y_3),…(x_n,y_n)}<br>$$<br><strong>选择最优切分变量j与切分点s</strong>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。<br>$$<br>\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]<br>$$<br><strong>用选定的(j,s)对，划分区域并决定相应的输出值</strong><br>$$<br>R_1(j,s)={x|x^{(j)}\\le s},R_2(j,s)={x|x^{(j)} &gt;  s}<br>$$</p>\n<p>$$<br>\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i<br>$$</p>\n<p>$$<br>x\\epsilon R_m,m=1,2<br>$$</p>\n<p><strong>继续对两个子区域调用上述步骤</strong>，<strong>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</strong><br>$$<br>f(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)<br>$$<br>当输入空间划分确定时，可以用<strong>平方误差</strong>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。<br>$$<br>\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2<br>$$</p>\n<h4 id=\"2-4CART回归树实例详解\"><a href=\"#2-4CART回归树实例详解\" class=\"headerlink\" title=\"2.4CART回归树实例详解\"></a>2.4CART回归树实例详解</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">5.56</td>\n<td style=\"text-align:center\">5.70</td>\n<td style=\"text-align:center\">5.91</td>\n<td style=\"text-align:center\">6.40</td>\n<td style=\"text-align:center\">6.80</td>\n<td style=\"text-align:center\">7.05</td>\n<td style=\"text-align:center\">8.90</td>\n<td style=\"text-align:center\">8.70</td>\n<td style=\"text-align:center\">9.00</td>\n<td style=\"text-align:center\">9.05</td>\n</tr>\n</tbody>\n</table>\n<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。<br>$$<br>c_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56<br>$$</p>\n<p>$$<br>c_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+…+9.05)=7.50<br>$$</p>\n<p>$$<br>m(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72<br>$$</p>\n<p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$s$</th>\n<th style=\"text-align:center\">1.5</th>\n<th style=\"text-align:center\">2.5</th>\n<th style=\"text-align:center\">3.5</th>\n<th style=\"text-align:center\">4.5</th>\n<th style=\"text-align:center\">5.5</th>\n<th style=\"text-align:center\">6.5</th>\n<th style=\"text-align:center\">7.5</th>\n<th style=\"text-align:center\">8.5</th>\n<th style=\"text-align:center\">9.5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$m(s)$</td>\n<td style=\"text-align:center\">15.72</td>\n<td style=\"text-align:center\">12.07</td>\n<td style=\"text-align:center\">8.36</td>\n<td style=\"text-align:center\">5.78</td>\n<td style=\"text-align:center\">3.91</td>\n<td style=\"text-align:center\">1.93</td>\n<td style=\"text-align:center\">8.01</td>\n<td style=\"text-align:center\">11.73</td>\n<td style=\"text-align:center\">15.74</td>\n</tr>\n</tbody>\n</table>\n<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为<br>$$<br>T_1(x)=\\begin{cases}<br> &amp; 6.24,x&lt;6.5 \\<br> &amp; 8.91,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_1(x)=T_1(x)<br>$$</p>\n<p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>，如下表所示</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">-0.68</td>\n<td style=\"text-align:center\">-0.54</td>\n<td style=\"text-align:center\">-0.33</td>\n<td style=\"text-align:center\">0.16</td>\n<td style=\"text-align:center\">0.56</td>\n<td style=\"text-align:center\">0.81</td>\n<td style=\"text-align:center\">-0.01</td>\n<td style=\"text-align:center\">-0.21</td>\n<td style=\"text-align:center\">0.09</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n</tbody>\n</table>\n<p><strong>用f1(x)拟合训练数据得到平方误差</strong><br>$$<br>L(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93<br>$$<br>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到<br>$$<br>T_2(x)=\\begin{cases}<br> &amp; -0.52,x&lt;3.5 \\<br> &amp; 0.22,x\\ge 3.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_2(x)=f_1(x)+T_2(x)=<br>\\begin{cases}<br> &amp; 5.72,x&lt;3.5 \\<br> &amp; 6.46,3.5\\le x \\le 6.5 \\<br> &amp; 9.13,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>用f2(x)拟合训练数据的平方误差<br>$$<br>L(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79<br>$$<br>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示<br>$$<br>T_3(x)=\\begin{cases}<br> &amp; 0.15,x&lt;6.5 \\<br> &amp; -0.22,x\\ge 6.5<br>\\end{cases}<br>L(y,f_3(x))=0.47<br>$$</p>\n<p>$$<br>T_4(x)=\\begin{cases}<br> &amp; -0.16,x&lt;4.5 \\<br> &amp; 0.11,x\\ge 4.5<br>\\end{cases}<br>L(y,f_4(x))=0.30<br>$$</p>\n<p>$$<br>T_5(x)=\\begin{cases}<br> &amp; 0.07,x&lt;6.5 \\<br> &amp; -0.11,x\\ge 6.5<br>\\end{cases}<br>L(y,f_5(x))=0.23<br>$$</p>\n<p>$$<br>T_6(x)=\\begin{cases}<br> &amp; -0.15,x&lt;2.5 \\<br> &amp; 0.04,x\\ge 2.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_6(x)=f_5(x)+T_6(x)=T_1(x)+…+T_6(x)=<br>\\begin{cases}<br> &amp; 5.63,x&lt;2.5 \\<br> &amp; 5.82,2.5\\le x \\le 3.5 \\<br>  &amp; 6.56,3.5\\le x \\le 4.5 \\<br> &amp; 6.83,4.5\\le x \\le 6.5 \\<br>  &amp; 8.95,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。<br>$$<br>L(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71<br>$$</p>\n<h3 id=\"3-Sklearn实现随机森林\"><a href=\"#3-Sklearn实现随机森林\" class=\"headerlink\" title=\"3.Sklearn实现随机森林\"></a>3.Sklearn实现随机森林</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"></span><br><span class=\"line\">X,y=make_classification(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                        n_informative=<span class=\"number\">2</span>,n_redundant=<span class=\"number\">0</span>,</span><br><span class=\"line\">                        random_state=<span class=\"number\">0</span>,shuffle=<span class=\"number\">0</span>)</span><br><span class=\"line\">print(X[:<span class=\"number\">10</span>],y[:<span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"comment\">#  X</span></span><br><span class=\"line\"><span class=\"comment\"># [[-1.66853167 -1.29901346  0.2746472  -0.60362044]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-2.9728827  -1.08878294  0.70885958  0.42281857]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.13449871 -1.27403448  0.74355352  0.21035937]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#  y</span></span><br><span class=\"line\"><span class=\"comment\"># [0 0 0 0 0 0 0 0 0 0]</span></span><br><span class=\"line\"></span><br><span class=\"line\">clf=RandomForestClassifier(max_depth=<span class=\"number\">2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\">print(clf.feature_importances_)</span><br><span class=\"line\"><span class=\"comment\"># [ 0.17287856  0.80608704  0.01884792  0.00218648]</span></span><br><span class=\"line\">print(clf.predict([[<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]]))</span><br><span class=\"line\"><span class=\"comment\"># [1]</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-随机森林优缺点\"><a href=\"#4-随机森林优缺点\" class=\"headerlink\" title=\"4.随机森林优缺点\"></a>4.随机森林优缺点</h3><h4 id=\"4-1优点\"><a href=\"#4-1优点\" class=\"headerlink\" title=\"4.1优点\"></a>4.1优点</h4><ul>\n<li>决策树选择部分样本及部分特征，一定程度上避免过拟合。</li>\n<li>决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。</li>\n<li>能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。</li>\n<li>对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。</li>\n<li>训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。</li>\n<li>随机森林有oob，不需要单独划分交叉验证集。</li>\n</ul>\n<h4 id=\"4-2缺点\"><a href=\"#4-2缺点\" class=\"headerlink\" title=\"4.2缺点\"></a>4.2缺点</h4><ul>\n<li>可能有很多相似决策树，掩盖真实结果。</li>\n<li>对小数据或低维数据可能不能产生很好分类。</li>\n<li>产生众多决策树，算法较慢。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-随机森林简介\"><a href=\"#1-随机森林简介\" class=\"headerlink\" title=\"1.随机森林简介\"></a>1.随机森林简介</h3><p>随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于<strong>分类</strong>和<strong>回归</strong>问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中<strong>森林</strong>和<strong>随机</strong>的概念。</p>\n<h4 id=\"1-1集成学习\"><a href=\"#1-1集成学习\" class=\"headerlink\" title=\"1.1集成学习\"></a>1.1集成学习</h4><p><strong>集成学习</strong>是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。</p>\n<p>单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了<strong>森林</strong>。</p>\n<h4 id=\"1-2随机决策树\"><a href=\"#1-2随机决策树\" class=\"headerlink\" title=\"1.2随机决策树\"></a>1.2随机决策树</h4><p>我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。</p>\n<p>那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n&lt;N)，然后从所有特征中选出k个特征生成单个随机决策树，这便是随机森林中<strong>随机</strong>的概念。</p>\n<h4 id=\"1-3随机森林算法\"><a href=\"#1-3随机森林算法\" class=\"headerlink\" title=\"1.3随机森林算法\"></a>1.3随机森林算法</h4><p>由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？</p>\n<p>好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示</p>\n<ul>\n<li>从样本集N中有放回随机采样选出n个样本。</li>\n<li>从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。</li>\n<li>重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。</li>\n<li>对于新数据，经过每棵决策树投票分类。</li>\n</ul>\n<p><img src=\"机器学习之随机森林/机器学习之随机森林图片01.png\" alt=\"机器学习之随机森林图片01\"></p>\n<h3 id=\"2-CART算法\"><a href=\"#2-CART算法\" class=\"headerlink\" title=\"2.CART算法\"></a>2.CART算法</h3><p>随机森林包含众多决策树，能够用于<strong>分类</strong>和<strong>回归</strong>问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。</p>\n<h4 id=\"2-1CART分类树算法推导\"><a href=\"#2-1CART分类树算法推导\" class=\"headerlink\" title=\"2.1CART分类树算法推导\"></a>2.1CART分类树算法推导</h4><p>CART分类树预测离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为<br>$$<br>Gini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}<br>$$<br>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。<br>$$<br>Gini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2<br>$$<br>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。<br>$$<br>Gain_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)<br>$$<br>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。<br>$$<br>\\min_{i\\epsilon A}(Gain_Gini(D,A))<br>$$</p>\n<p>$$<br>\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain_Gini(D,A)))<br>$$</p>\n<h4 id=\"2-2CART分类树实例详解\"><a href=\"#2-2CART分类树实例详解\" class=\"headerlink\" title=\"2.2CART分类树实例详解\"></a>2.2CART分类树实例详解</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">名称</th>\n<th style=\"text-align:center\">体温</th>\n<th style=\"text-align:center\">胎生</th>\n<th style=\"text-align:center\">水生</th>\n<th style=\"text-align:center\">类标记</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">人</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蟒</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲑鱼</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲸</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蛙</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蜥</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝙蝠</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">猫</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豹纹鲨</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">海龟</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豪猪</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鳗</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝾螈</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">两栖类</td>\n</tr>\n</tbody>\n</table>\n<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。<br>$$<br>Gini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}<br>$$</p>\n<p>$$<br>Gini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}<br>$$</p>\n<p>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。<br>$$<br>Gain_Gini(D,体温)=\\frac{7}{15}<em>\\frac{20}{49}+\\frac{8}{15}</em>\\frac{42}{64}<br>$$</p>\n<h4 id=\"2-3CART回归树算法详解\"><a href=\"#2-3CART回归树算法详解\" class=\"headerlink\" title=\"2.3CART回归树算法详解\"></a>2.3CART回归树算法详解</h4><p>CART回归树预测连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。<br>$$<br>D={(x_1,y_1),(x_2,y_2),(x_3,y_3),…(x_n,y_n)}<br>$$<br><strong>选择最优切分变量j与切分点s</strong>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。<br>$$<br>\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]<br>$$<br><strong>用选定的(j,s)对，划分区域并决定相应的输出值</strong><br>$$<br>R_1(j,s)={x|x^{(j)}\\le s},R_2(j,s)={x|x^{(j)} &gt;  s}<br>$$</p>\n<p>$$<br>\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i<br>$$</p>\n<p>$$<br>x\\epsilon R_m,m=1,2<br>$$</p>\n<p><strong>继续对两个子区域调用上述步骤</strong>，<strong>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</strong><br>$$<br>f(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)<br>$$<br>当输入空间划分确定时，可以用<strong>平方误差</strong>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。<br>$$<br>\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2<br>$$</p>\n<h4 id=\"2-4CART回归树实例详解\"><a href=\"#2-4CART回归树实例详解\" class=\"headerlink\" title=\"2.4CART回归树实例详解\"></a>2.4CART回归树实例详解</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">5.56</td>\n<td style=\"text-align:center\">5.70</td>\n<td style=\"text-align:center\">5.91</td>\n<td style=\"text-align:center\">6.40</td>\n<td style=\"text-align:center\">6.80</td>\n<td style=\"text-align:center\">7.05</td>\n<td style=\"text-align:center\">8.90</td>\n<td style=\"text-align:center\">8.70</td>\n<td style=\"text-align:center\">9.00</td>\n<td style=\"text-align:center\">9.05</td>\n</tr>\n</tbody>\n</table>\n<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。<br>$$<br>c_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56<br>$$</p>\n<p>$$<br>c_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+…+9.05)=7.50<br>$$</p>\n<p>$$<br>m(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72<br>$$</p>\n<p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$s$</th>\n<th style=\"text-align:center\">1.5</th>\n<th style=\"text-align:center\">2.5</th>\n<th style=\"text-align:center\">3.5</th>\n<th style=\"text-align:center\">4.5</th>\n<th style=\"text-align:center\">5.5</th>\n<th style=\"text-align:center\">6.5</th>\n<th style=\"text-align:center\">7.5</th>\n<th style=\"text-align:center\">8.5</th>\n<th style=\"text-align:center\">9.5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$m(s)$</td>\n<td style=\"text-align:center\">15.72</td>\n<td style=\"text-align:center\">12.07</td>\n<td style=\"text-align:center\">8.36</td>\n<td style=\"text-align:center\">5.78</td>\n<td style=\"text-align:center\">3.91</td>\n<td style=\"text-align:center\">1.93</td>\n<td style=\"text-align:center\">8.01</td>\n<td style=\"text-align:center\">11.73</td>\n<td style=\"text-align:center\">15.74</td>\n</tr>\n</tbody>\n</table>\n<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为<br>$$<br>T_1(x)=\\begin{cases}<br> &amp; 6.24,x&lt;6.5 \\<br> &amp; 8.91,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_1(x)=T_1(x)<br>$$</p>\n<p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>，如下表所示</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">-0.68</td>\n<td style=\"text-align:center\">-0.54</td>\n<td style=\"text-align:center\">-0.33</td>\n<td style=\"text-align:center\">0.16</td>\n<td style=\"text-align:center\">0.56</td>\n<td style=\"text-align:center\">0.81</td>\n<td style=\"text-align:center\">-0.01</td>\n<td style=\"text-align:center\">-0.21</td>\n<td style=\"text-align:center\">0.09</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n</tbody>\n</table>\n<p><strong>用f1(x)拟合训练数据得到平方误差</strong><br>$$<br>L(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93<br>$$<br>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到<br>$$<br>T_2(x)=\\begin{cases}<br> &amp; -0.52,x&lt;3.5 \\<br> &amp; 0.22,x\\ge 3.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_2(x)=f_1(x)+T_2(x)=<br>\\begin{cases}<br> &amp; 5.72,x&lt;3.5 \\<br> &amp; 6.46,3.5\\le x \\le 6.5 \\<br> &amp; 9.13,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>用f2(x)拟合训练数据的平方误差<br>$$<br>L(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79<br>$$<br>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示<br>$$<br>T_3(x)=\\begin{cases}<br> &amp; 0.15,x&lt;6.5 \\<br> &amp; -0.22,x\\ge 6.5<br>\\end{cases}<br>L(y,f_3(x))=0.47<br>$$</p>\n<p>$$<br>T_4(x)=\\begin{cases}<br> &amp; -0.16,x&lt;4.5 \\<br> &amp; 0.11,x\\ge 4.5<br>\\end{cases}<br>L(y,f_4(x))=0.30<br>$$</p>\n<p>$$<br>T_5(x)=\\begin{cases}<br> &amp; 0.07,x&lt;6.5 \\<br> &amp; -0.11,x\\ge 6.5<br>\\end{cases}<br>L(y,f_5(x))=0.23<br>$$</p>\n<p>$$<br>T_6(x)=\\begin{cases}<br> &amp; -0.15,x&lt;2.5 \\<br> &amp; 0.04,x\\ge 2.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_6(x)=f_5(x)+T_6(x)=T_1(x)+…+T_6(x)=<br>\\begin{cases}<br> &amp; 5.63,x&lt;2.5 \\<br> &amp; 5.82,2.5\\le x \\le 3.5 \\<br>  &amp; 6.56,3.5\\le x \\le 4.5 \\<br> &amp; 6.83,4.5\\le x \\le 6.5 \\<br>  &amp; 8.95,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。<br>$$<br>L(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71<br>$$</p>\n<h3 id=\"3-Sklearn实现随机森林\"><a href=\"#3-Sklearn实现随机森林\" class=\"headerlink\" title=\"3.Sklearn实现随机森林\"></a>3.Sklearn实现随机森林</h3><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\" rel=\"noopener\">sklearn官方教程</a>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"></span><br><span class=\"line\">X,y=make_classification(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                        n_informative=<span class=\"number\">2</span>,n_redundant=<span class=\"number\">0</span>,</span><br><span class=\"line\">                        random_state=<span class=\"number\">0</span>,shuffle=<span class=\"number\">0</span>)</span><br><span class=\"line\">print(X[:<span class=\"number\">10</span>],y[:<span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"comment\">#  X</span></span><br><span class=\"line\"><span class=\"comment\"># [[-1.66853167 -1.29901346  0.2746472  -0.60362044]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-2.9728827  -1.08878294  0.70885958  0.42281857]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-1.13449871 -1.27403448  0.74355352  0.21035937]</span></span><br><span class=\"line\"><span class=\"comment\">#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#  y</span></span><br><span class=\"line\"><span class=\"comment\"># [0 0 0 0 0 0 0 0 0 0]</span></span><br><span class=\"line\"></span><br><span class=\"line\">clf=RandomForestClassifier(max_depth=<span class=\"number\">2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">clf.fit(X,y)</span><br><span class=\"line\">print(clf.feature_importances_)</span><br><span class=\"line\"><span class=\"comment\"># [ 0.17287856  0.80608704  0.01884792  0.00218648]</span></span><br><span class=\"line\">print(clf.predict([[<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]]))</span><br><span class=\"line\"><span class=\"comment\"># [1]</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-随机森林优缺点\"><a href=\"#4-随机森林优缺点\" class=\"headerlink\" title=\"4.随机森林优缺点\"></a>4.随机森林优缺点</h3><h4 id=\"4-1优点\"><a href=\"#4-1优点\" class=\"headerlink\" title=\"4.1优点\"></a>4.1优点</h4><ul>\n<li>决策树选择部分样本及部分特征，一定程度上避免过拟合。</li>\n<li>决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。</li>\n<li>能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。</li>\n<li>对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。</li>\n<li>训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。</li>\n<li>随机森林有oob，不需要单独划分交叉验证集。</li>\n</ul>\n<h4 id=\"4-2缺点\"><a href=\"#4-2缺点\" class=\"headerlink\" title=\"4.2缺点\"></a>4.2缺点</h4><ul>\n<li>可能有很多相似决策树，掩盖真实结果。</li>\n<li>对小数据或低维数据可能不能产生很好分类。</li>\n<li>产生众多决策树，算法较慢。</li>\n</ul>\n<h3 id=\"5-推广\"><a href=\"#5-推广\" class=\"headerlink\" title=\"5.推广\"></a>5.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"},{"title":"机器学习知识体系","date":"2018-03-24T04:30:14.000Z","comments":1,"toc":true,"_content":"\n### 1.什么是机器学习\n\n>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n\n上述为**百度百科**定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。\n\n+ 给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。\n\n![图片01](机器学习知识体系/图片01.png)\n\n\n\n+ 以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。\n\n![图片02](机器学习知识体系/图片02.png)\n\n### 2.机器学习体系概括\n\n机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。\n\n![图片03](机器学习知识体系/图片03.png)\n\n下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。\n\n![图片04](机器学习知识体系/图片04.png)\n\n机器学习算法中常用到的便是**监督学习**和**无监督学习**，监督学习包含**回归**和**分类**两方面，无监督学习为**聚类**。\n\n**监督学习（Supervised Learning）**\n\n当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为**回归分析（Regression Analysis）**和**分类（Classification）**两类。\n\n+ **回归分析（Regression Analysis）**：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。\n+ **分类（Classfication）**：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。\n\n**无监督学习（Unsupervised Learning）**\n\n我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。\n\n所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。\n\n### 3.如何开始学习\n\n开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。\n\n很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。\n\n---\n\n### 4.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](机器学习知识体系/推广.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/机器学习知识体系.md","raw":"---\ntitle: 机器学习知识体系\ndate: 2018-03-24 12:30:14\ntags: 机器学习\ncategories: 机器学习\ncomments: true\ntoc: true\n---\n\n### 1.什么是机器学习\n\n>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n\n上述为**百度百科**定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。\n\n+ 给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。\n\n![图片01](机器学习知识体系/图片01.png)\n\n\n\n+ 以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。\n\n![图片02](机器学习知识体系/图片02.png)\n\n### 2.机器学习体系概括\n\n机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。\n\n![图片03](机器学习知识体系/图片03.png)\n\n下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。\n\n![图片04](机器学习知识体系/图片04.png)\n\n机器学习算法中常用到的便是**监督学习**和**无监督学习**，监督学习包含**回归**和**分类**两方面，无监督学习为**聚类**。\n\n**监督学习（Supervised Learning）**\n\n当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为**回归分析（Regression Analysis）**和**分类（Classification）**两类。\n\n+ **回归分析（Regression Analysis）**：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。\n+ **分类（Classfication）**：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。\n\n**无监督学习（Unsupervised Learning）**\n\n我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。\n\n所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。\n\n### 3.如何开始学习\n\n开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。\n\n很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。\n\n---\n\n### 4.推广\n\n更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](机器学习知识体系/推广.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"机器学习知识体系","published":1,"updated":"2018-03-29T07:24:34.332Z","layout":"post","photos":[],"link":"","_id":"cji4rdze6001i2e017e8ta02f","content":"<h3 id=\"1-什么是机器学习\"><a href=\"#1-什么是机器学习\" class=\"headerlink\" title=\"1.什么是机器学习\"></a>1.什么是机器学习</h3><blockquote>\n<p>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。</p>\n</blockquote>\n<p>上述为<strong>百度百科</strong>定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。</p>\n<ul>\n<li>给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。</li>\n</ul>\n<p><img src=\"机器学习知识体系/图片01.png\" alt=\"图片01\"></p>\n<ul>\n<li>以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。</li>\n</ul>\n<p><img src=\"机器学习知识体系/图片02.png\" alt=\"图片02\"></p>\n<h3 id=\"2-机器学习体系概括\"><a href=\"#2-机器学习体系概括\" class=\"headerlink\" title=\"2.机器学习体系概括\"></a>2.机器学习体系概括</h3><p>机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。</p>\n<p><img src=\"机器学习知识体系/图片03.png\" alt=\"图片03\"></p>\n<p>下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。</p>\n<p><img src=\"机器学习知识体系/图片04.png\" alt=\"图片04\"></p>\n<p>机器学习算法中常用到的便是<strong>监督学习</strong>和<strong>无监督学习</strong>，监督学习包含<strong>回归</strong>和<strong>分类</strong>两方面，无监督学习为<strong>聚类</strong>。</p>\n<p><strong>监督学习（Supervised Learning）</strong></p>\n<p>当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为<strong>回归分析（Regression Analysis）</strong>和<strong>分类（Classification）</strong>两类。</p>\n<ul>\n<li><strong>回归分析（Regression Analysis）</strong>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。</li>\n<li><strong>分类（Classfication）</strong>：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。</li>\n</ul>\n<p><strong>无监督学习（Unsupervised Learning）</strong></p>\n<p>我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。</p>\n<p>所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。</p>\n<h3 id=\"3-如何开始学习\"><a href=\"#3-如何开始学习\" class=\"headerlink\" title=\"3.如何开始学习\"></a>3.如何开始学习</h3><p>开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。</p>\n<p>很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。</p>\n<hr>\n<h3 id=\"4-推广\"><a href=\"#4-推广\" class=\"headerlink\" title=\"4.推广\"></a>4.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"机器学习知识体系/推广.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-什么是机器学习\"><a href=\"#1-什么是机器学习\" class=\"headerlink\" title=\"1.什么是机器学习\"></a>1.什么是机器学习</h3><blockquote>\n<p>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。</p>\n</blockquote>\n<p>上述为<strong>百度百科</strong>定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。</p>\n<ul>\n<li>给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。</li>\n</ul>\n<p><img src=\"机器学习知识体系/图片01.png\" alt=\"图片01\"></p>\n<ul>\n<li>以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。</li>\n</ul>\n<p><img src=\"机器学习知识体系/图片02.png\" alt=\"图片02\"></p>\n<h3 id=\"2-机器学习体系概括\"><a href=\"#2-机器学习体系概括\" class=\"headerlink\" title=\"2.机器学习体系概括\"></a>2.机器学习体系概括</h3><p>机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。</p>\n<p><img src=\"机器学习知识体系/图片03.png\" alt=\"图片03\"></p>\n<p>下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。</p>\n<p><img src=\"机器学习知识体系/图片04.png\" alt=\"图片04\"></p>\n<p>机器学习算法中常用到的便是<strong>监督学习</strong>和<strong>无监督学习</strong>，监督学习包含<strong>回归</strong>和<strong>分类</strong>两方面，无监督学习为<strong>聚类</strong>。</p>\n<p><strong>监督学习（Supervised Learning）</strong></p>\n<p>当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为<strong>回归分析（Regression Analysis）</strong>和<strong>分类（Classification）</strong>两类。</p>\n<ul>\n<li><strong>回归分析（Regression Analysis）</strong>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。</li>\n<li><strong>分类（Classfication）</strong>：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。</li>\n</ul>\n<p><strong>无监督学习（Unsupervised Learning）</strong></p>\n<p>我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。</p>\n<p>所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。</p>\n<h3 id=\"3-如何开始学习\"><a href=\"#3-如何开始学习\" class=\"headerlink\" title=\"3.如何开始学习\"></a>3.如何开始学习</h3><p>开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。</p>\n<p>很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。</p>\n<hr>\n<h3 id=\"4-推广\"><a href=\"#4-推广\" class=\"headerlink\" title=\"4.推广\"></a>4.推广</h3><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"机器学习知识体系/推广.png\" alt=\"推广\"></p>\n"},{"title":"面向知乎的个性化推荐模型研究","date":"2018-03-12T05:18:48.000Z","toc":true,"comments":1,"_content":"### 面向知乎的个性化推荐模型研究\n《[面向知乎的个性化推荐模型研究](https://github.com/XiaoYiii/Paper/blob/master/Paper/%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6.pdf)》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。\n<div align=center>![论文图片01](http://img.blog.csdn.net/20180311003539161?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n<div align=center>![论文图片02](http://img.blog.csdn.net/20180311003612378?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n<div align=center>![论文图片03](http://img.blog.csdn.net/20180311003633906?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n<div align=center>![论文图片04](http://img.blog.csdn.net/20180311003651675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n<div align=center>![论文图片05](http://img.blog.csdn.net/20180311003709174?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...\n<div align=center>![公众号](http://img.blog.csdn.net/20180311001720557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n","source":"_posts/面向知乎的个性化推荐模型研究论文.md","raw":"---\ntitle: 面向知乎的个性化推荐模型研究\ndate: 2018-03-12 13:18:48\ntags: 推荐系统\ntoc: true\ncategories: 推荐系统\ncomments: true\n---\n### 面向知乎的个性化推荐模型研究\n《[面向知乎的个性化推荐模型研究](https://github.com/XiaoYiii/Paper/blob/master/Paper/%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6.pdf)》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。\n<div align=center>![论文图片01](http://img.blog.csdn.net/20180311003539161?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n<div align=center>![论文图片02](http://img.blog.csdn.net/20180311003612378?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n<div align=center>![论文图片03](http://img.blog.csdn.net/20180311003633906?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n<div align=center>![论文图片04](http://img.blog.csdn.net/20180311003651675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n<div align=center>![论文图片05](http://img.blog.csdn.net/20180311003709174?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n----------\n更多内容请关注公众号'谓之小一'，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...\n<div align=center>![公众号](http://img.blog.csdn.net/20180311001720557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n","slug":"面向知乎的个性化推荐模型研究论文","published":1,"updated":"2018-03-14T03:17:08.367Z","layout":"post","photos":[],"link":"","_id":"cji4rdze8001m2e01aoi33hcs","content":"<h3 id=\"面向知乎的个性化推荐模型研究\"><a href=\"#面向知乎的个性化推荐模型研究\" class=\"headerlink\" title=\"面向知乎的个性化推荐模型研究\"></a>面向知乎的个性化推荐模型研究</h3><p>《<a href=\"https://github.com/XiaoYiii/Paper/blob/master/Paper/%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6.pdf\" target=\"_blank\" rel=\"noopener\">面向知乎的个性化推荐模型研究</a>》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。</p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003539161?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片01\"></div></p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003612378?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片02\"></div></p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003633906?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片03\"></div></p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003651675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片04\"></div></p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003709174?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片05\"></div></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…</p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311001720557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"公众号\"></div></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"面向知乎的个性化推荐模型研究\"><a href=\"#面向知乎的个性化推荐模型研究\" class=\"headerlink\" title=\"面向知乎的个性化推荐模型研究\"></a>面向知乎的个性化推荐模型研究</h3><p>《<a href=\"https://github.com/XiaoYiii/Paper/blob/master/Paper/%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6.pdf\" target=\"_blank\" rel=\"noopener\">面向知乎的个性化推荐模型研究</a>》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。</p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003539161?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片01\"></div></p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003612378?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片02\"></div></p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003633906?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片03\"></div></p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003651675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片04\"></div></p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311003709174?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"论文图片05\"></div></p>\n<hr>\n<p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…</p>\n<p><div align=\"center\"><img src=\"http://img.blog.csdn.net/20180311001720557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1lpX0VyaWM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" alt=\"公众号\"></div></p>\n"},{"title":"机器学习之分类与回归树(CART)","date":"2018-04-22T13:58:53.000Z","mathjax":true,"comments":1,"_content":"\n### 1.分类与回归树简介\n\n分类与回归树的英文是*Classfication And Regression Tree*，缩写为CART。CART算法采用**二分递归分割的技术**将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为**True**和**False**，左分支取值为**True**，右分支取值为**False**，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。\n\n+ 如果待预测分类是离散型数据，则CART生成分类决策树。\n+ 如果待预测分类是连续性数据，则CART生成回归决策树。\n\n### 2.CART分类树\n\n#### 2.1算法详解\n\nCART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为\n\n$$\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\n$$\n根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。\n$$\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2\n$$\n如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n$$\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)\n$$\n对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。\n$$\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))\n$$\n$$\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))\n$$\n#### 2.2实例详解\n\n|  名称  | 体温 | 胎生 | 水生 | 类标记 |\n| :----: | :--: | :--: | :--: | :----: |\n|   人   | 恒温 |  是  |  否  | 哺乳类 |\n|  巨蟒  | 冷血 |  否  |  否  | 爬行类 |\n|  鲑鱼  | 冷血 |  否  |  是  |  鱼类  |\n|   鲸   | 恒温 |  是  |  是  | 哺乳类 |\n|   蛙   | 冷血 |  否  | 有时 |  鱼类  |\n|  巨蜥  | 冷血 |  否  |  否  | 爬行类 |\n|  蝙蝠  | 恒温 |  是  |  否  | 哺乳类 |\n|   猫   | 恒温 |  是  |  否  | 哺乳类 |\n| 豹纹鲨 | 冷血 |  是  |  是  |  鱼类  |\n|  海龟  | 冷血 |  否  | 有时 | 爬行类 |\n|  豪猪  | 恒温 |  是  |  否  | 哺乳类 |\n|   鳗   | 冷血 |  否  |  是  |  鱼类  |\n|  蝾螈  | 冷血 |  否  | 有时 | 两栖类 |\n\n针对上述离散型数据，按照**体温为恒温和非恒温**进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。\n\n$$\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}\n$$\n$$\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}\n$$\n然后计算得到特征**体温**下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。\n$$\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}\n$$\n### 3.CART回归树\n\n#### 3.1算法详解\n\nCART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n$$\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}\n$$\n**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。\n\n$$\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]\n$$\n**用选定的(j,s)对，划分区域并决定相应的输出值**\n$$\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}\n$$\n\n$$\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i\n$$\n\n$$\nx\\epsilon R_m,m=1,2\n$$\n\n**继续对两个子区域调用上述步骤**，**将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。**\n\n$$\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)\n$$\n当输入空间划分确定时，可以用**平方误差**来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。\n$$\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2\n$$\n\n#### 3.2实例详解\n\n| $x_i$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |\n| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |\n\n考虑如上所示的连续性变量，根据给定的数据点，考虑**1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5**切分点。对各切分点依次求出**R1,R2,c1,c2及m(s)**，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。\n\n$$\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56\n$$\n\n$$\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50\n$$\n\n$$\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72\n$$\n\n**依次改变(j,s)对，可以得到s及m(s)的计算结果**，如下表所示。\n\n|  $s$   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |\n| :----: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |\n| $m(s)$ | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |\n\n当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。**回归树T1(x)**为\n$$\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}\n$$\n\n$$\nf_1(x)=T_1(x)\n$$\n\n**然后我们利用f1(x)拟合训练数据的残差**，如下表所示\n\n| $x_i$ |   1   |   2   |   3   |  4   |  5   |  6   |   7   |   8   |  9   |  10  |\n| :---: | :---: | :---: | :---: | :--: | :--: | :--: | :---: | :---: | :--: | :--: |\n| $y_i$ | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |\n\n**用f1(x)拟合训练数据得到平方误差**\n$$\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93\n$$\n第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到\n$$\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}\n$$\n\n$$\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}\n$$\n\n用f2(x)拟合训练数据的平方误差\n$$\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79\n$$\n继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示\n$$\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47\n$$\n\n$$\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30\n$$\n\n$$\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23\n$$\n\n$$\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}\n$$\n\n$$\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}\n$$\n\n用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。\n$$\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71\n$$\n### 4.CART剪枝\n\n> 此处我们介绍代价复杂度剪枝算法\n\n我们将一颗充分生长的树称为**T0** ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下\n$$\nC_\\alpha(T)=C(T)+\\alpha|T|\n$$\n+ T为任意子树，|T|为子树T的叶子节点个数。\n+ α是参数，权衡拟合程度与树的复杂度。\n+ C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。\n\n**那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？**准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。\n\n+ 当α很小的时候，T0 是这样的最优子树.\n+ 当α很大的时候，单独一个根节点就是最优子树。\n\n尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行**交叉验证**，找到最优的那个子树作为我们的决策树。子树序列如下\n$$\nT_0>T_1>T_2>T_3>...>T_n\n$$\n**因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，**在此不再详细介绍。\n\n### 5.Sklearn实现\n\n我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\n#load data\niris=load_iris()\nX=iris.data\ny=iris.target\nclf=tree.DecisionTreeClassifier()\nclf=clf.fit(X,y)\n\n#export the decision tree\nimport graphviz\n#export_graphviz support a variety of aesthetic options\ndot_data=tree.export_graphviz(clf,out_file=None,\n                              feature_names=iris.feature_names,\n                              class_names=iris.target_names,\n                              filled=True,rounded=True,\n                              special_characters=True)\n\ngraph=graphviz.Source(dot_data)\ngraph.view()\n```\n\n![机器学习之分类与回归树CART图片01](机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png)\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","source":"_posts/机器学习之分类与回归树-CART.md","raw":"---\ntitle: 机器学习之分类与回归树(CART)\ndate: 2018-04-22 21:58:53\ntags: [机器学习,算法]\ncategories: 机器学习\nmathjax: true\ncomments: true\n---\n\n### 1.分类与回归树简介\n\n分类与回归树的英文是*Classfication And Regression Tree*，缩写为CART。CART算法采用**二分递归分割的技术**将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为**True**和**False**，左分支取值为**True**，右分支取值为**False**，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。\n\n+ 如果待预测分类是离散型数据，则CART生成分类决策树。\n+ 如果待预测分类是连续性数据，则CART生成回归决策树。\n\n### 2.CART分类树\n\n#### 2.1算法详解\n\nCART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为\n\n$$\nGini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\n$$\n根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。\n$$\nGini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2\n$$\n如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n$$\nGain\\_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)\n$$\n对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。\n$$\n\\min_{i\\epsilon A}(Gain\\_Gini(D,A))\n$$\n$$\n\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain\\_Gini(D,A)))\n$$\n#### 2.2实例详解\n\n|  名称  | 体温 | 胎生 | 水生 | 类标记 |\n| :----: | :--: | :--: | :--: | :----: |\n|   人   | 恒温 |  是  |  否  | 哺乳类 |\n|  巨蟒  | 冷血 |  否  |  否  | 爬行类 |\n|  鲑鱼  | 冷血 |  否  |  是  |  鱼类  |\n|   鲸   | 恒温 |  是  |  是  | 哺乳类 |\n|   蛙   | 冷血 |  否  | 有时 |  鱼类  |\n|  巨蜥  | 冷血 |  否  |  否  | 爬行类 |\n|  蝙蝠  | 恒温 |  是  |  否  | 哺乳类 |\n|   猫   | 恒温 |  是  |  否  | 哺乳类 |\n| 豹纹鲨 | 冷血 |  是  |  是  |  鱼类  |\n|  海龟  | 冷血 |  否  | 有时 | 爬行类 |\n|  豪猪  | 恒温 |  是  |  否  | 哺乳类 |\n|   鳗   | 冷血 |  否  |  是  |  鱼类  |\n|  蝾螈  | 冷血 |  否  | 有时 | 两栖类 |\n\n针对上述离散型数据，按照**体温为恒温和非恒温**进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。\n\n$$\nGini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}\n$$\n$$\nGini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}\n$$\n然后计算得到特征**体温**下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。\n$$\nGain\\_Gini(D,体温)=\\frac{7}{15}*\\frac{20}{49}+\\frac{8}{15}*\\frac{42}{64}\n$$\n### 3.CART回归树\n\n#### 3.1算法详解\n\nCART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n$$\nD=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...(x_n,y_n)\\}\n$$\n**选择最优切分变量j与切分点s**：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。\n\n$$\n\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]\n$$\n**用选定的(j,s)对，划分区域并决定相应的输出值**\n$$\nR_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)} >  s\\}\n$$\n\n$$\n\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i\n$$\n\n$$\nx\\epsilon R_m,m=1,2\n$$\n\n**继续对两个子区域调用上述步骤**，**将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。**\n\n$$\nf(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)\n$$\n当输入空间划分确定时，可以用**平方误差**来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。\n$$\n\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2\n$$\n\n#### 3.2实例详解\n\n| $x_i$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |\n| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |\n\n考虑如上所示的连续性变量，根据给定的数据点，考虑**1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5**切分点。对各切分点依次求出**R1,R2,c1,c2及m(s)**，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。\n\n$$\nc_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56\n$$\n\n$$\nc_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50\n$$\n\n$$\nm(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72\n$$\n\n**依次改变(j,s)对，可以得到s及m(s)的计算结果**，如下表所示。\n\n|  $s$   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |\n| :----: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |\n| $m(s)$ | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |\n\n当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。**回归树T1(x)**为\n$$\nT_1(x)=\\begin{cases}\n & 6.24,x<6.5 \\\\ \n & 8.91,x\\ge 6.5\n\\end{cases}\n$$\n\n$$\nf_1(x)=T_1(x)\n$$\n\n**然后我们利用f1(x)拟合训练数据的残差**，如下表所示\n\n| $x_i$ |   1   |   2   |   3   |  4   |  5   |  6   |   7   |   8   |  9   |  10  |\n| :---: | :---: | :---: | :---: | :--: | :--: | :--: | :---: | :---: | :--: | :--: |\n| $y_i$ | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |\n\n**用f1(x)拟合训练数据得到平方误差**\n$$\nL(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93\n$$\n第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到\n$$\nT_2(x)=\\begin{cases}\n & -0.52,x<3.5 \\\\ \n & 0.22,x\\ge 3.5\n\\end{cases}\n$$\n\n$$\nf_2(x)=f_1(x)+T_2(x)=\n\\begin{cases}\n & 5.72,x<3.5 \\\\ \n & 6.46,3.5\\le x \\le 6.5 \\\\\n & 9.13,x\\ge 6.5\n\\end{cases}\n$$\n\n用f2(x)拟合训练数据的平方误差\n$$\nL(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79\n$$\n继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示\n$$\nT_3(x)=\\begin{cases}\n & 0.15,x<6.5 \\\\ \n & -0.22,x\\ge 6.5\n\\end{cases}\nL(y,f_3(x))=0.47\n$$\n\n$$\nT_4(x)=\\begin{cases}\n & -0.16,x<4.5 \\\\ \n & 0.11,x\\ge 4.5\n\\end{cases}\nL(y,f_4(x))=0.30\n$$\n\n$$\nT_5(x)=\\begin{cases}\n & 0.07,x<6.5 \\\\ \n & -0.11,x\\ge 6.5\n\\end{cases}\nL(y,f_5(x))=0.23\n$$\n\n$$\nT_6(x)=\\begin{cases}\n & -0.15,x<2.5 \\\\ \n & 0.04,x\\ge 2.5\n\\end{cases}\n$$\n\n$$\nf_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=\n\\begin{cases}\n & 5.63,x<2.5 \\\\ \n & 5.82,2.5\\le x \\le 3.5 \\\\\n  & 6.56,3.5\\le x \\le 4.5 \\\\\n & 6.83,4.5\\le x \\le 6.5 \\\\\n  & 8.95,x\\ge 6.5 \n\\end{cases}\n$$\n\n用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。\n$$\nL(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71\n$$\n### 4.CART剪枝\n\n> 此处我们介绍代价复杂度剪枝算法\n\n我们将一颗充分生长的树称为**T0** ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下\n$$\nC_\\alpha(T)=C(T)+\\alpha|T|\n$$\n+ T为任意子树，|T|为子树T的叶子节点个数。\n+ α是参数，权衡拟合程度与树的复杂度。\n+ C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。\n\n**那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？**准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。\n\n+ 当α很小的时候，T0 是这样的最优子树.\n+ 当α很大的时候，单独一个根节点就是最优子树。\n\n尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行**交叉验证**，找到最优的那个子树作为我们的决策树。子树序列如下\n$$\nT_0>T_1>T_2>T_3>...>T_n\n$$\n**因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，**在此不再详细介绍。\n\n### 5.Sklearn实现\n\n我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\n#load data\niris=load_iris()\nX=iris.data\ny=iris.target\nclf=tree.DecisionTreeClassifier()\nclf=clf.fit(X,y)\n\n#export the decision tree\nimport graphviz\n#export_graphviz support a variety of aesthetic options\ndot_data=tree.export_graphviz(clf,out_file=None,\n                              feature_names=iris.feature_names,\n                              class_names=iris.target_names,\n                              filled=True,rounded=True,\n                              special_characters=True)\n\ngraph=graphviz.Source(dot_data)\ngraph.view()\n```\n\n![机器学习之分类与回归树CART图片01](机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png)\n\n### 6.推广\n\n更多内容请关注公众号**谓之小一**，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。\n\n![推广](http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png)","slug":"机器学习之分类与回归树-CART","published":1,"updated":"2018-04-30T07:12:17.076Z","layout":"post","photos":[],"link":"","_id":"cji4rdzea001p2e01e6fq6qvj","content":"<h3 id=\"1-分类与回归树简介\"><a href=\"#1-分类与回归树简介\" class=\"headerlink\" title=\"1.分类与回归树简介\"></a>1.分类与回归树简介</h3><p>分类与回归树的英文是<em>Classfication And Regression Tree</em>，缩写为CART。CART算法采用<strong>二分递归分割的技术</strong>将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为<strong>True</strong>和<strong>False</strong>，左分支取值为<strong>True</strong>，右分支取值为<strong>False</strong>，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。</p>\n<ul>\n<li>如果待预测分类是离散型数据，则CART生成分类决策树。</li>\n<li>如果待预测分类是连续性数据，则CART生成回归决策树。</li>\n</ul>\n<h3 id=\"2-CART分类树\"><a href=\"#2-CART分类树\" class=\"headerlink\" title=\"2.CART分类树\"></a>2.CART分类树</h3><h4 id=\"2-1算法详解\"><a href=\"#2-1算法详解\" class=\"headerlink\" title=\"2.1算法详解\"></a>2.1算法详解</h4><p>CART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为</p>\n<p>$$<br>Gini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}<br>$$<br>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。<br>$$<br>Gini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2<br>$$<br>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。<br>$$<br>Gain_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)<br>$$<br>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。<br>$$<br>\\min_{i\\epsilon A}(Gain_Gini(D,A))<br>$$<br>$$<br>\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain_Gini(D,A)))<br>$$</p>\n<h4 id=\"2-2实例详解\"><a href=\"#2-2实例详解\" class=\"headerlink\" title=\"2.2实例详解\"></a>2.2实例详解</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">名称</th>\n<th style=\"text-align:center\">体温</th>\n<th style=\"text-align:center\">胎生</th>\n<th style=\"text-align:center\">水生</th>\n<th style=\"text-align:center\">类标记</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">人</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蟒</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲑鱼</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲸</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蛙</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蜥</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝙蝠</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">猫</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豹纹鲨</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">海龟</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豪猪</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鳗</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝾螈</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">两栖类</td>\n</tr>\n</tbody>\n</table>\n<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。</p>\n<p>$$<br>Gini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}<br>$$<br>$$<br>Gini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}<br>$$<br>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。<br>$$<br>Gain_Gini(D,体温)=\\frac{7}{15}<em>\\frac{20}{49}+\\frac{8}{15}</em>\\frac{42}{64}<br>$$</p>\n<h3 id=\"3-CART回归树\"><a href=\"#3-CART回归树\" class=\"headerlink\" title=\"3.CART回归树\"></a>3.CART回归树</h3><h4 id=\"3-1算法详解\"><a href=\"#3-1算法详解\" class=\"headerlink\" title=\"3.1算法详解\"></a>3.1算法详解</h4><p>CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。<br>$$<br>D={(x_1,y_1),(x_2,y_2),(x_3,y_3),…(x_n,y_n)}<br>$$<br><strong>选择最优切分变量j与切分点s</strong>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。</p>\n<p>$$<br>\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]<br>$$<br><strong>用选定的(j,s)对，划分区域并决定相应的输出值</strong><br>$$<br>R_1(j,s)={x|x^{(j)}\\le s},R_2(j,s)={x|x^{(j)} &gt;  s}<br>$$</p>\n<p>$$<br>\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i<br>$$</p>\n<p>$$<br>x\\epsilon R_m,m=1,2<br>$$</p>\n<p><strong>继续对两个子区域调用上述步骤</strong>，<strong>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</strong></p>\n<p>$$<br>f(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)<br>$$<br>当输入空间划分确定时，可以用<strong>平方误差</strong>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。<br>$$<br>\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2<br>$$</p>\n<h4 id=\"3-2实例详解\"><a href=\"#3-2实例详解\" class=\"headerlink\" title=\"3.2实例详解\"></a>3.2实例详解</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">5.56</td>\n<td style=\"text-align:center\">5.70</td>\n<td style=\"text-align:center\">5.91</td>\n<td style=\"text-align:center\">6.40</td>\n<td style=\"text-align:center\">6.80</td>\n<td style=\"text-align:center\">7.05</td>\n<td style=\"text-align:center\">8.90</td>\n<td style=\"text-align:center\">8.70</td>\n<td style=\"text-align:center\">9.00</td>\n<td style=\"text-align:center\">9.05</td>\n</tr>\n</tbody>\n</table>\n<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。</p>\n<p>$$<br>c_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56<br>$$</p>\n<p>$$<br>c_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+…+9.05)=7.50<br>$$</p>\n<p>$$<br>m(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72<br>$$</p>\n<p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$s$</th>\n<th style=\"text-align:center\">1.5</th>\n<th style=\"text-align:center\">2.5</th>\n<th style=\"text-align:center\">3.5</th>\n<th style=\"text-align:center\">4.5</th>\n<th style=\"text-align:center\">5.5</th>\n<th style=\"text-align:center\">6.5</th>\n<th style=\"text-align:center\">7.5</th>\n<th style=\"text-align:center\">8.5</th>\n<th style=\"text-align:center\">9.5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$m(s)$</td>\n<td style=\"text-align:center\">15.72</td>\n<td style=\"text-align:center\">12.07</td>\n<td style=\"text-align:center\">8.36</td>\n<td style=\"text-align:center\">5.78</td>\n<td style=\"text-align:center\">3.91</td>\n<td style=\"text-align:center\">1.93</td>\n<td style=\"text-align:center\">8.01</td>\n<td style=\"text-align:center\">11.73</td>\n<td style=\"text-align:center\">15.74</td>\n</tr>\n</tbody>\n</table>\n<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为<br>$$<br>T_1(x)=\\begin{cases}<br> &amp; 6.24,x&lt;6.5 \\<br> &amp; 8.91,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_1(x)=T_1(x)<br>$$</p>\n<p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>，如下表所示</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">-0.68</td>\n<td style=\"text-align:center\">-0.54</td>\n<td style=\"text-align:center\">-0.33</td>\n<td style=\"text-align:center\">0.16</td>\n<td style=\"text-align:center\">0.56</td>\n<td style=\"text-align:center\">0.81</td>\n<td style=\"text-align:center\">-0.01</td>\n<td style=\"text-align:center\">-0.21</td>\n<td style=\"text-align:center\">0.09</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n</tbody>\n</table>\n<p><strong>用f1(x)拟合训练数据得到平方误差</strong><br>$$<br>L(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93<br>$$<br>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到<br>$$<br>T_2(x)=\\begin{cases}<br> &amp; -0.52,x&lt;3.5 \\<br> &amp; 0.22,x\\ge 3.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_2(x)=f_1(x)+T_2(x)=<br>\\begin{cases}<br> &amp; 5.72,x&lt;3.5 \\<br> &amp; 6.46,3.5\\le x \\le 6.5 \\<br> &amp; 9.13,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>用f2(x)拟合训练数据的平方误差<br>$$<br>L(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79<br>$$<br>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示<br>$$<br>T_3(x)=\\begin{cases}<br> &amp; 0.15,x&lt;6.5 \\<br> &amp; -0.22,x\\ge 6.5<br>\\end{cases}<br>L(y,f_3(x))=0.47<br>$$</p>\n<p>$$<br>T_4(x)=\\begin{cases}<br> &amp; -0.16,x&lt;4.5 \\<br> &amp; 0.11,x\\ge 4.5<br>\\end{cases}<br>L(y,f_4(x))=0.30<br>$$</p>\n<p>$$<br>T_5(x)=\\begin{cases}<br> &amp; 0.07,x&lt;6.5 \\<br> &amp; -0.11,x\\ge 6.5<br>\\end{cases}<br>L(y,f_5(x))=0.23<br>$$</p>\n<p>$$<br>T_6(x)=\\begin{cases}<br> &amp; -0.15,x&lt;2.5 \\<br> &amp; 0.04,x\\ge 2.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_6(x)=f_5(x)+T_6(x)=T_1(x)+…+T_6(x)=<br>\\begin{cases}<br> &amp; 5.63,x&lt;2.5 \\<br> &amp; 5.82,2.5\\le x \\le 3.5 \\<br>  &amp; 6.56,3.5\\le x \\le 4.5 \\<br> &amp; 6.83,4.5\\le x \\le 6.5 \\<br>  &amp; 8.95,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。<br>$$<br>L(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71<br>$$</p>\n<h3 id=\"4-CART剪枝\"><a href=\"#4-CART剪枝\" class=\"headerlink\" title=\"4.CART剪枝\"></a>4.CART剪枝</h3><blockquote>\n<p>此处我们介绍代价复杂度剪枝算法</p>\n</blockquote>\n<p>我们将一颗充分生长的树称为<strong>T0</strong> ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下<br>$$<br>C_\\alpha(T)=C(T)+\\alpha|T|<br>$$</p>\n<ul>\n<li>T为任意子树，|T|为子树T的叶子节点个数。</li>\n<li>α是参数，权衡拟合程度与树的复杂度。</li>\n<li>C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。</li>\n</ul>\n<p><strong>那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？</strong>准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。</p>\n<ul>\n<li>当α很小的时候，T0 是这样的最优子树.</li>\n<li>当α很大的时候，单独一个根节点就是最优子树。</li>\n</ul>\n<p>尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行<strong>交叉验证</strong>，找到最优的那个子树作为我们的决策树。子树序列如下<br>$$<br>T_0&gt;T_1&gt;T_2&gt;T_3&gt;…&gt;T_n<br>$$<br><strong>因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，</strong>在此不再详细介绍。</p>\n<h3 id=\"5-Sklearn实现\"><a href=\"#5-Sklearn实现\" class=\"headerlink\" title=\"5.Sklearn实现\"></a>5.Sklearn实现</h3><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">clf=tree.DecisionTreeClassifier()</span><br><span class=\"line\">clf=clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#export the decision tree</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> graphviz</span><br><span class=\"line\"><span class=\"comment\">#export_graphviz support a variety of aesthetic options</span></span><br><span class=\"line\">dot_data=tree.export_graphviz(clf,out_file=<span class=\"keyword\">None</span>,</span><br><span class=\"line\">                              feature_names=iris.feature_names,</span><br><span class=\"line\">                              class_names=iris.target_names,</span><br><span class=\"line\">                              filled=<span class=\"keyword\">True</span>,rounded=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                              special_characters=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph=graphviz.Source(dot_data)</span><br><span class=\"line\">graph.view()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png\" alt=\"机器学习之分类与回归树CART图片01\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-分类与回归树简介\"><a href=\"#1-分类与回归树简介\" class=\"headerlink\" title=\"1.分类与回归树简介\"></a>1.分类与回归树简介</h3><p>分类与回归树的英文是<em>Classfication And Regression Tree</em>，缩写为CART。CART算法采用<strong>二分递归分割的技术</strong>将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为<strong>True</strong>和<strong>False</strong>，左分支取值为<strong>True</strong>，右分支取值为<strong>False</strong>，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。</p>\n<ul>\n<li>如果待预测分类是离散型数据，则CART生成分类决策树。</li>\n<li>如果待预测分类是连续性数据，则CART生成回归决策树。</li>\n</ul>\n<h3 id=\"2-CART分类树\"><a href=\"#2-CART分类树\" class=\"headerlink\" title=\"2.CART分类树\"></a>2.CART分类树</h3><h4 id=\"2-1算法详解\"><a href=\"#2-1算法详解\" class=\"headerlink\" title=\"2.1算法详解\"></a>2.1算法详解</h4><p>CART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为</p>\n<p>$$<br>Gini(p)=\\sum_{k=1}^{m}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}<br>$$<br>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。<br>$$<br>Gini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|} \\right)^2<br>$$<br>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。<br>$$<br>Gain_Gini(D,A)=\\frac{|D1|}{|D|}Gini(D_1)+\\frac{|D1|}{|D|}Gini(D_2)<br>$$<br>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。<br>$$<br>\\min_{i\\epsilon A}(Gain_Gini(D,A))<br>$$<br>$$<br>\\min_{A\\epsilon Attribute}(\\min_{i\\epsilon A}(Gain_Gini(D,A)))<br>$$</p>\n<h4 id=\"2-2实例详解\"><a href=\"#2-2实例详解\" class=\"headerlink\" title=\"2.2实例详解\"></a>2.2实例详解</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">名称</th>\n<th style=\"text-align:center\">体温</th>\n<th style=\"text-align:center\">胎生</th>\n<th style=\"text-align:center\">水生</th>\n<th style=\"text-align:center\">类标记</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">人</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蟒</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲑鱼</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鲸</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蛙</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">巨蜥</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝙蝠</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">猫</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豹纹鲨</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">海龟</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">爬行类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">豪猪</td>\n<td style=\"text-align:center\">恒温</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">哺乳类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">鳗</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">是</td>\n<td style=\"text-align:center\">鱼类</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">蝾螈</td>\n<td style=\"text-align:center\">冷血</td>\n<td style=\"text-align:center\">否</td>\n<td style=\"text-align:center\">有时</td>\n<td style=\"text-align:center\">两栖类</td>\n</tr>\n</tbody>\n</table>\n<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。</p>\n<p>$$<br>Gini(D_1)=1-[ (\\frac{5}{7})^2+(\\frac{2}{7})^2]=\\frac{20}{49}<br>$$<br>$$<br>Gini(D_2)=1-[ (\\frac{3}{8})^2+(\\frac{3}{8})^2+(\\frac{2}{8})^2]=\\frac{42}{64}<br>$$<br>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。<br>$$<br>Gain_Gini(D,体温)=\\frac{7}{15}<em>\\frac{20}{49}+\\frac{8}{15}</em>\\frac{42}{64}<br>$$</p>\n<h3 id=\"3-CART回归树\"><a href=\"#3-CART回归树\" class=\"headerlink\" title=\"3.CART回归树\"></a>3.CART回归树</h3><h4 id=\"3-1算法详解\"><a href=\"#3-1算法详解\" class=\"headerlink\" title=\"3.1算法详解\"></a>3.1算法详解</h4><p>CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。<br>$$<br>D={(x_1,y_1),(x_2,y_2),(x_3,y_3),…(x_n,y_n)}<br>$$<br><strong>选择最优切分变量j与切分点s</strong>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。</p>\n<p>$$<br>\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]<br>$$<br><strong>用选定的(j,s)对，划分区域并决定相应的输出值</strong><br>$$<br>R_1(j,s)={x|x^{(j)}\\le s},R_2(j,s)={x|x^{(j)} &gt;  s}<br>$$</p>\n<p>$$<br>\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i<br>$$</p>\n<p>$$<br>x\\epsilon R_m,m=1,2<br>$$</p>\n<p><strong>继续对两个子区域调用上述步骤</strong>，<strong>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</strong></p>\n<p>$$<br>f(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\epsilon R_m)<br>$$<br>当输入空间划分确定时，可以用<strong>平方误差</strong>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。<br>$$<br>\\sum_{x_i\\epsilon R_m}(y_i-f(x_i))^2<br>$$</p>\n<h4 id=\"3-2实例详解\"><a href=\"#3-2实例详解\" class=\"headerlink\" title=\"3.2实例详解\"></a>3.2实例详解</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">5.56</td>\n<td style=\"text-align:center\">5.70</td>\n<td style=\"text-align:center\">5.91</td>\n<td style=\"text-align:center\">6.40</td>\n<td style=\"text-align:center\">6.80</td>\n<td style=\"text-align:center\">7.05</td>\n<td style=\"text-align:center\">8.90</td>\n<td style=\"text-align:center\">8.70</td>\n<td style=\"text-align:center\">9.00</td>\n<td style=\"text-align:center\">9.05</td>\n</tr>\n</tbody>\n</table>\n<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。</p>\n<p>$$<br>c_1=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{1}\\sum_{x_i\\epsilon R_1(1,1.5)}5.56=5.56<br>$$</p>\n<p>$$<br>c_2=\\frac{1}{N_m}\\sum_{x_i\\epsilon R_m(j,s)}y_i=\\frac{1}{9}\\sum_{x_i\\epsilon R_2(1,1.5)}(5.70+5.91+…+9.05)=7.50<br>$$</p>\n<p>$$<br>m(s)=\\min_{j,s}[\\min_{c_1}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum _{x_i\\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72<br>$$</p>\n<p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$s$</th>\n<th style=\"text-align:center\">1.5</th>\n<th style=\"text-align:center\">2.5</th>\n<th style=\"text-align:center\">3.5</th>\n<th style=\"text-align:center\">4.5</th>\n<th style=\"text-align:center\">5.5</th>\n<th style=\"text-align:center\">6.5</th>\n<th style=\"text-align:center\">7.5</th>\n<th style=\"text-align:center\">8.5</th>\n<th style=\"text-align:center\">9.5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$m(s)$</td>\n<td style=\"text-align:center\">15.72</td>\n<td style=\"text-align:center\">12.07</td>\n<td style=\"text-align:center\">8.36</td>\n<td style=\"text-align:center\">5.78</td>\n<td style=\"text-align:center\">3.91</td>\n<td style=\"text-align:center\">1.93</td>\n<td style=\"text-align:center\">8.01</td>\n<td style=\"text-align:center\">11.73</td>\n<td style=\"text-align:center\">15.74</td>\n</tr>\n</tbody>\n</table>\n<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为<br>$$<br>T_1(x)=\\begin{cases}<br> &amp; 6.24,x&lt;6.5 \\<br> &amp; 8.91,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_1(x)=T_1(x)<br>$$</p>\n<p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>，如下表所示</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$x_i$</th>\n<th style=\"text-align:center\">1</th>\n<th style=\"text-align:center\">2</th>\n<th style=\"text-align:center\">3</th>\n<th style=\"text-align:center\">4</th>\n<th style=\"text-align:center\">5</th>\n<th style=\"text-align:center\">6</th>\n<th style=\"text-align:center\">7</th>\n<th style=\"text-align:center\">8</th>\n<th style=\"text-align:center\">9</th>\n<th style=\"text-align:center\">10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">$y_i$</td>\n<td style=\"text-align:center\">-0.68</td>\n<td style=\"text-align:center\">-0.54</td>\n<td style=\"text-align:center\">-0.33</td>\n<td style=\"text-align:center\">0.16</td>\n<td style=\"text-align:center\">0.56</td>\n<td style=\"text-align:center\">0.81</td>\n<td style=\"text-align:center\">-0.01</td>\n<td style=\"text-align:center\">-0.21</td>\n<td style=\"text-align:center\">0.09</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n</tbody>\n</table>\n<p><strong>用f1(x)拟合训练数据得到平方误差</strong><br>$$<br>L(y,f_1(x))=\\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93<br>$$<br>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到<br>$$<br>T_2(x)=\\begin{cases}<br> &amp; -0.52,x&lt;3.5 \\<br> &amp; 0.22,x\\ge 3.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_2(x)=f_1(x)+T_2(x)=<br>\\begin{cases}<br> &amp; 5.72,x&lt;3.5 \\<br> &amp; 6.46,3.5\\le x \\le 6.5 \\<br> &amp; 9.13,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>用f2(x)拟合训练数据的平方误差<br>$$<br>L(y,f_2(x))=\\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79<br>$$<br>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示<br>$$<br>T_3(x)=\\begin{cases}<br> &amp; 0.15,x&lt;6.5 \\<br> &amp; -0.22,x\\ge 6.5<br>\\end{cases}<br>L(y,f_3(x))=0.47<br>$$</p>\n<p>$$<br>T_4(x)=\\begin{cases}<br> &amp; -0.16,x&lt;4.5 \\<br> &amp; 0.11,x\\ge 4.5<br>\\end{cases}<br>L(y,f_4(x))=0.30<br>$$</p>\n<p>$$<br>T_5(x)=\\begin{cases}<br> &amp; 0.07,x&lt;6.5 \\<br> &amp; -0.11,x\\ge 6.5<br>\\end{cases}<br>L(y,f_5(x))=0.23<br>$$</p>\n<p>$$<br>T_6(x)=\\begin{cases}<br> &amp; -0.15,x&lt;2.5 \\<br> &amp; 0.04,x\\ge 2.5<br>\\end{cases}<br>$$</p>\n<p>$$<br>f_6(x)=f_5(x)+T_6(x)=T_1(x)+…+T_6(x)=<br>\\begin{cases}<br> &amp; 5.63,x&lt;2.5 \\<br> &amp; 5.82,2.5\\le x \\le 3.5 \\<br>  &amp; 6.56,3.5\\le x \\le 4.5 \\<br> &amp; 6.83,4.5\\le x \\le 6.5 \\<br>  &amp; 8.95,x\\ge 6.5<br>\\end{cases}<br>$$</p>\n<p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。<br>$$<br>L(y,f_6(x))=\\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71<br>$$</p>\n<h3 id=\"4-CART剪枝\"><a href=\"#4-CART剪枝\" class=\"headerlink\" title=\"4.CART剪枝\"></a>4.CART剪枝</h3><blockquote>\n<p>此处我们介绍代价复杂度剪枝算法</p>\n</blockquote>\n<p>我们将一颗充分生长的树称为<strong>T0</strong> ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下<br>$$<br>C_\\alpha(T)=C(T)+\\alpha|T|<br>$$</p>\n<ul>\n<li>T为任意子树，|T|为子树T的叶子节点个数。</li>\n<li>α是参数，权衡拟合程度与树的复杂度。</li>\n<li>C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。</li>\n</ul>\n<p><strong>那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？</strong>准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。</p>\n<ul>\n<li>当α很小的时候，T0 是这样的最优子树.</li>\n<li>当α很大的时候，单独一个根节点就是最优子树。</li>\n</ul>\n<p>尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行<strong>交叉验证</strong>，找到最优的那个子树作为我们的决策树。子树序列如下<br>$$<br>T_0&gt;T_1&gt;T_2&gt;T_3&gt;…&gt;T_n<br>$$<br><strong>因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，</strong>在此不再详细介绍。</p>\n<h3 id=\"5-Sklearn实现\"><a href=\"#5-Sklearn实现\" class=\"headerlink\" title=\"5.Sklearn实现\"></a>5.Sklearn实现</h3><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#load data</span></span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">X=iris.data</span><br><span class=\"line\">y=iris.target</span><br><span class=\"line\">clf=tree.DecisionTreeClassifier()</span><br><span class=\"line\">clf=clf.fit(X,y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#export the decision tree</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> graphviz</span><br><span class=\"line\"><span class=\"comment\">#export_graphviz support a variety of aesthetic options</span></span><br><span class=\"line\">dot_data=tree.export_graphviz(clf,out_file=<span class=\"keyword\">None</span>,</span><br><span class=\"line\">                              feature_names=iris.feature_names,</span><br><span class=\"line\">                              class_names=iris.target_names,</span><br><span class=\"line\">                              filled=<span class=\"keyword\">True</span>,rounded=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                              special_characters=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph=graphviz.Source(dot_data)</span><br><span class=\"line\">graph.view()</span><br></pre></td></tr></table></figure>\n<p><img src=\"机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png\" alt=\"机器学习之分类与回归树CART图片01\"></p>\n<h3 id=\"6-推广\"><a href=\"#6-推广\" class=\"headerlink\" title=\"6.推广\"></a>6.推广</h3><p>更多内容请关注公众号<strong>谓之小一</strong>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p>\n<p><img src=\"http://p66yyzg4i.bkt.clouddn.com/%E6%8E%A8%E5%B9%BF.png\" alt=\"推广\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/Markdown写作教程/图片03.png","slug":"图片03.png","post":"cji4rdzd300032e01manfcwdl","modified":1,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/12.png","slug":"12.png","post":"cji4rdzd900072e01gu51z7ex","modified":1,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程图片02.png","slug":"Python之Sklearn使用教程图片02.png","post":"cji4rdzdh000d2e01qxrefsqi","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片03.png","slug":"机器学习之SVM支持向量机（一）图片03.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片05.png","slug":"机器学习之SVM支持向量机（一）图片05.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像06.png","slug":"机器学习之SVM支持向量机（二）图像06.png","post":"cji4rdzdu000t2e01koqemz5r","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/公式01.png","slug":"公式01.png","post":"cji4rdze100172e019x7ml08w","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/公式03.png","slug":"公式03.png","post":"cji4rdze100172e019x7ml08w","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/图片03.png","slug":"图片03.png","post":"cji4rdze100172e019x7ml08w","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.3.png","slug":"图片6.3.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程05.png","slug":"Python之Sklearn使用教程05.png","post":"cji4rdzdh000d2e01qxrefsqi","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之Apriori算法/机器学习之Apriori算法图片01.png","slug":"机器学习之Apriori算法图片01.png","post":"cji4rdzde000a2e01z3lad7yd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片04.png","slug":"机器学习之SVM支持向量机（一）图片04.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片11.png","slug":"机器学习之SVM支持向量机（一）图片11.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片14.png","slug":"机器学习之SVM支持向量机（一）图片14.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片17.png","slug":"机器学习之SVM支持向量机（一）图片17.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/公式02.png","slug":"公式02.png","post":"cji4rdze100172e019x7ml08w","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片02.png","slug":"机器学习之自适应增强Adaboost图片02.png","post":"cji4rdze2001a2e01jl6br241","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片03.png","slug":"机器学习之自适应增强Adaboost图片03.png","post":"cji4rdze2001a2e01jl6br241","modified":1,"renderable":0},{"_id":"source/_posts/机器学习知识体系/图片03.png","slug":"图片03.png","post":"cji4rdze6001i2e017e8ta02f","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片7.2.png","slug":"图片7.2.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/15.png","slug":"15.png","post":"cji4rdzd900072e01gu51z7ex","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归02.png","slug":"机器学习之Logistic回归02.png","post":"cji4rdzdr000n2e01u6930lhf","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片02.png","slug":"机器学习之SVM支持向量机（一）图片02.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之最大期望-EM-算法/机器学习之最大期望算法图片01.png","slug":"机器学习之最大期望算法图片01.png","post":"cji4rdzdz00142e01xw2dy74c","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之线性回归/图片02.png","slug":"图片02.png","post":"cji4rdze100172e019x7ml08w","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之随机森林/机器学习之随机森林图片01.png","slug":"机器学习之随机森林图片01.png","post":"cji4rdze5001f2e01pgkkqok2","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之分类与回归树-CART/机器学习之分类与回归树CART图片01.png","slug":"机器学习之分类与回归树CART图片01.png","post":"cji4rdzea001p2e01e6fq6qvj","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之决策树-C4-5算法/机器学习之决策树图片01.png","post":"cji4rdzdw000w2e018zqafriy","slug":"机器学习之决策树图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之决策树-C4-5算法/机器学习之决策树图片02.png","slug":"机器学习之决策树图片02.png","post":"cji4rdzdw000w2e018zqafriy","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片01.png","post":"cji4rdze3001c2e012vnymvzm","slug":"机器学习之梯度提升决策树图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之梯度提升决策树-GBDT/机器学习之梯度提升决策树图片02.png","post":"cji4rdze3001c2e012vnymvzm","slug":"机器学习之梯度提升决策树图片02.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片01.png","post":"cji4rdzdp000j2e01pi5l4zxb","slug":"机器学习值K均值算法图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片02.png","post":"cji4rdzdp000j2e01pi5l4zxb","slug":"机器学习值K均值算法图片02.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K均值-K-Means-算法/机器学习值K均值算法图片03.png","post":"cji4rdzdp000j2e01pi5l4zxb","slug":"机器学习值K均值算法图片03.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之自适应增强-Adaboost/机器学习之自适应增强Adaboost图片01.png","post":"cji4rdze2001a2e01jl6br241","slug":"机器学习之自适应增强Adaboost图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/Markdown写作教程/图片01.png","post":"cji4rdzd300032e01manfcwdl","slug":"图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/Markdown写作教程/图片02.png","slug":"图片02.png","post":"cji4rdzd300032e01manfcwdl","modified":1,"renderable":0},{"_id":"source/_posts/Markdown写作教程/推广.png","post":"cji4rdzd300032e01manfcwdl","slug":"推广.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之Logistic回归/推广.png","post":"cji4rdzdr000n2e01u6930lhf","slug":"推广.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归01.png","post":"cji4rdzdr000n2e01u6930lhf","slug":"机器学习之Logistic回归01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之Logistic回归/机器学习之Logistic回归03.png","post":"cji4rdzdr000n2e01u6930lhf","slug":"机器学习之Logistic回归03.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习知识体系/图片01.png","post":"cji4rdze6001i2e017e8ta02f","slug":"图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习知识体系/图片02.png","post":"cji4rdze6001i2e017e8ta02f","slug":"图片02.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习知识体系/图片04.png","slug":"图片04.png","post":"cji4rdze6001i2e017e8ta02f","modified":1,"renderable":0},{"_id":"source/_posts/机器学习知识体系/推广.png","post":"cji4rdze6001i2e017e8ta02f","slug":"推广.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片01.png","post":"cji4rdzdn000i2e01e0e80w3t","slug":"机器学习之K近邻算法图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片02.png","post":"cji4rdzdn000i2e01e0e80w3t","slug":"机器学习之K近邻算法图片02.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片03.png","post":"cji4rdzdn000i2e01e0e80w3t","slug":"机器学习之K近邻算法图片03.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片04.png","post":"cji4rdzdn000i2e01e0e80w3t","slug":"机器学习之K近邻算法图片04.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片05.png","post":"cji4rdzdn000i2e01e0e80w3t","slug":"机器学习之K近邻算法图片05.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之K近邻-KNN-算法/机器学习之K近邻算法图片06.png","post":"cji4rdzdn000i2e01e0e80w3t","slug":"机器学习之K近邻算法图片06.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像01.png","post":"cji4rdzdu000t2e01koqemz5r","slug":"机器学习之SVM支持向量机（二）图像01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像02.png","post":"cji4rdzdu000t2e01koqemz5r","slug":"机器学习之SVM支持向量机（二）图像02.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像03.png","post":"cji4rdzdu000t2e01koqemz5r","slug":"机器学习之SVM支持向量机（二）图像03.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图像05.png","post":"cji4rdzdu000t2e01koqemz5r","slug":"机器学习之SVM支持向量机（二）图像05.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之SVM支持向量机（二）/机器学习之SVM支持向量机（二）图片推广.png","post":"cji4rdzdu000t2e01koqemz5r","slug":"机器学习之SVM支持向量机（二）图片推广.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之线性回归/图片01.png","post":"cji4rdze100172e019x7ml08w","slug":"图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之线性回归/推广.png","post":"cji4rdze100172e019x7ml08w","slug":"推广.png","modified":1,"renderable":1},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.1.png","post":"cji4rdzcz00012e01ln70yhxw","slug":"图片6.1.png","modified":1,"renderable":1},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片3.3.png","slug":"图片3.3.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片4.1.png","slug":"图片4.1.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.2.png","slug":"图片6.2.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.4.png","slug":"图片6.4.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.5.png","slug":"图片6.5.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片6.6.png","slug":"图片6.6.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/图片7.1.png","slug":"图片7.1.png","post":"cji4rdzcz00012e01ln70yhxw","modified":1,"renderable":0},{"_id":"source/_posts/Mac+Hexo+GitHub博客搭建教程/推广.png","post":"cji4rdzcz00012e01ln70yhxw","slug":"推广.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程03.png","post":"cji4rdzdh000d2e01qxrefsqi","slug":"Python之Sklearn使用教程03.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程04.png","post":"cji4rdzdh000d2e01qxrefsqi","slug":"Python之Sklearn使用教程04.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程06.png","post":"cji4rdzdh000d2e01qxrefsqi","slug":"Python之Sklearn使用教程06.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程07.png","post":"cji4rdzdh000d2e01qxrefsqi","slug":"Python之Sklearn使用教程07.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程08.png","slug":"Python之Sklearn使用教程08.png","post":"cji4rdzdh000d2e01qxrefsqi","modified":1,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程09.png","post":"cji4rdzdh000d2e01qxrefsqi","slug":"Python之Sklearn使用教程09.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程10.png","post":"cji4rdzdh000d2e01qxrefsqi","slug":"Python之Sklearn使用教程10.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程11.png","post":"cji4rdzdh000d2e01qxrefsqi","slug":"Python之Sklearn使用教程11.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程图片01.png","slug":"Python之Sklearn使用教程图片01.png","post":"cji4rdzdh000d2e01qxrefsqi","modified":1,"renderable":0},{"_id":"source/_posts/Python之Sklearn使用教程/Python之Sklearn使用教程推广.png","post":"cji4rdzdh000d2e01qxrefsqi","slug":"Python之Sklearn使用教程推广.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片01.png","post":"cji4rdzds000p2e012599lzfd","slug":"机器学习之SVM支持向量机（一）图片01.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片06.png","slug":"机器学习之SVM支持向量机（一）图片06.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片07.png","slug":"机器学习之SVM支持向量机（一）图片07.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片08.png","slug":"机器学习之SVM支持向量机（一）图片08.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片09.png","slug":"机器学习之SVM支持向量机（一）图片09.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片10.png","slug":"机器学习之SVM支持向量机（一）图片10.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片12.png","slug":"机器学习之SVM支持向量机（一）图片12.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片13.png","slug":"机器学习之SVM支持向量机（一）图片13.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片15.png","slug":"机器学习之SVM支持向量机（一）图片15.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片16.png","slug":"机器学习之SVM支持向量机（一）图片16.png","post":"cji4rdzds000p2e012599lzfd","modified":1,"renderable":0},{"_id":"source/_posts/机器学习之SVM支持向量机（一）/机器学习之SVM支持向量机（一）图片推广.png.png","post":"cji4rdzds000p2e012599lzfd","slug":"机器学习之SVM支持向量机（一）图片推广.png.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/01.png","post":"cji4rdzd900072e01gu51z7ex","slug":"01.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/02.png","post":"cji4rdzd900072e01gu51z7ex","slug":"02.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/03.png","post":"cji4rdzd900072e01gu51z7ex","slug":"03.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/04.png","post":"cji4rdzd900072e01gu51z7ex","slug":"04.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/05.png","post":"cji4rdzd900072e01gu51z7ex","slug":"05.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/06.png","post":"cji4rdzd900072e01gu51z7ex","slug":"06.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/07.png","post":"cji4rdzd900072e01gu51z7ex","slug":"07.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/08.png","post":"cji4rdzd900072e01gu51z7ex","slug":"08.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/09.png","post":"cji4rdzd900072e01gu51z7ex","slug":"09.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/10.png","slug":"10.png","post":"cji4rdzd900072e01gu51z7ex","modified":1,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/11.png","post":"cji4rdzd900072e01gu51z7ex","slug":"11.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/13.png","post":"cji4rdzd900072e01gu51z7ex","slug":"13.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/14.png","slug":"14.png","post":"cji4rdzd900072e01gu51z7ex","modified":1,"renderable":0},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像18.png","post":"cji4rdzd900072e01gu51z7ex","slug":"图像18.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像19.png","post":"cji4rdzd900072e01gu51z7ex","slug":"图像19.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像20.png","post":"cji4rdzd900072e01gu51z7ex","slug":"图像20.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像21.png","post":"cji4rdzd900072e01gu51z7ex","slug":"图像21.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像22.png","post":"cji4rdzd900072e01gu51z7ex","slug":"图像22.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/图像23.png","post":"cji4rdzd900072e01gu51z7ex","slug":"图像23.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/图片16.png","post":"cji4rdzd900072e01gu51z7ex","slug":"图片16.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/图片17.png","post":"cji4rdzd900072e01gu51z7ex","slug":"图片17.png","modified":1,"renderable":1},{"_id":"source/_posts/Python之MatPlotLib使用教程/推广.png","post":"cji4rdzd900072e01gu51z7ex","slug":"推广.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cji4rdzcz00012e01ln70yhxw","category_id":"cji4rdzd600042e01yy8d7xwr","_id":"cji4rdzdj000f2e01j9ix92k6"},{"post_id":"cji4rdzdh000d2e01qxrefsqi","category_id":"cji4rdzdf000b2e01uio21srh","_id":"cji4rdzdp000k2e01nzshdu3w"},{"post_id":"cji4rdzd900072e01gu51z7ex","category_id":"cji4rdzdf000b2e01uio21srh","_id":"cji4rdzds000o2e010qjstsnh"},{"post_id":"cji4rdzdi000e2e01nfv5nofl","category_id":"cji4rdzdf000b2e01uio21srh","_id":"cji4rdzdt000q2e01aneokso8"},{"post_id":"cji4rdzdc00092e01l1xeovvs","category_id":"cji4rdzdf000b2e01uio21srh","_id":"cji4rdzdv000u2e01s4jqnleo"},{"post_id":"cji4rdzdr000n2e01u6930lhf","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdzdx000x2e01rthxgor9"},{"post_id":"cji4rdzde000a2e01z3lad7yd","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdzdy00112e01ondgqwl8"},{"post_id":"cji4rdzds000p2e012599lzfd","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdze000152e019q4jgtui"},{"post_id":"cji4rdzdu000t2e01koqemz5r","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdze100182e01w0v496tn"},{"post_id":"cji4rdzdn000i2e01e0e80w3t","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdze3001b2e01jouykytp"},{"post_id":"cji4rdzdw000w2e018zqafriy","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdze4001d2e01f6gwuwoh"},{"post_id":"cji4rdzdy00102e013zkegnvw","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdze6001g2e01uq54azfh"},{"post_id":"cji4rdzdp000j2e01pi5l4zxb","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdze7001j2e01k4km3cqz"},{"post_id":"cji4rdzdz00142e01xw2dy74c","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdze9001n2e01evkoc2ep"},{"post_id":"cji4rdze100172e019x7ml08w","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdzeb001q2e01mm8qqzdu"},{"post_id":"cji4rdze2001a2e01jl6br241","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdzec001u2e01o3zcg56g"},{"post_id":"cji4rdze3001c2e012vnymvzm","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdzec001w2e01ln5wzcv2"},{"post_id":"cji4rdze5001f2e01pgkkqok2","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdzed001y2e01btqmy5tg"},{"post_id":"cji4rdze6001i2e017e8ta02f","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdzed001z2e011nwlynko"},{"post_id":"cji4rdzea001p2e01e6fq6qvj","category_id":"cji4rdzdq000l2e011q91w2jr","_id":"cji4rdzed00202e012yenpbyr"},{"post_id":"cji4rdze8001m2e01aoi33hcs","category_id":"cji4rdzeb001t2e014nhmc42r","_id":"cji4rdzed00232e01znhfw5ed"}],"PostTag":[{"post_id":"cji4rdzcz00012e01ln70yhxw","tag_id":"cji4rdzd800052e01u8dp7shi","_id":"cji4rdzdw000v2e01imnubro0"},{"post_id":"cji4rdzcz00012e01ln70yhxw","tag_id":"cji4rdzdg000c2e01odqpcjna","_id":"cji4rdzdx000z2e01r24xqwuf"},{"post_id":"cji4rdzcz00012e01ln70yhxw","tag_id":"cji4rdzdk000h2e01g9ekdtlm","_id":"cji4rdzdz00132e01faz0aepc"},{"post_id":"cji4rdzcz00012e01ln70yhxw","tag_id":"cji4rdzdq000m2e0156eiairm","_id":"cji4rdze000162e012wjfiqve"},{"post_id":"cji4rdzd300032e01manfcwdl","tag_id":"cji4rdzdu000s2e01bjnsozb5","_id":"cji4rdze6001h2e01801vsq23"},{"post_id":"cji4rdzd300032e01manfcwdl","tag_id":"cji4rdzdq000m2e0156eiairm","_id":"cji4rdze7001k2e01z3tssk5j"},{"post_id":"cji4rdzd300032e01manfcwdl","tag_id":"cji4rdze100192e01jmtjm8vm","_id":"cji4rdze9001o2e01o2zihyj9"},{"post_id":"cji4rdzd900072e01gu51z7ex","tag_id":"cji4rdze4001e2e01wtbfgsox","_id":"cji4rdzeb001r2e01de74x2y1"},{"post_id":"cji4rdzdc00092e01l1xeovvs","tag_id":"cji4rdze4001e2e01wtbfgsox","_id":"cji4rdzec001v2e01chzdjf3a"},{"post_id":"cji4rdzde000a2e01z3lad7yd","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzed00222e013lvf2b9q"},{"post_id":"cji4rdzde000a2e01z3lad7yd","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzee00242e017yrrs4xi"},{"post_id":"cji4rdzdh000d2e01qxrefsqi","tag_id":"cji4rdzed00212e01pffvf9lr","_id":"cji4rdzef00272e01uutxiowj"},{"post_id":"cji4rdzdh000d2e01qxrefsqi","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzef00282e01bveog41a"},{"post_id":"cji4rdzdi000e2e01nfv5nofl","tag_id":"cji4rdze4001e2e01wtbfgsox","_id":"cji4rdzeg002a2e01awr2vba0"},{"post_id":"cji4rdzdn000i2e01e0e80w3t","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzei002d2e01eyaqr9tk"},{"post_id":"cji4rdzdn000i2e01e0e80w3t","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzei002e2e01fcku4ez5"},{"post_id":"cji4rdzdp000j2e01pi5l4zxb","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzek002h2e01xgbtiw9y"},{"post_id":"cji4rdzdp000j2e01pi5l4zxb","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzek002i2e01qzwll3dh"},{"post_id":"cji4rdzdr000n2e01u6930lhf","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzel002l2e018j05wy4l"},{"post_id":"cji4rdzdr000n2e01u6930lhf","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzel002m2e01qm856f88"},{"post_id":"cji4rdzds000p2e012599lzfd","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzen002p2e01vawjcxhv"},{"post_id":"cji4rdzds000p2e012599lzfd","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzen002q2e01lj8i9ccw"},{"post_id":"cji4rdzdu000t2e01koqemz5r","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzeo002t2e01yj49lz8w"},{"post_id":"cji4rdzdu000t2e01koqemz5r","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzeo002u2e015b32mpn5"},{"post_id":"cji4rdzdw000w2e018zqafriy","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzep002x2e01t35g461a"},{"post_id":"cji4rdzdw000w2e018zqafriy","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzeq002y2e01bf8v2vnd"},{"post_id":"cji4rdzdy00102e013zkegnvw","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzer00312e01q7t1wf2o"},{"post_id":"cji4rdzdy00102e013zkegnvw","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzes00322e01qtmcog46"},{"post_id":"cji4rdzdz00142e01xw2dy74c","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzet00352e01abex56q4"},{"post_id":"cji4rdzdz00142e01xw2dy74c","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzet00362e01c6ukad8k"},{"post_id":"cji4rdze100172e019x7ml08w","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzeu00392e01prxlzgd5"},{"post_id":"cji4rdze100172e019x7ml08w","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzeu003a2e01qik1ptll"},{"post_id":"cji4rdze2001a2e01jl6br241","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzew003d2e01f6et5pmo"},{"post_id":"cji4rdze2001a2e01jl6br241","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzew003e2e0107vc0xyd"},{"post_id":"cji4rdze3001c2e012vnymvzm","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzex003h2e01zgrpb632"},{"post_id":"cji4rdze3001c2e012vnymvzm","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzex003i2e01ee4haa0i"},{"post_id":"cji4rdze5001f2e01pgkkqok2","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzey003l2e01iyqkygca"},{"post_id":"cji4rdze5001f2e01pgkkqok2","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzey003m2e01kqhwquu6"},{"post_id":"cji4rdze6001i2e017e8ta02f","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzez003o2e01vs9ohdb5"},{"post_id":"cji4rdze8001m2e01aoi33hcs","tag_id":"cji4rdzey003n2e01gw4ipcr7","_id":"cji4rdzez003q2e011gq7n620"},{"post_id":"cji4rdzea001p2e01e6fq6qvj","tag_id":"cji4rdzeb001s2e01l6w8toog","_id":"cji4rdzf0003s2e015v1njsz4"},{"post_id":"cji4rdzea001p2e01e6fq6qvj","tag_id":"cji4rdzec001x2e0135r4laeo","_id":"cji4rdzf1003t2e01rrr82gyp"}],"Tag":[{"name":"Mac","_id":"cji4rdzd800052e01u8dp7shi"},{"name":"Hexo","_id":"cji4rdzdg000c2e01odqpcjna"},{"name":"GitHub","_id":"cji4rdzdk000h2e01g9ekdtlm"},{"name":"博客","_id":"cji4rdzdq000m2e0156eiairm"},{"name":"Markdown","_id":"cji4rdzdu000s2e01bjnsozb5"},{"name":"教程","_id":"cji4rdze100192e01jmtjm8vm"},{"name":"python","_id":"cji4rdze4001e2e01wtbfgsox"},{"name":"机器学习","_id":"cji4rdzeb001s2e01l6w8toog"},{"name":"算法","_id":"cji4rdzec001x2e0135r4laeo"},{"name":"Python","_id":"cji4rdzed00212e01pffvf9lr"},{"name":"推荐系统","_id":"cji4rdzey003n2e01gw4ipcr7"}]}}