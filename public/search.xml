<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习之决策树(C4.5算法)]]></title>
    <url>%2F2018%2F04%2F19%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91-C4-5%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1.决策树简介我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。 序号 天气 温度 湿度 风速 高尔夫 1 晴 炎热 高 弱 进行 2 晴 炎热 高 强 进行 3 阴 炎热 高 弱 取消 4 雨 适中 高 弱 取消 5 雨 寒冷 正常 弱 取消 6 雨 寒冷 正常 强 进行 7 阴 寒冷 正常 强 进行 8 晴 适中 高 弱 进行 9 晴 寒冷 正常 弱 进行 10 雨 适中 正常 弱 进行 11 晴 适中 正常 强 进行 12 阴 适中 高 强 进行 13 阴 炎热 正常 弱 取消 14 雨 适中 高 强 取消 正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。 2.C4.5算法上古之神赐予你智慧：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。 C4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设 类标记元组训练集记为$D$，$|D|$表示元组个数，例如上述数据$|D|=14$。 类标号记为$m$，例如上述数据$m=2$，分别为进行、取消。 属性集合记为为$C$，例如$C_1$为天气情况，$C_iD$是$D$中$C_i$类元组的集合，$|C_iD|$为$C_iD$中元组个数。 属性标号记为$n$，例如上述数据$n=2$，分别为天气、温度、湿度、风速。 $p_i$表示类别$i$的概率，比如$p(进行)=\frac{9}{14}$。 2.1信息增益信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。 计算类别信息熵:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息 Info(D)=-\sum_{i=1}^{m}p_ilog_2(p_i) Info(D)=-\frac{9}{14}log_2\frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940计算每个属性的信息熵:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。 Info_A(D)=\sum_{j=1}^{v}\frac{|D_j|}{|D|}*Info(D_j) Info(天气)=\frac{5}{14}*[-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5}]+\frac{4}{14}*[-\frac{4}{4}log_2\frac{4}{4}]+\frac{5}{14}*[-\frac{3}{5}log_2\frac{3}{5}-\frac{2}{5}log_2\frac{2}{5}]=0.694 Info(温度)=0.911,Info(湿度)=0.739,Info(风速)=0.892计算信息增益:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。 Gain(A)=Info(D)-Info_A(D) Gain(天气)=Info(D)-Info(天气)=0.940-0.694=0.246 Gain(温度)=0.029,Gain(湿度)=0.15,Gain(风速)=0.048但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。 2.2信息增益率计算属性分裂信息度量:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用信息增益 / 内在信息表示，信息增益率会导致属性的重要性随着内在信息的增大而减小（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它），这样算是对单纯用信息增益有所补偿。信息增益率定义如下 SplitInfo_A(D)=-\sum_{j=1}^{v}\frac{|D_j|}{|D|}*log_2\frac{|D_j|}{|D|} SplitInfo(天气)=-\frac{5}{14}*log_2\frac{5}{14}-\frac{5}{14}*log_2\frac{5}{14}-\frac{4}{14}*log_2\frac{4}{14}=1.577 SplitInfo(温度)=1.556,SplitInfo(湿度)=1.0,SplitInfo(风速)=0.985 GainRatio(A)=\frac{Gain(A)}{SplitInfo(A)} GainRatio(天气)=\frac{Gain(天气)}{SplitInfo(天气)}=\frac{0.246}{1.577}=0.155 GainRatio(温度)=0.0186,GainRatio(湿度)=0.151,GainRatio(风速)=0.048天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。 3.树剪枝决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。 3.1先剪枝先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法 当决策树达到一定的高度就停止决策树的生长。 到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。 计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。 3.2后剪枝后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。 把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为 \frac{\sum (E_i+0.5*L)}{\sum N_i}这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。 假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(e为分布的固有属性，可以统计出来)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。 E(subtree\_err\_count)=N*e var(subtree\_err\_count)=\sqrt{N*e*(1-e)}把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为 E(leaf\_err\_count)=N*e使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝 E(subtree\_err\_count)-var(subtree\_err\_count)>E(leaf\_err\_count)上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。 4.Sklearn实现决策树我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。 1234567891011121314151617181920212223from sklearn.datasets import load_irisfrom sklearn import tree#引入数据iris=load_iris()X=iris.datay=iris.target#训练数据和模型,采用ID3或C4.5训练clf=tree.DecisionTreeClassifier(criterion='entropy')clf=clf.fit(X,y)#引入graphviz模块用来导出图,结果图如下所示import graphvizdot_data=tree.export_graphviz(clf,out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True,rounded=True, special_characters=True)graph=graphviz.Source(dot_data)graph.view() 5.实际使用技巧 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。 训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (sample_weight) 的和归一化为相同的值。 考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。 通过 export 功能可以可视化您的决策树。使用 max_depth=3作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。 填充树的样本数量会增加树的每个附加级别。使用 max_depth 来控制树的大小防止过拟合。 通过使用 min_samples_split 和 min_samples_leaf 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之Sklearn使用教程]]></title>
    <url>%2F2018%2F04%2F15%2FPython%E4%B9%8BSklearn%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.Sklearn简介Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点： 简单高效的数据挖掘和数据分析工具 让每个人能够在复杂环境中重复使用 建立NumPy、Scipy、MatPlotLib之上 2.Sklearn安装Sklearn安装要求Python(&gt;=2.7 or &gt;=3.3)、NumPy (&gt;= 1.8.2)、SciPy (&gt;= 0.13.3)。如果已经安装NumPy和SciPy，安装scikit-learn可以使用pip install -U scikit-learn。 3.Sklearn通用学习模式Sklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，4.Sklearn datasets中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过MatPlotLib等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。 123456789101112131415161718192021222324252627282930313233from sklearn import datasets#引入数据集,sklearn包含众多数据集from sklearn.model_selection import train_test_split#将数据分为测试集和训练集from sklearn.neighbors import KNeighborsClassifier#利用邻近点方式训练数据###引入数据###iris=datasets.load_iris()#引入iris鸢尾花数据,iris数据包含4个特征变量iris_X=iris.data#特征变量iris_y=iris.target#目标值X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)#利用train_test_split进行将训练集和测试集进行分开，test_size占30%print(y_train)#我们看到训练数据的特征值分为3类'''[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] '''###训练数据###knn=KNeighborsClassifier()#引入训练方法knn.fit(X_train,y_train)#进行填充测试数据进行训练###预测数据###print(knn.predict(X_test))#预测特征值'''[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0 1 2 1 0 0 1 0 2]'''print(y_test)#真实特征值'''[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0 1 2 1 0 0 1 0 2]''' 4.Sklearn datasetsSklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的load_iris数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过load_sample_images()来引入图片。 除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。 123456789from sklearn import datasets#引入数据集#构造的各种参数可以根据自己需要调整X,y=datasets.make_regression(n_samples=100,n_features=1,n_targets=1,noise=1)###绘制构造的数据###import matplotlib.pyplot as pltplt.figure()plt.scatter(X,y)plt.show() 5.Sklearn Model的属性和功能数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数y=0.3x+1，我们可通过_coef得到模型的系数为0.3，通过_intercept得到模型的截距为1。 1234567891011121314151617181920212223242526272829from sklearn import datasetsfrom sklearn.linear_model import LinearRegression#引入线性回归模型###引入数据###load_data=datasets.load_boston()data_X=load_data.datadata_y=load_data.targetprint(data_X.shape)#(506, 13)data_X共13个特征变量###训练数据###model=LinearRegression()model.fit(data_X,data_y)model.predict(data_X[:4,:])#预测前4个数据###属性和功能###print(model.coef_)'''[ -1.07170557e-01 4.63952195e-02 2.08602395e-02 2.68856140e+00 -1.77957587e+01 3.80475246e+00 7.51061703e-04 -1.47575880e+00 3.05655038e-01 -1.23293463e-02 -9.53463555e-01 9.39251272e-03 -5.25466633e-01]'''print(model.intercept_)#36.4911032804print(model.get_params())#得到模型的参数#&#123;'copy_X': True, 'normalize': False, 'n_jobs': 1, 'fit_intercept': True&#125;print(model.score(data_X,data_y))#对训练情况进行打分#0.740607742865 6.Sklearn数据预处理数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。 例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。 123456789101112131415from sklearn import preprocessingimport numpy as npa=np.array([[10,2.7,3.6], [-100,5,-2], [120,20,40]],dtype=np.float64)print(a)print(preprocessing.scale(a))#将值的相差度减小'''[[ 10. 2.7 3.6] [-100. 5. -2. ] [ 120. 20. 40[[ 0. -0.85170713 -0.55138018] [-1.22474487 -0.55187146 -0.852133 ] [ 1.22474487 1.40357859 1.40351318]]''' 我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为0.511111111111，预处理后模型评分为0.933333333333，可以看到预处理对模型评分有很大程度的提升。 12345678910111213141516171819from sklearn.model_selection import train_test_splitfrom sklearn.datasets.samples_generator import make_classificationfrom sklearn.svm import SVCimport matplotlib.pyplot as plt###生成的数据如下图所示###plt.figureX,y=make_classification(n_samples=300,n_features=2,n_redundant=0,n_informative=2, random_state=22,n_clusters_per_class=1,scale=100)plt.scatter(X[:,0],X[:,1],c=y)plt.show()###利用minmax方式对数据进行规范化###X=preprocessing.minmax_scale(X)#feature_range=(-1,1)可设置重置范围X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)clf=SVC()clf.fit(X_train,y_train)print(clf.score(X_test,y_test))#0.933333333333#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升 7.交叉验证交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。 机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：训练集、验证集和测试集。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。 以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。 12345678910111213141516171819from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier###引入数据###iris=load_iris()X=iris.datay=iris.target###训练数据###X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)#引入交叉验证,数据分为5组进行训练from sklearn.model_selection import cross_val_scoreknn=KNeighborsClassifier(n_neighbors=5)#选择邻近的5个点scores=cross_val_score(knn,X,y,cv=5,scoring='accuracy')#评分方式为accuracyprint(scores)#每组的评分结果#[ 0.96666667 1. 0.93333333 0.96666667 1. ]5组数据print(scores.mean())#平均评分结果#0.973333333333 那么是否n_neighbor=5便是最好呢，我们来调整参数来看模型最终训练分数。 12345678910111213141516171819202122from sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import cross_val_score#引入交叉验证import matplotlib.pyplot as plt###引入数据###iris=datasets.load_iris()X=iris.datay=iris.target###设置n_neighbors的值为1到30,通过绘图来看训练分数###k_range=range(1,31)k_score=[]for k in k_range: knn=KNeighborsClassifier(n_neighbors=k) scores=cross_val_score(knn,X,y,cv=10,scoring='accuracy')#for classfication k_score.append(loss.mean())plt.figure()plt.plot(k_range,k_score)plt.xlabel('Value of k for KNN')plt.ylabel('CrossValidation accuracy')plt.show()#K过大会带来过拟合问题,我们可以选择12-18之间的值 我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择2-fold Cross Validation,Leave-One-Out Cross Validation等方法来分割数据，比较不同方法和参数得到最优结果。 我们将上述代码中的循环部分改变一下，评分函数改为neg_mean_squared_error，便得到对于不同参数时的损失函数。 1234for k in k_range: knn=KNeighborsClassifier(n_neighbors=k) loss=-cross_val_score(knn,X,y,cv=10,scoring='neg_mean_squared_error')# for regression k_score.append(loss.mean()) 8.过拟合问题什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。 我们先举例如何辨别overfitting问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。 12345678910111213141516171819202122232425from sklearn.model_selection import learning_curvefrom sklearn.datasets import load_digitsfrom sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as np#引入数据digits=load_digits()X=digits.datay=digits.target#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下train_size,train_loss,test_loss=learning_curve( SVC(gamma=0.1),X,y,cv=10,scoring='neg_mean_squared_error', train_sizes=[0.1,0.25,0.5,0.75,1])train_loss_mean=-np.mean(train_loss,axis=1)test_loss_mean=-np.mean(test_loss,axis=1)plt.figure()#将每一步进行打印出来plt.plot(train_size,train_loss_mean,'o-',color='r',label='Training')plt.plot(train_size,test_loss_mean,'o-',color='g',label='Cross-validation')plt.legend('best')plt.show() 如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。 下面我们通过修改gamma参数来修正过拟合问题。 1234567891011121314151617181920212223242526from sklearn.model_selection import validation_curve#将learning_curve改为validation_curvefrom sklearn.datasets import load_digitsfrom sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as np#引入数据digits=load_digits()X=digits.datay=digits.target#改变param来观察Loss函数情况param_range=np.logspace(-6,-2.3,5)train_loss,test_loss=validation_curve( SVC(),X,y,param_name='gamma',param_range=param_range,cv=10, scoring='neg_mean_squared_error')train_loss_mean=-np.mean(train_loss,axis=1)test_loss_mean=-np.mean(test_loss,axis=1)plt.figure()plt.plot(param_range,train_loss_mean,'o-',color='r',label='Training')plt.plot(param_range,test_loss_mean,'o-',color='g',label='Cross-validation')plt.xlabel('gamma')plt.ylabel('loss')plt.legend(loc='best')plt.show() 通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。 9.保存模型我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。 123456789101112131415161718from sklearn import svmfrom sklearn import datasets#引入和训练数据iris=datasets.load_iris()X,y=iris.data,iris.targetclf=svm.SVC()clf.fit(X,y)#引入sklearn中自带的保存模块from sklearn.externals import joblib#保存modeljoblib.dump(clf,'sklearn_save/clf.pkl')#重新加载model，只有保存一次后才能加载modelclf3=joblib.load('sklearn_save/clf.pkl')print(clf3.predict(X[0:1]))#存放model能够更快的获得以前的结果 10.推广更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。]]></content>
      <categories>
        <category>Python库</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之SVM支持向量机（二）]]></title>
    <url>%2F2018%2F04%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1.知识回顾机器学习之SVM支持向量机（一）中我们介绍了SVM损失函数、最大间隔分类、为什么SVM能形成最大间隔分类器、核函数、SVM中Gaussian Kernel的使用知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。 2.函数间隔和几何间隔上文我们从logistic Regression损失函数推导出SVM损失函数，本文我们采用另一种方法得到SVM损失函数。首先定义超平面可以用分类函数$f(x)=w^Tx+b$表示，当$f(x)$等于0的时候，$x$便是位于超平面上的点，而$f(x)$大于0对应$y=1$的数据点，$f(x)$小于0对应于$y=-1$的点，如下图所示： 在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到超平面的远近，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确。因此我们用$y*(w^Tx+b)$的正负性来判定分类的正确性，于是引出函数间隔的概念。 定义函数间隔$\hat{\gamma}$： \hat{\gamma}=y(w^Tx+b)=yf(x)而超平面$(w,b)$关于训练数据集T中所有样本点$(x_i,y_i)$的函数间隔最小值，便为超平面$(w,b)$关于训练数据集T的函数间隔： \hat{\gamma}=min\hat{\gamma}但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$，则函数间隔的值$f(x)$却变成了原来的2倍(虽然此时超平面没有改变)，所以只有函数间隔还是不够的。 但我们可以对法向量$w$增加些约束条件，从而引出真正定义点到超平面的距离。假设对于一点$x$，令其垂直投影到超平面上的点对应为$x_0$，$w$是垂直于超平面的一个向量，$r$为样本$x$到分类间隔的距离，如下图所示。 x=x_0+\gamma\frac{w}{||w||}其中$||w||$表示范数，又由于$x_0$是超平面上的点，满足$f(x_0)=0$，代入超平面的方程$w^Tx+b=0$，我们得到： \gamma=\frac{w^T+b}{||w||}=\frac{f(x)}{||w||}为了得到$\gamma$绝对值，将$\gamma$乘上相应类别$y$，即可得到几何间隔： \tilde{r}=yr=\frac{\hat{\gamma}}{||w||}从上述定义我们能够看到，几何间隔就是函数间隔除以$||w||$，而且函数间隔$y(w^Tx+b)=yf(x)$，实际上就是$|f(x)|$，几何间隔$\frac{f(x)}{||w||}$才是直观上的点到超平面的距离。 对一个数据点进行分类，当超平面离数据点的间隔越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化间隔值。于是最大间隔分类器的目标函数定义为$max\tilde{\gamma}$。同时需满足如下条件 y_i(w^Tx_i+b)=\hat{\gamma_i}\ge\hat{\gamma},i=1,2,3,...,n此处令函数间隔$\hat{\gamma}$等于1（之所以令$\hat{\gamma}=1$是为了方便推导，且这样做对目标函数的优化没有影响）。则上述目标函数转换成： max\frac{1}{||w||},s.t.,y_i(w^Tx_i+b)\ge1,i=1,2,3,...,n3.原始问题到对偶问题的求解接着考虑我们之前的目标函数，由于求$\frac{1}{||w||}$的最大值相当于求$\frac{1}{2}||w||^2$的最小值，所以目标函数转换为 min\frac{1}{2}||w||^2 ,s.t.,y_i(w^Tx_i+b)\ge1,i=1,2,3,...,n现在目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。由于此问题的特殊结构，我们可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解。 那什么是拉格朗日对偶性呢？简单来说就是通过给每一个约束条件加上一个拉格朗日乘子$\alpha$，定义拉格朗日函数为： L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^{n}\alpha_i(y_i(w^Tx_i+b)-1)然后令： \theta(w)=\max_{\alpha_i\ge0}L(w,b,\alpha)当某个条件不满足时，例如$y_i(w^Tx+b)&lt;1$，那么有$\theta(w)=\infty$。而当所有约束条件都满足时，则有$\theta(w)=\frac{1}{2}||w||^2$，亦即最初要最小化的量。目标函数则转换为： \min_{w,b}\theta(w)=\min_{w,b}\max_{\alpha_i\ge0}L(w,b,\alpha)=p^*这里用$p^*$表示这个问题的最优值，和最初的问题是等价的。如果直接求解那么我们将面对$w,b$两个参数，而$\alpha_i$又是不等式约束，这个求解过程不好做，我们把最小和最大的位置交换一下： \max_{\alpha_i\ge0}\min_{w,b}L(w,b,\alpha)=d^*交换以后的新问题就是原始问题的对偶问题，新问题的最优值用$d^$表示，而且有$d^\le p^$，在*满足某些条件的情况下，这两者相等，此时便可以通过求解对偶问题来间接的求解原始问题。 此处满足某些条件的情况下，两者等价，此处的满足某些条件便是满足KKT条件。KKT最优化数学模型表示成下列标准形式: \min f(x) s.t. h_j(x)=0,j=1,2,3,...,p g_k(x)\le0,k=1,2,3,...,q x\in X\subset R^n其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别为等式约束和不等式约束的数量。 凸优化概念:$X\subset R^n$为一凸集，$f:X-&gt;R$为一凸函数。凸优化便是寻找一点$x^\in X$，是的每一$x\in X$满足$f(x^)\le f(x)$。 KKT条件意义是非线性规划问题能有最优化解法的必要和充分条件。 KKT条件就是上面最优化数学模型的标准形式中的最小点$x^*$必须满足下面的条件: h_j(x^*)=0,j=1,2,3,...,p g_k(x^*)\le 0,k=1,2,3,...,q \nabla f(x^*)+\sum_{j=1}^{p}\lambda_j\nabla h_j(x^*)+\sum_{k=1}^{q}\mu_k \nabla g_k(x^*)=0 \lambda_j \neq0,\mu \ge0,\mu_k g_k(x^*)=0此处我们不做详细证明为什么满足KKT条件。原始问题通过满足KKT条件，已经转换成对偶问题。求解对偶问题首先要让$L(w,b,\alpha)$关于$w,b$的最小化，然后求对$\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。下面为具体求解过程。 首先固定$\alpha$，要让L关于$w,b$最小化，我们分别对$w,b$求偏导数。 \frac{\partial L}{\partial w}=0 \Rightarrow w= \sum_{i=1}^{n}\alpha_iy_ix_i \frac{\partial L}{\partial b}=0 \Rightarrow \sum_{i=1}^{n}\alpha_iy_i=0将上述结果代入到之前的L得到： L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^{n}\alpha_i(y_i(w^Tx_i+b)-1) =\frac{1}{2}w^Tw-\sum_{i=1}^{n}\alpha_iy_iw^Tx_i-\sum_{i=1}^{n}\alpha_iy_ib+\sum_{i=1}^{n}\alpha_i =\frac{1}{2}w^T\sum_{i=1}^{n}\alpha_iy_ix_i-\sum_{i=1}^{n}\alpha_iy_iw^Tx_i-\sum_{i=1}^{n}\alpha_iy_ib+\sum_{i=1}^{n}\alpha_i =\frac{1}{2}w^T\sum_{i=1}^{n}\alpha_iy_ix_i-w^T\sum_{i=1}^{n}\alpha_iy_ix_i-\sum_{i=1}^{n}\alpha_iy_ib+\sum_{i=1}^{n}\alpha_i =-\frac{1}{2}w^T\sum_{i=1}^{n}\alpha_iy_ix_i-\sum_{i=1}^{n}\alpha_iy_ib+\sum_{i=1}^{n}\alpha_i =-\frac{1}{2}w^T\sum_{i=1}^{n}\alpha_iy_ix_i-b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i =-\frac{1}{2}(\sum_{i=1}^{n}\alpha_iy_ix_i)^T\sum_{i=1}^{n}\alpha_iy_ix_i-b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i =-\frac{1}{2}\sum_{i=1}^{n}\alpha_iy_ix_i^T\sum_{i=1}^{n}\alpha_iy_ix_i-b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i =-\frac{1}{2}\sum_{i=1,j=1}^{n}\alpha_iy_ix_i^T\alpha_jy_jx_j-b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i =\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j然后求对$\alpha$的极大，即是关于对偶问题的最优化问题。 \max_{\alpha}\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^{n}\alpha_iy_ix_i^T\alpha_jy_jx_j s.t.,\alpha_i\ge0,i=1,2,3,...,n \sum_{i=1}^{n}\alpha_iy_i=0我们已经知道$x_i,x_j$的值，便可利用SMO算法求解$\alpha_i$，此处不详细介绍SMO算法。同时根据$w=\sum_{i=1}^{n}\alpha_iy_ix_i$我们便可求出$w$，然后通过下式得到$b$。 b^*=-\frac{\max_{i:y(i)=-1}w^Tx_i+\min_{i:y(i)=1}w^Tx_i}{2}至此我们便可得出分类超平面和分类决策函数。 4.松弛变量处理outliers方法实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。 为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即 y_i(w^Tx_i+b)\ge1-\varepsilon_i,i=1,2,3,...,n其中$\varepsilon$称为松弛变量，对应数据点$x_i$允许偏离分类决策函数的量。当然如果我们允许$\varepsilon_i$任意大的话，那任意的超平面都是符合条件的。所以我们在原来的目标函数后面再加上一项，使得这些$\varepsilon_i$的总和也要尽量小。 \min\frac{1}{2}||w||^2+C\sum_{i=1}^{n}\varepsilon_i s.t.,y_i(w^Tx_i+b)\ge1-\varepsilon_i,i=1,2,3,...,n \varepsilon_i\ge0,i=1,2,3,...,n此处和机器学习之SVM支持向量机（一）中的损失函数不同的是加入$\varepsilon_i$后损失函数第一项便不为0。下述目标函数中第一项相当于现在的$\varepsilon_i$。 min_{\theta}C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^{2}那么现在用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示: L(w,b,\varepsilon,\alpha,r)=\frac{1}{2}||w||^2+C\sum_{i=1}^{n}\varepsilon_i-\sum_{i=1}^{n}\alpha_i(y_i(w^Tx_i+b)-1+\varepsilon_i)-\sum_{i=1}^{n}r_i\varepsilon_i分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。 5.Sklearn实现SVM支持向量机我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在机器学习之SVM支持向量机（一）中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇SVM核函数的应用。 5.1线性123456789101112131415161718192021222324252627from sklearn import svmimport numpy as npimport matplotlib.pyplot as pltnp.random.seed(0)x=np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]#正态分布产生数字20行2列y=[0]*20+[1]*20#20个class0,20个class1clf=svm.SVC(kernel='linear')#使用线性核clf.fit(x,y)w=clf.coef_[0]#获取wa=-w[0]/w[1]#斜率#画图xx=np.linspace(-5,5)yy=a*xx-(clf.intercept_[0])/w[1]b=clf.support_vectors_[0]yy_down=a*xx+(b[1]-a*b[0])b=clf.support_vectors_[-1]yy_up=a*xx+(b[1]-a*b[0])plt.figure(figsize=(8,4))plt.plot(xx,yy)plt.plot(xx,yy_down)plt.plot(xx,yy_up)plt.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=80)plt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.Paired)plt.axis('tight')plt.show() 5.2非线性123456789101112131415161718192021222324252627282930313233343536import numpy as npimport matplotlib.pyplot as pltfrom sklearn.datasets import make_moonsfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import LinearSVCX, y = make_moons( n_samples=100, noise=0.15, random_state=42 )def plot_dataset(X, y, axes): plt.plot( X[:,0][y==0], X[:,1][y==0], "bs" ) plt.plot( X[:,0][y==1], X[:,1][y==1], "g^" ) plt.axis( axes ) plt.grid( True, which="both" ) plt.xlabel(r"$x_l$") plt.ylabel(r"$x_2$")# contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）def plot_predict(clf, axes): x0s = np.linspace(axes[0], axes[1], 100) x1s = np.linspace(axes[2], axes[3], 100) x0, x1 = np.meshgrid( x0s, x1s ) X = np.c_[x0.ravel(), x1.ravel()] y_pred = clf.predict( X ).reshape( x0.shape ) y_decision = clf.decision_function( X ).reshape( x0.shape ) plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=0.5 ) plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=0.2 )polynomial_svm_clf = Pipeline([ ("poly_featutres", PolynomialFeatures(degree=3)), ("scaler", StandardScaler()), ("svm_clf", LinearSVC(C=10, loss="hinge", random_state=42) ) ])#多项式核函数polynomial_svm_clf.fit( X, y )plot_dataset( X, y, [-1.5, 2.5, -1, 1.5] )plot_predict( polynomial_svm_clf, [-1.5, 2.5, -1, 1.5] )plt.show() 6.推广更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之SVM支持向量机（一）]]></title>
    <url>%2F2018%2F03%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。 1.SVM损失函数针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。 首先我们先复习下Logistic Regression Function h_{\theta}=\frac{1}{1+e^{-\theta^Tx}}如果$y=1$，我们希望$h_{\theta}\approx1$，那么$\theta^Tx\gg0$。如果$y=0$，我们希望$h_{\theta}\approx0$，那么$\theta^Tx\ll0$。我们以Logistic Regression为例 LR Cost Example=-\left( (ylogh_\theta(x))+(1-y)log(1-h_\theta(x))\right) =-ylog\frac{1}{1+e^{-\theta^Tx}}-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}}) 当$y=1$时，此时$\theta^Tx\gg0$，上述公式为$-ylog\frac{1}{1+e^{-\theta^Tx}}$，其中$z=\theta^Tx$。我们将曲线分为两段，下图中取$z=1$点，粉色线部分我们定义为$cost_1(z)$。 当$y=0$时，此时$\theta^Tx\ll0$，上述公式为$-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})$，其中$z=\theta^Tx$。我们将曲线分为两段，下图中取$z=-1$点，粉色线部分我们定义为$cost_0(z)$。 $cost_1(z)$与$cost_0(z)$便是我们希望的Cost Function曲线，和Logistic Function曲线非常接近，$cost_1(z)$与$cost_0(z)$分别代表y=1和y=0时的目标函数定义。 Logistic Regression的损失函数: min_{\theta}\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}(-logh_{\theta}(x^{(i)}))+(1-y^{(i)})(-log(1-h_{\theta}(x^{(i)})))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}因此对于SVM，我们得到: min_{\theta}\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}因为常数项对我们结果没有影响，因此去掉上述方程式之中的m。Logstic Regression损失函数中包含两项，训练样本的代价项和正则项，形式类似于$A+\lambda B$，我们通过设置$\lambda$来平衡这两项。对于SVM来说，我们依照惯例设置损失函数为$CA+B$，利用C对两项进行平衡。其中C与Logistic Regression损失函数中的$\frac{1}{\lambda}$作用一致，因此我们便得到SVM的损失函数。 min_{\theta}C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^{2}2.最大间隔分类SVM希望最小化代价参数，应该如何做呢？我们现在来看当损失函数最小时，我们需要做什么。 min_{\theta}C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^{2} 当为正样本时$y=1$，我们希望$\theta^Tx\ge1$，而不是 $\theta^Tx\ge0$。 当为正样本时$y=0$，我们希望$\theta^Tx\le-1$，而不是$\theta^Tx\le0$。 我们设C为非常大的值，例如1000000。 当$y^{(i)}=1$时，$\theta^Tx^{(i)}\ge1$，此时SVM损失函数中第一项为0。 当$y^{(i)}=0$时，$\theta^Tx^{(i)}\le-1$，此时SVM损失函数中第一项为0。 那么我们便得到: minC*0+\frac{1}{2}\sum_{i=1}^{m}\theta_j^2 约束条件1:如果$y^{(i)}=1$，$\theta^Tx^{(i)}\ge1$。 约束条件2:如果$y^{(i)}=0$，$\theta^Tx^{(i)}\le-1$。 SVM是一个最大间隔分类器，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在模块3中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。 我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。 3.SVM最大间隔分类首先我们来看两个向量内积的表现形式。假设向量u,v均为二维向量，我们知道u,v的内积为$u^Tv=u_1v_1+u_2v_2$，表现在坐标上便为下图所示。首先将v向量投影到u向量上，记长度为p。其中p值有正负，与u方向相同为正，方向相反为负。uv两向量内积可以表示为 u^Tv=||u||\cdot||v||\cdot cos\theta=||u||\cdot p=u_1v_1+u_2v_2 现在我们来看SVM损失函数: min_{\theta}C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^{2}由于C设置的非常大，那么SVM损失函数为: min_{\theta} \frac{1}{2}\sum_{j=1}^{n}\theta_j^2 约束条件1:如果$y^{(i)}=1$，$\theta^Tx^{(i)}\ge1$。 约束条件2:如果$y^{(i)}=0$，$\theta^Tx^{(i)}\le-1$。 下面我们举例说明SVM，为了简化，假设n=2,$\theta_0=0$，我们得到 min\frac{1}{2}\sum_{j=1}^{2}\theta_j^2=\frac{1}{2}{(\theta_1^2+\theta_2^2)}=\frac{1}{2}\left (\sqrt{\theta_1^2+\theta_2^2} \right )^2=\frac{1}{2}||\theta||^2我们来更深层次的理解$\theta^Tx^{(i)}$，表现形式和上述$u^Tv$相同。利用坐标表示则为 \theta^Tx^{(i)}=p^{(i)}\cdot||\theta||=\theta_1x_1^{(i)}+\theta_2x_1^{(2)}$\theta^Tx^{(i)}$我们可以利用$p^{(i)}\cdot||\theta||$表示，同时SVM随时函数目标是极小化$||\theta^2||$。 下面有两种分类方式，SVM为什么要选择第二种当作分类呢。我们想最小化$||\theta||$，并且要满足$p^{(i)}\cdot ||\theta||\ge1$，但左边坐标中$p^{(1)}$较小，那么我们便要让$||\theta||$更大，不满足最小化$||\theta||$的需求。坐标2中$p^{(1)}$更大，那么$||\theta||$便较小，满足最小化$||\theta||$的需求。SVM便是通过最大化分类间隔来让$||\theta||$更小，这便是SVM中为什么要最大化分类间隔。 4.核函数上述介绍线性分类，但对于非线性问题我们如何解决呢？对于非线性的决策边界，我们可以利用多项式拟合的方式进行预测。对于下面图片中的决策边界，我们令$f_1=x_1,f2=x_2,f3=x_1x_2,f4=x_1^2,f5=x_2^2,…$ $f_1,f_2,f_3…$为提取出来的特征。 定义预测方程$h_{\theta}(x)$为多项式sigmoid函数。$h_{\theta}(x)=g(\theta_0f_0+\theta_1f_1+\theta_2f_2+…+\theta_nf_n)$，其中$f_n$为x的幂次项组合。 当$\theta_0f_0+\theta_1f_1+\theta_2f_2+…+\theta_nf_n\ge0$时$h_{\theta}(x)=1$，否则$h_{\theta}(x)=0$。 那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。 x和l越相似，f越接近于1。x和l相差越远，f越接近于0。 下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。 下面我们来看SVM核分类预测的结果。引入核函数后，代数上的区别在于f变了，原来的f是$x_1,x_1^2…$，即$x_i$的幂次项乘积，另外几何来说可以更直观的表示如何分类。 假如我们将坐标上的所有数据点分为两类，红色圈内希望预测为y=1，圈外希望预测为y=0。通过训练数据集，我们得到一组$\theta(\theta_0,\theta_1,\theta_2,\theta_3)$值为$(-0.5,1,1,0)$以及三个landmark点(L1,L2,L3)。具体如何选取landmark点和训练生成$\theta$值在下面会详细介绍。 对于每个数据集内的点，我们首先计算它到(L1,L2,L3)各自的相似度，也就是核函数的值$f_1,f_2,f_3$，然后带入多项式$\theta_0f_0+\theta_1f_1+\theta_2f_2+…+\theta_nf_n$，当多项式大于0时预测结果为类内点，表示为正样本，y=1。否则预测为负样本，y=0。 5.SVM中Gaussian Kernel的使用上述中我们利用到$l^{(1)},l^{(2)},l^{(3)}$，但是我们如何达到这些landmark呢。首先我们来看L点的选取，上述提到Gaussian kernel $f_i$的计算。 f_i=similarity(x,l^{(i)})=exp\left ( -\frac{||x-l^{(i)}||^2}{2\sigma^2}\right)我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。 那么在这m个训练数据中，每一个训练数据$x^{(i)}$所得的特征向量（核函数）f中，总有一维向量的值为1（因为$x^{(i)}=l^{(i)}$）。于是每个特征向量f有m+1为维度，在SVM训练中，将Gaussian Kernel带入SVM损失函数，通过最小化该函数就可于得到参数$\theta$，并根据该参数$\theta$进行预测。 $\theta^Tf\ge0$，预测 y=1。 $\theta^Tf\le0$，预测y=0。 如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。 最后我们介绍下如何选取C和$\sigma^2$。由于$C=\frac{1}{\lambda}$，所以 C大，$\lambda$小，overfit，产生low bias，high variance。 C小，$\lambda$大，underfoot，underfoot，产生high bias，low variance。 对于方差$\sigma^2$ $\sigma^2$大，x-f相似性图像较为扁平。 $\sigma^2小$，x-f相似性图像较为窄尖。 通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。 由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。 5.推广更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之Logistic回归]]></title>
    <url>%2F2018%2F03%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BLogistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.Logistic回归简介线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。 2.Sigmoid函数为什么选择Sigmoid函数呢？我们目标是寻找函数进行分类，首先假设任意多类的分类问题（不仅是两类）。Exponential假设第i个体征对第k类问题的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第k类的概率正比于 exp(w_{k1}x_1+…+w_{kn}x_n)。因为一个数据点属于各类的概率之和为1，所以可以得到 P(y=k)=\frac{exp(\sum_{i=1}^{n}w_{ki}{x_i})}{\sum_{k'}exp(\sum_{i=1}^{n}w_{k'i}x_i)}现在回到两类（0,1）的情况，此时分母上只有两项 P(y=1)=\frac{exp(\sum_{i=1}^{n}w_{1i}{x_i})}{exp(\sum_{i=1}^{n}w_{1i}x_i)+exp(\sum_{i=1}^{n}w_{0i}x_i)}公式分子、分母同时除以分子，并设$w_i=w_{1i}-w_{0i}$，则有 P(y=1)=\frac{1}{1+exp(-\sum_{i=1}^{n}w_ix_i)}上述公式便是Logistic函数，参数$w_i$表示第i个特征对1类的贡献与0类的贡献的差值。 Sigmoid Function: f(x)=\frac{1}{1+e^{-x}}Sigmoid函数具有如下性质 函数连续且单调递增 函数关于（0,0.5）对称 $x\in(-\infty,\infty)$时$y\in(0,1)$ 123456789101112131415161718192021#plot sigmoid function import numpy as npimport matplotlib.pyplot as plt##sigmoid functionx=np.arange(-5,5,0.1)y=1/(1+np.exp(-x))#plotplt.figure()plt.plot(x,y,color='red',linewidth='2')ax=plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.spines['bottom'].set_position(('data',0))ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data',0))plt.xlabel('independent variable')plt.ylabel('dependent variable')plt.show() 3.Logistic回归推导 特征向量$X=(x_0,x_1,x_2…x_n)$，默认$x_0=1$。 $\theta=(\theta_0,\theta_1,\theta_2…\theta_n)$ $n$表示特征数量 $m$表示训练数据数量 线性回归函数为$z=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n=\theta^TX$。对于Logistic回归来说，其思想也基于线性回归（Logistic回归属于广义线性回归模型）。结合线性回归和Sigmoid函数，将线性回归得到的结果映射到Sigmoid函数之中，我们便得到目标函数。 h(X)=\frac{1}{1+e^{-\theta^TX}}我们可以把$h(X)$看成样本数据的概率密度函数，当$h(X)0.5$判断当前数据属于B类。对于上述函数$h(X)$，接下来我们需要做的便是怎样去估计参数$\theta$。 条件概率$P(y=1|X)$为某事件发生的概率，Logistic回归模型可以表示为 P(y=1|X)=\pi(X)=\frac{1}{1+e^{-\theta^TX}}条件概率$P(y=0|X)$为某事件不发生的概率，Logistic回归模型可以表示为 P(y=0|X)=1-\pi(X)=\frac{1}{1+e^{\theta^TX}}因此我们可以得到事件的发生比为 odds=\frac{P(y=1|X)}{P(y=0|X)}事件的发生和不发生为相互独立事件，样本数据结果记录为$(y_1,y_2…y_m)$。设$p_i=P(y_i=1|X_i)$为给定条件下得到$y_i=1$的概率，同样$1-p_i=P(y_i=0|X_i)$的概率，所以得到一个观测值的概率为$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$，最后参数估计时我们可以采用极大似然估计。 各个观测样本之间相互独立，那么它们的联合分布为各边缘分布的乘积，得到如下极大似然函数 L(\theta)=\prod_{i=1}^{m}[\pi(X_i)]^{y_i}[1-\pi(X_i)]^{1-y_i}目标便是求得使这一似然函数值最大的参数估计，于是函数取对数得到 lnL(\theta)=\sum_{i=1}^{m}\left \{ y_iln[\pi(X_i)] +(1-y_i)ln[1-\pi(X_i)] \right \} =\sum_{i=1}^{m}ln[1-\pi(X_i)]+\sum_{i=1}^{m}y_iln\frac{\pi(X_i)}{1-\pi(X_i)} =\sum_{i=1}^{m}ln[1-\pi(X_i)]+\sum_{i=1}^{m}y_i\theta^TX =\sum_{i=1}^{m}-ln[1+e^{\theta^Tx}]+\sum_{i=1}^{m}y_i\theta^TX通过上面得到的结论来求解使得似然函数最大化的参数向量，此处我们利用梯度下降算法求$\theta$。首先在前面乘上负的系数$-\frac{1}{m}$，所以$J(\theta)$最小时的$\theta$为最佳参数。 J(\theta)=-\frac{1}{m}lnL(\theta) =-\frac{1}{m}\left \{\sum_{i=1}^{m}-ln[1+e^{\theta^Tx}]+\sum_{i=1}^{m}y_i\theta^TX \right\}4.梯度下降算法4.1梯度下降算法简述实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。 从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。 4.2 梯度下降算法相关概念求解梯度下降算法之前，我们先了解相关概念。 步长（Learning Rate）：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。 特征（Feature）：即上述描述的$X$ 假设函数（Hypothesis Function）：监督学习中，为了拟合输入样本，而使用假设函数。 损失函数（Loss Function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。Logistic损失函数为 J(\theta)=-\frac{1}{m}\left \{\sum_{i=1}^{m}-ln[1+e^{\theta^Tx}]+\sum_{i=1}^{m}y_i\theta^TX \right\}我们利用梯度下降算法，目标便是找到一组$\theta$使得$J(\theta)$达到最小。 4.3梯度下降算法过程 随机选取一组$\theta$。 不断变化$\theta$，让$J(\theta)$变小，$\alpha$为学习步长。 \theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) 直到$J(\theta)$得到最小值，$\frac{\partial}{\partial\theta_k}J(\theta)$为$J(\theta)$对$\theta_k$的偏导。 \frac{\partial J(\theta)}{\partial\theta_j}=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{1+e^{\theta^TX}}e^{\theta^TX}X_{ij}-\sum_{i=1}^{m}y_iX_{ij} =\frac{1}{m}\sum_{i=1}^{m}X_{ij}[\frac{e^{\theta^TX}}{1+e^{\theta^Tx}}-y_i] =\frac{1}{m}\sum_{i=1}^{m}X_{ij}[\pi(X_i)-y_i]因此梯度下降算法的迭代最终表述为 \theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}X_{ij}[\pi(X_i)-y_i]梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\theta$时我们便能得到Logistic函数。 5.Logistic回归实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import StandardScalerdef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'cyan', 'gray') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot class samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl) # highlight test samples if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], c='blue', alpha=1.0, linewidth=1, marker='o', s=55, label='test set')iris = datasets.load_iris()X = iris.data[:, [2, 3]]y = iris.targetX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)#为了追求机器学习的最佳性能，我们将特征缩放sc = StandardScaler()sc.fit(X_train)#估算每个特征的平均值和标准差X_train_std=sc.transform(X_train)#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性X_test_std=sc.transform(X_test)X_combined_std = np.vstack((X_train_std, X_test_std))y_combined = np.hstack((y_train, y_test))#训练感知机模型lr = LogisticRegression(C=1000.0,random_state=0)#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序lr.fit(X_train_std, y_train)lr.predict_proba(X_test_std)#绘图plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105,150))plt.xlabel('petal length [standardized]')plt.ylabel('petal width [standardized]')plt.legend(loc='upper left')plt.show() 6.推广更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。 参考 1. https://www.zhihu.com/people/maigo/activities &#8617; 2. https://blog.csdn.net/programmer_wei/article/details/52072939 &#8617; 3. https://blog.csdn.net/javaisnotgood/article/details/78873819 &#8617;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之线性回归]]></title>
    <url>%2F2018%2F03%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.线性回归分析（ Linear Regression Analysis）线性回归分析（Regression Analysis）：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于监督学习。上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。 2.模型表达建立数学模型之前，我们先定义如下变量。 $x_i$表示输入数据（Feature） $y_i$表示输出数据（Target） $(x_i,y_i)$表示一组训练数据（Training example） m表示训练数据的个数 n表示特征数量 监督学习目标便是根据给定的训练数据，可以得到函数方法，使得假设函数$h$(hypothesis)满足$h(x)-&gt;y$。针对线性回归而言，函数$h(x)$表达式为 h(x)=\theta_0+\theta_1*x_i+\theta_2*x_2+...+\theta_n*x_n$为$方便我们使用矩阵来表达，$h(x)=\theta^T*x$，其中$\theta^T$为$\theta$的转置。为求解函数$h(x)$，我们希望找出一组$\theta$，使得$h(x)-y$无限趋近0，此处我们引入梯度下降算法求解问题。 3.梯度下降算法3.1梯度下降算法简述实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。 从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。 3.2 梯度下降算法相关概念求解梯度下降算法之前，我们先了解相关概念。 步长（Learning Rate）：步长决定梯度下降算法过程中，每步沿梯度负方向前进的长度。 特征（Feature）：即上述描述的$x_i,y_i$ 假设函数（Hypothesis Function）：监督学习中，为了拟合输入样本，而使用假设函数$h(x)$ 损失函数（Loss Function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小，意味着拟合的程度越好，对应的模型参数即为最优参数。线性回归损失函数为J(\theta)=\frac{1}{2m}*\sum_{i=1}^{n}(h(x)^{(i)}-y^{(i)})我们利用梯度下降算法，目标便是找到一组$\theta$使得$J(\theta)$达到最小。 3.3梯度下降算法过程 随机选取一组$\theta$。 不断变化$\theta$，让$J(\theta)$变小。\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)$j=0,1,2…n$，$\theta_j$是n+1个值同时变化。$\alpha$表示学习速率，目标求最小值，因此沿负梯度方向下降，故$\theta$前为负号。$\alpha\frac{\partial}{\partial\theta_j}$是对$J(\theta)$的偏导。 直到$J(\theta)​$得到最小值。 $\alpha\frac{\partial}{\partial\theta_j}$是对$J(\theta)$的偏导求解过程如下： \frac{\partial}{\partial\theta_j}J(\theta)=\frac{\partial}{\partial\theta_j}\frac{1}{2m}(h_\theta(x)-y)^2=2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)=(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)=(h_\theta(x)-y)\cdot x_j因此梯度下降算法的最终表述为Repeat Until Convergence{ \theta_j:=\theta_j-\alpha\sum_{i=1}^{n}((h_\theta(x^{(i)})-y^{(i)})\cdot x_j) for every $j$}梯度下降算法需多次迭代、算法复杂度为$O(kn^2)$。当利用梯度下降算法求得一组$\theta$时我们便能得到线性回归函数。 4.线性回归算法实现为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址为http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。1234567891011121314151617181920212223242526272829303132333435363738394041424344import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn import linear_modelfrom mpl_toolkits.mplot3d import axes3dimport seaborn as sns#read_csvreaddata=pd.read_csv('data/Advertising.csv')data=np.array(readdata.values)#训练数据X_train=data[0:150,1:3]Y_train=data[0:150,3]#测试数据X_test=data[150:200,1:3]Y_test=data[150:200,3]#回归分析regr = linear_model.LinearRegression()#进行training set和test set的fit，即是训练的过程regr.fit(X_train, Y_train)# 打印出相关系数和截距等信息print('Coefficients: \n', regr.coef_)print('Intercept: ', regr.intercept_)# The mean square errorprint("Residual sum of squares: %.2f" % np.mean((regr.predict(X_test) - Y_test) ** 2))# Explained variance score: 1 is perfect predictionprint('Variance score: %.2f' % regr.score(X_test, Y_test))#得出回归函数 并自定义数据X_line=np.linspace(0,300)Y_line=np.linspace(0,50)Z_line=0.04699836*X_line+0.17913965*Y_line+3.00431061176#画图fig=plt.figure()ax = plt.subplot(111, projection='3d') # 创建一个三维的绘图工程ax.scatter(data[:,1],data[:,2],data[:,3],c='red',) # 绘制数据点ax.plot(X_line,Y_line,Z_line,c='blue')#绘制回归曲线plt.show() 其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。 5.推广更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习知识体系]]></title>
    <url>%2F2018%2F03%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[1.什么是机器学习 机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。 上述为百度百科定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。 给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。 以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。 2.机器学习体系概括机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。 下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。 机器学习算法中常用到的便是监督学习和无监督学习，监督学习包含回归和分类两方面，无监督学习为聚类。 监督学习（Supervised Learning） 当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为回归分析（Regression Analysis）和分类（Classification）两类。 回归分析（Regression Analysis）：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。 分类（Classfication）：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。 无监督学习（Unsupervised Learning） 我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。 所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。 3.如何开始学习开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。 很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。 4.推广更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown写作教程]]></title>
    <url>%2F2018%2F03%2F18%2FMarkdown%E5%86%99%E4%BD%9C%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[博客搭建教程写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。 1.为什么选择Markdown首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用印象笔记至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。 2.标题标题通过#的个数进行区分，Markdown共支持6级标题。 3.字体设置3.1粗体文字前后加**来表示粗体。 1**粗体** 粗体 3.2斜体文字前后加*来表示斜体。 1*斜体* 斜体 3.3粗斜体文字前后加***来表示粗斜体。 1***粗斜体*** 粗斜体 3.4下划线文字前后加&lt;u&gt; &lt;/u&gt;来表示下划线。 1&lt;u&gt;下滑线&lt;/u&gt; 下划线 3.5删除线文字前后加~~来表示删除线。 1~~删除线~~ 删除线 3.6标记文字前后加` 来表示标记，该符号位于Esc键下面。 1`标记` 标记 3.7Html标签1&lt;font face="微软雅黑" color="red" size="6"&gt;字体及字体颜色和大小&lt;/font&gt; 字体及字体颜色和大小 4.列表4.1有序列表采用1. 后加空格形式表示有序列表。 1231. 有序列表12. 有序列表23. 有序列表3 有序列表1 有序列表2 有序列表3 4.2无序列表采用+ - * =符号表示无序列表，支持多级嵌套。 12345+ 有序列表1+ + 有序列表1.1+ + 有序列表1.2+ 有序列表2+ 有序列表3 无序列表1 无序列表1.1 无序列表1.2 无序列表2 无序列表3 4.3未完成列表采用- []表示未完成任务，各符号间均有空格。 123- [ ] 未完成任务1- [ ] 未完成任务2- [ ] 未完成任务3 [ ] 未完成任务1 [ ] 未完成任务2 [ ] 未完成任务3 4.4已完成任务采用- [x]表示已完成任务，各符号间均有空格。同时可直接在未完成任务间打勾来转换成已完成任务。 123- [x] 已完成任务1- [x] 已完成任务2- [x] 已完成任务3 [ ] 已完成任务1 [ ] 已完成任务2 [ ] 已完成任务3 5.表格表格对齐方式 居左：:—— 居中：:——:或——- 居由：——: 1234| 标题1 | 标题2 | 标题3 || :-------------- | :-------------: | --------------: || 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 || 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 | 标题1 标题2 标题3 居左测试文本1.1 居中测试文本2.1 居右测试文本3.1 居左测试文本1.2 居中测试文本2.2 居右测试文本3.2 6.段落和换行6.1首行缩进方式 &amp;emsp;中文空格 &amp;ensp;半中文空格 &amp;nbsp;英文空格 输入法切换到全角双击空格 6.2换行 ` `换行处连续打两个空格 换行处使用&lt;br&gt;进行换行 6.3空行 ` ` 空行处连续打两个空格 换行处使用&lt;br&gt;进行空行 6.引用和代码块6.1引用若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。 1234&gt; 引用1&gt; &gt; 引用1.1&gt; &gt; 引用1.2&gt; 引用2 引用1 引用1.1 引用1.2 引用2 6.2代码块代码前后添加`表示代码块。12345```markdown​```Pythonprint(&apos;代码块&apos;)​ 123```pythonprint（'代码块'） 7.链接7.1图片链接采用![]()来表示图片链接。 1![图片名称](链接地址) 7.2文字链接采用[]()表示文字链接。 1[链接名称](链接地址) 文字链接 7.3参考链接采用[ ]:表示参考链接，注意符号后有空格。 1[ ]: url title 8.分割线上下文无关时可使用分割符进行分开。 连续多个- (&gt;=3) 连续多个* （&gt;=3） 连续多个下划线_ （&gt;=3） 123---分割线***分割线___分割线 9.脚注和注释9.1脚注采用`:表示脚注，注意空格。 1[^]: 脚注 9.2注释采用&lt;!----&gt;表示注释. 1&lt;!--注释--&gt; 11.转义Markdown通过反斜杠\来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。 123456789101112\\反斜线\`反引号\*星号\_下划线\&#123;&#125;花括号\[]方括号\()括弧\#井字号\+加号\-减号\.英文句点\!感叹号 \\反斜线`反引号*星号_下划线\{}花括号[]方括号()括弧#井字号+加号-减号.英文句点!感叹号 12.目录采用[TOC]来生成文章目录。 1[TOC] 13.推广更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。]]></content>
      <tags>
        <tag>Markdown</tag>
        <tag>博客</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac+Hexo+GitHub博客搭建教程]]></title>
    <url>%2F2018%2F03%2F16%2FMac%2BHexo%2BGitHub%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.为什么写博客以前利用Jekyll+Github搭建博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。以前学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。 2.Mac+Hexo+GitHub博客现在博客主流的就是Jekyll和Hexo两种格式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo，选择Hexo的主要原因。 Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。 Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人审美原因)。 选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是百度爬虫无法爬去博客内容，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。 3.博客本地环境搭建3.1安装Node.js和GitMac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录/usr/local/bin目录下。测试Node.js和npm，出现下述信息则安装成功。 12node -vv8.10.0 12npm -v5.6.0 Git官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本即可。 Git —version git version 2.15.0 3.2安装HexoNode.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。 1sudo npm install -g hexo 3.3博客初始化创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。 1cd myblog 执行下述命令初始化本地博客，下载一些列文件。 1hexo init 执行下述命令安装npm。 1sudo npm install 执行下述命令生成本地html文件并开启服务器，然后通过http://localhost:4000查看本地博客。 12hexo ghexo s 4.本地博客关联GitHub4.1本地博客代码上传GitHub注册并登陆GitHub账号后，新建仓库，名称必须为user.github.io，如weizhixiaoyi.github.io。 终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。 1vim _config.yml 打开后至文档最后部分，将deploy配置如下。 1234deploy: type: git repository: https://github.com/weizhixiaoyi/weizhixiaoyi.github.io.git branch: master 其中将repository中weizhixiaoyi改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。 12hexo ghexo d 若执行hexo g出错则执行npm install hexo --save，若执行hexo d出错则执行npm install hexo-deployer-git --save。错误修正后再次执行hexo g和hexo d。 若未关联GitHub，执行hexo d时会提示输入GitHub账号用户名和密码，即: 12username for 'https://github.com':password for 'https://github.com': hexo d执行成功后便可通过https://weizhixiaoyi.github.io访问博客，看到的内容和http://localhost:4000相同。 4.2添加ssh keys到GitHub添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则进入新生成ssh key。 执行下述命令生成新的ssh key，将your_email@example.com改成自己以注册的GitHub邮箱地址。默认会在~/.ssh/id_rsa.pub中生成id_rsa和id_rsa.pub文件。 1ssh-keygen -t rsa -C "your_email@exampl" Mac下利用open ~/.ssh打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key路径GitHub-&gt;Setting-&gt;SSH keys-&gt;add SSH key界面即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。 此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过hexo g和hexo d便可更新到GitHub之中，通过https://weizhixiaoyi.github.io访问便可看到更新内容。 5.更换Hexo主题可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。 终端cd到myblog目录下执行如下所示命令。 1git clone https://github.com/iissnan/hexo-theme-next themes/next 将blog目录下_config.yml里的theme的名称landscape更改为next。 执行如下命令（每次部署文章的步骤） 12hexo g //生成缓存和静态文件hexo d //重新部署到服务器 当本地博客部署到服务器后，网页端无变化时可以采用下述命令。 1hexo clean //清楚缓存文件(db.json)和已生成的静态文件(public) 6.配置Hexo-theme-next主题Hexo-theme-next主题便为精于心、简于形，简介的界面下能够呈现丰富的内容，访问next官网查看配置内容。配置文件主要修改next中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。 6.1增加标签、分类、归档页首先将next/config.yml文件中将menu中tags catagories archive前面的#。例如增加标签页，通过hexo new page &#39;tags&#39;增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为tags。利用hexo g和hexo d将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。 6.2增加喜欢界面喜欢界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。 从GitHub上https://github.com/weizhixiaoyi 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。 12image_stream: jquery: false 在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。 1234567menu: home: / || home about: /about/ || user favorite: /favorite/ || heart tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive 然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。 123456&#123;% stream %&#125;&#123;% figure https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg[《万物理论》]（https://movie.douban.com/subject/24815950/）%&#125;&#123;% endstream %&#125; 6.3文章阅读统计文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app_id和app_key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。 注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app_id和app_key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。 1234leancloud_visitors: enable: true app_id: Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz #&lt;app_id&gt; app_key: qJejurdHKM06N75OQedX4SDK #&lt;app_key&gt; 6.4增加百度统计百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到代码获取。 123456789&lt;script&gt;var _hmt = _hmt || [];(function() &#123; var hm = document.createElement("script"); hm.src = "https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645"; var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);&#125;)();&lt;/script&gt; 将代码中b54e835b3551fd0696954b3aedf5d645复制到next主题_config.yml的baidu_analytics中。接下来通过代码安装检查来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。 6.4增加评论功能多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。 进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题_config.yml中的livere_uid处便可，重新提交网站便可看到评论专区。编写文章时应在头部添加comments: true 7.绑定个人域名现在使用的域名weizhixiaoyi.github.io是github提供的二级域名，也可绑定自己的个性域名weizhixiaoyi.com。域名是在阿里云购买，年费为55元，也可以在狗爹https://sg.godaddy.com购买，购买好域名之后便可以直接解析。 7.1GitHub端在next主题中source文件夹中创建CNAME文件，没有后缀名，然后将个人域名weizhixiaoyi.com添加进CNAME文件即可，然后通过hexo g hexo d重新部署网站。 7.2域名解析如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。 记录类型：CNAME 主机记录：@ 解析线路：默认 记录值：weizhixiaoyi.github.io 解析成功后，等待几分钟便可登陆weizhixiaoyi.com查看网站内容。 7.博客SEO优化SEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。 7.1确认收录情况在谷歌上搜索site:weizhixiaoyi.com，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。 7.1网站身份验证验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过shadowsock+搬瓦工自行搭建。 进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/_partials/，打开head.swig文件，在theme_google_site_verification处添加如下信息。 123&#123;% if theme.google_site_verification %&#125; &lt;meta name="google-site-verification" content="E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo" /&gt;&#123;% endif %&#125; 然后回到myblog文件夹下将_config.yml中google_site_vertification设置为true。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用hexo g和hexo d更新博客内容，至此网站身份验证结束。 7.2添加Sitemapsitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。 首先安装针对谷歌的插件npm install hexo-generator-sitemap --save，然后进入myblog文件夹下将sitemap设置如下。 123# sitemapsitemap: path: sitemap.xml 7.3谷歌收录博客谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过site:weizhixiaoyi.com能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。 另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。 8.ToDoList 寻找更好的方法解决百度爬虫无法爬取博客内容的问题 博客增加转发功能 9.推广更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>Mac</tag>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之MatPlotLib使用教程]]></title>
    <url>%2F2018%2F03%2F14%2FPython%E4%B9%8BMatPlotLib%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.Matplotlib简介 Matplotlib是非常强大的python画图工具 Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 2.Matplotlib安装1pip3 install matplotlib#python3 3.Matplotlib引入123import matplotlib.pyplot as plt#为方便简介为pltimport numpy as np#画图过程中会使用numpyimport pandas as pd#画图过程中会使用pandas 4.Matplotlib基本应用123456x=np.linspace(-1,1,50)#定义x数据范围y1=2*x+1#定义y数据范围y2=x**2plt.figure()#定义一个图像窗口plt.plot(x,y)#plot()画出曲线plt.show()#显示图像 4.1figure图像matplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。1234567x=np.linspace(-3,3,50)#50为生成的样本数y1=2*x+1y2=x**2plt.figure(num=1,figsize=(8,5))#定义编号为1 大小为(8,5)plt.plot(x,y1,color='red',linewidth=2,linestyle='--')#颜色为红色，线宽度为2，线风格为--plt.plot(x,y2)#进行画图plt.show()#显示图 4.2设置坐标轴1234567891011x=np.linspace(-3,3,50)y1=2*x+1y2=x**2plt.figure(num=2,figsize=(8,5))plt.plot(x,y1,color='red',linewidth=2,linestyle='-')plt.plot(x,y2)#进行画图plt.xlim(-1,2)plt.ylim(-2,3)plt.xlabel("I'm x")plt.ylabel("I'm y")plt.show() 自定义坐标轴1234567891011121314151617x=np.linspace(-3,3,50)y1=2*x+1y2=x**2plt.figure(num=2,figsize=(8,5))plt.plot(x,y1,color='red',linewidth=2,linestyle='-')plt.plot(x,y2)#进行画图plt.xlim(-1,2)plt.ylim(-2,3)plt.xlabel("I'm x")plt.ylabel("I'm y")new_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位print(new_ticks)#[-1. -0.25 0.5 1.25 2. ]plt.xticks(new_ticks)#进行替换新下标plt.yticks([-2,-1,1,2,], [r'$really\ bad$','$bad$','$well$','$really\ well$'])plt.show() 设置边框属性12345678910111213141516x=np.linspace(-3,3,50)y1=2*x+1y2=x**2plt.figure(num=2,figsize=(8,5))plt.plot(x,y1,color='red',linewidth=2,linestyle='--')plt.plot(x,y2)#进行画图plt.xlim(-1,2)plt.ylim(-2,3)new_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位plt.xticks(new_ticks)#进行替换新下标plt.yticks([-2,-1,1,2,], [r'$really\ bad$','$bad$','$well$','$really\ well$'])ax=plt.gca()#gca=get current axisax.spines['right'].set_color('none')#边框属性设置为none 不显示ax.spines['top'].set_color('none')plt.show() 调整移动坐标轴1234567891011121314151617181920x=np.linspace(-3,3,50)y1=2*x+1y2=x**2plt.figure(num=2,figsize=(8,5))plt.plot(x,y1,color='red',linewidth=2,linestyle='--')plt.plot(x,y2)#进行画图plt.xlim(-1,2)plt.ylim(-2,3)new_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位plt.xticks(new_ticks)#进行替换新下标plt.yticks([-2,-1,1,2,], [r'$really\ bad$','$bad$','$well$','$really\ well$'])ax=plt.gca()#gca=get current axisax.spines['right'].set_color('none')#边框属性设置为none 不显示ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、noneax.spines['bottom'].set_position(('data', 0))#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、dataax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data',0))#坐标中心点在(0,0)位置plt.show() 4.3添加图例matplotlib中legend图例帮助我们展示数据对应的图像名称。123456789101112131415x=np.linspace(-3,3,50)y1=2*x+1y2=x**2plt.figure(num=2,figsize=(8,5))plt.xlim(-1,2)plt.ylim(-2,3)new_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位plt.xticks(new_ticks)#进行替换新下标plt.yticks([-2,-1,1,2,], [r'$really\ bad$','$bad$','$well$','$really\ well$'])l1,=plt.plot(x,y1,color='red',linewidth=2,linestyle='--',label='linear line')l2,=plt.plot(x,y2,label='square line')#进行画图plt.legend(loc='best')#显示在最好的位置plt.show()#显示图 调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数123456789101112131415plt.legend(handles=[l1, l2], labels=['up', 'down'], loc='best')#loc有很多参数 其中best自分配最佳位置''' 'best' : 0, 'upper right' : 1, 'upper left' : 2, 'lower left' : 3, 'lower right' : 4, 'right' : 5, 'center left' : 6, 'center right' : 7, 'lower center' : 8, 'upper center' : 9, 'center' : 10, ''' 4.4标注123456789101112131415161718192021222324252627x=np.linspace(-3,3,50)y = 2*x + 1plt.figure(num=1, figsize=(8, 5))plt.plot(x, y,)#移动坐标轴ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.spines['bottom'].set_position(('data', 0))ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data', 0))#标注信息x0=1y0=2*x0+1plt.scatter(x0,y0,s=50,color='b')plt.plot([x0,x0],[y0,0],'k--',lw=2.5)#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细#xycoords='data'是基于数据的值来选位置，xytext=(+30,-30)和textcoords='offset points'对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置plt.annotate(r'$2x0+1=%s$' % y0, xy=(x0, y0), xycoords='data', xytext=(+30, -30), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.2"))#添加注视text（-3.7,3）表示选取text位置 空格需要用\进行转译 fontdict设置文本字体 plt.text(-3.7, 3, r'$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$', fontdict=&#123;'size': 16, 'color': 'r'&#125;)plt.show() 4.5能见度调整123456789101112131415161718192021x=np.linspace(-3, 3, 50)y=0.1*xplt.figure()plt.plot(x, y, linewidth=10, zorder=1)plt.ylim(-2, 2)#移动坐标轴ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.spines['bottom'].set_position(('data', 0))ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data', 0))#label.set_fontsize(12)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序for label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontsize(12) label.set_bbox(dict(facecolor='red', edgecolor='None', alpha=0.7, zorder=2))plt.show() 5.画图种类5.1Scatter散点图1234567891011n=1024X=np.random.normal(0,1,n)#每一个点的X值Y=np.random.normal(0,1,n)#每一个点的Y值T=np.arctan2(Y,X)#arctan2返回给定的X和Y值的反正切值#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴plt.scatter(X,Y,s=75,c=T,alpha=0.5)plt.xlim(-1.5,1.5)plt.xticks(())#忽略xticksplt.ylim(-1.5,1.5)plt.yticks(())#忽略yticksplt.show() 5.2条形图123456789101112131415161718#基本图形n=12X=np.arange(n)Y1=(1-X/float(n))*np.random.uniform(0.5,1,n)Y2=(1-X/float(n))*np.random.uniform(0.5,1,n)plt.bar(X,+Y1,facecolor='#9999ff',edgecolor='white')plt.bar(X,-Y2,facecolor='#ff9999',edgecolor='white')#标记值for x,y in zip(X,Y1):#zip表示可以传递两个值 plt.text(x+0.4,y+0.05,'%.2f'%y,ha='center',va='bottom')#ha表示横向对齐 bottom表示向下对齐for x,y in zip(X,Y2): plt.text(x+0.4,-y-0.05,'%.2f'%y,ha='center',va='top')plt.xlim(-0.5,n)plt.xticks(())#忽略xticksplt.ylim(-1.25,1.25)plt.yticks(())#忽略yticksplt.show() 5.3等高线图123456789101112131415n=256x=np.linspace(-3,3,n)y=np.linspace(-3,3,n)X,Y=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中def f(x,y): return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)plt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5C=plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=0.5)#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10plt.clabel(C,inline=True,fontsize=10)plt.xticks(())#隐藏坐标轴plt.yticks(())plt.show() 5.4Image图片利用matplotlib打印出图像123456789a = np.array([0.313660827978, 0.365348418405, 0.423733120134, 0.365348418405, 0.439599930621, 0.525083754405, 0.423733120134, 0.525083754405, 0.651536351379]).reshape(3,3)#origin='lower'代表的就是选择的原点位置plt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')#cmap为color mapplt.colorbar(shrink=.92)#右边颜色说明 shrink参数是将图片长度变为原来的92%plt.xticks(())plt.yticks(())plt.show() 出图方式 此处采用内插法中的nearest-neighbor 5.53D图像1234567891011121314151617import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3Dfig=plt.figure()#定义图像窗口ax=Axes3D(fig)#在窗口上添加3D坐标轴#将X和Y值编织成栅格X=np.arange(-4,4,0.25)Y=np.arange(-4,4,0.25)X,Y=np.meshgrid(X,Y)R=np.sqrt(X**2+Y**2)Z=np.sin(R)#高度值#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度ax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride表示图像中分割线的跨图#添加XY平面等高线 投影到z平面ax.contourf(X,Y,Z,zdir='z',offset=-2,cmap=plt.get_cmap('rainbow'))#把图像进行投影的图形 offset表示比0坐标轴低两个位置ax.set_zlim(-2,2)plt.show() 6.多图合并显示6.1Subplot多合一显示均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。 12345678910111213plt.figure()plt.subplot(2,1,1)#表示整个图像分割成2行2列，当前位置为1plt.plot([0,1],[0,1])#横坐标变化为[0,1] 竖坐标变化为[0,2]plt.subplot(2,3,4)plt.plot([0,1],[0,2])plt.subplot(2,3,5)plt.plot([0,1],[0,3])plt.subplot(2,3,6)plt.plot([0,1],[0,4])plt.show() 不均匀图中图 12345678910111213plt.figure()plt.subplot(2,1,1)#将整个窗口分割成2行1列，当前位置表示第一个图plt.plot([0,1],[0,1])#横坐标变化为[0,1],竖坐标变化为[0,1]plt.subplot(2,3,4)#将整个窗口分割成2行3列，当前位置为4plt.plot([0,1],[0,2])plt.subplot(2,3,5)plt.plot([0,1],[0,3])plt.subplot(2,3,6)plt.plot([0,1],[0,4])plt.show() 6.2SubPlot分格显示方法一 123456789101112131415161718192021import matplotlib.gridspec as gridspec#引入新模块plt.figure()'''使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1'''ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3) # stands for axesax1.plot([1, 2], [1, 2])ax1.set_title('ax1_title')#设置图的标题#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2ax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1ax4 = plt.subplot2grid((3, 3), (2, 0))ax4.scatter([1, 2], [2, 2])ax4.set_xlabel('ax4_x')ax4.set_ylabel('ax4_y')ax5 = plt.subplot2grid((3, 3), (2, 1)) 方法二 12345678plt.figure()gs = gridspec.GridSpec(3, 3)#将图像分割成3行3列ax6 = plt.subplot(gs[0, :])#gs[0:1]表示图占第0行和所有列ax7 = plt.subplot(gs[1, :2])#gs[1,:2]表示图占第1行和第二列前的所有列ax8 = plt.subplot(gs[1:, 2])ax9 = plt.subplot(gs[-1, 0])ax10 = plt.subplot(gs[-1, -2])#gs[-1.-2]表示这个图占倒数第1行和倒数第2行plt.show() 方法三 1234567'''建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114'''f, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, sharex=True, sharey=True)ax11.scatter([1,2], [1,2])ax11.scatter 坐标范围x为[1,2]，y为[1,2]plt.tight_layout()#表示紧凑显示图像plt.show() 6.3图中图123456789101112131415161718192021222324252627fig=plt.figure()#创建数据x=[1,2,3,4,5,6,7]y=[1,3,4,2,5,8,6]#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。left, bottom, width, height = 0.1, 0.1, 0.8, 0.8ax1 = fig.add_axes([left, bottom, width, height]) # main axesax1.plot(x, y, 'r')#绘制大图，颜色为redax1.set_xlabel('x')#横坐标名称为xax1.set_ylabel('y')ax1.set_title('title')#图名称为title#绘制小图，注意坐标系位置和大小的改变ax2 = fig.add_axes([0.2, 0.6, 0.25, 0.25])ax2.plot(y, x, 'b')#颜色为buueax2.set_xlabel('x')ax2.set_ylabel('y')ax2.set_title('title inside 1')#绘制第二个小兔plt.axes([0.6, 0.2, 0.25, 0.25])plt.plot(y[::-1], x, 'g')#将y进行逆序plt.xlabel('x')plt.ylabel('y')plt.title('title inside 2')plt.show() 6.4次坐标轴12345678910111213x=np.arange(0,10,0.1)y1=0.5*x**2y2=-1*y1fig, ax1 = plt.subplots()ax2 = ax1.twinx()#镜像显示ax1.plot(x, y1, 'g-')ax2.plot(x, y2, 'b-')ax1.set_xlabel('X data')ax1.set_ylabel('Y1 data', color='g')#第一个y坐标轴ax2.set_ylabel('Y2 data', color='b')#第二个y坐标轴plt.show() 7.动画12345678910111213141516171819from matplotlib import animation#引入新模块fig,ax=plt.subplots()x=np.arange(0,2*np.pi,0.01)#数据为0~2PI范围内的正弦曲线line,=ax.plot(x,np.sin(x))# line表示列表#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧def animate(i): line.set_ydata(np.sin(x+i/100)) return line,#构造开始帧函数initdef init(): line.set_ydata(np.sin(x)) return line,# frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 # blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。ani=animation.FuncAnimation(fig=fig,func=animate,frames=200,init_func=init,interval=20,blit=False)plt.show() MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。 更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。]]></content>
      <categories>
        <category>Python库</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之NumPy使用教程]]></title>
    <url>%2F2018%2F03%2F13%2FPython%E4%B9%8BNumPy%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.NumPy概述NumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点： 强大的N维数组对象Array 成熟的函数库 用于集成C/C++和Fortran代码的工具 实用的线性代数、傅立叶变换和随机生成函数 2.NumPy安装1pip install numpy或pip3 install numpy 3.NumPy引入1import numpy as np#为了方便实用numpy 采用np简写 4.NumPy方法123456array=np.array([[1,2,3],[4,5,6]])#将列表转换为矩阵 并转换为int类型print(array)'''[[1 2 3] [4 5 6]] ''' 4.1NumPy属性123456print('array of dim:',array.ndim)#矩阵的维度#array of dim:2print('array of shape',array.shape)#矩阵的行数和列数#array of shape:(2,3)print('number of size:',array.size)#元素的个数#number of size:6 4.2NumPy创建Array array:创建数组 dtype:指定数据类型 zeros:创建数据全为0 ones:创建数据全为1 empty:创建数据接近0 arange:指定范围内创建数据 linspace创建线段 创建数组123a=np.array([1,2,3])print(a)#[1,2,3] 指定数据dtype123456a=np.array([1,2,3],dtype=np.int)#指定为int类型print(a.dtype)#int 64b=np.array([1,2,3],dtype=np.float)#指定为float类型print(b.dtype)#float 64 创建特定数据123456a=np.array([[1,2,3],[4,5,6]])#矩阵 2行3列print(a)'''[[1 2 3] [4 5 6]] ''' 创建全0数组123456a=np.zeros((2,3))#数据全0 2行3列print(a)'''[[0 0 0] [0 0 0]] ''' 创建全1数组 指定特定类型dtype123456a=np.zeros((2,3),dtype=np.int)#数据全1 2行3列 同时指定类型print(a)'''[[1 1 1] [1 1 1]] ''' 创建全空数组 每个值接近0123456a=np.empty(2,3)#数据全为empty 3行4列print(a)'''[[ 0.00000000e+000 0.00000000e+000 2.12704693e-314] [ 2.12706024e-314 2.12706024e-314 2.12706024e-314]] ''' 用array创建连续数组123a=np.arange(1,10,2)#1到10的数据 2步长print(a)#[1 3 5 7 9] 用reshape改变数据形状123456a=np.arange(6).reshape(2,3)print(a)'''[[0 1 2] [3 4 5]] ''' 用linspace创建线段形数据123456a=np.linspace(1,10,20)#开始端1 结束端5 分割成10个数据 生成线段print(a)'''[ 1. 1.44444444 1.88888889 2.33333333 2.77777778 3.22222222 3.66666667 4.11111111 4.55555556 5. ] ''' 4.3NumPy基础运算基础运算之加、减、三角函数等1234567891011121314151617181920212223242526272829a=np.array([10,20,30,40])b=np.arange(4) #array[0,1,2,3]c=a+b#加法运算print(c)#[10,21,32,43]c=a-b#减法运算print(c)#[10.19,28,37]c=10*np.sin(a)#三角函数运算#[-5.44021111, 9.12945251, -9.88031624, 7.4511316 ]print(b&lt;3)#逻辑判断#[ True True True False]d=np.random.random((2,3))#随机生成2行3列的矩阵print(d)'''[[ 0.21116981 0.0804489 0.51855475] [ 0.38359164 0.55852973 0.73218811]]'''print(np.sum(d))#元素求和#2.48448292958print(np.max(d))#元素求最大值#0.732188108709print(np.min(d))#元素求最小值#0.0804488978886 多维矩阵运算123456789a=np.array([[1,1],[0,1]])b=np.arange(4).reshape((2,2))c=np.dot(a,b)#或c=a.dot(b)矩阵运算print(c)'''[[2 4] [2 3]] ''' 对行或列执行查找运算12345678910a=np.array([[1,2],[3,4]])print(a)'''[[1,2] [3,4]] '''print(np.max(a,axis=0))#axis=0时是对列进行操作#[3,4]print(np.min(a,axis=1))#axis=1是对行进行操作#[1,3] 矩阵索引操作1234567891011121314151617181920212223A=np.arange(2,14).reshape(3,4)print(A)'''[[2,3,4,5] [6,7,8,9] [10,11,12,13]] '''print(np.argmax(A))#矩阵中最大元素的索引#11print(np.argmin(A))#矩阵中最小元素的索引#0print(np.mean(A))#或者np.average(A)求解矩阵均值#7.5print(np.cumsum(A))#矩阵累加函数#[2 5 9 14 20 27 35 44 54 65 77 90]print(np.diff(A))#矩阵累差函数'''[[1 1 1] [1 1 1] [1 1 1]] '''print(np.nonzero(A))#将非0元素的行与列坐标分割开来#(array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3])) 矩阵排序、转置、替换操作12345678910111213141516171819202122232425262728A=np.arange(14,2,-1).reshape((3,4))print(A)'''[[14 13 12 11] [10 9 8 7] [ 6 5 4 3]] '''print(np.sort(A))#排序'''[[11 12 13 14] [ 7 8 9 10] [ 3 4 5 6]] '''print(np.transpose(A))'''[[14 10 6] [13 9 5] [12 8 4] [11 7 3]] '''print(np.clip(A,5,9))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换'''[[9 9 9 9] [9 9 8 7] [6 5 5 5]] ''' 5.索引一维索引123456789A=np.arange(0,12)print(A)#[ 0 1 2 3 4 5 6 7 8 9 10 11]print(A[1])#一维索引#1A=np.arange(0,12).reshape((3,4))print(A[0])#[0,1,2,3] 二维索引12345678910111213141516171819202122232425262728293031323334353637A=np.arange(0,12).reshape((3,4))print(A)'''[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] '''print(A[1][1])#或者A[1,1]#5print(A[1,1:3])#切片处理#[5,6]for row in A: print(A)'''[0 1 2 3][4 5 6 7][ 8 9 10 11] '''for col in A: print(col)'''[0 4 8][1 5 9][ 2 6 10][ 3 7 11] '''for item in A.flat: print(item)'''01...1011''' 6.NumPy之Array合并123456789A=np.array([1,1,1])B=np.array([2,2,2])print(np.vstack((A,B)))#上下合并'''[[1 1 1] [2 2 2]] '''print(np.hstack((A,B)))#左右合并#[1 1 1 2 2 2] 增加维度12345678910111213141516A=np.array([1,1,1])print(A.shape)#(3,)print(A[np.newaxis,:])#[[1 1 1]]print(A[np.newaxis,:].shape)#newaxis增加维度#(1,3)print(A[:,np.newaxis])'''[[1] [1] [1]] '''print(A[:,np.newaxis].shape)#（3,1） 多矩阵合并1234567891011121314151617181920212223A = np.array([1,1,1])[:,np.newaxis]B = np.array([2,2,2])[:,np.newaxis]print(np.concatenate((A,B,B,A),axis=0))#0表示上下合并'''[[1] [1] [1] [2] [2] [2] [2] [2] [2] [1] [1] [1]] '''print(np.concatenate((A,B,B,A),axis=1))#1表示左右合并'''[[1 2 2 1] [1 2 2 1] [1 2 2 1]] ''' 7.NumPy分割1234567891011121314151617181920212223242526272829A=np.arange(12).reshape((3,4))print(A)'''[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] '''print(np.split(A,3,axis=0))#横向分割成3部分 或者np.vsplit(A,3)#[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])]print(np.split(A,2,axis=1))#竖向分割成2部分 或者np.hsplit(A,2)'''[array([[0, 1], [4, 5], [8, 9]]), array([[ 2, 3], [ 6, 7], [10, 11]])] ''' print(np.array_split(A,3,axis=1))#不等量分割成3部分'''[array([[0, 1], [4, 5], [8, 9]]), array([[ 2], [ 6], [10]]), array([[ 3], [ 7], [11]])]''' 8.NumPy中copy和deep copy‘=’赋值方式会带有关联性12345678910111213141516a=np.arange(4)print(a)#[1 2 3 4]b=ac=ad=bprint(b is a)#Trueprint(c is a)#Trueprint(d is a)#Trueb[0]=5#改变b的值，a,c,d同样会进行改变print(a)#[5 2 3 4] ‘copy()’赋值方式没有关联性1234567a=np.arange(4)#deep copyprint(a)#[0 1 2 3]b=a.copy()a[0]=5print(b)#值并不发生改变#[0 1 2 3] 更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。如果感觉不错的话，可以资助1元钱当作鼓励，Thank you谢谢!]]></content>
      <categories>
        <category>Python库</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之Pandas使用教程]]></title>
    <url>%2F2018%2F03%2F12%2FPython%E4%B9%8BPandas%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.Pandas概述 Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。 Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。 Pandas提供大量能使我们快速便捷地处理数据的函数和方法。 Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。 2.Pandas安装1pip3 install pandas 3.Pandas引入1import pandas as pd#为了方便实用pandas 采用pd简写 4.Pandas数据结构4.1Series12345678910111213import numpy as npimport pandas as pds=pd.Series([1,2,3,np.nan,5,6])print(s)#索引在左边 值在右边'''0 1.01 2.02 3.03 NaN4 5.05 6.0dtype: float64 ''' 4.2DataFrameDataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106dates=pd.date_range('20180310',periods=6)df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置print(df)#输出6行4列的表格''' A B C D2018-03-10 -0.092889 -0.503172 0.692763 -1.2613132018-03-11 -0.895628 -2.300249 -1.098069 0.4689862018-03-12 0.084732 -1.275078 1.638007 -0.2911452018-03-13 -0.561528 0.431088 0.430414 1.0659392018-03-14 1.485434 -0.341404 0.267613 -1.4933662018-03-15 -1.671474 0.110933 1.688264 -0.910599 '''print(df['B'])'''2018-03-10 -0.9272912018-03-11 -0.4068422018-03-12 -0.0883162018-03-13 -1.6310552018-03-14 -0.9299262018-03-15 -0.010904Freq: D, Name: B, dtype: float64 '''#创建特定数据的DataFramedf_1=pd.DataFrame(&#123;'A' : 1., 'B' : pd.Timestamp('20180310'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical(["test","train","test","train"]), 'F' : 'foo' &#125;)print(df_1)''' A B C D E F0 1.0 2018-03-10 1.0 3 test foo1 1.0 2018-03-10 1.0 3 train foo2 1.0 2018-03-10 1.0 3 test foo3 1.0 2018-03-10 1.0 3 train foo'''print(df_1.dtypes)'''A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object'''print(df_1.index)#行的序号#Int64Index([0, 1, 2, 3], dtype='int64')print(df_1.columns)#列的序号名字#Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')print(df_1.values)#把每个值进行打印出来'''[[1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo'] [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo'] [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'test' 'foo'] [1.0 Timestamp('2018-03-10 00:00:00') 1.0 3 'train' 'foo']] '''print(df_1.describe())#数字总结''' A C Dcount 4.0 4.0 4.0mean 1.0 1.0 3.0std 0.0 0.0 0.0min 1.0 1.0 3.025% 1.0 1.0 3.050% 1.0 1.0 3.075% 1.0 1.0 3.0max 1.0 1.0 3.0'''print(df_1.T)#翻转数据''' 0 1 2 \A 1 1 1 B 2018-03-10 00:00:00 2018-03-10 00:00:00 2018-03-10 00:00:00 C 1 1 1 D 3 3 3 E test train test F foo foo foo 3 A 1 B 2018-03-10 00:00:00 C 1 D 3 E train F foo '''print(df_1.sort_index(axis=1, ascending=False))#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示''' F E D C B A0 foo test 3 1.0 2018-03-10 1.01 foo train 3 1.0 2018-03-10 1.02 foo test 3 1.0 2018-03-10 1.03 foo train 3 1.0 2018-03-10 1.0'''print(df_1.sort_values(by='E'))#按值进行排序''' A B C D E F0 1.0 2018-03-10 1.0 3 test foo2 1.0 2018-03-10 1.0 3 test foo1 1.0 2018-03-10 1.0 3 train foo3 1.0 2018-03-10 1.0 3 train foo''' 5.Pandas选择数据123456789101112131415161718192021dates=pd.date_range('20180310',periods=6)df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=['A','B','C','D'])#生成6行4列位置print(df)''' A B C D2018-03-10 -0.520509 -0.136602 -0.516984 1.3575052018-03-11 0.332656 -0.094633 0.382384 -0.9143392018-03-12 0.499960 1.576897 2.128730 2.1974652018-03-13 0.540385 0.427337 -0.591381 0.1265032018-03-14 0.191962 1.237843 1.903370 2.1553662018-03-15 -0.188331 -0.578581 -0.845854 -0.056373 '''print(df['A'])#或者df.A 选择某列'''2018-03-10 -0.5205092018-03-11 0.3326562018-03-12 0.4999602018-03-13 0.5403852018-03-14 0.1919622018-03-15 -0.188331''' 切片选择12345678910111213print(df[0:3], df['20180310':'20180314'])#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择''' A B C D2018-03-10 -0.520509 -0.136602 -0.516984 1.3575052018-03-11 0.332656 -0.094633 0.382384 -0.9143392018-03-12 0.499960 1.576897 2.128730 2.197465 A B C D2018-03-10 -0.520509 -0.136602 -0.516984 1.3575052018-03-11 0.332656 -0.094633 0.382384 -0.9143392018-03-12 0.499960 1.576897 2.128730 2.1974652018-03-13 0.540385 0.427337 -0.591381 0.1265032018-03-14 0.191962 1.237843 1.903370 2.155366 ''' 根据标签loc-行标签进行选择数据123456print(df.loc['20180312', ['A','B']])#按照行标签进行选择 精确选择 '''A 0.499960B 1.576897Name: 2018-03-12 00:00:00, dtype: float64''' 根据序列iloc-行号进行选择数据1234567891011121314151617print(df.iloc[3, 1])#输出第三行第一列的数据#0.427336827399print(df.iloc[3:5,0:2])#进行切片选择 ''' A B2018-03-13 0.540385 0.4273372018-03-14 0.191962 1.237843 '''print(df.iloc[[1,2,4],[0,2]])#进行不连续筛选''' A C2018-03-11 0.332656 0.3823842018-03-12 0.499960 2.1287302018-03-14 0.191962 1.903370 ''' 根据混合的两种ix1234567print(df.ix[:3, ['A', 'C']])''' A C2018-03-10 -0.919275 -1.3560372018-03-11 0.010171 -0.3800102018-03-12 0.285251 -1.174265 ''' 根据判断筛选12345678print(df[df.A &gt; 0])#筛选出df.A大于0的元素 布尔条件筛选''' A B C D2018-03-11 0.332656 -0.094633 0.382384 -0.9143392018-03-12 0.499960 1.576897 2.128730 2.1974652018-03-13 0.540385 0.427337 -0.591381 0.1265032018-03-14 0.191962 1.237843 1.903370 2.155366 ''' 6.Pandas设置数据根据loc和iloc设置12345678910111213141516171819202122232425dates = pd.date_range('20180310', periods=6)df = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])print(df)''' A B C D2018-03-10 0 1 2 32018-03-11 4 5 6 72018-03-12 8 9 1111 112018-03-13 12 13 14 152018-03-14 16 17 18 192018-03-15 20 21 22 23'''df.iloc[2,2] = 999#单点设置df.loc['2018-03-13', 'D'] = 999print(df)''' A B C D2018-03-10 0 1 2 32018-03-11 0 5 6 72018-03-12 0 9 999 112018-03-13 0 13 14 9992018-03-14 0 17 18 192018-03-15 0 21 22 23''' 根据条件设置1234567891011df[df.A&gt;0]=999#将df.A大于0的值改变print(df)''' A B C D2018-03-10 0 1 2 32018-03-11 999 5 6 72018-03-12 999 9 999 112018-03-13 999 13 14 9992018-03-14 999 17 18 192018-03-15 999 21 22 23 ''' 根据行或列设置1234567891011df['F']=np.nanprint(df)''' A B C D2018-03-10 0 1 2 NaN2018-03-11 999 5 6 NaN2018-03-12 999 9 999 NaN2018-03-13 999 13 14 NaN2018-03-14 999 17 18 NaN2018-03-15 999 21 22 NaN ''' 添加数据1234567891011df['E'] = pd.Series([1,2,3,4,5,6], index=pd.date_range('20180313', periods=6))#增加一列print(df)''' A B C D E2018-03-10 0 1 2 NaN NaN2018-03-11 999 5 6 NaN NaN2018-03-12 999 9 999 NaN NaN2018-03-13 999 13 14 NaN 1.02018-03-14 999 17 18 NaN 2.02018-03-15 999 21 22 NaN 3.0''' 7.Pandas处理丢失数据处理数据中NaN数据1234567891011121314dates = pd.date_range('20180310', periods=6)df = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])df.iloc[0,1]=np.nandf.iloc[1,2]=np.nanprint(df)''' A B C D2018-03-10 0 NaN 2.0 32018-03-11 4 5.0 NaN 72018-03-12 8 9.0 10.0 112018-03-13 12 13.0 14.0 152018-03-14 16 17.0 18.0 192018-03-15 20 21.0 22.0 23''' 使用dropna（）函数去掉NaN的行或列12345678print(df.dropna(axis=0,how='any'#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop''' A B C D2018-03-12 8 9.0 10.0 112018-03-13 12 13.0 14.0 152018-03-14 16 17.0 18.0 192018-03-15 20 21.0 22.0 23 ''' 使用fillna（）函数替换NaN值12345678910print(df.fillna(value=0))#将NaN值替换为0''' A B C D2018-03-10 0 0.0 2.0 32018-03-11 4 5.0 0.0 72018-03-12 8 9.0 10.0 112018-03-13 12 13.0 14.0 152018-03-14 16 17.0 18.0 192018-03-15 20 21.0 22.0 23 ''' 使用isnull()函数判断数据是否丢失123456789101112print(pd.isnull(df))#矩阵用布尔来进行表示 是nan为ture 不是nan为false''' A B C D2018-03-10 False True False False2018-03-11 False False True False2018-03-12 False False False False2018-03-13 False False False False2018-03-14 False False False False2018-03-15 False False False False '''print(np.any(df.isnull()))#判断数据中是否会存在NaN值#True 8.Pandas导入导出pandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看官方资料123data=pd.read_csv('test1.csv')#读取csv文件data.to_pickle('test2.pickle')#将资料存取成pickle文件 #其他文件导入导出方式相同 9.Pandas合并数据axis合并方向1234567891011121314151617df1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])df2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])df3 = pd.DataFrame(np.ones((3,4))*2, columns=['a','b','c','d'])res = pd.concat([df1, df2, df3], axis=0, ignore_index=True)#0表示竖项合并 1表示横项合并 ingnore_index重置序列index index变为0 1 2 3 4 5 6 7 8print(res)''' a b c d0 0.0 0.0 0.0 0.01 0.0 0.0 0.0 0.02 0.0 0.0 0.0 0.03 1.0 1.0 1.0 1.04 1.0 1.0 1.0 1.05 1.0 1.0 1.0 1.06 2.0 2.0 2.0 2.07 2.0 2.0 2.0 2.08 2.0 2.0 2.0 2.0 ''' join合并方式123456789101112131415161718192021222324252627282930313233343536373839404142df1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'], index=[1,2,3])df2 = pd.DataFrame(np.ones((3,4))*1, columns=['b','c','d', 'e'], index=[2,3,4])print(df1)''' a b c d1 0.0 0.0 0.0 0.02 0.0 0.0 0.0 0.03 0.0 0.0 0.0 0.0 '''print(df2)''' b c d e2 1.0 1.0 1.0 1.03 1.0 1.0 1.0 1.04 1.0 1.0 1.0 1.0 '''res=pd.concat([df1,df2],axis=1,join='outer')#行往外进行合并print(res)''' a b c d b c d e1 0.0 0.0 0.0 0.0 NaN NaN NaN NaN2 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.03 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.04 NaN NaN NaN NaN 1.0 1.0 1.0 1.0 '''res=pd.concat([df1,df2],axis=1,join='outer')#行相同的进行合并print(res)''' a b c d b c d e2 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.03 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0'''res=pd.concat([df1,df2],axis=1,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充print(res)''' a b c d b c d e1 0.0 0.0 0.0 0.0 NaN NaN NaN NaN2 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.03 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0''' append添加数据1234567891011121314151617181920212223242526df1 = pd.DataFrame(np.ones((3,4))*0, columns=['a','b','c','d'])df2 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])df3 = pd.DataFrame(np.ones((3,4))*1, columns=['a','b','c','d'])s1 = pd.Series([1,2,3,4], index=['a','b','c','d'])res=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置indexprint(res)''' a b c d0 0.0 0.0 0.0 0.01 0.0 0.0 0.0 0.02 0.0 0.0 0.0 0.03 1.0 1.0 1.0 1.04 1.0 1.0 1.0 1.05 1.0 1.0 1.0 1.0'''res=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置indexprint(res)''' a b c d0 0.0 0.0 0.0 0.01 0.0 0.0 0.0 0.02 0.0 0.0 0.0 0.03 1.0 2.0 3.0 4.0''' 10.Pandas合并merge依据一组key合并12345678910111213141516171819202122232425262728293031left = pd.DataFrame(&#123;'key': ['K0', 'K1', 'K2', 'K3'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3']&#125;)print(left)''' A B key0 A0 B0 K01 A1 B1 K12 A2 B2 K23 A3 B3 K3'''right = pd.DataFrame(&#123;'key': ['K0', 'K1', 'K2', 'K3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']&#125;)print(right)''' C D key0 C0 D0 K01 C1 D1 K12 C2 D2 K23 C3 D3 K3'''res=pd.merge(left,right,on='key')print(res)''' A B key C D0 A0 B0 K0 C0 D01 A1 B1 K1 C1 D12 A2 B2 K2 C2 D23 A3 B3 K3 C3 D3''' 依据两组key合并1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465left = pd.DataFrame(&#123;'key1': ['K0', 'K0', 'K1', 'K2'], 'key2': ['K0', 'K1', 'K0', 'K1'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3']&#125;)print(left)''' A B key1 key20 A0 B0 K0 K01 A1 B1 K0 K12 A2 B2 K1 K03 A3 B3 K2 K1 '''right = pd.DataFrame(&#123;'key1': ['K0', 'K1', 'K1', 'K2'], 'key2': ['K0', 'K0', 'K0', 'K0'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']&#125;)print(right)''' C D key1 key20 C0 D0 K0 K01 C1 D1 K1 K02 C2 D2 K1 K03 C3 D3 K2 K0 '''res=pd.merge(left,right,on=['key1','key2'],how='inner')#内联合并print(res)''' A B key1 key2 C D0 A0 B0 K0 K0 C0 D01 A2 B2 K1 K0 C1 D12 A2 B2 K1 K0 C2 D2'''res=pd.merge(left,right,on=['key1','key2'],how='outer')#外联合并print(res)''' A B key1 key2 C D0 A0 B0 K0 K0 C0 D01 A1 B1 K0 K1 NaN NaN2 A2 B2 K1 K0 C1 D13 A2 B2 K1 K0 C2 D24 A3 B3 K2 K1 NaN NaN5 NaN NaN K2 K0 C3 D3'''res=pd.merge(left,right,on=['key1','key2'],how='left')#左联合并''' A B key1 key2 C D0 A0 B0 K0 K0 C0 D01 A1 B1 K0 K1 NaN NaN2 A2 B2 K1 K0 C1 D13 A2 B2 K1 K0 C2 D24 A3 B3 K2 K1 NaN NaN'''res=pd.merge(left,right,on=['key1','key2'],how='right')#右联合并print(res)''' A B key1 key2 C D0 A0 B0 K0 K0 C0 D01 A2 B2 K1 K0 C1 D12 A2 B2 K1 K0 C2 D23 NaN NaN K2 K0 C3 D3''' Indicator合并1234567891011121314151617181920212223242526272829303132333435df1 = pd.DataFrame(&#123;'col1':[0,1], 'col_left':['a','b']&#125;)print(df1)''' col1 col_left0 0 a1 1 b '''df2 = pd.DataFrame(&#123;'col1':[1,2,2],'col_right':[2,2,2]&#125;)print(df2)''' col1 col_right0 1 21 2 22 2 2 '''res=pd.merge(df1,df2,on='col1',how='outer',indicator=True)#依据col1进行合并 并启用indicator=True输出每项合并方式print(res)''' col1 col_left col_right _merge0 0 a NaN left_only1 1 b 2.0 both2 2 NaN 2.0 right_only3 2 NaN 2.0 right_only'''res = pd.merge(df1, df2, on='col1', how='outer', indicator='indicator_column')#自定义indicator column名称print(res)''' col1 col_left col_right indicator_column0 0 a NaN left_only1 1 b 2.0 both2 2 NaN 2.0 right_only3 2 NaN 2.0 right_only''' 依据index合并1234567891011121314151617181920212223242526272829303132333435363738left = pd.DataFrame(&#123;'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2']&#125;, index=['K0', 'K1', 'K2'])print(left)''' A BK0 A0 B0K1 A1 B1K2 A2 B2 '''right = pd.DataFrame(&#123;'C': ['C0', 'C2', 'C3'], 'D': ['D0', 'D2', 'D3']&#125;, index=['K0', 'K2', 'K3'])print(right)''' C DK0 C0 D0K2 C2 D2K3 C3 D3'''res=pd.merge(left,right,left_index=True,right_index=True,how='outer')#根据index索引进行合并 并选择外联合并print(res)''' A B C DK0 A0 B0 C0 D0K1 A1 B1 NaN NaNK2 A2 B2 C2 D2K3 NaN NaN C3 D3'''res=pd.merge(left,right,left_index=True,right_index=True,how='inner')print(res)''' A B C DK0 A0 B0 C0 D0K2 A2 B2 C2 D2''' 更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…]]></content>
      <categories>
        <category>Python库</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向知乎的个性化推荐模型研究]]></title>
    <url>%2F2018%2F03%2F12%2F%E9%9D%A2%E5%90%91%E7%9F%A5%E4%B9%8E%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[面向知乎的个性化推荐模型研究《面向知乎的个性化推荐模型研究》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。 更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活…]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
</search>
